record_number,buggy_code,fixed_code,code_similarity
132001,"@Override public FlowDetails createClassifierOutFlow(String nodeId,String flowKey,Match match,SfcRspInfo sfcRspInfo){
  if (Strings.isNullOrEmpty(flowKey) || sfcRspInfo == null || Strings.isNullOrEmpty(nodeId)) {
    LOG.error(""String_Node_Str"");
    return null;
  }
  LOG.info(""String_Node_Str"");
  List<Action> theActions=SfcScfOfUtils.buildNshActions(sfcRspInfo);
  InstructionsBuilder isb;
  DpnIdType classifierNodeDpnId=LogicalClassifierDataGetter.getDpnIdFromNodeName(nodeId);
  DpnIdType firstHopDataplaneId=logicalSffDataGetter.getFirstHopDataplaneId(sfcRspInfo.getRsp()).orElseThrow(IllegalArgumentException::new);
  if (classifierNodeDpnId.equals(firstHopDataplaneId)) {
    LOG.info(""String_Node_Str"",this.getTransportIngressTable());
    isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
    SfcOpenflowUtils.appendGotoTableInstruction(isb,this.getTransportIngressTable());
  }
 else {
    String theTunnelIf=logicalSffDataGetter.getInterfaceBetweenDpnIds(classifierNodeDpnId,firstHopDataplaneId).orElseThrow(RuntimeException::new);
    LOG.info(""String_Node_Str"",theTunnelIf,classifierNodeDpnId.getValue(),firstHopDataplaneId.getValue());
    List<Action> actionList=logicalSffDataGetter.getEgressActionsForTunnelInterface(theTunnelIf,theActions.size());
    if (!actionList.isEmpty()) {
      theActions.addAll(actionList);
    }
    isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
  }
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(this.getClassifierTable()).setKey(new FlowKey(new FlowId(flowKey))).setPriority(SfcScfOfUtils.FLOW_PRIORITY_CLASSIFIER).setMatch(match).setInstructions(isb.build());
  return classifierHandler.addRspRelatedFlowIntoNode(nodeId,flowb,sfcRspInfo.getNshNsp());
}","@Override public FlowDetails createClassifierOutFlow(String nodeId,String flowKey,Match match,SfcRspInfo sfcRspInfo){
  Preconditions.checkNotNull(flowKey,""String_Node_Str"");
  Preconditions.checkNotNull(sfcRspInfo,""String_Node_Str"");
  Preconditions.checkNotNull(nodeId,""String_Node_Str"");
  List<Action> theActions=SfcScfOfUtils.buildNshActions(sfcRspInfo);
  InstructionsBuilder isb;
  DpnIdType classifierNodeDpnId=LogicalClassifierDataGetter.getDpnIdFromNodeName(nodeId);
  DpnIdType firstHopDataplaneId=logicalSffDataGetter.getFirstHopDataplaneId(sfcRspInfo.getRsp()).orElseThrow(IllegalArgumentException::new);
  if (classifierNodeDpnId.equals(firstHopDataplaneId)) {
    LOG.info(""String_Node_Str"",this.getTransportIngressTable());
    isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
    SfcOpenflowUtils.appendGotoTableInstruction(isb,this.getTransportIngressTable());
  }
 else {
    String theTunnelIf=logicalSffDataGetter.getInterfaceBetweenDpnIds(classifierNodeDpnId,firstHopDataplaneId).orElseThrow(RuntimeException::new);
    LOG.info(""String_Node_Str"",theTunnelIf,classifierNodeDpnId.getValue(),firstHopDataplaneId.getValue());
    List<Action> actionList=logicalSffDataGetter.getEgressActionsForTunnelInterface(theTunnelIf,theActions.size());
    if (!actionList.isEmpty()) {
      theActions.addAll(actionList);
    }
    isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
  }
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(this.getClassifierTable()).setKey(new FlowKey(new FlowId(flowKey))).setPriority(SfcScfOfUtils.FLOW_PRIORITY_CLASSIFIER).setMatch(match).setInstructions(isb.build());
  return classifierHandler.addRspRelatedFlowIntoNode(nodeId,flowb,sfcRspInfo.getNshNsp());
}",0.931805477920626
132002,"@Override public FlowDetails createClassifierRelayFlow(String nodeId,String flowKey,SfcRspInfo sfcRspInfo,String classifierName){
  SffName lastSff=sfcRspInfo.getLastSffName();
  SffName classifierSff=new SffName(classifierName);
  SffDataPlaneLocator returnSffDpl=SfcModelUtil.searchSrcDplInConnectedSffs(lastSff,classifierSff);
  if (returnSffDpl == null) {
    return null;
  }
  SffDataPlaneLocator1 ofsDpl=returnSffDpl.getAugmentation(SffDataPlaneLocator1.class);
  if (ofsDpl == null) {
    return null;
  }
  FlowBuilder fb=SfcScfOfUtils.createClassifierMacChainingRelayFlow(nodeId,flowKey,ofsDpl.getOfsPort().getPortId(),sfcRspInfo.getNshNsp(),sfcRspInfo.getNshStartNsi(),sfcRspInfo.getNshEndNsi());
  return classifierHandler.addRspRelatedFlowIntoNode(nodeId,fb,sfcRspInfo.getNshNsp());
}","@Override public FlowDetails createClassifierRelayFlow(String nodeId,String flowKey,SfcRspInfo sfcRspInfo,String classifierName){
  SffName lastSff=sfcRspInfo.getLastSffName();
  SffName classifierSff=new SffName(classifierName);
  SffDataPlaneLocator returnSffDpl=SfcModelUtil.searchSrcDplInConnectedSffs(lastSff,classifierSff);
  if (returnSffDpl == null) {
    LOG.error(""String_Node_Str"",lastSff,classifierSff);
    return null;
  }
  SffDataPlaneLocator1 ofsDpl=returnSffDpl.getAugmentation(SffDataPlaneLocator1.class);
  if (ofsDpl == null) {
    LOG.error(""String_Node_Str"",returnSffDpl);
    return null;
  }
  FlowBuilder fb=SfcScfOfUtils.createClassifierMacChainingRelayFlow(nodeId,flowKey,ofsDpl.getOfsPort().getPortId(),sfcRspInfo.getNshNsp(),sfcRspInfo.getNshStartNsi(),sfcRspInfo.getNshEndNsi());
  return classifierHandler.addRspRelatedFlowIntoNode(nodeId,fb,sfcRspInfo.getNshNsp());
}",0.939304655274013
132003,"@Override public FlowDetails createClassifierInFlow(String nodeId,String flowKey,SfcRspInfo sfcRspInfo,Long outPort){
  SffName classifier=new SffName(SfcProviderServiceForwarderAPI.getSffName(nodeId));
  ServiceFunctionForwarder sff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(classifier);
  List<SffDataPlaneLocator> sffDataPlaneLocatorList=sff.getSffDataPlaneLocator();
  TerminationPoint terminationPoint=null;
  for (  SffDataPlaneLocator dpl : sffDataPlaneLocatorList) {
    SffDplChainTerminationAugment terminationDpl=dpl.getAugmentation(SffDplChainTerminationAugment.class);
    if (terminationDpl != null) {
      terminationPoint=terminationDpl.getTerminationPoint();
      break;
    }
  }
  if (terminationPoint == null) {
    return null;
  }
  String classifierNodeName=SfcOvsUtil.getOpenFlowNodeIdForSff(SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(classifier));
  FlowBuilder fb=SfcScfOfUtils.createMacChainClassifierInFlow(classifierNodeName,String.format(""String_Node_Str"",flowKey,sfcRspInfo.getNshNsp().toString()),terminationPoint.getPortId(),terminationPoint.getMacAddress().getValue(),sfcRspInfo.getNshNsp(),sfcRspInfo.getNshStartNsi());
  return classifierHandler.addRspRelatedFlowIntoNode(classifierNodeName,fb,sfcRspInfo.getNshNsp());
}","@Override public FlowDetails createClassifierInFlow(String nodeId,String flowKey,SfcRspInfo sfcRspInfo,Long outPort){
  SffName classifier=new SffName(SfcProviderServiceForwarderAPI.getSffName(nodeId));
  ServiceFunctionForwarder sff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(classifier);
  List<SffDataPlaneLocator> sffDataPlaneLocatorList=sff.getSffDataPlaneLocator();
  TerminationPoint terminationPoint=null;
  for (  SffDataPlaneLocator dpl : sffDataPlaneLocatorList) {
    SffDplChainTerminationAugment terminationDpl=dpl.getAugmentation(SffDplChainTerminationAugment.class);
    if (terminationDpl != null) {
      terminationPoint=terminationDpl.getTerminationPoint();
      break;
    }
  }
  if (terminationPoint == null) {
    LOG.error(""String_Node_Str"",classifier);
    return null;
  }
  String classifierNodeName=SfcOvsUtil.getOpenFlowNodeIdForSff(SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(classifier));
  if (classifierNodeName == null) {
    LOG.error(""String_Node_Str"",classifier);
    return null;
  }
  FlowBuilder fb=SfcScfOfUtils.createMacChainClassifierInFlow(classifierNodeName,String.format(""String_Node_Str"",flowKey,sfcRspInfo.getNshNsp().toString()),terminationPoint.getPortId(),terminationPoint.getMacAddress().getValue(),sfcRspInfo.getNshNsp(),sfcRspInfo.getNshStartNsi());
  return classifierHandler.addRspRelatedFlowIntoNode(classifierNodeName,fb,sfcRspInfo.getNshNsp());
}",0.946212952799122
132004,"@Override public FlowDetails createClassifierOutFlow(String nodeId,String flowKey,Match match,SfcRspInfo sfcRspInfo){
  SffName firstSffName=sfcRspInfo.getFirstSffName();
  SffName classifier=new SffName(SfcProviderServiceForwarderAPI.getSffName(nodeId));
  SffDataPlaneLocator outputDpl=SfcModelUtil.searchSrcDplInConnectedSffs(classifier,firstSffName);
  if (outputDpl == null) {
    return null;
  }
  SffDataPlaneLocator1 ofsDpl=outputDpl.getAugmentation(SffDataPlaneLocator1.class);
  if (ofsDpl == null) {
    return null;
  }
  FlowBuilder fb=SfcScfOfUtils.createMacChainClassifierOutFlow(nodeId,String.format(""String_Node_Str"",flowKey,ofsDpl.getOfsPort().getPortId()),match,ofsDpl.getOfsPort().getPortId(),sfcRspInfo.getNshNsp(),sfcRspInfo.getNshStartNsi());
  return classifierHandler.addRspRelatedFlowIntoNode(nodeId,fb,sfcRspInfo.getNshNsp());
}","@Override public FlowDetails createClassifierOutFlow(String nodeId,String flowKey,Match match,SfcRspInfo sfcRspInfo){
  SffName firstSffName=sfcRspInfo.getFirstSffName();
  SffName classifier=new SffName(SfcProviderServiceForwarderAPI.getSffName(nodeId));
  SffDataPlaneLocator outputDpl=SfcModelUtil.searchSrcDplInConnectedSffs(classifier,firstSffName);
  if (outputDpl == null) {
    LOG.error(""String_Node_Str"",classifier,firstSffName);
    return null;
  }
  SffDataPlaneLocator1 ofsDpl=outputDpl.getAugmentation(SffDataPlaneLocator1.class);
  if (ofsDpl == null) {
    LOG.error(""String_Node_Str"",outputDpl);
    return null;
  }
  FlowBuilder fb=SfcScfOfUtils.createMacChainClassifierOutFlow(nodeId,String.format(""String_Node_Str"",flowKey,ofsDpl.getOfsPort().getPortId()),match,ofsDpl.getOfsPort().getPortId(),sfcRspInfo.getNshNsp(),sfcRspInfo.getNshStartNsi());
  return classifierHandler.addRspRelatedFlowIntoNode(nodeId,fb,sfcRspInfo.getNshNsp());
}",0.9437706725468578
132005,"/** 
 * create classifier out flow for MAC Chaining. The function returns true if successful. The function returns false if unsuccessful.
 * @param nodeName flow table node name
 * @param flowKey  flow key
 * @param outPort  flow out port
 * @param gwMac      Gateway MAC to recovery original MAC addresses
 * @param pathId     chain path ID
 * @param startIndex   Firt hop in the chain
 * @return          create flow result
 */
public static FlowBuilder createMacChainClassifierInFlow(String nodeName,String flowKey,String outPort,String gwMac,Long pathId,short startIndex){
  int order=0;
  VirtualMacAddress vmac=VirtualMacAddress.getForwardAddress(pathId,0);
  if (nodeName == null || flowKey == null) {
    return null;
  }
  MatchBuilder mb=new MatchBuilder();
  SfcOpenflowUtils.addMatchDstMac(mb,vmac.getHop(startIndex).getValue());
  Action macDst=SfcOpenflowUtils.createActionSetDlDst(gwMac,order++);
  Action out=SfcOpenflowUtils.createActionOutPort(Integer.parseInt(outPort),order);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(Integer.valueOf(FLOW_PRIORITY_CLASSIFIER + 1)).setMatch(mb.build()).setInstructions(SfcOpenflowUtils.createInstructionsBuilder(SfcOpenflowUtils.createActionsInstructionBuilder(macDst,out)).build());
  return flowb;
}","/** 
 * create classifier out flow for MAC Chaining. The function returns true if successful. The function returns false if unsuccessful.
 * @param nodeName flow table node name
 * @param flowKey  flow key
 * @param outPort  flow out port
 * @param gwMac      Gateway MAC to recovery original MAC addresses
 * @param pathId     chain path ID
 * @param startIndex   Firt hop in the chain
 * @return          create flow result
 */
public static FlowBuilder createMacChainClassifierInFlow(String nodeName,String flowKey,String outPort,String gwMac,Long pathId,short startIndex){
  Preconditions.checkNotNull(flowKey,""String_Node_Str"");
  Preconditions.checkNotNull(nodeName,""String_Node_Str"");
  MatchBuilder mb=new MatchBuilder();
  VirtualMacAddress vmac=VirtualMacAddress.getForwardAddress(pathId,0);
  SfcOpenflowUtils.addMatchDstMac(mb,vmac.getHop(startIndex).getValue());
  int order=0;
  Action macDst=SfcOpenflowUtils.createActionSetDlDst(gwMac,order++);
  Action out=SfcOpenflowUtils.createActionOutPort(Integer.parseInt(outPort),order);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(Integer.valueOf(FLOW_PRIORITY_CLASSIFIER + 1)).setMatch(mb.build()).setInstructions(SfcOpenflowUtils.createInstructionsBuilder(SfcOpenflowUtils.createActionsInstructionBuilder(macDst,out)).build());
  return flowb;
}",0.900965319985699
132006,"/** 
 * create classifier relay flow for MAC Chaining. The function returns true if successful. The function returns false if unsuccessful.
 * @param nodeName flow table node name
 * @param flowKey  flow key
 * @param outPort  flow out port
 * @param pathId   chain path ID
 * @param startIndex  Firt hop in the chain
 * @param lastIndex   Last hop in the chain
 * @return          create relay result
 */
public static FlowBuilder createClassifierMacChainingRelayFlow(String nodeName,String flowKey,String outPort,Long pathId,short startIndex,short lastIndex){
  int order=0;
  VirtualMacAddress vmac=VirtualMacAddress.getForwardAddress(pathId,0);
  if (nodeName == null || flowKey == null) {
    return null;
  }
  MatchBuilder mb=new MatchBuilder();
  SfcOpenflowUtils.addMatchDstMac(mb,vmac.getHop(lastIndex).getValue());
  Action macDst=SfcOpenflowUtils.createActionSetDlDst(vmac.getHop(startIndex).getValue(),order++);
  Action out=SfcOpenflowUtils.createActionOutPort(Integer.parseInt(outPort),order);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(Integer.valueOf(FLOW_PRIORITY_CLASSIFIER)).setMatch(mb.build()).setInstructions(SfcOpenflowUtils.createInstructionsBuilder(SfcOpenflowUtils.createActionsInstructionBuilder(macDst,out)).build());
  return flowb;
}","/** 
 * create classifier relay flow for MAC Chaining. The function returns true if successful. The function returns false if unsuccessful.
 * @param nodeName flow table node name
 * @param flowKey  flow key
 * @param outPort  flow out port
 * @param pathId   chain path ID
 * @param startIndex  Firt hop in the chain
 * @param lastIndex   Last hop in the chain
 * @return          create relay result
 */
public static FlowBuilder createClassifierMacChainingRelayFlow(String nodeName,String flowKey,String outPort,Long pathId,short startIndex,short lastIndex){
  Preconditions.checkNotNull(flowKey,""String_Node_Str"");
  Preconditions.checkNotNull(nodeName,""String_Node_Str"");
  MatchBuilder mb=new MatchBuilder();
  VirtualMacAddress vmac=VirtualMacAddress.getForwardAddress(pathId,0);
  SfcOpenflowUtils.addMatchDstMac(mb,vmac.getHop(lastIndex).getValue());
  int order=0;
  Action macDst=SfcOpenflowUtils.createActionSetDlDst(vmac.getHop(startIndex).getValue(),order++);
  Action out=SfcOpenflowUtils.createActionOutPort(Integer.parseInt(outPort),order);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(Integer.valueOf(FLOW_PRIORITY_CLASSIFIER)).setMatch(mb.build()).setInstructions(SfcOpenflowUtils.createInstructionsBuilder(SfcOpenflowUtils.createActionsInstructionBuilder(macDst,out)).build());
  return flowb;
}",0.9015985790408526
132007,"/** 
 * Get a FlowBuilder object w/ the classifier relay flow.
 * @param flowKey flow key
 * @param sfcRspInfo nsh header
 * @return the FlowBuilder containing the classifier relay flow
 */
public static FlowBuilder createClassifierRelayFlow(String flowKey,SfcRspInfo sfcRspInfo){
  if (flowKey == null || sfcRspInfo == null || sfcRspInfo.getVxlanIpDst() == null) {
    return null;
  }
  String dstIp=sfcRspInfo.getVxlanIpDst().getValue();
  List<Action> theActions=new ArrayList<>();
  theActions.add(SfcOpenflowUtils.createActionNxLoadTunGpeNp(OpenflowConstants.TUN_GPE_NP_NSH,theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxSetTunIpv4Dst(dstIp,theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxMoveNsp(theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxMoveNsi(theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxMoveNsc1(theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxMoveNsc2(theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionOutPort(OutputPortValues.INPORT.toString(),theActions.size()));
  InstructionsBuilder isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
  FlowBuilder flowb=new FlowBuilder();
  MatchBuilder mb=SfcOpenflowUtils.getNshMatches(sfcRspInfo.getNshNsp(),sfcRspInfo.getNshEndNsi());
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(FLOW_PRIORITY_CLASSIFIER).setMatch(mb.build()).setInstructions(isb.build());
  return flowb;
}","/** 
 * Get a FlowBuilder object w/ the classifier relay flow.
 * @param flowKey flow key
 * @param sfcRspInfo nsh header
 * @return the FlowBuilder containing the classifier relay flow
 */
public static FlowBuilder createClassifierRelayFlow(String flowKey,SfcRspInfo sfcRspInfo){
  Preconditions.checkNotNull(flowKey,""String_Node_Str"");
  Preconditions.checkNotNull(sfcRspInfo,""String_Node_Str"");
  Preconditions.checkNotNull(sfcRspInfo.getVxlanIpDst(),""String_Node_Str"");
  String dstIp=sfcRspInfo.getVxlanIpDst().getValue();
  List<Action> theActions=new ArrayList<>();
  theActions.add(SfcOpenflowUtils.createActionNxLoadTunGpeNp(OpenflowConstants.TUN_GPE_NP_NSH,theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxSetTunIpv4Dst(dstIp,theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxMoveNsp(theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxMoveNsi(theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxMoveNsc1(theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionNxMoveNsc2(theActions.size()));
  theActions.add(SfcOpenflowUtils.createActionOutPort(OutputPortValues.INPORT.toString(),theActions.size()));
  InstructionsBuilder isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
  FlowBuilder flowb=new FlowBuilder();
  MatchBuilder mb=SfcOpenflowUtils.getNshMatches(sfcRspInfo.getNshNsp(),sfcRspInfo.getNshEndNsi());
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(FLOW_PRIORITY_CLASSIFIER).setMatch(mb.build()).setInstructions(isb.build());
  return flowb;
}",0.9367404175755688
132008,"/** 
 * create classifier out flow for MAC Chaining. The function returns true if successful. The function returns false if unsuccessful.
 * @param nodeName flow table node name
 * @param flowKey  flow key
 * @param match    flow match
 * @param outPort  flow out port
 * @param pathId   chain path ID
 * @param startIndex  Firt hop in the chain
 * @return          create flow result
 */
public static FlowBuilder createMacChainClassifierOutFlow(String nodeName,String flowKey,Match match,String outPort,Long pathId,short startIndex){
  int order=0;
  VirtualMacAddress vmac=VirtualMacAddress.getForwardAddress(pathId,0);
  if (nodeName == null || flowKey == null) {
    return null;
  }
  Action macDst=SfcOpenflowUtils.createActionSetDlDst(vmac.getHop(startIndex).getValue(),order++);
  Action out=SfcOpenflowUtils.createActionOutPort(Integer.parseInt(outPort),order);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(Integer.valueOf(FLOW_PRIORITY_CLASSIFIER)).setMatch(match).setInstructions(SfcOpenflowUtils.createInstructionsBuilder(SfcOpenflowUtils.createActionsInstructionBuilder(macDst,out)).build());
  return flowb;
}","/** 
 * create classifier out flow for MAC Chaining. The function returns true if successful. The function returns false if unsuccessful.
 * @param nodeName flow table node name
 * @param flowKey  flow key
 * @param match    flow match
 * @param outPort  flow out port
 * @param pathId   chain path ID
 * @param startIndex  Firt hop in the chain
 * @return          create flow result
 */
public static FlowBuilder createMacChainClassifierOutFlow(String nodeName,String flowKey,Match match,String outPort,Long pathId,short startIndex){
  Preconditions.checkNotNull(flowKey,""String_Node_Str"");
  Preconditions.checkNotNull(nodeName,""String_Node_Str"");
  int order=0;
  VirtualMacAddress vmac=VirtualMacAddress.getForwardAddress(pathId,0);
  Action macDst=SfcOpenflowUtils.createActionSetDlDst(vmac.getHop(startIndex).getValue(),order++);
  Action out=SfcOpenflowUtils.createActionOutPort(Integer.parseInt(outPort),order);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(Integer.valueOf(FLOW_PRIORITY_CLASSIFIER)).setMatch(match).setInstructions(SfcOpenflowUtils.createInstructionsBuilder(SfcOpenflowUtils.createActionsInstructionBuilder(macDst,out)).build());
  return flowb;
}",0.928486764124852
132009,"/** 
 * Get a FlowBuilder object w/ the classifier 'in' flow.
 * @param flowKey flow key
 * @param sfcRspInfo nsh header
 * @param outPort flow out port
 * @return create in result
 */
public static FlowBuilder createClassifierInFlow(String flowKey,SfcRspInfo sfcRspInfo,Long outPort){
  if (flowKey == null || sfcRspInfo == null || sfcRspInfo.getVxlanIpDst() == null) {
    return null;
  }
  MatchBuilder mb=SfcOpenflowUtils.getNshMatches(sfcRspInfo.getNshNsp(),sfcRspInfo.getNshEndNsi());
  List<Action> theActions=new ArrayList<>();
  theActions.add(SfcOpenflowUtils.createActionNxPopNsh(theActions.size()));
  theActions.add(outPort == null ? SfcOpenflowUtils.createActionOutPort(OutputPortValues.INPORT.toString(),theActions.size()) : SfcOpenflowUtils.createActionOutPort(outPort.intValue(),theActions.size()));
  InstructionsBuilder isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(FLOW_PRIORITY_CLASSIFIER).setMatch(mb.build()).setInstructions(isb.build());
  return flowb;
}","/** 
 * Get a FlowBuilder object w/ the classifier 'in' flow.
 * @param flowKey flow key
 * @param sfcRspInfo nsh header
 * @param outPort flow out port
 * @return create in result
 */
public static FlowBuilder createClassifierInFlow(String flowKey,SfcRspInfo sfcRspInfo,Long outPort){
  Preconditions.checkNotNull(flowKey,""String_Node_Str"");
  Preconditions.checkNotNull(sfcRspInfo,""String_Node_Str"");
  Preconditions.checkNotNull(sfcRspInfo.getVxlanIpDst(),""String_Node_Str"");
  MatchBuilder mb=SfcOpenflowUtils.getNshMatches(sfcRspInfo.getNshNsp(),sfcRspInfo.getNshEndNsi());
  List<Action> theActions=new ArrayList<>();
  theActions.add(SfcOpenflowUtils.createActionNxPopNsh(theActions.size()));
  theActions.add(outPort == null ? SfcOpenflowUtils.createActionOutPort(OutputPortValues.INPORT.toString(),theActions.size()) : SfcOpenflowUtils.createActionOutPort(outPort.intValue(),theActions.size()));
  InstructionsBuilder isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(FLOW_PRIORITY_CLASSIFIER).setMatch(mb.build()).setInstructions(isb.build());
  return flowb;
}",0.9162886597938144
132010,"/** 
 * create classifier out flow. The function returns true if successful. The function returns false if unsuccessful. Get a FlowBuilder object w/ the classifier 'out' flow.
 * @param flowKey flow key
 * @param match flow match
 * @param sfcRspInfo nsh header
 * @param outPort flow out port
 * @return create flow result
 */
public static FlowBuilder createClassifierOutFlow(String flowKey,Match match,SfcRspInfo sfcRspInfo,Long outPort){
  if (flowKey == null || sfcRspInfo == null || sfcRspInfo.getVxlanIpDst() == null) {
    return null;
  }
  String dstIp=sfcRspInfo.getVxlanIpDst().getValue();
  List<Action> theActions=buildNshActions(sfcRspInfo);
  theActions.add(SfcOpenflowUtils.createActionNxSetTunIpv4Dst(dstIp,theActions.size()));
  theActions.add(outPort == null ? SfcOpenflowUtils.createActionOutPort(OutputPortValues.INPORT.toString(),theActions.size()) : SfcOpenflowUtils.createActionOutPort(outPort.intValue(),theActions.size()));
  InstructionsBuilder isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(FLOW_PRIORITY_CLASSIFIER).setMatch(match).setInstructions(isb.build());
  return flowb;
}","/** 
 * create classifier out flow. The function returns true if successful. The function returns false if unsuccessful. Get a FlowBuilder object w/ the classifier 'out' flow.
 * @param flowKey flow key
 * @param match flow match
 * @param sfcRspInfo nsh header
 * @param outPort flow out port
 * @return create flow result
 */
public static FlowBuilder createClassifierOutFlow(String flowKey,Match match,SfcRspInfo sfcRspInfo,Long outPort){
  Preconditions.checkNotNull(flowKey,""String_Node_Str"");
  Preconditions.checkNotNull(sfcRspInfo,""String_Node_Str"");
  Preconditions.checkNotNull(sfcRspInfo.getVxlanIpDst(),""String_Node_Str"");
  String dstIp=sfcRspInfo.getVxlanIpDst().getValue();
  List<Action> theActions=buildNshActions(sfcRspInfo);
  theActions.add(SfcOpenflowUtils.createActionNxSetTunIpv4Dst(dstIp,theActions.size()));
  theActions.add(outPort == null ? SfcOpenflowUtils.createActionOutPort(OutputPortValues.INPORT.toString(),theActions.size()) : SfcOpenflowUtils.createActionOutPort(outPort.intValue(),theActions.size()));
  InstructionsBuilder isb=SfcOpenflowUtils.wrapActionsIntoApplyActionsInstruction(theActions);
  FlowBuilder flowb=new FlowBuilder();
  flowb.setId(new FlowId(flowKey)).setTableId(TABLE_INDEX_CLASSIFIER).setKey(new FlowKey(new FlowId(flowKey))).setPriority(FLOW_PRIORITY_CLASSIFIER).setMatch(match).setInstructions(isb.build());
  return flowb;
}",0.9242819843342036
132011,"/** 
 * Process a list of classifier switches objects, adding or removing flows for the entire impacted RSP.
 * @param theAcl the ACL object to install
 * @param addClassifier true when adding the classifier flows, false when deleting them
 * @param classifierList the list of  {@link SclServiceFunctionForwarder} in which theclassifier flows will be installed
 * @return the list of all the relevant flows to be installed
 */
public List<FlowDetails> processClassifierList(Acl theAcl,boolean addClassifier,List<SclServiceFunctionForwarder> classifierList){
  return classifierList.stream().map(classifier -> processClassifier(classifier,theAcl,addClassifier)).peek(theFlows -> LOG.info(""String_Node_Str"",theFlows.size())).reduce(new ArrayList<>(),(dstList,theList) -> Stream.concat(dstList.stream(),theList.stream()).collect(Collectors.toList()));
}","/** 
 * Process a list of classifier switches objects, adding or removing flows for the entire impacted RSP.
 * @param theAcl the ACL object to install
 * @param onAddClassifier true when adding the classifier flows, false when deleting them
 * @param classifierList the list of  {@link SclServiceFunctionForwarder} in which theclassifier flows will be installed
 * @return the list of all the relevant flows to be installed
 */
public List<FlowDetails> processClassifierList(Acl theAcl,boolean onAddClassifier,List<SclServiceFunctionForwarder> classifierList){
  return classifierList.stream().map(classifier -> processClassifier(classifier,theAcl,onAddClassifier)).peek(theFlows -> LOG.info(""String_Node_Str"",theFlows.size())).reduce(new ArrayList<>(),(dstList,theList) -> Stream.concat(dstList.stream(),theList.stream()).collect(Collectors.toList()));
}",0.992966002344666
132012,"public SfcRspInfo getNshMetaC2(Long nshMetaC2){
  this.nshMetaC2=nshMetaC2;
  return this;
}","public Long getNshMetaC2(){
  return this.nshMetaC2;
}",0.6027397260273972
132013,"@Test public void removeClassifierLegacyScenario(){
  when(sff.getSffDataPlaneLocator()).thenReturn(new ArrayList<>());
  PowerMockito.when(SfcOvsUtil.getDpdkOfPort(anyString(),anyString())).thenReturn(null);
  BareClassifier classifierInterface=Mockito.spy(new BareClassifier(sff));
  doReturn(Optional.of(FIRST_SF_NODE_NAME)).when(classifierInterface).getNodeName(anyString());
  OpenflowClassifierProcessor classifierManager=new OpenflowClassifierProcessor(readWriteTransaction,new LogicallyAttachedClassifier(dataGetter),classifierInterface);
  List<FlowDetails> theFlows=classifierManager.processClassifier(sffClassifier,acl,false);
  Assert.assertFalse(theFlows.isEmpty());
  Assert.assertEquals(1 + 1,theFlows.size());
}","@Test public void removeClassifierLegacyScenario(){
  when(sff.getSffDataPlaneLocator()).thenReturn(new ArrayList<>());
  PowerMockito.when(SfcOvsUtil.getDpdkOfPort(anyString(),anyString())).thenReturn(null);
  BareClassifier bareClassifier=Mockito.spy(new BareClassifier(sff));
  doReturn(Optional.of(FIRST_SF_NODE_NAME)).when(bareClassifier).getNodeName(anyString());
  OpenflowClassifierProcessor classifierManager=new OpenflowClassifierProcessor(readWriteTransaction,new LogicallyAttachedClassifier(dataGetter),bareClassifier);
  List<FlowDetails> theFlows=classifierManager.processClassifier(sffClassifier,acl,false);
  Assert.assertFalse(theFlows.isEmpty());
  Assert.assertEquals(1 + 1,theFlows.size());
}",0.9312022237665044
132014,"@Test public void removeClassifierDpdkScenario(){
  when(sff.getSffDataPlaneLocator()).thenReturn(new ArrayList<>());
  PowerMockito.when(SfcOvsUtil.getDpdkOfPort(anyString(),anyString())).thenReturn(null);
  BareClassifier classifierInterface=Mockito.spy(new BareClassifier(sff));
  doReturn(Optional.of(FIRST_SF_NODE_NAME)).when(classifierInterface).getNodeName(anyString());
  OpenflowClassifierProcessor classifierManager=new OpenflowClassifierProcessor(readWriteTransaction,new LogicallyAttachedClassifier(dataGetter),classifierInterface);
  List<FlowDetails> theFlows=classifierManager.processClassifier(sffClassifier,acl,false);
  Assert.assertFalse(theFlows.isEmpty());
  Assert.assertEquals(1 + 1,theFlows.size());
}","@Test public void removeClassifierDpdkScenario(){
  when(sff.getSffDataPlaneLocator()).thenReturn(new ArrayList<>());
  PowerMockito.when(SfcOvsUtil.getDpdkOfPort(anyString(),anyString())).thenReturn(null);
  BareClassifier bareClassifier=Mockito.spy(new BareClassifier(sff));
  doReturn(Optional.of(FIRST_SF_NODE_NAME)).when(bareClassifier).getNodeName(anyString());
  OpenflowClassifierProcessor classifierManager=new OpenflowClassifierProcessor(readWriteTransaction,new LogicallyAttachedClassifier(dataGetter),bareClassifier);
  List<FlowDetails> theFlows=classifierManager.processClassifier(sffClassifier,acl,false);
  Assert.assertFalse(theFlows.isEmpty());
  Assert.assertEquals(1 + 1,theFlows.size());
}",0.9310104529616724
132015,"@Test public void addClassifierOK(){
  when(dataGetter.getFirstHopDataplaneId(any(RenderedServicePath.class))).thenReturn(Optional.of(FIRST_SF_DATAPLANE_ID));
  LogicallyAttachedClassifier classifierInterface=Mockito.spy(new LogicallyAttachedClassifier(dataGetter));
  doReturn(Optional.of(FIRST_SF_NODE_NAME)).when(classifierInterface).getNodeName(anyString());
  OpenflowClassifierProcessor classifierManager=new OpenflowClassifierProcessor(readWriteTransaction,classifierInterface,new BareClassifier());
  PowerMockito.when(SfcOvsUtil.getDpdkOfPort(anyString(),anyString())).thenReturn(null);
  List<FlowDetails> theFlows=classifierManager.processClassifier(sffClassifier,acl,true);
  Assert.assertFalse(theFlows.isEmpty());
  Assert.assertEquals(2,theFlows.size());
}","@Test public void addClassifierOK(){
  when(dataGetter.getFirstHopDataplaneId(any(RenderedServicePath.class))).thenReturn(Optional.of(FIRST_SF_DATAPLANE_ID));
  doReturn(Optional.of(FIRST_SF_NODE_NAME)).when(classifierInterface).getNodeName(anyString());
  OpenflowClassifierProcessor classifierManager=new OpenflowClassifierProcessor(readWriteTransaction,classifierInterface,new BareClassifier());
  PowerMockito.when(SfcOvsUtil.getDpdkOfPort(anyString(),anyString())).thenReturn(null);
  List<FlowDetails> theFlows=classifierManager.processClassifier(sffClassifier,acl,true);
  Assert.assertFalse(theFlows.isEmpty());
  Assert.assertEquals(2,theFlows.size());
}",0.9246861924686192
132016,"@Test public void removeClassifierOK(){
  when(dataGetter.getFirstHopDataplaneId(any(RenderedServicePath.class))).thenReturn(Optional.of(FIRST_SF_DATAPLANE_ID));
  LogicallyAttachedClassifier classifierInterface=Mockito.spy(new LogicallyAttachedClassifier(dataGetter));
  doReturn(Optional.of(FIRST_SF_NODE_NAME)).when(classifierInterface).getNodeName(anyString());
  OpenflowClassifierProcessor classifierManager=new OpenflowClassifierProcessor(readWriteTransaction,classifierInterface,new BareClassifier());
  List<FlowDetails> theFlows=classifierManager.processClassifier(sffClassifier,acl,false);
  Assert.assertFalse(theFlows.isEmpty());
  theFlows.forEach(flow -> LOG.info(""String_Node_Str"",flow.getFlow()));
  Assert.assertEquals(1,theFlows.size());
}","@Test public void removeClassifierOK(){
  when(dataGetter.getFirstHopDataplaneId(any(RenderedServicePath.class))).thenReturn(Optional.of(FIRST_SF_DATAPLANE_ID));
  doReturn(Optional.of(FIRST_SF_NODE_NAME)).when(classifierInterface).getNodeName(anyString());
  OpenflowClassifierProcessor classifierManager=new OpenflowClassifierProcessor(readWriteTransaction,classifierInterface,new BareClassifier());
  List<FlowDetails> theFlows=classifierManager.processClassifier(sffClassifier,acl,false);
  Assert.assertFalse(theFlows.isEmpty());
  theFlows.forEach(flow -> LOG.info(""String_Node_Str"",flow.getFlow()));
  Assert.assertEquals(1,theFlows.size());
}",0.9232954545454546
132017,"public SfcTableIndexMapperBuilder setPathMapperTable(short externalPathMapperTable){
  this.externalPathMapperTable=externalPathMapperTable;
  return this;
}","public SfcTableIndexMapperBuilder setPathMapperTable(short mapperTable){
  this.externalPathMapperTable=mapperTable;
  return this;
}",0.903448275862069
132018,"public SfcTableIndexMapperBuilder setTransportIngressTable(short externalTransportIngressTable){
  this.externalTransportIngressTable=externalTransportIngressTable;
  return this;
}","public SfcTableIndexMapperBuilder setTransportIngressTable(short ingressTable){
  this.externalTransportIngressTable=ingressTable;
  return this;
}",0.8841463414634146
132019,"public SfcTableIndexMapperBuilder setNextHopTable(short externalNextHopTable){
  this.externalNextHopTable=externalNextHopTable;
  return this;
}","public SfcTableIndexMapperBuilder setNextHopTable(short nextHopTable){
  this.externalNextHopTable=nextHopTable;
  return this;
}",0.9416058394160584
132020,"public SfcTableIndexMapperBuilder setTransportEgressTable(short externalTransportEgressTable){
  this.externalTransportEgressTable=externalTransportEgressTable;
  return this;
}","public SfcTableIndexMapperBuilder setTransportEgressTable(short transportEgressTable){
  this.externalTransportEgressTable=transportEgressTable;
  return this;
}",0.9526627218934912
132021,"public SfcTableIndexMapperBuilder setClassifierTable(short externalTransportClassifierTable){
  this.externalTransportClassifierTable=externalTransportClassifierTable;
  return this;
}","public SfcTableIndexMapperBuilder setClassifierTable(short classifierTable){
  this.externalTransportClassifierTable=classifierTable;
  return this;
}",0.8862275449101796
132022,"public SfcTableIndexMapperBuilder setPathMapperAclTable(short externalPathMapperAclTable){
  this.externalPathMapperAclTable=externalPathMapperAclTable;
  return this;
}","public SfcTableIndexMapperBuilder setPathMapperAclTable(short mapperAclTable){
  this.externalPathMapperAclTable=mapperAclTable;
  return this;
}",0.910828025477707
132023,"/** 
 * Submits callable for execution by given ExecutorService. Thanks to this wrapper method, boolean result will be returned instead of Future. <p>
 * @param callable Callable
 * @param executor ExecutorService
 * @return true if callable completed successfully, otherwise false.
 */
public static Object submitCallable(Callable<?> callable,ExecutorService executor){
  Object result=null;
  Future<?> future=executor.submit(callable);
  try {
    result=future.get();
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.warn(""String_Node_Str"",callable.toString(),e);
  }
  return result;
}","/** 
 * Submits callable for execution by given ExecutorService. Thanks to this wrapper method, boolean result will be returned instead of Future. <p>
 * @param callable Callable
 * @param executorService ExecutorService
 * @return true if callable completed successfully, otherwise false.
 */
public static Object submitCallable(Callable<?> callable,ExecutorService executorService){
  Object result=null;
  Future<?> future=executorService.submit(callable);
  try {
    result=future.get();
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.warn(""String_Node_Str"",callable.toString(),e);
  }
  return result;
}",0.9830234438156832
132024,"@Test public void testGetManagerNodeByIpV6(){
  OvsdbNodeAugmentationBuilder ovsdbNodeAugmentationBuilder=new OvsdbNodeAugmentationBuilder();
  org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress ipAddress=new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress(new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.Ipv6Address(IPV6_ADDRESS));
  ovsdbNodeAugmentationBuilder.setConnectionInfo(new ConnectionInfoBuilder().setRemoteIp(ipAddress).build());
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setNodeId(new NodeId(IPV6_ADDRESS)).addAugmentation(OvsdbNodeAugmentation.class,ovsdbNodeAugmentationBuilder.build());
  InstanceIdentifier<Node> nodeIID=SfcOvsUtil.buildOvsdbNodeIID(IPV6_ADDRESS);
  boolean transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(nodeIID,nodeBuilder.build(),LogicalDatastoreType.OPERATIONAL);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  Node node=SfcOvsUtil.getManagerNodeByIp(new IpAddress(new Ipv6Address(IPV6_ADDRESS)));
  assertNotNull(""String_Node_Str"",node);
  assertEquals(""String_Node_Str"",node.getKey().getNodeId().getValue(),IPV6_ADDRESS);
}","@Test public void testGetManagerNodeByIpV6(){
  OvsdbNodeAugmentationBuilder ovsdbNodeAugmentationBuilder=new OvsdbNodeAugmentationBuilder();
  org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress ipAddress=new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress(new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.Ipv6Address(IPV6_ADDRESS));
  ovsdbNodeAugmentationBuilder.setConnectionInfo(new ConnectionInfoBuilder().setRemoteIp(ipAddress).build());
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setNodeId(new NodeId(IPV6_ADDRESS)).addAugmentation(OvsdbNodeAugmentation.class,ovsdbNodeAugmentationBuilder.build());
  InstanceIdentifier<Node> ovsdbNodeIID=SfcOvsUtil.buildOvsdbNodeIID(IPV6_ADDRESS);
  boolean transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(ovsdbNodeIID,nodeBuilder.build(),LogicalDatastoreType.OPERATIONAL);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  Node node=SfcOvsUtil.getManagerNodeByIp(new IpAddress(new Ipv6Address(IPV6_ADDRESS)));
  assertNotNull(""String_Node_Str"",node);
  assertEquals(""String_Node_Str"",node.getKey().getNodeId().getValue(),IPV6_ADDRESS);
}",0.994336569579288
132025,"@Test public void testAugmentSffWithOpenFlowNodeId1() throws Exception {
  final String ofNodeId=""String_Node_Str"";
  OvsdbNodeAugmentationBuilder ovsdbNodeAugmentationBuilder=new OvsdbNodeAugmentationBuilder();
  org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress ipAddress=new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress(new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.Ipv4Address(TEST_IP_ADDRESS));
  ovsdbNodeAugmentationBuilder.setConnectionInfo(new ConnectionInfoBuilder().setRemoteIp(ipAddress).build());
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setKey(new NodeKey(new NodeId(""String_Node_Str""))).addAugmentation(OvsdbNodeAugmentation.class,ovsdbNodeAugmentationBuilder.build());
  InstanceIdentifier<Node> nodeIID=InstanceIdentifier.create(NetworkTopology.class).child(Topology.class,new TopologyKey(SouthboundConstants.OVSDB_TOPOLOGY_ID)).child(Node.class,new NodeKey(new NodeId(""String_Node_Str"")));
  boolean transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(nodeIID,nodeBuilder.build(),LogicalDatastoreType.CONFIGURATION);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.setKey(new ServiceFunctionForwarderKey(new SffName(TEST_STRING))).setIpMgmtAddress(new IpAddress(new Ipv4Address(TEST_IP_ADDRESS)));
  ServiceFunctionForwarder serviceFunctionForwarder=serviceFunctionForwarderBuilder.build();
  assertNotNull(""String_Node_Str"",serviceFunctionForwarder);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getKey().getName().getValue(),TEST_STRING);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getIpMgmtAddress().getIpv4Address().getValue(),TEST_IP_ADDRESS);
  OvsdbBridgeAugmentationBuilder ovsdbBridgeAugmentationBuilder=new OvsdbBridgeAugmentationBuilder();
  ovsdbBridgeAugmentationBuilder.setDatapathId(new DatapathId(TEST_DATA_PATH));
  InstanceIdentifier<OvsdbBridgeAugmentation> bridgeEntryIID=InstanceIdentifier.create(NetworkTopology.class).child(Topology.class,new TopologyKey(SouthboundConstants.OVSDB_TOPOLOGY_ID)).child(Node.class,new NodeKey(new NodeId(TEST_IP_ADDRESS + ""String_Node_Str"" + TEST_BRIDGE_NAME))).augmentation(OvsdbBridgeAugmentation.class);
  transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(bridgeEntryIID,ovsdbBridgeAugmentationBuilder.build(),LogicalDatastoreType.OPERATIONAL);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  IpBuilder ipBuilder=new IpBuilder();
  ipBuilder.setIp(new IpAddress(new Ipv4Address(IP_ADDRESS))).setPort(new PortNumber(5000));
  DataPlaneLocatorBuilder dataPlaneLocatorBuilder=new DataPlaneLocatorBuilder();
  dataPlaneLocatorBuilder.setTransport(VxlanGpe.class).setLocatorType(ipBuilder.build());
  List<SffDataPlaneLocator> sffDataPlaneLocators=new ArrayList<>();
  SffDataPlaneLocatorBuilder sffDataPlaneLocatorBuilder=new SffDataPlaneLocatorBuilder();
  SffDataPlaneLocatorName sffDplName=new SffDataPlaneLocatorName(""String_Node_Str"");
  sffDataPlaneLocatorBuilder.setName(sffDplName).setKey(new SffDataPlaneLocatorKey(sffDplName)).setDataPlaneLocator(dataPlaneLocatorBuilder.build());
  sffDataPlaneLocators.add(sffDataPlaneLocatorBuilder.build());
  OvsBridgeBuilder ovsBridgeBuilder=new OvsBridgeBuilder();
  ovsBridgeBuilder.setBridgeName(TEST_BRIDGE_NAME);
  SffOvsBridgeAugmentationBuilder sffOvsBridgeAugmentationBuilder=new SffOvsBridgeAugmentationBuilder();
  sffOvsBridgeAugmentationBuilder.setOvsBridge(ovsBridgeBuilder.build());
  serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.addAugmentation(SffOvsBridgeAugmentation.class,sffOvsBridgeAugmentationBuilder.build()).setKey(new ServiceFunctionForwarderKey(new SffName(TEST_STRING))).setSffDataPlaneLocator(sffDataPlaneLocators);
  serviceFunctionForwarder=SfcOvsUtil.augmentSffWithOpenFlowNodeId(serviceFunctionForwarderBuilder.build());
  assertNotNull(""String_Node_Str"",serviceFunctionForwarder);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getAugmentation(SffOvsBridgeAugmentation.class).getOvsBridge().getOpenflowNodeId(),ofNodeId);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getKey().getName().getValue(),TEST_STRING);
}","@Test public void testAugmentSffWithOpenFlowNodeId1() throws Exception {
  final String ofNodeId=""String_Node_Str"";
  OvsdbNodeAugmentationBuilder ovsdbNodeAugmentationBuilder=new OvsdbNodeAugmentationBuilder();
  org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress ipAddress=new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress(new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.Ipv4Address(TEST_IP_ADDRESS));
  ovsdbNodeAugmentationBuilder.setConnectionInfo(new ConnectionInfoBuilder().setRemoteIp(ipAddress).build());
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setKey(new NodeKey(new NodeId(""String_Node_Str""))).addAugmentation(OvsdbNodeAugmentation.class,ovsdbNodeAugmentationBuilder.build());
  InstanceIdentifier<Node> ovsdbNodeIID=InstanceIdentifier.create(NetworkTopology.class).child(Topology.class,new TopologyKey(SouthboundConstants.OVSDB_TOPOLOGY_ID)).child(Node.class,new NodeKey(new NodeId(""String_Node_Str"")));
  boolean transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(ovsdbNodeIID,nodeBuilder.build(),LogicalDatastoreType.CONFIGURATION);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.setKey(new ServiceFunctionForwarderKey(new SffName(TEST_STRING))).setIpMgmtAddress(new IpAddress(new Ipv4Address(TEST_IP_ADDRESS)));
  ServiceFunctionForwarder serviceFunctionForwarder=serviceFunctionForwarderBuilder.build();
  assertNotNull(""String_Node_Str"",serviceFunctionForwarder);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getKey().getName().getValue(),TEST_STRING);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getIpMgmtAddress().getIpv4Address().getValue(),TEST_IP_ADDRESS);
  OvsdbBridgeAugmentationBuilder ovsdbBridgeAugmentationBuilder=new OvsdbBridgeAugmentationBuilder();
  ovsdbBridgeAugmentationBuilder.setDatapathId(new DatapathId(TEST_DATA_PATH));
  InstanceIdentifier<OvsdbBridgeAugmentation> bridgeEntryIID=InstanceIdentifier.create(NetworkTopology.class).child(Topology.class,new TopologyKey(SouthboundConstants.OVSDB_TOPOLOGY_ID)).child(Node.class,new NodeKey(new NodeId(TEST_IP_ADDRESS + ""String_Node_Str"" + TEST_BRIDGE_NAME))).augmentation(OvsdbBridgeAugmentation.class);
  transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(bridgeEntryIID,ovsdbBridgeAugmentationBuilder.build(),LogicalDatastoreType.OPERATIONAL);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  IpBuilder ipBuilder=new IpBuilder();
  ipBuilder.setIp(new IpAddress(new Ipv4Address(IP_ADDRESS))).setPort(new PortNumber(5000));
  DataPlaneLocatorBuilder dataPlaneLocatorBuilder=new DataPlaneLocatorBuilder();
  dataPlaneLocatorBuilder.setTransport(VxlanGpe.class).setLocatorType(ipBuilder.build());
  List<SffDataPlaneLocator> sffDataPlaneLocators=new ArrayList<>();
  SffDataPlaneLocatorBuilder sffDataPlaneLocatorBuilder=new SffDataPlaneLocatorBuilder();
  SffDataPlaneLocatorName sffDplName=new SffDataPlaneLocatorName(""String_Node_Str"");
  sffDataPlaneLocatorBuilder.setName(sffDplName).setKey(new SffDataPlaneLocatorKey(sffDplName)).setDataPlaneLocator(dataPlaneLocatorBuilder.build());
  sffDataPlaneLocators.add(sffDataPlaneLocatorBuilder.build());
  OvsBridgeBuilder ovsBridgeBuilder=new OvsBridgeBuilder();
  ovsBridgeBuilder.setBridgeName(TEST_BRIDGE_NAME);
  SffOvsBridgeAugmentationBuilder sffOvsBridgeAugmentationBuilder=new SffOvsBridgeAugmentationBuilder();
  sffOvsBridgeAugmentationBuilder.setOvsBridge(ovsBridgeBuilder.build());
  serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.addAugmentation(SffOvsBridgeAugmentation.class,sffOvsBridgeAugmentationBuilder.build()).setKey(new ServiceFunctionForwarderKey(new SffName(TEST_STRING))).setSffDataPlaneLocator(sffDataPlaneLocators);
  serviceFunctionForwarder=SfcOvsUtil.augmentSffWithOpenFlowNodeId(serviceFunctionForwarderBuilder.build());
  assertNotNull(""String_Node_Str"",serviceFunctionForwarder);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getAugmentation(SffOvsBridgeAugmentation.class).getOvsBridge().getOpenflowNodeId(),ofNodeId);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getKey().getName().getValue(),TEST_STRING);
}",0.9984116178806444
132026,"@Test public void testGetManagerNodeByIpV6(){
  OvsdbNodeAugmentationBuilder ovsdbNodeAugmentationBuilder=new OvsdbNodeAugmentationBuilder();
  org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress ipAddress=new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress(new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.Ipv6Address(IPV6_ADDRESS));
  ovsdbNodeAugmentationBuilder.setConnectionInfo(new ConnectionInfoBuilder().setRemoteIp(ipAddress).build());
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setNodeId(new NodeId(IPV6_ADDRESS)).addAugmentation(OvsdbNodeAugmentation.class,ovsdbNodeAugmentationBuilder.build());
  InstanceIdentifier<Node> nodeIID=SfcOvsUtil.buildOvsdbNodeIID(IPV6_ADDRESS);
  boolean transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(nodeIID,nodeBuilder.build(),LogicalDatastoreType.OPERATIONAL);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  Node node=SfcOvsUtil.getManagerNodeByIp(new IpAddress(new Ipv6Address(IPV6_ADDRESS)));
  assertNotNull(""String_Node_Str"",node);
  assertEquals(""String_Node_Str"",node.getKey().getNodeId().getValue(),IPV6_ADDRESS);
}","@Test public void testGetManagerNodeByIpV6(){
  OvsdbNodeAugmentationBuilder ovsdbNodeAugmentationBuilder=new OvsdbNodeAugmentationBuilder();
  org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress ipAddress=new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress(new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.Ipv6Address(IPV6_ADDRESS));
  ovsdbNodeAugmentationBuilder.setConnectionInfo(new ConnectionInfoBuilder().setRemoteIp(ipAddress).build());
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setNodeId(new NodeId(IPV6_ADDRESS)).addAugmentation(OvsdbNodeAugmentation.class,ovsdbNodeAugmentationBuilder.build());
  InstanceIdentifier<Node> ovsdbNodeIID=SfcOvsUtil.buildOvsdbNodeIID(IPV6_ADDRESS);
  boolean transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(ovsdbNodeIID,nodeBuilder.build(),LogicalDatastoreType.OPERATIONAL);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  Node node=SfcOvsUtil.getManagerNodeByIp(new IpAddress(new Ipv6Address(IPV6_ADDRESS)));
  assertNotNull(""String_Node_Str"",node);
  assertEquals(""String_Node_Str"",node.getKey().getNodeId().getValue(),IPV6_ADDRESS);
}",0.994336569579288
132027,"@Test public void testAugmentSffWithOpenFlowNodeId1() throws Exception {
  final String ofNodeId=""String_Node_Str"";
  OvsdbNodeAugmentationBuilder ovsdbNodeAugmentationBuilder=new OvsdbNodeAugmentationBuilder();
  org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress ipAddress=new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress(new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.Ipv4Address(TEST_IP_ADDRESS));
  ovsdbNodeAugmentationBuilder.setConnectionInfo(new ConnectionInfoBuilder().setRemoteIp(ipAddress).build());
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setKey(new NodeKey(new NodeId(""String_Node_Str""))).addAugmentation(OvsdbNodeAugmentation.class,ovsdbNodeAugmentationBuilder.build());
  InstanceIdentifier<Node> nodeIID=InstanceIdentifier.create(NetworkTopology.class).child(Topology.class,new TopologyKey(SouthboundConstants.OVSDB_TOPOLOGY_ID)).child(Node.class,new NodeKey(new NodeId(""String_Node_Str"")));
  boolean transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(nodeIID,nodeBuilder.build(),LogicalDatastoreType.CONFIGURATION);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.setKey(new ServiceFunctionForwarderKey(new SffName(TEST_STRING))).setIpMgmtAddress(new IpAddress(new Ipv4Address(TEST_IP_ADDRESS)));
  ServiceFunctionForwarder serviceFunctionForwarder=serviceFunctionForwarderBuilder.build();
  assertNotNull(""String_Node_Str"",serviceFunctionForwarder);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getKey().getName().getValue(),TEST_STRING);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getIpMgmtAddress().getIpv4Address().getValue(),TEST_IP_ADDRESS);
  OvsdbBridgeAugmentationBuilder ovsdbBridgeAugmentationBuilder=new OvsdbBridgeAugmentationBuilder();
  ovsdbBridgeAugmentationBuilder.setDatapathId(new DatapathId(TEST_DATA_PATH));
  InstanceIdentifier<OvsdbBridgeAugmentation> bridgeEntryIID=InstanceIdentifier.create(NetworkTopology.class).child(Topology.class,new TopologyKey(SouthboundConstants.OVSDB_TOPOLOGY_ID)).child(Node.class,new NodeKey(new NodeId(TEST_IP_ADDRESS + ""String_Node_Str"" + TEST_BRIDGE_NAME))).augmentation(OvsdbBridgeAugmentation.class);
  transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(bridgeEntryIID,ovsdbBridgeAugmentationBuilder.build(),LogicalDatastoreType.OPERATIONAL);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  IpBuilder ipBuilder=new IpBuilder();
  ipBuilder.setIp(new IpAddress(new Ipv4Address(IP_ADDRESS))).setPort(new PortNumber(5000));
  DataPlaneLocatorBuilder dataPlaneLocatorBuilder=new DataPlaneLocatorBuilder();
  dataPlaneLocatorBuilder.setTransport(VxlanGpe.class).setLocatorType(ipBuilder.build());
  List<SffDataPlaneLocator> sffDataPlaneLocators=new ArrayList<>();
  SffDataPlaneLocatorBuilder sffDataPlaneLocatorBuilder=new SffDataPlaneLocatorBuilder();
  SffDataPlaneLocatorName sffDplName=new SffDataPlaneLocatorName(""String_Node_Str"");
  sffDataPlaneLocatorBuilder.setName(sffDplName).setKey(new SffDataPlaneLocatorKey(sffDplName)).setDataPlaneLocator(dataPlaneLocatorBuilder.build());
  sffDataPlaneLocators.add(sffDataPlaneLocatorBuilder.build());
  OvsBridgeBuilder ovsBridgeBuilder=new OvsBridgeBuilder();
  ovsBridgeBuilder.setBridgeName(TEST_BRIDGE_NAME);
  SffOvsBridgeAugmentationBuilder sffOvsBridgeAugmentationBuilder=new SffOvsBridgeAugmentationBuilder();
  sffOvsBridgeAugmentationBuilder.setOvsBridge(ovsBridgeBuilder.build());
  serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.addAugmentation(SffOvsBridgeAugmentation.class,sffOvsBridgeAugmentationBuilder.build()).setKey(new ServiceFunctionForwarderKey(new SffName(TEST_STRING))).setSffDataPlaneLocator(sffDataPlaneLocators);
  serviceFunctionForwarder=SfcOvsUtil.augmentSffWithOpenFlowNodeId(serviceFunctionForwarderBuilder.build());
  assertNotNull(""String_Node_Str"",serviceFunctionForwarder);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getAugmentation(SffOvsBridgeAugmentation.class).getOvsBridge().getOpenflowNodeId(),ofNodeId);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getKey().getName().getValue(),TEST_STRING);
}","@Test public void testAugmentSffWithOpenFlowNodeId1() throws Exception {
  final String ofNodeId=""String_Node_Str"";
  OvsdbNodeAugmentationBuilder ovsdbNodeAugmentationBuilder=new OvsdbNodeAugmentationBuilder();
  org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress ipAddress=new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.IpAddress(new org.opendaylight.yang.gen.v1.urn.ietf.params.xml.ns.yang.ietf.inet.types.rev130715.Ipv4Address(TEST_IP_ADDRESS));
  ovsdbNodeAugmentationBuilder.setConnectionInfo(new ConnectionInfoBuilder().setRemoteIp(ipAddress).build());
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setKey(new NodeKey(new NodeId(""String_Node_Str""))).addAugmentation(OvsdbNodeAugmentation.class,ovsdbNodeAugmentationBuilder.build());
  InstanceIdentifier<Node> ovsdbNodeIID=InstanceIdentifier.create(NetworkTopology.class).child(Topology.class,new TopologyKey(SouthboundConstants.OVSDB_TOPOLOGY_ID)).child(Node.class,new NodeKey(new NodeId(""String_Node_Str"")));
  boolean transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(ovsdbNodeIID,nodeBuilder.build(),LogicalDatastoreType.CONFIGURATION);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.setKey(new ServiceFunctionForwarderKey(new SffName(TEST_STRING))).setIpMgmtAddress(new IpAddress(new Ipv4Address(TEST_IP_ADDRESS)));
  ServiceFunctionForwarder serviceFunctionForwarder=serviceFunctionForwarderBuilder.build();
  assertNotNull(""String_Node_Str"",serviceFunctionForwarder);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getKey().getName().getValue(),TEST_STRING);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getIpMgmtAddress().getIpv4Address().getValue(),TEST_IP_ADDRESS);
  OvsdbBridgeAugmentationBuilder ovsdbBridgeAugmentationBuilder=new OvsdbBridgeAugmentationBuilder();
  ovsdbBridgeAugmentationBuilder.setDatapathId(new DatapathId(TEST_DATA_PATH));
  InstanceIdentifier<OvsdbBridgeAugmentation> bridgeEntryIID=InstanceIdentifier.create(NetworkTopology.class).child(Topology.class,new TopologyKey(SouthboundConstants.OVSDB_TOPOLOGY_ID)).child(Node.class,new NodeKey(new NodeId(TEST_IP_ADDRESS + ""String_Node_Str"" + TEST_BRIDGE_NAME))).augmentation(OvsdbBridgeAugmentation.class);
  transactionSuccessful=SfcDataStoreAPI.writePutTransactionAPI(bridgeEntryIID,ovsdbBridgeAugmentationBuilder.build(),LogicalDatastoreType.OPERATIONAL);
  assertTrue(""String_Node_Str"",transactionSuccessful);
  IpBuilder ipBuilder=new IpBuilder();
  ipBuilder.setIp(new IpAddress(new Ipv4Address(IP_ADDRESS))).setPort(new PortNumber(5000));
  DataPlaneLocatorBuilder dataPlaneLocatorBuilder=new DataPlaneLocatorBuilder();
  dataPlaneLocatorBuilder.setTransport(VxlanGpe.class).setLocatorType(ipBuilder.build());
  List<SffDataPlaneLocator> sffDataPlaneLocators=new ArrayList<>();
  SffDataPlaneLocatorBuilder sffDataPlaneLocatorBuilder=new SffDataPlaneLocatorBuilder();
  SffDataPlaneLocatorName sffDplName=new SffDataPlaneLocatorName(""String_Node_Str"");
  sffDataPlaneLocatorBuilder.setName(sffDplName).setKey(new SffDataPlaneLocatorKey(sffDplName)).setDataPlaneLocator(dataPlaneLocatorBuilder.build());
  sffDataPlaneLocators.add(sffDataPlaneLocatorBuilder.build());
  OvsBridgeBuilder ovsBridgeBuilder=new OvsBridgeBuilder();
  ovsBridgeBuilder.setBridgeName(TEST_BRIDGE_NAME);
  SffOvsBridgeAugmentationBuilder sffOvsBridgeAugmentationBuilder=new SffOvsBridgeAugmentationBuilder();
  sffOvsBridgeAugmentationBuilder.setOvsBridge(ovsBridgeBuilder.build());
  serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.addAugmentation(SffOvsBridgeAugmentation.class,sffOvsBridgeAugmentationBuilder.build()).setKey(new ServiceFunctionForwarderKey(new SffName(TEST_STRING))).setSffDataPlaneLocator(sffDataPlaneLocators);
  serviceFunctionForwarder=SfcOvsUtil.augmentSffWithOpenFlowNodeId(serviceFunctionForwarderBuilder.build());
  assertNotNull(""String_Node_Str"",serviceFunctionForwarder);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getAugmentation(SffOvsBridgeAugmentation.class).getOvsBridge().getOpenflowNodeId(),ofNodeId);
  assertEquals(""String_Node_Str"",serviceFunctionForwarder.getKey().getName().getValue(),TEST_STRING);
}",0.9984116178806444
132028,"public static void setON(boolean on){
  SfcProviderDebug.on=on;
}","public static void setON(boolean value){
  SfcProviderDebug.on=value;
}",0.8970588235294118
132029,"/** 
 * Performs validation of a service function path.
 * @param serviceFunctionPath a candidate SFP that is being added / updated in a currently open transaction
 * @return true when validation is passed (i.e. when the SFs contained inthe SFP are coherent (type-wise) with the type definitions in the associated SFC; false afterwards)
 * @throws DataValidationFailedWithMessageException when validation cannot be performed because some of the referenced SFs / SFCs do not exist
 */
protected boolean validateServiceFunctionPath(ServiceFunctionPath serviceFunctionPath) throws DataValidationFailedWithMessageException {
  if (serviceFunctionPath != null) {
    LOG.debug(""String_Node_Str"",serviceFunctionPath.getName());
    if (serviceFunctionPath.getServicePathHop() == null) {
      LOG.info(""String_Node_Str"" + ""String_Node_Str"");
      return true;
    }
    int numberOfSpecifiedSFs=serviceFunctionPath.getServicePathHop().size();
    List<String> sfChainTypes;
    try {
      sfChainTypes=SfcDatastoreCache.getSfChainToSfTypeList().get(serviceFunctionPath.getServiceChainName());
    }
 catch (    ExecutionException e) {
      LOG.debug(""String_Node_Str"",e);
      throw ValidationConstants.SFP_FAILED_CAN_COMMIT_EXCEPTION_SFC_MISSING;
    }
    if (sfChainTypes == null || sfChainTypes.isEmpty()) {
      LOG.error(""String_Node_Str"");
      throw ValidationConstants.SFP_FAILED_CAN_COMMIT_EXCEPTION_SFC_MISSING;
    }
    LOG.debug(""String_Node_Str"",serviceFunctionPath.getServiceChainName().getValue(),serviceFunctionPath.getName().getValue(),sfChainTypes.size(),numberOfSpecifiedSFs);
    if (sfChainTypes.size() < numberOfSpecifiedSFs) {
      LOG.error(""String_Node_Str"",sfChainTypes.size(),numberOfSpecifiedSFs);
      return false;
    }
    boolean errorFound=false;
    for (int i=0; i < numberOfSpecifiedSFs; i++) {
      SfName sfName=serviceFunctionPath.getServicePathHop().get(i).getServiceFunctionName();
      if (sfName == null) {
        continue;
      }
      String sfChainTypeName=sfChainTypes.get(serviceFunctionPath.getServicePathHop().get(i).getHopNumber());
      String sfTypeNameFromSFP;
      try {
        sfTypeNameFromSFP=SfcDatastoreCache.getSfToSfTypeCache().get(sfName);
      }
 catch (      ExecutionException e) {
        LOG.debug(""String_Node_Str"",e);
        throw ValidationConstants.SFP_FAILED_CAN_COMMIT_EXCEPTION_SF_MISSING;
      }
      if (!sfChainTypeName.equals(sfTypeNameFromSFP)) {
        LOG.error(""String_Node_Str"",i,sfChainTypeName,sfTypeNameFromSFP);
        errorFound=true;
        break;
      }
    }
    if (!errorFound) {
      LOG.info(""String_Node_Str"");
      return true;
    }
  }
  return false;
}","/** 
 * Performs validation of a service function path.
 * @param serviceFunctionPath a candidate SFP that is being added / updated in a currently open transaction
 * @return true when validation is passed (i.e. when the SFs contained inthe SFP are coherent (type-wise) with the type definitions in the associated SFC; false afterwards)
 * @throws DataValidationFailedWithMessageException when validation cannot be performed because some of the referenced SFs / SFCs do not exist
 */
@SuppressWarnings(""String_Node_Str"") protected boolean validateServiceFunctionPath(ServiceFunctionPath serviceFunctionPath) throws DataValidationFailedWithMessageException {
  if (serviceFunctionPath != null) {
    LOG.debug(""String_Node_Str"",serviceFunctionPath.getName());
    if (serviceFunctionPath.getServicePathHop() == null) {
      LOG.info(""String_Node_Str"" + ""String_Node_Str"");
      return true;
    }
    int numberOfSpecifiedSFs=serviceFunctionPath.getServicePathHop().size();
    List<String> sfChainTypes;
    try {
      sfChainTypes=SfcDatastoreCache.getSfChainToSfTypeList().get(serviceFunctionPath.getServiceChainName());
    }
 catch (    ExecutionException e) {
      LOG.debug(""String_Node_Str"",e);
      throw ValidationConstants.SFP_FAILED_CAN_COMMIT_EXCEPTION_SFC_MISSING;
    }
    if (sfChainTypes == null || sfChainTypes.isEmpty()) {
      LOG.error(""String_Node_Str"");
      throw ValidationConstants.SFP_FAILED_CAN_COMMIT_EXCEPTION_SFC_MISSING;
    }
    LOG.debug(""String_Node_Str"",serviceFunctionPath.getServiceChainName().getValue(),serviceFunctionPath.getName().getValue(),sfChainTypes.size(),numberOfSpecifiedSFs);
    if (sfChainTypes.size() < numberOfSpecifiedSFs) {
      LOG.error(""String_Node_Str"",sfChainTypes.size(),numberOfSpecifiedSFs);
      return false;
    }
    boolean errorFound=false;
    for (int i=0; i < numberOfSpecifiedSFs; i++) {
      SfName sfName=serviceFunctionPath.getServicePathHop().get(i).getServiceFunctionName();
      if (sfName == null) {
        continue;
      }
      String sfChainTypeName=sfChainTypes.get(serviceFunctionPath.getServicePathHop().get(i).getHopNumber());
      String sfTypeNameFromSFP;
      try {
        sfTypeNameFromSFP=SfcDatastoreCache.getSfToSfTypeCache().get(sfName);
      }
 catch (      ExecutionException e) {
        LOG.debug(""String_Node_Str"",e);
        throw ValidationConstants.SFP_FAILED_CAN_COMMIT_EXCEPTION_SF_MISSING;
      }
      if (!sfChainTypeName.equals(sfTypeNameFromSFP)) {
        LOG.error(""String_Node_Str"",i,sfChainTypeName,sfTypeNameFromSFP);
        errorFound=true;
        break;
      }
    }
    if (!errorFound) {
      LOG.info(""String_Node_Str"");
      return true;
    }
  }
  return false;
}",0.9931316131427512
132030,"@Test public void testCreateReadServiceNode(){
  SfcProviderServiceNodeAPI.putServiceNode(serviceNode);
  ServiceNode serviceNode=SfcProviderServiceNodeAPI.readServiceNodeByName(new SnName(SERVICE_NODE_NAME));
  assertNotNull(""String_Node_Str"",serviceNode);
  assertEquals(""String_Node_Str"",serviceNode.getName().getValue(),SERVICE_NODE_NAME);
  assertEquals(""String_Node_Str"",serviceNode.getIpMgmtAddress().getIpv4Address().getValue(),SERVICE_NODE_IP_MANAGEMENT_ADDRESS);
}","@Test public void testCreateReadServiceNode(){
  SfcProviderServiceNodeAPI.putServiceNode(serviceNode);
  serviceNode=SfcProviderServiceNodeAPI.readServiceNodeByName(new SnName(SERVICE_NODE_NAME));
  assertNotNull(""String_Node_Str"",serviceNode);
  assertEquals(""String_Node_Str"",serviceNode.getName().getValue(),SERVICE_NODE_NAME);
  assertEquals(""String_Node_Str"",serviceNode.getIpMgmtAddress().getIpv4Address().getValue(),SERVICE_NODE_IP_MANAGEMENT_ADDRESS);
}",0.9871794871794872
132031,"@SuppressWarnings(""String_Node_Str"") private <U extends DataObject>boolean writeMergeTransaction(InstanceIdentifier<U> addIID,U data){
  long timeout=5000L;
  int attempt=0;
  WriteTransaction transaction=null;
  do {
    attempt++;
    try {
      transaction=mountpoint.newWriteOnlyTransaction();
    }
 catch (    RuntimeException e) {
      if (e.getCause().getClass().equals(NetconfDocumentedException.class)) {
        LOG.warn(""String_Node_Str"",attempt,e.getCause());
        try {
          Thread.sleep(timeout);
          timeout+=1000L;
        }
 catch (        InterruptedException i) {
          LOG.error(""String_Node_Str"",i);
        }
      }
 else {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
 while (attempt <= 5 && transaction == null);
  if (transaction == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  try {
    transaction.merge(Preconditions.checkNotNull(datastoreType),addIID,data);
    CheckedFuture<Void,TransactionCommitFailedException> submitFuture=transaction.submit();
    submitFuture.checkedGet();
    return true;
  }
 catch (  TransactionCommitFailedException e) {
    LOG.error(""String_Node_Str"",e);
    return false;
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    return false;
  }
}","@SuppressWarnings(""String_Node_Str"") private <U extends DataObject>boolean writeMergeTransaction(InstanceIdentifier<U> addIID,U dataObject){
  long timeout=5000L;
  int attempt=0;
  WriteTransaction transaction=null;
  do {
    attempt++;
    try {
      transaction=mountpoint.newWriteOnlyTransaction();
    }
 catch (    RuntimeException e) {
      if (e.getCause().getClass().equals(NetconfDocumentedException.class)) {
        LOG.warn(""String_Node_Str"",attempt,e.getCause());
        try {
          Thread.sleep(timeout);
          timeout+=1000L;
        }
 catch (        InterruptedException i) {
          LOG.error(""String_Node_Str"",i);
        }
      }
 else {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
 while (attempt <= 5 && transaction == null);
  if (transaction == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  try {
    transaction.merge(Preconditions.checkNotNull(datastoreType),addIID,dataObject);
    CheckedFuture<Void,TransactionCommitFailedException> submitFuture=transaction.submit();
    submitFuture.checkedGet();
    return true;
  }
 catch (  TransactionCommitFailedException e) {
    LOG.error(""String_Node_Str"",e);
    return false;
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    return false;
  }
}",0.9953198127925116
132032,"@Override public void setFlowWriter(SfcOfFlowWriterInterface sfcOfFlowWriter){
  this.sfcOfFlowWriter=sfcOfFlowWriter;
}","@Override public void setFlowWriter(SfcOfFlowWriterInterface writer){
  this.sfcOfFlowWriter=writer;
}",0.918918918918919
132033,"public void setFlowProgrammer(SfcOfFlowProgrammerInterface sfcFlowProgrammer){
  this.sfcFlowProgrammer=sfcFlowProgrammer;
}","public void setFlowProgrammer(SfcOfFlowProgrammerInterface flowProgrammer){
  this.sfcFlowProgrammer=flowProgrammer;
}",0.975206611570248
132034,"/** 
 * Update the operational datastore with information related to the rendered path.
 * @param theGraph the graph on which the just-rendered path was based
 * @param rsp the rendered service path
 */
public void updateOperationalDSInfo(SffGraph theGraph,RenderedServicePath rsp){
}","/** 
 * Update the operational datastore with information related to the rendered path.
 * @param theGraph the graph on which the just-rendered path was based
 * @param updatedRsp the rendered service path
 */
public void updateOperationalDSInfo(SffGraph theGraph,RenderedServicePath updatedRsp){
}",0.9690721649484536
132035,"public void setSfcPotRspProcessor(SfcPotNetconfIoam sfcPotNetconfIoam){
  this.sfcPotNetconfIoam=sfcPotNetconfIoam;
}","public void setSfcPotRspProcessor(SfcPotNetconfIoam netconfIoam){
  this.sfcPotNetconfIoam=netconfIoam;
}",0.927927927927928
132036,"public boolean init(String rspName,int sfSize,final Class<? extends TimeResolution> refreshPeriodTimeUnits,Long refreshPeriodValue,BitMaskOptions ioamPotProfileBitMask,Long ioamPotNumProfiles){
  long prime;
  long secret;
  List<Coeffs> coeffs=new ArrayList<>();
  List<Long> shares=new ArrayList<>();
  List<Lpcs> lpcs=new ArrayList<>();
  if (polyClassList == null) {
    polyClassList=sfcPotPolyClassAPI.getPolyClassList(rspName);
    if (polyClassList == null) {
      polyClassList=new ArrayList<>();
    }
  }
  this.refreshPeriodTimeUnits=refreshPeriodTimeUnits;
  this.refreshPeriodValue=refreshPeriodValue;
  this.ioamPotProfileBitMask=ioamPotProfileBitMask;
  this.ioamPotNumProfiles=ioamPotNumProfiles;
  sfcPotPolyClassAPI.setNumProfiles(ioamPotNumProfiles);
  SfcPotConfigGenerator configGenerator=new SfcPotConfigGenerator(sfSize);
  configGenerator.generateScvConfig();
  for (long j=0; j < ioamPotNumProfiles; j++) {
    prime=configGenerator.getPrime();
    secret=configGenerator.getSecret();
    for (int i=1; i < sfSize; i++) {
      coeffs.add(new CoeffsBuilder().setCoeff(configGenerator.getCoeff(i)).build());
      lpcs.add(new LpcsBuilder().setLpc(configGenerator.getLpc(i).longValue()).build());
    }
    for (int i=0; i < sfSize; i++) {
      shares.add(configGenerator.getSecretShare(i));
    }
    polyClassList.add(new SfcPotPolyClass(prime,secret,coeffs,shares,lpcs,sfSize));
    coeffs=new ArrayList<>();
    shares=new ArrayList<>();
    lpcs=new ArrayList<>();
  }
  sfcPotPolyClassAPI.putPolyClassList(rspName,polyClassList);
  return true;
}","public boolean init(String rspName,int sfSize,final Class<? extends TimeResolution> newRefreshPeriodTimeUnits,Long newRefreshPeriodValue,BitMaskOptions newIoamPotProfileBitMask,Long newIoamPotNumProfiles){
  long prime;
  long secret;
  List<Coeffs> coeffs=new ArrayList<>();
  List<Long> shares=new ArrayList<>();
  List<Lpcs> lpcs=new ArrayList<>();
  if (polyClassList == null) {
    polyClassList=sfcPotPolyClassAPI.getPolyClassList(rspName);
    if (polyClassList == null) {
      polyClassList=new ArrayList<>();
    }
  }
  this.refreshPeriodTimeUnits=newRefreshPeriodTimeUnits;
  this.refreshPeriodValue=newRefreshPeriodValue;
  this.ioamPotProfileBitMask=newIoamPotProfileBitMask;
  this.ioamPotNumProfiles=newIoamPotNumProfiles;
  sfcPotPolyClassAPI.setNumProfiles(newIoamPotNumProfiles);
  SfcPotConfigGenerator configGenerator=new SfcPotConfigGenerator(sfSize);
  configGenerator.generateScvConfig();
  for (long j=0; j < newIoamPotNumProfiles; j++) {
    prime=configGenerator.getPrime();
    secret=configGenerator.getSecret();
    for (int i=1; i < sfSize; i++) {
      coeffs.add(new CoeffsBuilder().setCoeff(configGenerator.getCoeff(i)).build());
      lpcs.add(new LpcsBuilder().setLpc(configGenerator.getLpc(i).longValue()).build());
    }
    for (int i=0; i < sfSize; i++) {
      shares.add(configGenerator.getSecretShare(i));
    }
    polyClassList.add(new SfcPotPolyClass(prime,secret,coeffs,shares,lpcs,sfSize));
    coeffs=new ArrayList<>();
    shares=new ArrayList<>();
    lpcs=new ArrayList<>();
  }
  sfcPotPolyClassAPI.putPolyClassList(rspName,polyClassList);
  return true;
}",0.9843161856963614
132037,"@Test public void createSfTestAuthFail(){
  TackerManager tackerManager=TackerManager.builder().setBaseUri(BASE_URI).setTackerPort(BASE_PORT).setKeystonePort(KEYSTONE_PORT).setAuth(Auth.builder().setTenantName(""String_Node_Str"").setPasswordCredentials(new PasswordCredentials(""String_Node_Str"",""String_Node_Str"")).build()).build();
  ServiceFunctionType sfType=new ServiceFunctionTypeBuilder().setType(new SftTypeName(""String_Node_Str"")).build();
  boolean result=tackerManager.createSf(sfType);
  LOG.debug(""String_Node_Str"" + !result);
  Assert.assertFalse(result);
}","@Test public void createSfTestAuthFail(){
  TackerManager badCredentialsTackerManager=TackerManager.builder().setBaseUri(BASE_URI).setTackerPort(BASE_PORT).setKeystonePort(KEYSTONE_PORT).setAuth(Auth.builder().setTenantName(""String_Node_Str"").setPasswordCredentials(new PasswordCredentials(""String_Node_Str"",""String_Node_Str"")).build()).build();
  ServiceFunctionType sfType=new ServiceFunctionTypeBuilder().setType(new SftTypeName(""String_Node_Str"")).build();
  boolean result=badCredentialsTackerManager.createSf(sfType);
  LOG.debug(""String_Node_Str"" + !result);
  Assert.assertFalse(result);
}",0.9725557461406518
132038,"/** 
 * Creates a table path from a node ID and table ID.
 * @param nodeId the ID of the node
 * @param tableId the ID of the table
 * @return the {@link InstanceIdentifier}
 */
public static final InstanceIdentifier<Table> createTablePath(final NodeId nodeId,final short tableId){
  return createNodePath(nodeId).builder().augmentation(FlowCapableNode.class).child(Table.class,new TableKey(tableId)).build();
}","/** 
 * Creates a table path from a node ID and table ID.
 * @param nodeId the ID of the node
 * @param tableId the ID of the table
 * @return the {@link InstanceIdentifier}
 */
public static InstanceIdentifier<Table> createTablePath(final NodeId nodeId,final short tableId){
  return createNodePath(nodeId).builder().augmentation(FlowCapableNode.class).child(Table.class,new TableKey(tableId)).build();
}",0.9926470588235294
132039,"/** 
 * Creates an Instance Identifier (path) for node with specified id.
 * @param nodeId the ID of the node
 * @return the {@link InstanceIdentifier}
 */
public static final InstanceIdentifier<Node> createNodePath(final NodeId nodeId){
  return InstanceIdentifier.builder(Nodes.class).child(Node.class,new NodeKey(nodeId)).build();
}","/** 
 * Creates an Instance Identifier (path) for node with specified id.
 * @param nodeId the ID of the node
 * @return the {@link InstanceIdentifier}
 */
public static InstanceIdentifier<Node> createNodePath(final NodeId nodeId){
  return InstanceIdentifier.builder(Nodes.class).child(Node.class,new NodeKey(nodeId)).build();
}",0.9909638554216867
132040,"protected void setupSfc(){
  dataBroker=getDataBroker();
  SfcDataStoreAPI.setDataProviderAux(dataBroker);
  SfcProviderRpc.setDataProviderAux(dataBroker);
  sfcIids=new SfcInstanceIdentifiers();
}","protected void setupSfc(){
  dataBroker=getDataBroker();
  SfcDataStoreAPI.setDataProviderAux(dataBroker);
  sfcIids=new SfcInstanceIdentifiers();
}",0.8579710144927536
132041,"protected void setupSfc(){
  dataBroker=getDataBroker();
  SfcDataStoreAPI.setDataProviderAux(dataBroker);
  SfcProviderRpc.setDataProviderAux(dataBroker);
  sfcIids=new SfcInstanceIdentifiers();
}","protected void setupSfc(){
  dataBroker=getDataBroker();
  SfcDataStoreAPI.setDataProviderAux(dataBroker);
  sfcIids=new SfcInstanceIdentifiers();
}",0.8579710144927536
132042,"@Before public void init(){
  dataBroker=getDataBroker();
  sfcProviderRpc=new SfcProviderRpc();
  SfcProviderRpc.setDataProviderAux(dataBroker);
  SfcDataStoreAPI.setDataProviderAux(dataBroker);
}","@Before public void init(){
  dataBroker=getDataBroker();
  sfcProviderRpc=new SfcProviderRpc(dataBroker);
  SfcDataStoreAPI.setDataProviderAux(dataBroker);
}",0.8901408450704226
132043,"/** 
 * Install an ACE entry, belonging to the given ACL, on the SFF identified through the specified nodeName. This method is called on result of classifier addition / removal.
 * @param nodeName      the compute node data-plane ID where the ACL is about to be written
 * @param theSff        the SFF to which the classifier is connected
 * @param theScfName    the name of the classifier
 * @param aclName       the name of the ACL
 * @param theIfName     the interface we want to classify
 * @param theAce        the ACE
 * @return              a List of {@link FlowDetails} having all the generated flows, which will be later installed
 */
public List<FlowDetails> processAceByProcessor(String nodeName,ServiceFunctionForwarder theSff,String theScfName,String aclName,String theIfName,Ace theAce,Optional<RspName> rspName){
  List<FlowDetails> theFlows=new ArrayList<>();
  String ruleName=theAce.getRuleName();
  if (Strings.isNullOrEmpty(ruleName)) {
    LOG.error(""String_Node_Str"");
    return Collections.emptyList();
  }
  LOG.info(""String_Node_Str"",nodeName,theIfName);
  Optional<Long> inPort=macChainingClassifier.getInPort(theIfName,nodeName);
  Match match=inPort.map(port -> String.format(""String_Node_Str"",nodeName,port)).map(NodeConnectorId::new).map(connectorId -> new SfcScfMatch().setPortMatch(connectorId)).map(scfMatch -> scfMatch.setAclMatch(theAce.getMatches())).orElseThrow(IllegalArgumentException::new).build();
  String flowKey=classifierHandler.buildFlowKeyName(theScfName,aclName,ruleName,""String_Node_Str"");
  if (addClassifier) {
    LOG.info(""String_Node_Str"");
    theFlows.add(macChainingClassifier.initClassifierTable(nodeName));
    Optional.ofNullable(macChainingClassifier.createClassifierOutFlow(flowKey,match,rspName.get(),nodeName)).ifPresent(theFlows::add);
  }
 else {
    LOG.info(""String_Node_Str"");
    theFlows.add(classifierHandler.deleteFlowFromTable(nodeName,flowKey,ClassifierGeniusIntegration.getClassifierTable()));
  }
  if (classifierHandler.usesLogicalInterfaces(theSff)) {
    return theFlows;
  }
  List<FlowDetails> theReverseRspFlows=processReverseRsp(rspName.get(),theScfName,aclName,nodeName,theAce.getRuleName(),inPort.get(),theSff);
  theFlows.addAll(theReverseRspFlows);
  LOG.debug(""String_Node_Str"",theFlows.size());
  return theFlows;
}","/** 
 * Install an ACE entry, belonging to the given ACL, on the SFF identified through the specified nodeName. This method is called on result of classifier addition / removal.
 * @param nodeName      the compute node data-plane ID where the ACL is about to be written
 * @param theSff        the SFF to which the classifier is connected
 * @param theScfName    the name of the classifier
 * @param aclName       the name of the ACL
 * @param theIfName     the interface we want to classify
 * @param theAce        the ACE
 * @return              a List of {@link FlowDetails} having all the generated flows, which will be later installed
 */
@Override public List<FlowDetails> processAceByProcessor(String nodeName,ServiceFunctionForwarder theSff,String theScfName,String aclName,String theIfName,Ace theAce,Optional<RspName> rspName){
  List<FlowDetails> theFlows=new ArrayList<>();
  String ruleName=theAce.getRuleName();
  if (Strings.isNullOrEmpty(ruleName)) {
    LOG.error(""String_Node_Str"");
    return Collections.emptyList();
  }
  LOG.info(""String_Node_Str"",nodeName,theIfName);
  Optional<Long> inPort=macChainingClassifier.getInPort(theIfName,nodeName);
  Match match=inPort.map(port -> String.format(""String_Node_Str"",nodeName,port)).map(NodeConnectorId::new).map(connectorId -> new SfcScfMatch().setPortMatch(connectorId)).map(scfMatch -> scfMatch.setAclMatch(theAce.getMatches())).orElseThrow(IllegalArgumentException::new).build();
  String flowKey=classifierHandler.buildFlowKeyName(theScfName,aclName,ruleName,""String_Node_Str"");
  if (addClassifier) {
    LOG.info(""String_Node_Str"");
    theFlows.add(macChainingClassifier.initClassifierTable(nodeName));
    Optional.ofNullable(macChainingClassifier.createClassifierOutFlow(flowKey,match,rspName.get(),nodeName)).ifPresent(theFlows::add);
  }
 else {
    LOG.info(""String_Node_Str"");
    theFlows.add(classifierHandler.deleteFlowFromTable(nodeName,flowKey,ClassifierGeniusIntegration.getClassifierTable()));
  }
  if (classifierHandler.usesLogicalInterfaces(theSff)) {
    return theFlows;
  }
  List<FlowDetails> theReverseRspFlows=processReverseRsp(rspName.get(),theScfName,aclName,nodeName,theAce.getRuleName(),inPort.get(),theSff);
  theFlows.addAll(theReverseRspFlows);
  LOG.debug(""String_Node_Str"",theFlows.size());
  return theFlows;
}",0.9978364344439636
132044,"void configureMacChainingNextHopFlow(final String sffNodeName,final String vmac,final String dstSfMac,final String nextVMac,final boolean l2Tranparent);","void configureMacChainingNextHopFlow(String sffNodeName,String vmac,String dstSfMac,String nextVMac,boolean l2Tranparent);",0.8905109489051095
132045,"void configureMacChainingSfTransportEgressFlow(final String sffNodeName,final String dstMac,String port,final String vmac);","void configureMacChainingSfTransportEgressFlow(String sffNodeName,String dstMac,String port,String vmac);",0.9210526315789472
132046,"public abstract ServiceFunctionType getServiceFunctionType(final SfName sfName,long rspId);","public abstract ServiceFunctionType getServiceFunctionType(SfName sfName,long rspId);",0.9659090909090908
132047,"private long toLong(){
  long mac=0;
  for (int i=0; i < 6; i++) {
    long tmp=(macAddr[i] & 0xffL) << ((5 - i) * 8);
    mac|=tmp;
  }
  return mac;
}","private long toLong(){
  long mac=0;
  for (int i=0; i < 6; i++) {
    long tmp=(macAddr[i] & 0xffL) << (5 - i) * 8;
    mac|=tmp;
  }
  return mac;
}",0.9933774834437086
132048,"private byte[] toByte(long address){
  byte[] addressInBytes=new byte[]{(byte)((address >> 40) & 0xff),(byte)((address >> 32) & 0xff),(byte)((address >> 24) & 0xff),(byte)((address >> 16) & 0xff),(byte)((address >> 8) & 0xff),(byte)((address >> 0) & 0xff)};
  return addressInBytes;
}","private byte[] toByte(long address){
  byte[] addressInBytes=new byte[]{(byte)(address >> 40 & 0xff),(byte)(address >> 32 & 0xff),(byte)(address >> 24 & 0xff),(byte)(address >> 16 & 0xff),(byte)(address >> 8 & 0xff),(byte)(address >> 0 & 0xff)};
  return addressInBytes;
}",0.6618705035971223
132049,"public VirtualMacAddress(int flags,long port,int chainId){
  if (port >= MAX_PORT || port < 0) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",MAX_PORT));
  }
  final int flagsMask=(MAX_FLAGS - 1) << (24 - FLAGS_LEN);
  if ((flags & (~flagsMask)) != 0) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",flags));
  }
  this.chainId=chainId;
  this.port=(int)port;
  this.flags=flags;
  this.macAddr=toByte(BASE_OUI | flags | port << (CID_LEN + SFID_LEN) | chainId << (SFID_LEN) | (int)(Math.pow(2,SFID_LEN) - 1));
}","public VirtualMacAddress(int flags,long port,int chainId){
  if (port >= MAX_PORT || port < 0) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",MAX_PORT));
  }
  final int flagsMask=MAX_FLAGS - 1 << 24 - FLAGS_LEN;
  if ((flags & ~flagsMask) != 0) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",flags));
  }
  this.chainId=chainId;
  this.port=(int)port;
  this.flags=flags;
  this.macAddr=toByte(BASE_OUI | flags | port << CID_LEN + SFID_LEN | chainId << SFID_LEN | (int)(Math.pow(2,SFID_LEN) - 1));
}",0.9838420107719928
132050,"/** 
 * Put the service function monitor information into the OPERATIONAL datastore.
 * @param monInfo Service Function monitoring information
 * @param sfName Service Function name
 * @return true if monInfo was successfully put, fasle otherwise
 */
public static boolean putServiceFunctionMonitor(MonitoringInfo monInfo,SfName sfName){
  boolean ret=false;
  SfcSfDescMonBuilder sfDescMonBuilder=null;
  SfcSfDescMon sfDescMon=null;
  ServiceFunctionState dataSfcStateObject;
  printTraceStart(LOG);
  if (dataBroker != null) {
    sfDescMonBuilder=new SfcSfDescMonBuilder().setMonitoringInfo(monInfo);
    ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(sfName);
    InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
    dataSfcStateObject=SfcDataStoreAPI.readTransactionAPI(sfStateIID,LogicalDatastoreType.OPERATIONAL);
    if (dataSfcStateObject != null) {
      ServiceFunctionState1 sf1Temp=dataSfcStateObject.getAugmentation(ServiceFunctionState1.class);
      if (sf1Temp != null) {
        SfcSfDescMon sfDescMonTemp=sf1Temp.getSfcSfDescMon();
        sfDescMonBuilder.setDescriptionInfo(sfDescMonTemp.getDescriptionInfo());
      }
    }
    sfDescMon=sfDescMonBuilder.build();
    ServiceFunctionState1 sfState1=new ServiceFunctionState1Builder().setSfcSfDescMon(sfDescMon).build();
    ServiceFunctionState serviceFunctionState=new ServiceFunctionStateBuilder().setKey(serviceFunctionStateKey).addAugmentation(ServiceFunctionState1.class,sfState1).build();
    if (dataSfcStateObject != null) {
      ret=SfcProviderServiceFunctionAPI.mergeServiceFunctionState(serviceFunctionState);
    }
 else {
      ret=SfcProviderServiceFunctionAPI.putServiceFunctionState(serviceFunctionState);
    }
  }
 else {
    LOG.error(""String_Node_Str"");
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * Put the service function monitor information into the OPERATIONAL datastore.
 * @param monInfo Service Function monitoring information
 * @param sfName Service Function name
 * @return true if monInfo was successfully put, fasle otherwise
 */
public static boolean putServiceFunctionMonitor(MonitoringInfo monInfo,SfName sfName){
  boolean ret=false;
  SfcSfDescMonBuilder sfDescMonBuilder;
  SfcSfDescMon sfDescMon;
  ServiceFunctionState dataSfcStateObject;
  printTraceStart(LOG);
  if (dataBroker != null) {
    sfDescMonBuilder=new SfcSfDescMonBuilder().setMonitoringInfo(monInfo);
    ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(sfName);
    InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
    dataSfcStateObject=SfcDataStoreAPI.readTransactionAPI(sfStateIID,LogicalDatastoreType.OPERATIONAL);
    if (dataSfcStateObject != null) {
      ServiceFunctionState1 sf1Temp=dataSfcStateObject.getAugmentation(ServiceFunctionState1.class);
      if (sf1Temp != null) {
        SfcSfDescMon sfDescMonTemp=sf1Temp.getSfcSfDescMon();
        sfDescMonBuilder.setDescriptionInfo(sfDescMonTemp.getDescriptionInfo());
      }
    }
    sfDescMon=sfDescMonBuilder.build();
    ServiceFunctionState1 sfState1=new ServiceFunctionState1Builder().setSfcSfDescMon(sfDescMon).build();
    ServiceFunctionState serviceFunctionState=new ServiceFunctionStateBuilder().setKey(serviceFunctionStateKey).addAugmentation(ServiceFunctionState1.class,sfState1).build();
    if (dataSfcStateObject != null) {
      ret=SfcProviderServiceFunctionAPI.mergeServiceFunctionState(serviceFunctionState);
    }
 else {
      ret=SfcProviderServiceFunctionAPI.putServiceFunctionState(serviceFunctionState);
    }
  }
 else {
    LOG.error(""String_Node_Str"");
  }
  printTraceStop(LOG);
  return ret;
}",0.9974226804123713
132051,"/** 
 * put the service function description information into the OPERATIONAL datastore.
 * @param descInfo Service Function description information
 * @param sfName Service Function name
 * @return true if descInfo was successfully put, false otherwise
 */
public static boolean putServiceFunctionDescription(DescriptionInfo descInfo,SfName sfName){
  boolean ret=false;
  SfcSfDescMonBuilder sfDescMonBuilder=null;
  SfcSfDescMon sfDescMon=null;
  ServiceFunctionState dataSfcStateObject;
  printTraceStart(LOG);
  if (dataBroker != null) {
    sfDescMonBuilder=new SfcSfDescMonBuilder().setDescriptionInfo(descInfo);
    ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(sfName);
    InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
    dataSfcStateObject=SfcDataStoreAPI.readTransactionAPI(sfStateIID,LogicalDatastoreType.OPERATIONAL);
    if (dataSfcStateObject != null) {
      ServiceFunctionState1 sf1Temp=dataSfcStateObject.getAugmentation(ServiceFunctionState1.class);
      if (sf1Temp != null) {
        SfcSfDescMon sfDescMonTemp=sf1Temp.getSfcSfDescMon();
        sfDescMonBuilder.setMonitoringInfo(sfDescMonTemp.getMonitoringInfo());
      }
    }
    sfDescMon=sfDescMonBuilder.build();
    ServiceFunctionState1 sfState1=new ServiceFunctionState1Builder().setSfcSfDescMon(sfDescMon).build();
    ServiceFunctionState serviceFunctionState=new ServiceFunctionStateBuilder().setKey(serviceFunctionStateKey).addAugmentation(ServiceFunctionState1.class,sfState1).build();
    if (dataSfcStateObject != null) {
      ret=SfcProviderServiceFunctionAPI.mergeServiceFunctionState(serviceFunctionState);
    }
 else {
      ret=SfcProviderServiceFunctionAPI.putServiceFunctionState(serviceFunctionState);
    }
  }
 else {
    LOG.error(""String_Node_Str"");
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * put the service function description information into the OPERATIONAL datastore.
 * @param descInfo Service Function description information
 * @param sfName Service Function name
 * @return true if descInfo was successfully put, false otherwise
 */
public static boolean putServiceFunctionDescription(DescriptionInfo descInfo,SfName sfName){
  boolean ret=false;
  SfcSfDescMonBuilder sfDescMonBuilder;
  SfcSfDescMon sfDescMon;
  ServiceFunctionState dataSfcStateObject;
  printTraceStart(LOG);
  if (dataBroker != null) {
    sfDescMonBuilder=new SfcSfDescMonBuilder().setDescriptionInfo(descInfo);
    ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(sfName);
    InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
    dataSfcStateObject=SfcDataStoreAPI.readTransactionAPI(sfStateIID,LogicalDatastoreType.OPERATIONAL);
    if (dataSfcStateObject != null) {
      ServiceFunctionState1 sf1Temp=dataSfcStateObject.getAugmentation(ServiceFunctionState1.class);
      if (sf1Temp != null) {
        SfcSfDescMon sfDescMonTemp=sf1Temp.getSfcSfDescMon();
        sfDescMonBuilder.setMonitoringInfo(sfDescMonTemp.getMonitoringInfo());
      }
    }
    sfDescMon=sfDescMonBuilder.build();
    ServiceFunctionState1 sfState1=new ServiceFunctionState1Builder().setSfcSfDescMon(sfDescMon).build();
    ServiceFunctionState serviceFunctionState=new ServiceFunctionStateBuilder().setKey(serviceFunctionStateKey).addAugmentation(ServiceFunctionState1.class,sfState1).build();
    if (dataSfcStateObject != null) {
      ret=SfcProviderServiceFunctionAPI.mergeServiceFunctionState(serviceFunctionState);
    }
 else {
      ret=SfcProviderServiceFunctionAPI.putServiceFunctionState(serviceFunctionState);
    }
  }
 else {
    LOG.error(""String_Node_Str"");
  }
  printTraceStop(LOG);
  return ret;
}",0.9974398361495136
132052,"public GetSFDescriptionOutput getSFDescriptionInfoFromNetconf(String nodeName){
  GetSFDescriptionOutput ret=null;
  printTraceStart(LOG);
  ServiceFunctionDescriptionMonitorReportService service=getSfDescriptionMonitorService(nodeName);
  if (service != null) {
    Future<RpcResult<GetSFDescriptionOutput>> result=service.getSFDescription();
    RpcResult<GetSFDescriptionOutput> output;
    try {
      output=result.get();
      if (output.isSuccessful()) {
        ret=output.getResult();
        LOG.info(""String_Node_Str"");
      }
 else {
        LOG.error(""String_Node_Str"");
      }
    }
 catch (    InterruptedException|ExecutionException e) {
      LOG.error(""String_Node_Str"");
    }
  }
  printTraceStop(LOG);
  return ret;
}","public GetSFDescriptionOutput getSFDescriptionInfoFromNetconf(String nodeName){
  GetSFDescriptionOutput ret=null;
  printTraceStart(LOG);
  ServiceFunctionDescriptionMonitorReportService service=getSfDescriptionMonitorService(nodeName);
  if (service != null) {
    Future<RpcResult<GetSFDescriptionOutput>> result=service.getSFDescription();
    RpcResult<GetSFDescriptionOutput> output;
    try {
      output=result.get();
      if (output.isSuccessful()) {
        ret=output.getResult();
        LOG.info(""String_Node_Str"");
      }
 else {
        LOG.error(""String_Node_Str"");
      }
    }
 catch (    InterruptedException|ExecutionException e) {
      LOG.error(""String_Node_Str"",e);
    }
  }
  printTraceStop(LOG);
  return ret;
}",0.9986504723346828
132053,"public GetSFMonitoringInfoOutput getSFMonitorInfoFromNetconf(String nodeName){
  GetSFMonitoringInfoOutput ret=null;
  printTraceStart(LOG);
  ServiceFunctionDescriptionMonitorReportService service=getSfDescriptionMonitorService(nodeName);
  if (service != null) {
    Future<RpcResult<GetSFMonitoringInfoOutput>> result=service.getSFMonitoringInfo();
    RpcResult<GetSFMonitoringInfoOutput> output;
    try {
      output=result.get();
      if (output.isSuccessful()) {
        ret=output.getResult();
        LOG.info(""String_Node_Str"");
      }
 else {
        LOG.error(""String_Node_Str"");
      }
    }
 catch (    InterruptedException|ExecutionException e) {
      LOG.error(""String_Node_Str"");
    }
  }
  printTraceStop(LOG);
  return ret;
}","public GetSFMonitoringInfoOutput getSFMonitorInfoFromNetconf(String nodeName){
  GetSFMonitoringInfoOutput ret=null;
  printTraceStart(LOG);
  ServiceFunctionDescriptionMonitorReportService service=getSfDescriptionMonitorService(nodeName);
  if (service != null) {
    Future<RpcResult<GetSFMonitoringInfoOutput>> result=service.getSFMonitoringInfo();
    RpcResult<GetSFMonitoringInfoOutput> output;
    try {
      output=result.get();
      if (output.isSuccessful()) {
        ret=output.getResult();
        LOG.info(""String_Node_Str"");
      }
 else {
        LOG.error(""String_Node_Str"");
      }
    }
 catch (    InterruptedException|ExecutionException e) {
      LOG.error(""String_Node_Str"",e);
    }
  }
  printTraceStop(LOG);
  return ret;
}",0.9986702127659576
132054,"public void setSfcProviderSfDescriptionMonitorAPI(SfcProviderSfDescriptionMonitorAPI broker){
  getSfDescMon=broker;
}","public void setSfcProviderSfDescriptionMonitorAPI(SfcProviderSfDescriptionMonitorAPI descriptionMonitorAPI){
  getSfDescMon=descriptionMonitorAPI;
}",0.8421052631578947
132055,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node node=(Node)entry.getValue();
      String nodeName=node.getNodeId().getValue();
      LOG.debug(""String_Node_Str"",node.toString());
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    Node node=null;
    if (entry.getKey().getTargetType() == NetconfNode.class) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      NetconfNode nnode=(NetconfNode)entry.getValue();
      LOG.info(""String_Node_Str"",nodeName);
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if (entry.getKey().getTargetType() == NetconfNode.class && !dataCreatedObject.containsKey(entry.getKey())) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      if (!nodeName.equals(""String_Node_Str"")) {
        NetconfNode nnode=(NetconfNode)entry.getValue();
        NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
switch (csts) {
case Connected:
{
            LOG.debug(""String_Node_Str"",nodeId.getValue());
            List<AvailableCapability> capabilities=nnode.getAvailableCapabilities().getAvailableCapability();
            LOG.debug(""String_Node_Str"",capabilities);
            if (isServiceFunction(nnode)) {
              DescriptionInfo descInfo=SfcNetconfServiceFunctionAPI.getServiceFunctionDescription(nodeName);
              String type=descInfo.getType();
              if (type == null || type.isEmpty()) {
                LOG.error(""String_Node_Str"");
                break;
              }
              SftTypeName sfType=SfcProviderServiceTypeAPI.readServiceFunctionType(new SftTypeName(type)).getType();
              if (sfType == null) {
                LOG.error(""String_Node_Str"",type);
                break;
              }
              SfName sfNodeName=new SfName(nodeName);
              ServiceFunction sf=SfcNetconfServiceFunctionAPI.buildServiceFunctionFromNetconf(sfNodeName,descInfo.getDataPlaneIp(),descInfo.getDataPlanePort(),sfType);
              if (SfcProviderServiceFunctionAPI.putServiceFunction(sf)) {
                LOG.info(""String_Node_Str"",nodeName);
                SfcNetconfServiceFunctionAPI.putServiceFunctionDescription(descInfo,sfNodeName);
                MonitoringInfo monInfo=SfcNetconfServiceFunctionAPI.getServiceFunctionMonitor(nodeName);
                if (monInfo != null) {
                  SfcNetconfServiceFunctionAPI.putServiceFunctionMonitor(monInfo,sfNodeName);
                }
              }
 else {
                LOG.error(""String_Node_Str"",nodeName);
              }
              SfDescriptionMonitoringThread monitoringThread=new SfDescriptionMonitoringThread(nodeName);
              Thread thread=new Thread(monitoringThread);
              thread.start();
            }
 else {
              ServiceFunctionForwarder sff=SfcNetconfServiceForwarderAPI.buildServiceForwarderFromNetconf(nodeName,nnode);
              if (SfcProviderServiceForwarderAPI.putServiceFunctionForwarder(sff)) {
                LOG.info(""String_Node_Str"",nodeName);
              }
 else {
                LOG.error(""String_Node_Str"",nodeName);
              }
            }
            break;
          }
case Connecting:
{
          LOG.info(""String_Node_Str"",nodeName);
          if (SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarder(new SffName(nodeName))) {
            LOG.info(""String_Node_Str"",nodeName);
          }
 else {
            LOG.error(""String_Node_Str"",nodeName);
          }
          break;
        }
case UnableToConnect:
{
        LOG.info(""String_Node_Str"",nodeName);
        if (SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarder(new SffName(nodeName))) {
          LOG.info(""String_Node_Str"",nodeName);
        }
 else {
          LOG.error(""String_Node_Str"",nodeName);
        }
        break;
      }
default :
    break;
}
}
}
}
printTraceStop(LOG);
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node node=(Node)entry.getValue();
      LOG.debug(""String_Node_Str"",node.toString());
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    if (entry.getKey().getTargetType() == NetconfNode.class) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeName);
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if (entry.getKey().getTargetType() == NetconfNode.class && !dataCreatedObject.containsKey(entry.getKey())) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      if (!CONTROLLER_CONFIG.equals(nodeName)) {
        NetconfNode nnode=(NetconfNode)entry.getValue();
        NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
switch (csts) {
case Connected:
{
            LOG.debug(""String_Node_Str"",nodeId.getValue());
            List<AvailableCapability> capabilities=nnode.getAvailableCapabilities().getAvailableCapability();
            LOG.debug(""String_Node_Str"",capabilities);
            if (isServiceFunction(nnode)) {
              DescriptionInfo descInfo=SfcNetconfServiceFunctionAPI.getServiceFunctionDescription(nodeName);
              String type=descInfo.getType();
              if (type == null || type.isEmpty()) {
                LOG.error(""String_Node_Str"");
                break;
              }
              SftTypeName sfType=SfcProviderServiceTypeAPI.readServiceFunctionType(new SftTypeName(type)).getType();
              if (sfType == null) {
                LOG.error(""String_Node_Str"",type);
                break;
              }
              SfName sfNodeName=new SfName(nodeName);
              ServiceFunction sf=SfcNetconfServiceFunctionAPI.buildServiceFunctionFromNetconf(sfNodeName,descInfo.getDataPlaneIp(),descInfo.getDataPlanePort(),sfType);
              if (SfcProviderServiceFunctionAPI.putServiceFunction(sf)) {
                LOG.info(""String_Node_Str"",nodeName);
                SfcNetconfServiceFunctionAPI.putServiceFunctionDescription(descInfo,sfNodeName);
                MonitoringInfo monInfo=SfcNetconfServiceFunctionAPI.getServiceFunctionMonitor(nodeName);
                if (monInfo != null) {
                  SfcNetconfServiceFunctionAPI.putServiceFunctionMonitor(monInfo,sfNodeName);
                }
              }
 else {
                LOG.error(""String_Node_Str"",nodeName);
              }
              SfDescriptionMonitoringThread monitoringThread=new SfDescriptionMonitoringThread(nodeName);
              Thread thread=new Thread(monitoringThread);
              thread.start();
            }
 else {
              ServiceFunctionForwarder sff=SfcNetconfServiceForwarderAPI.buildServiceForwarderFromNetconf(nodeName,nnode);
              if (SfcProviderServiceForwarderAPI.putServiceFunctionForwarder(sff)) {
                LOG.info(""String_Node_Str"",nodeName);
              }
 else {
                LOG.error(""String_Node_Str"",nodeName);
              }
            }
            break;
          }
case Connecting:
{
          LOG.info(""String_Node_Str"",nodeName);
          if (SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarder(new SffName(nodeName))) {
            LOG.info(""String_Node_Str"",nodeName);
          }
 else {
            LOG.error(""String_Node_Str"",nodeName);
          }
          break;
        }
case UnableToConnect:
{
        LOG.info(""String_Node_Str"",nodeName);
        if (SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarder(new SffName(nodeName))) {
          LOG.info(""String_Node_Str"",nodeName);
        }
 else {
          LOG.error(""String_Node_Str"",nodeName);
        }
        break;
      }
default :
    break;
}
}
}
}
printTraceStop(LOG);
}",0.2630533816142974
132056,"public static Action createActionPktIn(final int pktLength,final int order){
  OutputActionBuilder output=new OutputActionBuilder();
  output.setMaxLength(new Integer(0xffff));
  Uri controllerPort=new Uri(OutputPortValues.CONTROLLER.toString());
  output.setOutputNodeConnector(controllerPort);
  ActionBuilder ab=createActionBuilder(order);
  ab.setAction(new OutputActionCaseBuilder().setOutputAction(output.build()).build());
  return ab.build();
}","public static Action createActionPktIn(final int pktLength,final int order){
  OutputActionBuilder output=new OutputActionBuilder();
  output.setMaxLength(0xffff);
  Uri controllerPort=new Uri(OutputPortValues.CONTROLLER.toString());
  output.setOutputNodeConnector(controllerPort);
  ActionBuilder ab=createActionBuilder(order);
  ab.setAction(new OutputActionCaseBuilder().setOutputAction(output.build()).build());
  return ab.build();
}",0.9854096520763188
132057,"public static Action createActionPushMpls(int order){
  PushMplsActionBuilder push=new PushMplsActionBuilder();
  push.setEthernetType(new Integer(ETHERTYPE_MPLS_UCAST));
  PushMplsActionCaseBuilder pushMplsCase=new PushMplsActionCaseBuilder();
  pushMplsCase.setPushMplsAction(push.build());
  ActionBuilder ab=createActionBuilder(order);
  ab.setAction(pushMplsCase.build());
  return ab.build();
}","public static Action createActionPushMpls(int order){
  PushMplsActionBuilder push=new PushMplsActionBuilder();
  push.setEthernetType(ETHERTYPE_MPLS_UCAST);
  PushMplsActionCaseBuilder pushMplsCase=new PushMplsActionCaseBuilder();
  pushMplsCase.setPushMplsAction(push.build());
  ActionBuilder ab=createActionBuilder(order);
  ab.setAction(pushMplsCase.build());
  return ab.build();
}",0.983481575603558
132058,"/** 
 * Add an etherType match to an existing MatchBuilder.
 * @param match the Match object to which we want to add an EtherTypeMatch
 * @param etherType the ethernet type
 */
public static void addMatchEtherType(MatchBuilder match,final long etherType){
  EthernetMatchBuilder ethernetMatch=new EthernetMatchBuilder();
  EthernetTypeBuilder ethTypeBuilder=new EthernetTypeBuilder();
  ethTypeBuilder.setType(new EtherType(etherType));
  ethernetMatch.setEthernetType(ethTypeBuilder.build());
  match.setEthernetMatch(mergeEthernetMatch(match,ethernetMatch));
}","/** 
 * Add an etherType match to an existing MatchBuilder.
 * @param match the Match object to which we want to add an EtherTypeMatch
 * @param etherType the Ethernet type
 */
public static void addMatchEtherType(MatchBuilder match,final long etherType){
  EthernetMatchBuilder ethernetMatch=new EthernetMatchBuilder();
  EthernetTypeBuilder ethTypeBuilder=new EthernetTypeBuilder();
  ethTypeBuilder.setType(new EtherType(etherType));
  ethernetMatch.setEthernetType(ethTypeBuilder.build());
  match.setEthernetMatch(mergeEthernetMatch(match,ethernetMatch));
}",0.998220640569395
132059,"public void setIID(InstanceIdentifier<?> iID){
  this.iID=iID;
}","public void setIID(InstanceIdentifier<?> instanceIdentifier){
  this.instanceIdentifier=instanceIdentifier;
}",0.7052023121387283
132060,"public UpdateOpenFlowTableOffsets(short sfcOffsetTable,short sfcAppEgressTable){
  this.sfcOffsetTable=sfcOffsetTable;
  this.sfcAppEgressTable=sfcAppEgressTable;
}","UpdateOpenFlowTableOffsets(short sfcOffsetTable,short sfcAppEgressTable){
  this.sfcOffsetTable=sfcOffsetTable;
  this.sfcAppEgressTable=sfcAppEgressTable;
}",0.9781931464174456
132061,"/** 
 * Verify that the given tableOffset and optional maxTable is in range
 * @param tableOffset the tableOffset to verify
 * @param maxTable optionally the number of tables beyond tableOffset to be used
 * @return a valid TableId or null if invalid
 */
public TableId verifyMaxTableId(short tableOffset,short maxTable){
  try {
    return new TableId((short)(tableOffset + maxTable));
  }
 catch (  IllegalArgumentException e) {
    LOG.error(""String_Node_Str"",tableOffset,maxTable);
    return null;
  }
}","/** 
 * Verify that the given tableOffset and optional maxTable is in range.
 * @param tableOffset the tableOffset to verify
 * @param maxTable optionally the number of tables beyond tableOffset to be used
 * @return a valid TableId or null if invalid
 */
public TableId verifyMaxTableId(short tableOffset,short maxTable){
  try {
    return new TableId((short)(tableOffset + maxTable));
  }
 catch (  IllegalArgumentException e) {
    LOG.error(""String_Node_Str"",tableOffset,maxTable);
    return null;
  }
}",0.9990167158308751
132062,"/** 
 * Process an OpenFlow Renderer configuration change. Only creates and updates are handled
 * @param config the configuration details
 */
private void processConfig(SfcOfRendererConfig config){
  if (verifyMaxTableId(config.getSfcOfTableOffset(),this.sfcOfFlowProgrammer.getMaxTableOffset()) == null) {
    return;
  }
  if (verifyMaxTableId(config.getSfcOfAppEgressTableOffset(),(short)0) == null) {
    return;
  }
  final int MAGIC_NUMBER_IN_SFCOFLOWPROGRAMMERIMPL=2;
  if (config.getSfcOfTableOffset() < MAGIC_NUMBER_IN_SFCOFLOWPROGRAMMERIMPL) {
    LOG.error(""String_Node_Str"",config.getSfcOfTableOffset());
    return;
  }
  if (config.getSfcOfAppEgressTableOffset() < 0) {
    LOG.error(""String_Node_Str"",config.getSfcOfAppEgressTableOffset());
    return;
  }
  if (config.getSfcOfAppEgressTableOffset() >= config.getSfcOfTableOffset() && config.getSfcOfAppEgressTableOffset() <= config.getSfcOfTableOffset() + this.sfcOfFlowProgrammer.getMaxTableOffset()) {
    LOG.error(""String_Node_Str"",config.getSfcOfAppEgressTableOffset(),config.getSfcOfTableOffset(),config.getSfcOfTableOffset() + this.sfcOfFlowProgrammer.getMaxTableOffset());
    return;
  }
  UpdateOpenFlowTableOffsets updateThread=new UpdateOpenFlowTableOffsets(config.getSfcOfTableOffset(),config.getSfcOfAppEgressTableOffset());
  try {
    threadExecutor.submit(updateThread);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e.toString());
  }
}","/** 
 * Process an OpenFlow Renderer configuration change. Only creates and updates are handled
 * @param config the configuration details
 */
private void processConfig(SfcOfRendererConfig config){
  if (verifyMaxTableId(config.getSfcOfTableOffset(),this.sfcOfFlowProgrammer.getMaxTableOffset()) == null) {
    return;
  }
  if (verifyMaxTableId(config.getSfcOfAppEgressTableOffset(),(short)0) == null) {
    return;
  }
  if (config.getSfcOfTableOffset() < MAGIC_NUMBER_IN_SFCOFLOWPROGRAMMERIMPL) {
    LOG.error(""String_Node_Str"",config.getSfcOfTableOffset());
    return;
  }
  if (config.getSfcOfAppEgressTableOffset() < 0) {
    LOG.error(""String_Node_Str"",config.getSfcOfAppEgressTableOffset());
    return;
  }
  if (config.getSfcOfAppEgressTableOffset() >= config.getSfcOfTableOffset() && config.getSfcOfAppEgressTableOffset() <= config.getSfcOfTableOffset() + this.sfcOfFlowProgrammer.getMaxTableOffset()) {
    LOG.error(""String_Node_Str"",config.getSfcOfAppEgressTableOffset(),config.getSfcOfTableOffset(),config.getSfcOfTableOffset() + this.sfcOfFlowProgrammer.getMaxTableOffset());
    return;
  }
  UpdateOpenFlowTableOffsets updateThread=new UpdateOpenFlowTableOffsets(config.getSfcOfTableOffset(),config.getSfcOfAppEgressTableOffset());
  threadExecutor.submit(updateThread);
}",0.6043956043956044
132063,"@Override public void onDataChanged(AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  for (  Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    if (entry.getValue() instanceof SfcOfRendererConfig) {
      LOG.info(""String_Node_Str"",((SfcOfRendererConfig)entry.getValue()));
      processConfig((SfcOfRendererConfig)entry.getValue());
    }
  }
  for (  Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if (entry.getValue() instanceof SfcOfRendererConfig) {
      LOG.info(""String_Node_Str"",((SfcOfRendererConfig)entry.getValue()));
      processConfig((SfcOfRendererConfig)entry.getValue());
    }
  }
}","@Override public void onDataChanged(AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  for (  Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    if (entry.getValue() instanceof SfcOfRendererConfig) {
      LOG.info(""String_Node_Str"",entry.getValue());
      processConfig((SfcOfRendererConfig)entry.getValue());
    }
  }
  for (  Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if (entry.getValue() instanceof SfcOfRendererConfig) {
      LOG.info(""String_Node_Str"",entry.getValue());
      processConfig((SfcOfRendererConfig)entry.getValue());
    }
  }
}",0.9659763313609468
132064,"@Override public void onDataTreeChanged(@Nonnull Collection<DataTreeModification<RenderedServicePath>> collection){
  for (  DataTreeModification<RenderedServicePath> modification : collection) {
    DataObjectModification<RenderedServicePath> rootNode=modification.getRootNode();
switch (rootNode.getModificationType()) {
case WRITE:
case SUBTREE_MODIFIED:
      if (rootNode.getDataBefore() == null && rootNode.getDataAfter() != null) {
        LOG.info(""String_Node_Str"",rootNode.getDataBefore());
        sfcOfRspProcessor.processRenderedServicePath(rootNode.getDataAfter());
      }
 else       if (rootNode.getDataAfter().equals(rootNode.getDataBefore())) {
        LOG.info(""String_Node_Str"",rootNode.getDataAfter(),rootNode.getDataBefore());
        sfcOfRspProcessor.deleteRenderedServicePath(rootNode.getDataBefore());
        sfcOfRspProcessor.processRenderedServicePath(rootNode.getDataAfter());
      }
    break;
case DELETE:
  if (rootNode.getDataBefore() != null) {
    LOG.info(""String_Node_Str"",rootNode.getDataBefore());
    sfcOfRspProcessor.deleteRenderedServicePath(rootNode.getDataBefore());
  }
break;
}
}
}","@Override public void onDataTreeChanged(@Nonnull Collection<DataTreeModification<RenderedServicePath>> collection){
  for (  DataTreeModification<RenderedServicePath> modification : collection) {
    DataObjectModification<RenderedServicePath> rootNode=modification.getRootNode();
switch (rootNode.getModificationType()) {
case WRITE:
case SUBTREE_MODIFIED:
      if (rootNode.getDataBefore() == null && rootNode.getDataAfter() != null) {
        LOG.info(""String_Node_Str"",rootNode.getDataBefore());
        sfcOfRspProcessor.processRenderedServicePath(rootNode.getDataAfter());
      }
 else       if (rootNode.getDataAfter().equals(rootNode.getDataBefore())) {
        LOG.info(""String_Node_Str"",rootNode.getDataAfter(),rootNode.getDataBefore());
        sfcOfRspProcessor.deleteRenderedServicePath(rootNode.getDataBefore());
        sfcOfRspProcessor.processRenderedServicePath(rootNode.getDataAfter());
      }
    break;
case DELETE:
  if (rootNode.getDataBefore() != null) {
    LOG.info(""String_Node_Str"",rootNode.getDataBefore());
    sfcOfRspProcessor.deleteRenderedServicePath(rootNode.getDataBefore());
  }
break;
default :
break;
}
}
}",0.9925405879771828
132065,"private void buildGroup(ServiceFunctionGroup sfg,boolean isAdd){
  try {
    List<SfcServiceFunction> sfs=sfg.getSfcServiceFunction();
    SfName sfName=new SfName(sfs.get(0).getName());
    ServiceFunction sf=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
    SffName sffName=sf.getSfDataPlaneLocator().get(0).getServiceFunctionForwarder();
    String sffNodeId=null;
    sffNodeId=getSffOpenFlowNodeName(sffName);
    if (sffNodeId == null) {
      LOG.warn(""String_Node_Str"",sffName);
      return;
    }
    ServiceFunctionGroupAlgorithm algorithm=SfcProviderServiceFunctionGroupAlgAPI.readServiceFunctionGroupAlg(sfg.getAlgorithm());
    List<GroupBucketInfo> bucketsInfo=new ArrayList<GroupBucketInfo>();
    ServiceFunctionForwarder sff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(sffName);
    int index=0;
    for (    SfcServiceFunction sfcServiceFunction : sfg.getSfcServiceFunction()) {
      sfName=new SfName(sfcServiceFunction.getName());
      sf=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
      ServiceFunctionDictionary sffSfDict=sfcOfProviderUtils.getSffSfDictionary(sff,sfName);
      String outPort=sfcOfProviderUtils.getDictPortInfoPort(sff,sffSfDict);
      bucketsInfo.add(buildBucket(sf,outPort,index));
      index++;
    }
    this.sfcOfFlowProgrammer.configureGroup(sffName.getValue(),sffNodeId,sfg.getName(),sfg.getGroupId(),algorithm.getAlgorithmType().getIntValue(),bucketsInfo,isAdd);
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"" + sfg,e);
  }
}","private void buildGroup(ServiceFunctionGroup sfg,boolean isAdd){
  List<SfcServiceFunction> sfs=sfg.getSfcServiceFunction();
  SfName sfName=new SfName(sfs.get(0).getName());
  ServiceFunction sf=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
  SffName sffName=sf.getSfDataPlaneLocator().get(0).getServiceFunctionForwarder();
  String sffNodeId=null;
  sffNodeId=getSffOpenFlowNodeName(sffName);
  if (sffNodeId == null) {
    LOG.warn(""String_Node_Str"",sffName);
    return;
  }
  ServiceFunctionGroupAlgorithm algorithm=SfcProviderServiceFunctionGroupAlgAPI.readServiceFunctionGroupAlg(sfg.getAlgorithm());
  List<GroupBucketInfo> bucketsInfo=new ArrayList<>();
  ServiceFunctionForwarder sff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(sffName);
  int index=0;
  for (  SfcServiceFunction sfcServiceFunction : sfg.getSfcServiceFunction()) {
    sfName=new SfName(sfcServiceFunction.getName());
    sf=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
    ServiceFunctionDictionary sffSfDict=sfcOfProviderUtils.getSffSfDictionary(sff,sfName);
    String outPort=sfcOfProviderUtils.getDictPortInfoPort(sff,sffSfDict);
    bucketsInfo.add(buildBucket(sf,outPort,index));
    index++;
  }
  this.sfcOfFlowProgrammer.configureGroup(sffName.getValue(),sffNodeId,sfg.getName(),sfg.getGroupId(),algorithm.getAlgorithmType().getIntValue(),bucketsInfo,isAdd);
}",0.5698667577724633
132066,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataCreatedConfigurationObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionGroup) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)entry.getValue()).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)entry.getValue();
      buildGroup(sfg,true);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunctionGroup && (!(dataCreatedConfigurationObject.containsKey(entry.getKey()))))) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)entry.getValue()).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)entry.getValue();
      buildGroup(sfg,true);
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier<?> instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=change.getOriginalData().get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionGroup) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)dataObject).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)dataObject;
      buildGroup(sfg,false);
    }
  }
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataCreatedConfigurationObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionGroup) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)entry.getValue()).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)entry.getValue();
      buildGroup(sfg,true);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionGroup && !dataCreatedConfigurationObject.containsKey(entry.getKey())) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)entry.getValue()).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)entry.getValue();
      buildGroup(sfg,true);
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier<?> instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=change.getOriginalData().get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionGroup) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)dataObject).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)dataObject;
      buildGroup(sfg,false);
    }
  }
}",0.998085513720485
132067,"protected static ObjectNode getDataPlaneLocatorObjectNode(DataPlaneLocator dataPlaneLocator){
  if (dataPlaneLocator == null) {
    return null;
  }
  ObjectMapper mapper=new ObjectMapper();
  ObjectNode locatorNode=null;
  if (dataPlaneLocator.getLocatorType() != null) {
    locatorNode=mapper.createObjectNode();
    String type=dataPlaneLocator.getLocatorType().getImplementedInterface().getSimpleName().toLowerCase();
switch (type) {
case FUNCTION:
      Function functionLocator=(Function)dataPlaneLocator.getLocatorType();
    locatorNode.put(_FUNCTION_NAME,functionLocator.getFunctionName());
  break;
case IP:
Ip ipLocator=(Ip)dataPlaneLocator.getLocatorType();
if (ipLocator.getIp() != null) {
locatorNode.put(_IP,convertIpAddress(ipLocator.getIp()));
if (ipLocator.getPort() != null) {
  locatorNode.put(_PORT,ipLocator.getPort().getValue());
}
}
break;
case LISP:
Lisp lispLocator=(Lisp)dataPlaneLocator.getLocatorType();
if (lispLocator.getEid() != null) locatorNode.put(_EID,convertIpAddress(lispLocator.getEid()));
break;
case MAC:
Mac macLocator=(Mac)dataPlaneLocator.getLocatorType();
if (macLocator.getMac() != null) locatorNode.put(_MAC,macLocator.getMac().getValue());
locatorNode.put(_VLAN_ID,macLocator.getVlanId());
}
}
if (dataPlaneLocator.getTransport() != null) {
if (locatorNode == null) {
locatorNode=mapper.createObjectNode();
}
locatorNode.put(_TRANSPORT,getDataPlaneLocatorTransport(dataPlaneLocator));
}
return locatorNode;
}","protected static ObjectNode getDataPlaneLocatorObjectNode(DataPlaneLocator dataPlaneLocator){
  if (dataPlaneLocator == null) {
    return null;
  }
  ObjectMapper mapper=new ObjectMapper();
  ObjectNode locatorNode=null;
  if (dataPlaneLocator.getLocatorType() != null) {
    locatorNode=mapper.createObjectNode();
    String type=dataPlaneLocator.getLocatorType().getImplementedInterface().getSimpleName().toLowerCase();
switch (type) {
case FUNCTION:
      Function functionLocator=(Function)dataPlaneLocator.getLocatorType();
    locatorNode.put(FUNCTION_NAME,functionLocator.getFunctionName());
  break;
case IP:
Ip ipLocator=(Ip)dataPlaneLocator.getLocatorType();
if (ipLocator.getIp() != null) {
locatorNode.put(IP,convertIpAddress(ipLocator.getIp()));
if (ipLocator.getPort() != null) {
  locatorNode.put(PORT,ipLocator.getPort().getValue());
}
}
break;
case LISP:
Lisp lispLocator=(Lisp)dataPlaneLocator.getLocatorType();
if (lispLocator.getEid() != null) {
locatorNode.put(EID,convertIpAddress(lispLocator.getEid()));
}
break;
case MAC:
Mac macLocator=(Mac)dataPlaneLocator.getLocatorType();
if (macLocator.getMac() != null) {
locatorNode.put(MAC,macLocator.getMac().getValue());
}
locatorNode.put(VLAN_ID,macLocator.getVlanId());
break;
default :
break;
}
}
if (dataPlaneLocator.getTransport() != null) {
if (locatorNode == null) {
locatorNode=mapper.createObjectNode();
}
locatorNode.put(TRANSPORT,getDataPlaneLocatorTransport(dataPlaneLocator));
}
return locatorNode;
}",0.9540660088465464
132068,"protected static ObjectNode getSffSfDataPlaneLocatorObjectNode(SffSfDataPlaneLocator sffSfDpl){
  if (sffSfDpl == null) {
    return null;
  }
  ObjectMapper mapper=new ObjectMapper();
  ObjectNode sffSfDplNode=mapper.createObjectNode();
  if (sffSfDpl.getSfDplName() != null) {
    sffSfDplNode.put(_SF_DPL_NAME,sffSfDpl.getSfDplName().getValue());
  }
  if (sffSfDpl.getSffDplName() != null) {
    sffSfDplNode.put(_SFF_DPL_NAME,sffSfDpl.getSffDplName().getValue());
  }
  return sffSfDplNode;
}","protected static ObjectNode getSffSfDataPlaneLocatorObjectNode(SffSfDataPlaneLocator sffSfDpl){
  if (sffSfDpl == null) {
    return null;
  }
  ObjectMapper mapper=new ObjectMapper();
  ObjectNode sffSfDplNode=mapper.createObjectNode();
  if (sffSfDpl.getSfDplName() != null) {
    sffSfDplNode.put(SF_DPL_NAME,sffSfDpl.getSfDplName().getValue());
  }
  if (sffSfDpl.getSffDplName() != null) {
    sffSfDplNode.put(SFF_DPL_NAME,sffSfDpl.getSffDplName().getValue());
  }
  return sffSfDplNode;
}",0.997983870967742
132069,"protected static String getDataPlaneLocatorTransport(DataPlaneLocator dataPlaneLocator){
  if (dataPlaneLocator == null || dataPlaneLocator.getTransport() == null) {
    return null;
  }
  String transport=null;
switch (dataPlaneLocator.getTransport().getSimpleName().toLowerCase()) {
case VXLAN_GPE:
    transport=SERVICE_LOCATOR_PREFIX + _VXLAN_GPE;
  break;
case GRE:
transport=SERVICE_LOCATOR_PREFIX + _GRE;
break;
case OTHER:
transport=SERVICE_LOCATOR_PREFIX + _OTHER;
break;
default :
transport=SERVICE_LOCATOR_PREFIX + _OTHER;
}
return transport;
}","protected static String getDataPlaneLocatorTransport(DataPlaneLocator dataPlaneLocator){
  if (dataPlaneLocator == null || dataPlaneLocator.getTransport() == null) {
    return null;
  }
  String transport=null;
switch (dataPlaneLocator.getTransport().getSimpleName().toLowerCase()) {
case VXLAN_GPE:
    transport=SERVICE_LOCATOR_PREFIX + VXLAN_GPE_DPL;
  break;
case GRE:
transport=SERVICE_LOCATOR_PREFIX + GRE;
break;
case OTHER:
transport=SERVICE_LOCATOR_PREFIX + OTHER;
break;
default :
transport=SERVICE_LOCATOR_PREFIX + OTHER;
break;
}
return transport;
}",0.9865711727842436
132070,"public void setDataProvider(DataBroker r){
  dataProvider=r;
}","public void setDataProvider(DataBroker broker){
  dataProvider=broker;
}",0.9253731343283582
132071,"public static void setDataProviderAux(DataBroker r){
  dataProvider=r;
}","public static void setDataProviderAux(DataBroker broker){
  dataProvider=broker;
}",0.935064935064935
132072,"public void setIID(InstanceIdentifier<?> iID){
  this.iID=iID;
}","public void setIID(InstanceIdentifier<?> instanceIdentifier){
  this.instanceIdentifier=instanceIdentifier;
}",0.7052023121387283
132073,"public UpdateOpenFlowTableOffsets(short sfcOffsetTable,short sfcAppEgressTable){
  this.sfcOffsetTable=sfcOffsetTable;
  this.sfcAppEgressTable=sfcAppEgressTable;
}","UpdateOpenFlowTableOffsets(short sfcOffsetTable,short sfcAppEgressTable){
  this.sfcOffsetTable=sfcOffsetTable;
  this.sfcAppEgressTable=sfcAppEgressTable;
}",0.9781931464174456
132074,"/** 
 * Verify that the given tableOffset and optional maxTable is in range
 * @param tableOffset the tableOffset to verify
 * @param maxTable optionally the number of tables beyond tableOffset to be used
 * @return a valid TableId or null if invalid
 */
public TableId verifyMaxTableId(short tableOffset,short maxTable){
  try {
    return new TableId((short)(tableOffset + maxTable));
  }
 catch (  IllegalArgumentException e) {
    LOG.error(""String_Node_Str"",tableOffset,maxTable);
    return null;
  }
}","/** 
 * Verify that the given tableOffset and optional maxTable is in range.
 * @param tableOffset the tableOffset to verify
 * @param maxTable optionally the number of tables beyond tableOffset to be used
 * @return a valid TableId or null if invalid
 */
public TableId verifyMaxTableId(short tableOffset,short maxTable){
  try {
    return new TableId((short)(tableOffset + maxTable));
  }
 catch (  IllegalArgumentException e) {
    LOG.error(""String_Node_Str"",tableOffset,maxTable);
    return null;
  }
}",0.9990167158308751
132075,"/** 
 * Process an OpenFlow Renderer configuration change. Only creates and updates are handled
 * @param config the configuration details
 */
private void processConfig(SfcOfRendererConfig config){
  if (verifyMaxTableId(config.getSfcOfTableOffset(),this.sfcOfFlowProgrammer.getMaxTableOffset()) == null) {
    return;
  }
  if (verifyMaxTableId(config.getSfcOfAppEgressTableOffset(),(short)0) == null) {
    return;
  }
  final int MAGIC_NUMBER_IN_SFCOFLOWPROGRAMMERIMPL=2;
  if (config.getSfcOfTableOffset() < MAGIC_NUMBER_IN_SFCOFLOWPROGRAMMERIMPL) {
    LOG.error(""String_Node_Str"",config.getSfcOfTableOffset());
    return;
  }
  if (config.getSfcOfAppEgressTableOffset() < 0) {
    LOG.error(""String_Node_Str"",config.getSfcOfAppEgressTableOffset());
    return;
  }
  if (config.getSfcOfAppEgressTableOffset() >= config.getSfcOfTableOffset() && config.getSfcOfAppEgressTableOffset() <= config.getSfcOfTableOffset() + this.sfcOfFlowProgrammer.getMaxTableOffset()) {
    LOG.error(""String_Node_Str"",config.getSfcOfAppEgressTableOffset(),config.getSfcOfTableOffset(),config.getSfcOfTableOffset() + this.sfcOfFlowProgrammer.getMaxTableOffset());
    return;
  }
  UpdateOpenFlowTableOffsets updateThread=new UpdateOpenFlowTableOffsets(config.getSfcOfTableOffset(),config.getSfcOfAppEgressTableOffset());
  try {
    threadExecutor.submit(updateThread);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e.toString());
  }
}","/** 
 * Process an OpenFlow Renderer configuration change. Only creates and updates are handled
 * @param config the configuration details
 */
private void processConfig(SfcOfRendererConfig config){
  if (verifyMaxTableId(config.getSfcOfTableOffset(),this.sfcOfFlowProgrammer.getMaxTableOffset()) == null) {
    return;
  }
  if (verifyMaxTableId(config.getSfcOfAppEgressTableOffset(),(short)0) == null) {
    return;
  }
  if (config.getSfcOfTableOffset() < MAGIC_NUMBER_IN_SFCOFLOWPROGRAMMERIMPL) {
    LOG.error(""String_Node_Str"",config.getSfcOfTableOffset());
    return;
  }
  if (config.getSfcOfAppEgressTableOffset() < 0) {
    LOG.error(""String_Node_Str"",config.getSfcOfAppEgressTableOffset());
    return;
  }
  if (config.getSfcOfAppEgressTableOffset() >= config.getSfcOfTableOffset() && config.getSfcOfAppEgressTableOffset() <= config.getSfcOfTableOffset() + this.sfcOfFlowProgrammer.getMaxTableOffset()) {
    LOG.error(""String_Node_Str"",config.getSfcOfAppEgressTableOffset(),config.getSfcOfTableOffset(),config.getSfcOfTableOffset() + this.sfcOfFlowProgrammer.getMaxTableOffset());
    return;
  }
  UpdateOpenFlowTableOffsets updateThread=new UpdateOpenFlowTableOffsets(config.getSfcOfTableOffset(),config.getSfcOfAppEgressTableOffset());
  threadExecutor.submit(updateThread);
}",0.6043956043956044
132076,"@Override public void onDataChanged(AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  for (  Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    if (entry.getValue() instanceof SfcOfRendererConfig) {
      LOG.info(""String_Node_Str"",((SfcOfRendererConfig)entry.getValue()));
      processConfig((SfcOfRendererConfig)entry.getValue());
    }
  }
  for (  Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if (entry.getValue() instanceof SfcOfRendererConfig) {
      LOG.info(""String_Node_Str"",((SfcOfRendererConfig)entry.getValue()));
      processConfig((SfcOfRendererConfig)entry.getValue());
    }
  }
}","@Override public void onDataChanged(AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  for (  Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    if (entry.getValue() instanceof SfcOfRendererConfig) {
      LOG.info(""String_Node_Str"",entry.getValue());
      processConfig((SfcOfRendererConfig)entry.getValue());
    }
  }
  for (  Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if (entry.getValue() instanceof SfcOfRendererConfig) {
      LOG.info(""String_Node_Str"",entry.getValue());
      processConfig((SfcOfRendererConfig)entry.getValue());
    }
  }
}",0.9659763313609468
132077,"@Override public void onDataTreeChanged(@Nonnull Collection<DataTreeModification<RenderedServicePath>> collection){
  for (  DataTreeModification<RenderedServicePath> modification : collection) {
    DataObjectModification<RenderedServicePath> rootNode=modification.getRootNode();
switch (rootNode.getModificationType()) {
case WRITE:
case SUBTREE_MODIFIED:
      if (rootNode.getDataBefore() == null && rootNode.getDataAfter() != null) {
        LOG.info(""String_Node_Str"",rootNode.getDataBefore());
        sfcOfRspProcessor.processRenderedServicePath(rootNode.getDataAfter());
      }
 else       if (rootNode.getDataAfter().equals(rootNode.getDataBefore())) {
        LOG.info(""String_Node_Str"",rootNode.getDataAfter(),rootNode.getDataBefore());
        sfcOfRspProcessor.deleteRenderedServicePath(rootNode.getDataBefore());
        sfcOfRspProcessor.processRenderedServicePath(rootNode.getDataAfter());
      }
    break;
case DELETE:
  if (rootNode.getDataBefore() != null) {
    LOG.info(""String_Node_Str"",rootNode.getDataBefore());
    sfcOfRspProcessor.deleteRenderedServicePath(rootNode.getDataBefore());
  }
break;
}
}
}","@Override public void onDataTreeChanged(@Nonnull Collection<DataTreeModification<RenderedServicePath>> collection){
  for (  DataTreeModification<RenderedServicePath> modification : collection) {
    DataObjectModification<RenderedServicePath> rootNode=modification.getRootNode();
switch (rootNode.getModificationType()) {
case WRITE:
case SUBTREE_MODIFIED:
      if (rootNode.getDataBefore() == null && rootNode.getDataAfter() != null) {
        LOG.info(""String_Node_Str"",rootNode.getDataBefore());
        sfcOfRspProcessor.processRenderedServicePath(rootNode.getDataAfter());
      }
 else       if (rootNode.getDataAfter().equals(rootNode.getDataBefore())) {
        LOG.info(""String_Node_Str"",rootNode.getDataAfter(),rootNode.getDataBefore());
        sfcOfRspProcessor.deleteRenderedServicePath(rootNode.getDataBefore());
        sfcOfRspProcessor.processRenderedServicePath(rootNode.getDataAfter());
      }
    break;
case DELETE:
  if (rootNode.getDataBefore() != null) {
    LOG.info(""String_Node_Str"",rootNode.getDataBefore());
    sfcOfRspProcessor.deleteRenderedServicePath(rootNode.getDataBefore());
  }
break;
default :
break;
}
}
}",0.9925405879771828
132078,"private void buildGroup(ServiceFunctionGroup sfg,boolean isAdd){
  try {
    List<SfcServiceFunction> sfs=sfg.getSfcServiceFunction();
    SfName sfName=new SfName(sfs.get(0).getName());
    ServiceFunction sf=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
    SffName sffName=sf.getSfDataPlaneLocator().get(0).getServiceFunctionForwarder();
    String sffNodeId=null;
    sffNodeId=getSffOpenFlowNodeName(sffName);
    if (sffNodeId == null) {
      LOG.warn(""String_Node_Str"",sffName);
      return;
    }
    ServiceFunctionGroupAlgorithm algorithm=SfcProviderServiceFunctionGroupAlgAPI.readServiceFunctionGroupAlg(sfg.getAlgorithm());
    List<GroupBucketInfo> bucketsInfo=new ArrayList<GroupBucketInfo>();
    ServiceFunctionForwarder sff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(sffName);
    int index=0;
    for (    SfcServiceFunction sfcServiceFunction : sfg.getSfcServiceFunction()) {
      sfName=new SfName(sfcServiceFunction.getName());
      sf=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
      ServiceFunctionDictionary sffSfDict=sfcOfProviderUtils.getSffSfDictionary(sff,sfName);
      String outPort=sfcOfProviderUtils.getDictPortInfoPort(sff,sffSfDict);
      bucketsInfo.add(buildBucket(sf,outPort,index));
      index++;
    }
    this.sfcOfFlowProgrammer.configureGroup(sffName.getValue(),sffNodeId,sfg.getName(),sfg.getGroupId(),algorithm.getAlgorithmType().getIntValue(),bucketsInfo,isAdd);
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"" + sfg,e);
  }
}","private void buildGroup(ServiceFunctionGroup sfg,boolean isAdd){
  List<SfcServiceFunction> sfs=sfg.getSfcServiceFunction();
  SfName sfName=new SfName(sfs.get(0).getName());
  ServiceFunction sf=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
  SffName sffName=sf.getSfDataPlaneLocator().get(0).getServiceFunctionForwarder();
  String sffNodeId=null;
  sffNodeId=getSffOpenFlowNodeName(sffName);
  if (sffNodeId == null) {
    LOG.warn(""String_Node_Str"",sffName);
    return;
  }
  ServiceFunctionGroupAlgorithm algorithm=SfcProviderServiceFunctionGroupAlgAPI.readServiceFunctionGroupAlg(sfg.getAlgorithm());
  List<GroupBucketInfo> bucketsInfo=new ArrayList<>();
  ServiceFunctionForwarder sff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(sffName);
  int index=0;
  for (  SfcServiceFunction sfcServiceFunction : sfg.getSfcServiceFunction()) {
    sfName=new SfName(sfcServiceFunction.getName());
    sf=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
    ServiceFunctionDictionary sffSfDict=sfcOfProviderUtils.getSffSfDictionary(sff,sfName);
    String outPort=sfcOfProviderUtils.getDictPortInfoPort(sff,sffSfDict);
    bucketsInfo.add(buildBucket(sf,outPort,index));
    index++;
  }
  this.sfcOfFlowProgrammer.configureGroup(sffName.getValue(),sffNodeId,sfg.getName(),sfg.getGroupId(),algorithm.getAlgorithmType().getIntValue(),bucketsInfo,isAdd);
}",0.5698667577724633
132079,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataCreatedConfigurationObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionGroup) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)entry.getValue()).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)entry.getValue();
      buildGroup(sfg,true);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunctionGroup && (!(dataCreatedConfigurationObject.containsKey(entry.getKey()))))) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)entry.getValue()).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)entry.getValue();
      buildGroup(sfg,true);
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier<?> instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=change.getOriginalData().get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionGroup) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)dataObject).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)dataObject;
      buildGroup(sfg,false);
    }
  }
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataCreatedConfigurationObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionGroup) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)entry.getValue()).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)entry.getValue();
      buildGroup(sfg,true);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionGroup && !dataCreatedConfigurationObject.containsKey(entry.getKey())) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)entry.getValue()).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)entry.getValue();
      buildGroup(sfg,true);
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier<?> instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=change.getOriginalData().get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionGroup) {
      LOG.info(""String_Node_Str"",((ServiceFunctionGroup)dataObject).getName());
      ServiceFunctionGroup sfg=(ServiceFunctionGroup)dataObject;
      buildGroup(sfg,false);
    }
  }
}",0.998085513720485
132080,"private boolean removeVxlanGpeClassifier(ServiceFunctionClassifier scf){
  Optional<Acl> theAcl=extractAcl(scf);
  if (!theAcl.isPresent() || !validateInputs(theAcl.get())) {
    LOG.error(""String_Node_Str"",scf);
    return false;
  }
  Map<RspName,List<Pair<HexString>>> rspPairList=new HashMap<RspName,List<Pair<HexString>>>();
  List<Ace> aceList=theAcl.get().getAccessListEntries().getAce();
  for (  Ace ace : aceList) {
    Optional<RspName> rspName=Optional.ofNullable(ace.getActions()).map(theActions -> theActions.getAugmentation(Actions1.class)).map(actions1 -> (AclRenderedServicePath)actions1.getSfcAction()).map(aclRsp -> new RspName(aclRsp.getRenderedServicePath()));
    if (!rspName.isPresent()) {
      LOG.error(""String_Node_Str"",scf);
    }
    List<Pair<HexString>> pairList=rspPairList.get(rspName.get());
    if (pairList == null) {
      pairList=new ArrayList<>();
      rspPairList.put(rspName.get(),pairList);
    }
    pairList.add(getMaskAndMatch(ace.getMatches()));
  }
  List<SclServiceFunctionForwarder> sfflist=scf.getSclServiceFunctionForwarder();
  if (sfflist == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  for (  SclServiceFunctionForwarder sclSff : sfflist) {
    SffName sffName=new SffName(sclSff.getName());
    Optional<String> itfName=getInterfaceNameFromClassifier(sclSff);
    if (!itfName.isPresent()) {
      LOG.error(""String_Node_Str"");
    }
    IpAddress sffIp=SfcVppUtils.getSffFirstDplIp(sffName);
    DataBroker mountPoint=SfcVppUtils.getSffMountpoint(this.nodeManager.getMountPointService(),sffName);
    for (    RspName rsp : rspPairList.keySet()) {
      SffInfo sffInfo=getFirstSffInfoInRsp(rsp);
      RspName reverseRspName=getReverseRspName(rsp);
      RenderedServicePath reverseRenderedServicePath=getRenderedServicePath(reverseRspName);
      if (reverseRenderedServicePath == null) {
        LOG.error(""String_Node_Str"");
        return false;
      }
      Long reversePathId=reverseRenderedServicePath.getPathId();
      List<RenderedServicePathHop> hopList=reverseRenderedServicePath.getRenderedServicePathHop();
      if (hopList == null || hopList.isEmpty()) {
        LOG.error(""String_Node_Str"",reverseRenderedServicePath.getName().getValue());
        return false;
      }
      RenderedServicePathHop lastRspHop=Iterables.getLast(hopList);
      if (lastRspHop == null) {
        LOG.error(""String_Node_Str"");
        return false;
      }
      Short reverseServiceIndex=(short)(lastRspHop.getServiceIndex() - 1);
      List<Pair<HexString>> pairList=rspPairList.get(rsp);
      int length=pairList.size();
      int index=0;
      List<String> tableKeyList=new ArrayList<>();
      List<HexString> matchList=new ArrayList<>();
      for (      Pair<HexString> maskMatch : pairList) {
        String classifyTableKey=SfcVppUtils.getSavedClassifyTableKey(sffName.getValue(),rsp.getValue(),index);
        tableKeyList.add(classifyTableKey);
        matchList.add(maskMatch.getMatch());
        index++;
      }
      SfcVppUtils.disableIngressAcl(mountPoint,itfName.get(),SfcVppUtils.buildClassifyTableKey(0),sffName.getValue());
      SfcVppUtils.removeVppClassifier(mountPoint,sffName,tableKeyList,matchList);
      SfcVppUtils.removeClassifierVxlanGpeNsh(mountPoint,sffName,SFC_BD_NAME,sffIp,sffInfo.ip,sffInfo.pathId,sffInfo.serviceIndex);
      SfcVppUtils.removeNshEntry(mountPoint,reversePathId,reverseServiceIndex,sffName.getValue());
      SfcVppUtils.removeNshMap(mountPoint,reversePathId,reverseServiceIndex,reversePathId,reverseServiceIndex,sffName.getValue());
      SfcVppUtils.removeVxlanGpeNsh(sffInfo.mountPoint,sffInfo.sffName,sffInfo.ip,sffIp,reversePathId,reverseServiceIndex);
    }
  }
  return true;
}","private boolean removeVxlanGpeClassifier(ServiceFunctionClassifier scf){
  Optional<Acl> theAcl=extractAcl(scf);
  if (!theAcl.isPresent() || !validateInputs(theAcl.get())) {
    LOG.error(""String_Node_Str"",scf);
    return false;
  }
  Map<RspName,List<Pair<HexString>>> rspPairList=new HashMap<RspName,List<Pair<HexString>>>();
  List<Ace> aceList=theAcl.get().getAccessListEntries().getAce();
  for (  Ace ace : aceList) {
    Optional<RspName> rspName=Optional.ofNullable(ace.getActions()).map(theActions -> theActions.getAugmentation(Actions1.class)).map(actions1 -> (AclRenderedServicePath)actions1.getSfcAction()).map(aclRsp -> new RspName(aclRsp.getRenderedServicePath()));
    if (!rspName.isPresent()) {
      LOG.error(""String_Node_Str"",scf);
    }
    List<Pair<HexString>> pairList=rspPairList.get(rspName.get());
    if (pairList == null) {
      pairList=new ArrayList<>();
      rspPairList.put(rspName.get(),pairList);
    }
    pairList.add(getMaskAndMatch(ace.getMatches()));
  }
  List<SclServiceFunctionForwarder> sfflist=scf.getSclServiceFunctionForwarder();
  if (sfflist == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  for (  SclServiceFunctionForwarder sclSff : sfflist) {
    SffName sffName=new SffName(sclSff.getName());
    Optional<String> itfName=getInterfaceNameFromClassifier(sclSff);
    if (!itfName.isPresent()) {
      LOG.error(""String_Node_Str"");
    }
    IpAddress sffIp=SfcVppUtils.getSffFirstDplIp(sffName);
    DataBroker mountPoint=SfcVppUtils.getSffMountpoint(this.nodeManager.getMountPointService(),sffName);
    for (    RspName rsp : rspPairList.keySet()) {
      SffInfo sffInfo=getFirstSffInfoInRsp(rsp);
      RspName reverseRspName=getReverseRspName(rsp);
      RenderedServicePath reverseRenderedServicePath=getRenderedServicePath(reverseRspName);
      if (reverseRenderedServicePath == null) {
        LOG.error(""String_Node_Str"");
        return false;
      }
      Long reversePathId=reverseRenderedServicePath.getPathId();
      List<RenderedServicePathHop> hopList=reverseRenderedServicePath.getRenderedServicePathHop();
      if (hopList == null || hopList.isEmpty()) {
        LOG.error(""String_Node_Str"",reverseRenderedServicePath.getName().getValue());
        return false;
      }
      RenderedServicePathHop lastRspHop=Iterables.getLast(hopList);
      if (lastRspHop == null) {
        LOG.error(""String_Node_Str"");
        return false;
      }
      Short reverseServiceIndex=(short)(lastRspHop.getServiceIndex() - 1);
      List<Pair<HexString>> pairList=rspPairList.get(rsp);
      int length=pairList.size();
      int index=0;
      List<String> tableKeyList=new ArrayList<>();
      List<HexString> matchList=new ArrayList<>();
      for (      Pair<HexString> maskMatch : pairList) {
        String classifyTableKey=SfcVppUtils.getSavedClassifyTableKey(sffName.getValue(),rsp.getValue(),index);
        tableKeyList.add(classifyTableKey);
        matchList.add(maskMatch.getMatch());
        index++;
      }
      SfcVppUtils.disableIngressAcl(mountPoint,itfName.get(),SfcVppUtils.buildClassifyTableKey(0),sffName.getValue());
      SfcVppUtils.removeVppClassifier(mountPoint,sffName,tableKeyList,matchList);
      SfcVppUtils.removeNshMap(mountPoint,reversePathId,reverseServiceIndex,reversePathId,reverseServiceIndex,sffName.getValue());
      SfcVppUtils.removeNshEntry(mountPoint,reversePathId,reverseServiceIndex,sffName.getValue());
      SfcVppUtils.removeVxlanGpeNsh(sffInfo.mountPoint,sffInfo.sffName,sffInfo.ip,sffIp,reversePathId,reverseServiceIndex);
      SfcVppUtils.removeClassifierVxlanGpeNsh(mountPoint,sffName,SFC_BD_NAME,sffIp,sffInfo.ip,sffInfo.pathId,sffInfo.serviceIndex);
    }
  }
  return true;
}",0.9000268744961032
132081,"@Override protected void add(RenderedServicePaths newDataObject){
  LOG.info(""String_Node_Str"");
  newDataObject.getRenderedServicePath().forEach(rspProcessor::updateRsp);
}","@Override protected void add(RenderedServicePath newDataObject){
  LOG.info(""String_Node_Str"");
  this.rspProcessor.updateRsp(newDataObject);
}",0.7278481012658228
132082,"@Override protected void update(RenderedServicePaths originalDataObject,RenderedServicePaths updatedDataObject){
  LOG.info(""String_Node_Str"");
  updatedDataObject.getRenderedServicePath().forEach(rspProcessor::updateRsp);
}","@Override protected void update(RenderedServicePath originalDataObject,RenderedServicePath updatedDataObject){
}",0.6666666666666666
132083,"public RenderedPathListener(DataBroker dataBroker,VppRspProcessor rspProcessor){
  this.rspProcessor=rspProcessor;
  final DataTreeIdentifier<RenderedServicePaths> treeId=new DataTreeIdentifier<>(LogicalDatastoreType.OPERATIONAL,InstanceIdentifier.create(RenderedServicePaths.class));
  vppRspListenerRegistration=dataBroker.registerDataTreeChangeListener(treeId,this);
}","public RenderedPathListener(DataBroker dataBroker,VppRspProcessor rspProcessor){
  this.rspProcessor=rspProcessor;
  final DataTreeIdentifier<RenderedServicePath> treeId=new DataTreeIdentifier<>(LogicalDatastoreType.OPERATIONAL,InstanceIdentifier.create(RenderedServicePaths.class).child(RenderedServicePath.class));
  vppRspListenerRegistration=dataBroker.registerDataTreeChangeListener(treeId,this);
}",0.9560723514211886
132084,"@Override protected void remove(RenderedServicePaths removedDataObject){
  LOG.info(""String_Node_Str"");
  removedDataObject.getRenderedServicePath().forEach(rspProcessor::deleteRsp);
}","@Override protected void remove(RenderedServicePath removedDataObject){
  LOG.info(""String_Node_Str"");
  this.rspProcessor.deleteRsp(removedDataObject);
}",0.7455621301775148
132085,"public void deleteRsp(RenderedServicePath renderedServicePath){
  boolean ret=false;
  Preconditions.checkNotNull(renderedServicePath);
  Long pathId=renderedServicePath.getPathId();
  Short serviceIndex=renderedServicePath.getStartingIndex();
  DataBroker previousMountPoint=null;
  DataBroker currentMountpoint=null;
  SffName previousSffName=null;
  SffName currentSffName=null;
  SfName sfName;
  List<IpAddress> ipList=null;
  IpAddress localIp=null;
  IpAddress remoteIp;
  IpAddress preLocalIp=null;
  if (renderedServicePath.getRenderedServicePathHop() == null || renderedServicePath.getRenderedServicePathHop().isEmpty()) {
    LOG.warn(""String_Node_Str"",renderedServicePath.getName().getValue());
    return;
  }
  Iterator<RenderedServicePathHop> rspHopIterator=renderedServicePath.getRenderedServicePathHop().iterator();
  while (rspHopIterator.hasNext()) {
    RenderedServicePathHop hop=rspHopIterator.next();
    previousSffName=currentSffName;
    previousMountPoint=currentMountpoint;
    preLocalIp=localIp;
    currentSffName=hop.getServiceFunctionForwarder();
    currentMountpoint=SfcVppUtils.getSffMountpoint(this.nodeManager.getMountPointService(),currentSffName);
    if (currentMountpoint == null) {
      LOG.error(""String_Node_Str"",renderedServicePath.getName().getValue(),currentSffName.getValue());
      return;
    }
    sfName=hop.getServiceFunctionName();
    serviceIndex=hop.getServiceIndex();
    ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
    if (serviceFunction == null) {
      LOG.error(""String_Node_Str"",sfName.getValue());
      return;
    }
    ipList=SfcVppUtils.getSffSfIps(currentSffName,sfName);
    if (ipList == null || ipList.isEmpty()) {
      LOG.error(""String_Node_Str"",currentSffName.getValue(),renderedServicePath.getName().getValue());
      return;
    }
    localIp=ipList.get(0);
    remoteIp=ipList.get(1);
    SfLocatorProxyAugmentation sfDplProxyAug=SfcVppUtils.getSfDplProxyAugmentation(serviceFunction,currentSffName);
    if (sfDplProxyAug == null) {
      ret=SfcVppUtils.removeVxlanGpeNsh(currentMountpoint,currentSffName,localIp,remoteIp,pathId,serviceIndex);
      if (!ret) {
        LOG.error(""String_Node_Str"",renderedServicePath.getName().getValue(),currentSffName.getValue());
        return;
      }
    }
 else {
      remoteIp=SfcVppUtils.getSfProxyDplIp(sfDplProxyAug);
      if (remoteIp == null) {
        LOG.error(""String_Node_Str"",currentSffName.getValue(),renderedServicePath.getName().getValue());
        return;
      }
      ret=SfcVppUtils.removeVxlanNsh(currentMountpoint,currentSffName,localIp,remoteIp,pathId,serviceIndex);
      if (!ret) {
        LOG.error(""String_Node_Str"",renderedServicePath.getName().getValue(),currentSffName.getValue());
        return;
      }
    }
    if (previousSffName != null && !previousSffName.equals(currentSffName)) {
      SfcVppUtils.removeVxlanGpePort(previousMountPoint,preLocalIp,localIp,0L,previousSffName.getValue());
      SfcVppUtils.removeVxlanGpePort(currentMountpoint,localIp,preLocalIp,0L,currentSffName.getValue());
    }
  }
}","public void deleteRsp(RenderedServicePath renderedServicePath){
  boolean ret=false;
  Preconditions.checkNotNull(renderedServicePath);
  Long pathId=renderedServicePath.getPathId();
  Short serviceIndex=renderedServicePath.getStartingIndex();
  DataBroker previousMountPoint=null;
  DataBroker currentMountpoint=null;
  SffName previousSffName=null;
  SffName currentSffName=null;
  SfName sfName;
  List<IpAddress> ipList=null;
  IpAddress localIp=null;
  IpAddress remoteIp;
  IpAddress preLocalIp=null;
  if (renderedServicePath.getRenderedServicePathHop() == null || renderedServicePath.getRenderedServicePathHop().isEmpty()) {
    LOG.warn(""String_Node_Str"",renderedServicePath.getName().getValue());
    return;
  }
  Iterator<RenderedServicePathHop> rspHopIterator=renderedServicePath.getRenderedServicePathHop().iterator();
  while (rspHopIterator.hasNext()) {
    RenderedServicePathHop hop=rspHopIterator.next();
    previousSffName=currentSffName;
    previousMountPoint=currentMountpoint;
    preLocalIp=localIp;
    currentSffName=hop.getServiceFunctionForwarder();
    currentMountpoint=SfcVppUtils.getSffMountpoint(this.nodeManager.getMountPointService(),currentSffName);
    if (currentMountpoint == null) {
      LOG.error(""String_Node_Str"",renderedServicePath.getName().getValue(),currentSffName.getValue());
      return;
    }
    sfName=hop.getServiceFunctionName();
    serviceIndex=hop.getServiceIndex();
    ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunction(sfName);
    if (serviceFunction == null) {
      LOG.error(""String_Node_Str"",sfName.getValue());
      return;
    }
    ipList=SfcVppUtils.getSffSfIps(currentSffName,sfName);
    if (ipList == null || ipList.isEmpty()) {
      LOG.error(""String_Node_Str"",currentSffName.getValue(),renderedServicePath.getName().getValue());
      return;
    }
    localIp=ipList.get(0);
    remoteIp=ipList.get(1);
    SfLocatorProxyAugmentation sfDplProxyAug=SfcVppUtils.getSfDplProxyAugmentation(serviceFunction,currentSffName);
    if (sfDplProxyAug == null) {
      ret=SfcVppUtils.removeVxlanGpeNsh(currentMountpoint,currentSffName,localIp,remoteIp,pathId,serviceIndex);
      if (!ret) {
        LOG.error(""String_Node_Str"",renderedServicePath.getName().getValue(),currentSffName.getValue());
        return;
      }
    }
 else {
      remoteIp=SfcVppUtils.getSfProxyDplIp(sfDplProxyAug);
      if (remoteIp == null) {
        LOG.error(""String_Node_Str"",currentSffName.getValue(),renderedServicePath.getName().getValue());
        return;
      }
      ret=SfcVppUtils.removeVxlanNsh(currentMountpoint,currentSffName,localIp,remoteIp,pathId,serviceIndex);
      if (!ret) {
        LOG.error(""String_Node_Str"",renderedServicePath.getName().getValue(),currentSffName.getValue());
        return;
      }
    }
    if (previousSffName != null && !previousSffName.equals(currentSffName)) {
      ret=SfcVppUtils.removeVxlanGpeNsh(previousMountPoint,previousSffName,preLocalIp,localIp,pathId,serviceIndex);
      if (!ret) {
        LOG.error(""String_Node_Str"",renderedServicePath.getName().getValue(),previousSffName.getValue());
        return;
      }
    }
  }
}",0.9513513513513514
132086,"public static void removeNshEntry(final DataBroker dataBroker,final Long nsp,final Short nsi,String vppNode){
  final DataBroker vppDataBroker=dataBroker;
  final WriteTransaction wTx=vppDataBroker.newWriteOnlyTransaction();
  final InstanceIdentifier<NshEntry> nshEntryIid=InstanceIdentifier.create(NshEntries.class).child(NshEntry.class,new NshEntryKey(buildNshEntryKey(nsp,nsi)));
  wTx.delete(LogicalDatastoreType.CONFIGURATION,nshEntryIid);
  addFuturesCallback(wTx);
}","public static void removeNshEntry(final DataBroker dataBroker,final Long nsp,final Short nsi,String vppNode){
  final DataBroker vppDataBroker=dataBroker;
  final WriteTransaction wTx=vppDataBroker.newWriteOnlyTransaction();
  String nshEntryKey=buildNshEntryKey(nsp,nsi);
  final InstanceIdentifier<NshEntry> nshEntryIid=InstanceIdentifier.create(VppNsh.class).child(NshEntries.class).child(NshEntry.class,new NshEntryKey(nshEntryKey));
  LOG.info(""String_Node_Str"",nshEntryKey,vppNode);
  wTx.delete(LogicalDatastoreType.CONFIGURATION,nshEntryIid);
  addFuturesCallback(wTx);
}",0.6571699905033238
132087,"public static void removeVxlanGpePort(final DataBroker dataBroker,final IpAddress local,final IpAddress remote,Long vni,String vppNode){
  final DataBroker vppDataBroker=dataBroker;
  final WriteTransaction wTx=vppDataBroker.newWriteOnlyTransaction();
  final KeyedInstanceIdentifier<Interface,InterfaceKey> interfaceIid=InstanceIdentifier.create(Interfaces.class).child(Interface.class,new InterfaceKey(buildVxlanGpePortKey(remote)));
  wTx.delete(LogicalDatastoreType.CONFIGURATION,interfaceIid);
  addFuturesCallback(wTx);
}","public static void removeVxlanGpePort(final DataBroker dataBroker,final IpAddress local,final IpAddress remote,Long vni,String vppNode){
  String interfaceKey=buildVxlanGpePortKey(remote);
  LOG.info(""String_Node_Str"",interfaceKey,vppNode);
  if (decrementVxlanGpeRefCnt(interfaceKey,vppNode) > 0) {
    return;
  }
  final DataBroker vppDataBroker=dataBroker;
  final WriteTransaction wTx=vppDataBroker.newWriteOnlyTransaction();
  final KeyedInstanceIdentifier<Interface,InterfaceKey> interfaceIid=InstanceIdentifier.create(Interfaces.class).child(Interface.class,new InterfaceKey(interfaceKey));
  LOG.info(""String_Node_Str"",interfaceKey,vppNode);
  wTx.delete(LogicalDatastoreType.CONFIGURATION,interfaceIid);
  addFuturesCallback(wTx);
}",0.7864460204885737
132088,"public static boolean removeClassifierVxlanGpeNsh(final DataBroker dataBroker,final SffName sffName,String bridgeDomainName,final IpAddress localIp,final IpAddress remoteIp,final Long nsp,final Short nsi){
  Long vni=0L;
  Short nextNsi=nsi;
  nextNsi--;
  removeVxlanGpePort(dataBroker,localIp,remoteIp,vni,sffName.getValue());
  removeNshEntry(dataBroker,nsp,nsi,sffName.getValue());
  removeNshMap(dataBroker,nsp,nsi,nsp,nextNsi,sffName.getValue());
  return true;
}","public static boolean removeClassifierVxlanGpeNsh(final DataBroker dataBroker,final SffName sffName,String bridgeDomainName,final IpAddress localIp,final IpAddress remoteIp,final Long nsp,final Short nsi){
  Long vni=0L;
  removeNshMap(dataBroker,nsp,nsi,nsp,nsi,sffName.getValue());
  removeNshEntry(dataBroker,nsp,nsi,sffName.getValue());
  removeVxlanGpePort(dataBroker,localIp,remoteIp,vni,sffName.getValue());
  return true;
}",0.6911111111111111
132089,"public static void removeNshMap(final DataBroker dataBroker,final Long nsp,final Short nsi,final Long mappedNsp,final Short mappedNsi,String vppNode){
  final DataBroker vppDataBroker=dataBroker;
  final WriteTransaction wTx=vppDataBroker.newWriteOnlyTransaction();
  final InstanceIdentifier<NshMap> nshMapIid=InstanceIdentifier.create(NshMaps.class).child(NshMap.class,new NshMapKey(buildNshMapKey(nsp,nsi,mappedNsp,mappedNsi)));
  wTx.delete(LogicalDatastoreType.CONFIGURATION,nshMapIid);
  addFuturesCallback(wTx);
}","public static void removeNshMap(final DataBroker dataBroker,final Long nsp,final Short nsi,final Long mappedNsp,final Short mappedNsi,String vppNode){
  final DataBroker vppDataBroker=dataBroker;
  final WriteTransaction wTx=vppDataBroker.newWriteOnlyTransaction();
  String nshMapKey=buildNshMapKey(nsp,nsi,mappedNsp,mappedNsi);
  final InstanceIdentifier<NshMap> nshMapIid=InstanceIdentifier.create(VppNsh.class).child(NshMaps.class).child(NshMap.class,new NshMapKey(nshMapKey));
  LOG.info(""String_Node_Str"",nshMapKey,vppNode);
  wTx.delete(LogicalDatastoreType.CONFIGURATION,nshMapIid);
  addFuturesCallback(wTx);
}",0.707638279192274
132090,"private static void addVxlanGpePort(final DataBroker dataBroker,final IpAddress local,final IpAddress remote,Long vni,String vppNode,String bridgeDomainName){
  final VxlanGpeBuilder vxlanGpeBuilder=new VxlanGpeBuilder();
  vxlanGpeBuilder.setLocal(local);
  vxlanGpeBuilder.setRemote(remote);
  vxlanGpeBuilder.setVni(new VxlanGpeVni(vni));
  vxlanGpeBuilder.setNextProtocol(VxlanGpeNextProtocol.Nsh);
  vxlanGpeBuilder.setEncapVrfId(0L);
  vxlanGpeBuilder.setDecapVrfId(0L);
  final RoutingBuilder routingBuilder=new RoutingBuilder();
  routingBuilder.setIpv4VrfId(0L);
  final InterfaceBuilder interfaceBuilder=new InterfaceBuilder();
  interfaceBuilder.setName(buildVxlanGpePortKey(remote));
  interfaceBuilder.setType(VxlanGpeTunnel.class);
  VppInterfaceAugmentationBuilder vppInterfaceAugmentationBuilder=new VppInterfaceAugmentationBuilder();
  vppInterfaceAugmentationBuilder.setVxlanGpe(vxlanGpeBuilder.build());
  vppInterfaceAugmentationBuilder.setRouting(routingBuilder.build());
  final L2Builder l2Builder=new L2Builder();
  final BridgeBasedBuilder bridgeBasedBuilder=new BridgeBasedBuilder();
  bridgeBasedBuilder.setBridgedVirtualInterface(false);
  bridgeBasedBuilder.setSplitHorizonGroup(Short.valueOf(""String_Node_Str""));
  bridgeBasedBuilder.setBridgeDomain(bridgeDomainName);
  l2Builder.setInterconnection(bridgeBasedBuilder.build());
  vppInterfaceAugmentationBuilder.setL2(l2Builder.build());
  interfaceBuilder.addAugmentation(VppInterfaceAugmentation.class,vppInterfaceAugmentationBuilder.build());
  interfaceBuilder.setEnabled(true);
  interfaceBuilder.setLinkUpDownTrapEnable(Interface.LinkUpDownTrapEnable.Enabled);
  final DataBroker vppDataBroker=dataBroker;
  final WriteTransaction wTx=vppDataBroker.newWriteOnlyTransaction();
  final KeyedInstanceIdentifier<Interface,InterfaceKey> interfaceIid=InstanceIdentifier.create(Interfaces.class).child(Interface.class,new InterfaceKey(interfaceBuilder.getName()));
  wTx.put(LogicalDatastoreType.CONFIGURATION,interfaceIid,interfaceBuilder.build());
  addFuturesCallback(wTx);
}","private static void addVxlanGpePort(final DataBroker dataBroker,final IpAddress local,final IpAddress remote,Long vni,String vppNode,String bridgeDomainName){
  String vxlanGpePortKey=buildVxlanGpePortKey(remote);
  LOG.info(""String_Node_Str"",vxlanGpePortKey,vppNode);
  if (incrementVxlanGpeRefCnt(vxlanGpePortKey,vppNode) > 1) {
    return;
  }
  final VxlanGpeBuilder vxlanGpeBuilder=new VxlanGpeBuilder();
  vxlanGpeBuilder.setLocal(local);
  vxlanGpeBuilder.setRemote(remote);
  vxlanGpeBuilder.setVni(new VxlanGpeVni(vni));
  vxlanGpeBuilder.setNextProtocol(VxlanGpeNextProtocol.Nsh);
  vxlanGpeBuilder.setEncapVrfId(0L);
  vxlanGpeBuilder.setDecapVrfId(0L);
  final RoutingBuilder routingBuilder=new RoutingBuilder();
  routingBuilder.setIpv4VrfId(0L);
  final InterfaceBuilder interfaceBuilder=new InterfaceBuilder();
  interfaceBuilder.setName(vxlanGpePortKey);
  interfaceBuilder.setType(VxlanGpeTunnel.class);
  VppInterfaceAugmentationBuilder vppInterfaceAugmentationBuilder=new VppInterfaceAugmentationBuilder();
  vppInterfaceAugmentationBuilder.setVxlanGpe(vxlanGpeBuilder.build());
  vppInterfaceAugmentationBuilder.setRouting(routingBuilder.build());
  final L2Builder l2Builder=new L2Builder();
  final BridgeBasedBuilder bridgeBasedBuilder=new BridgeBasedBuilder();
  bridgeBasedBuilder.setBridgedVirtualInterface(false);
  bridgeBasedBuilder.setSplitHorizonGroup(Short.valueOf(""String_Node_Str""));
  bridgeBasedBuilder.setBridgeDomain(bridgeDomainName);
  l2Builder.setInterconnection(bridgeBasedBuilder.build());
  vppInterfaceAugmentationBuilder.setL2(l2Builder.build());
  interfaceBuilder.addAugmentation(VppInterfaceAugmentation.class,vppInterfaceAugmentationBuilder.build());
  interfaceBuilder.setEnabled(true);
  interfaceBuilder.setLinkUpDownTrapEnable(Interface.LinkUpDownTrapEnable.Enabled);
  final DataBroker vppDataBroker=dataBroker;
  final WriteTransaction wTx=vppDataBroker.newWriteOnlyTransaction();
  final KeyedInstanceIdentifier<Interface,InterfaceKey> interfaceIid=InstanceIdentifier.create(Interfaces.class).child(Interface.class,new InterfaceKey(interfaceBuilder.getName()));
  wTx.put(LogicalDatastoreType.CONFIGURATION,interfaceIid,interfaceBuilder.build());
  addFuturesCallback(wTx);
}",0.9526916802610114
132091,"public static boolean removeVxlanGpeNsh(final DataBroker dataBroker,final SffName sffName,final IpAddress localIp,final IpAddress remoteIp,final Long nsp,final Short nsi){
  Short nextNsi=nsi;
  nextNsi--;
  Long vni=0L;
  removeVxlanGpePort(dataBroker,localIp,remoteIp,vni,sffName.getValue());
  removeNshEntry(dataBroker,nsp,nsi,sffName.getValue());
  removeNshEntry(dataBroker,nsp,nextNsi,sffName.getValue());
  removeNshMap(dataBroker,nsp,nsi,nsp,nextNsi,sffName.getValue());
  return true;
}","public static boolean removeVxlanGpeNsh(final DataBroker dataBroker,final SffName sffName,final IpAddress localIp,final IpAddress remoteIp,final Long nsp,final Short nsi){
  Long vni=0L;
  removeNshMap(dataBroker,nsp,nsi,nsp,nsi,sffName.getValue());
  removeNshEntry(dataBroker,nsp,nsi,sffName.getValue());
  removeVxlanGpePort(dataBroker,localIp,remoteIp,vni,sffName.getValue());
  return true;
}",0.723404255319149
132092,"/** 
 * Implemented from the AutoCloseable interface.
 */
@Override public void close() throws ExecutionException, InterruptedException {
  LOG.info(""String_Node_Str"");
  try {
    if (sfcOfFlowProgrammer != null) {
      sfcOfFlowProgrammer.shutdown();
    }
    if (pktInRegistration != null) {
      pktInRegistration.close();
    }
    openflowRspDataListener.close();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e.getMessage());
  }
}","/** 
 * Implemented from the AutoCloseable interface.
 */
@Override public void close() throws Exception {
  LOG.info(""String_Node_Str"");
  try {
    if (sfcOfFlowProgrammer != null) {
      sfcOfFlowProgrammer.shutdown();
    }
    if (pktInRegistration != null) {
      pktInRegistration.close();
    }
    openflowRspDataListener.close();
  }
  finally {
    openflowRspDataListener=null;
  }
}",0.8159437280187574
132093,"/** 
 * Implemented from the AutoCloseable interface.
 */
@Override public void close() throws ExecutionException, InterruptedException {
  LOG.info(""String_Node_Str"");
  try {
    if (sfcOfFlowProgrammer != null) {
      sfcOfFlowProgrammer.shutdown();
    }
    if (pktInRegistration != null) {
      pktInRegistration.close();
    }
    openflowRspDataListener.close();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e.getMessage());
  }
}","/** 
 * Implemented from the AutoCloseable interface.
 */
@Override public void close() throws Exception {
  LOG.info(""String_Node_Str"");
  try {
    if (sfcOfFlowProgrammer != null) {
      sfcOfFlowProgrammer.shutdown();
    }
    if (pktInRegistration != null) {
      pktInRegistration.close();
    }
    openflowRspDataListener.close();
  }
  finally {
    openflowRspDataListener=null;
  }
}",0.8159437280187574
132094,"public boolean isCapableNetconfDevice(Node node){
  NetconfNode netconfAugmentation=node.getAugmentation(NetconfNode.class);
  if (netconfAugmentation == null) {
    LOG.debug(""String_Node_Str"",node.getNodeId().getValue());
    return false;
  }
  AvailableCapabilities capabilities=netconfAugmentation.getAvailableCapabilities();
  return capabilities != null && capabilities.getAvailableCapability().containsAll(requiredCapabilities);
}","public boolean isCapableNetconfDevice(Node node){
  boolean ret=false;
  NetconfNode netconfAugmentation=node.getAugmentation(NetconfNode.class);
  if (netconfAugmentation == null) {
    LOG.debug(""String_Node_Str"",node.getNodeId().getValue());
    return ret;
  }
  AvailableCapabilities capabilities=netconfAugmentation.getAvailableCapabilities();
  if (capabilities != null) {
    List<String> availCapabilities=capabilities.getAvailableCapability().stream().map(AvailableCapability::getCapability).collect(Collectors.toList());
    ret=availCapabilities.containsAll(requiredCapabilities);
  }
  if (ret == false) {
    LOG.debug(""String_Node_Str"",node.getNodeId().getValue());
  }
  return ret;
}",0.7117750439367311
132095,"public void updateNode(Node node){
  NetconfNode netconfNode=node.getAugmentation(NetconfNode.class);
  Preconditions.checkNotNull(netconfNode);
  ConnectionStatus connectionStatus=netconfNode.getConnectionStatus();
  NodeId netconfNodeId=node.getNodeId();
  if (connectionStatus.equals(ConnectionStatus.Connected)) {
    InstanceIdentifier mountPointIid=getMountPointIid(netconfNodeId);
    DataBroker dataBroker=getNetconfNodeDataBroker(mountPointIid);
    if (dataBroker != null) {
      LOG.info(""String_Node_Str"",node.getNodeId().getValue());
      connectedNodes.put(netconfNodeId,node);
      activeMountPoints.put(netconfNodeId,dataBroker);
    }
 else {
      LOG.debug(""String_Node_Str"",netconfNodeId.getValue());
    }
  }
}","public void updateNode(Node node){
  NetconfNode netconfNode=node.getAugmentation(NetconfNode.class);
  Preconditions.checkNotNull(netconfNode);
  ConnectionStatus connectionStatus=netconfNode.getConnectionStatus();
  NodeId netconfNodeId=node.getNodeId();
  if (connectionStatus.equals(ConnectionStatus.Connected)) {
    InstanceIdentifier mountPointIid=getMountPointIid(netconfNodeId);
    DataBroker dataBroker=getNetconfNodeDataBroker(mountPointIid);
    if (dataBroker != null) {
      LOG.info(""String_Node_Str"",node.getNodeId().getValue());
      connectedNodes.put(netconfNodeId,node);
      activeMountPoints.put(netconfNodeId,dataBroker);
    }
 else {
      LOG.debug(""String_Node_Str"",netconfNodeId.getValue());
    }
  }
 else {
    LOG.debug(""String_Node_Str"",node.getNodeId().getValue());
  }
}",0.9520725388601036
132096,"public SfcScfOfDataListener(final DataBroker dataBroker,SfcScfOfProcessor sfcScfProcessor){
  this.dataBroker=dataBroker;
  this.sfcScfProcessor=sfcScfProcessor;
}","public SfcScfOfDataListener(final DataBroker dataBroker,SfcScfOfProcessor sfcScfProcessor){
  this.dataBroker=dataBroker;
  this.sfcScfProcessor=sfcScfProcessor;
  init();
}",0.9702380952380952
132097,"/** 
 * Implemented from the AutoCloseable interface.
 */
@Override public void close() throws ExecutionException, InterruptedException {
  LOG.info(""String_Node_Str"");
  try {
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e.getMessage());
  }
}","/** 
 * Implemented from the AutoCloseable interface.
 */
@Override public void close() throws ExecutionException, InterruptedException {
  LOG.info(""String_Node_Str"");
  try {
    sfcScfDataListener.close();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e.getMessage());
  }
}",0.9420289855072465
132098,"@Test public void pktInPurgeBuffering() throws InterruptedException {
  this.pktInHandler.setMaxBufferTime(1);
  this.pktInHandler.setPacketCountPurge(2);
  PacketReceived pkt=createPacket();
  assertEquals(this.pktInHandler.getBufferSize(),0);
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  assertEquals(this.pktInHandler.getBufferSize(),1);
  Thread.sleep(1);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  assertEquals(this.pktInHandler.getBufferSize(),1);
}","@Test public void pktInPurgeBuffering() throws InterruptedException {
  this.pktInHandler.setMaxBufferTime(1);
  this.pktInHandler.setPacketCountPurge(2);
  PacketReceived pkt=createPacket();
  assertEquals(this.pktInHandler.getBufferSize(),0);
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  assertEquals(this.pktInHandler.getBufferSize(),1);
  Thread.sleep(10);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  assertEquals(this.pktInHandler.getBufferSize(),1);
}",0.9995728321230244
132099,"@Test public void pktInBuffering(){
  this.pktInHandler.setMaxBufferTime(10000000);
  this.pktInHandler.setPacketCountPurge(1000);
  PacketReceived pkt=createPacket();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
}","@Test public void pktInBuffering(){
  this.pktInHandler.setMaxBufferTime(10000);
  this.pktInHandler.setPacketCountPurge(1000);
  PacketReceived pkt=createPacket();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
}",0.9980756895445798
132100,"@Test public void pktInBufferingTimeout() throws InterruptedException {
  this.pktInHandler.setMaxBufferTime(1);
  this.pktInHandler.setPacketCountPurge(1000);
  PacketReceived pkt=createPacket();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  Thread.sleep(1);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
}","@Test public void pktInBufferingTimeout() throws InterruptedException {
  this.pktInHandler.setMaxBufferTime(1);
  this.pktInHandler.setPacketCountPurge(1000);
  PacketReceived pkt=createPacket();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  Thread.sleep(10);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
}",0.999508116084604
132101,"@Test public void pktInPurgeBuffering() throws InterruptedException {
  this.pktInHandler.setMaxBufferTime(1);
  this.pktInHandler.setPacketCountPurge(2);
  PacketReceived pkt=createPacket();
  assertEquals(this.pktInHandler.getBufferSize(),0);
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  assertEquals(this.pktInHandler.getBufferSize(),1);
  Thread.sleep(1);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  assertEquals(this.pktInHandler.getBufferSize(),1);
}","@Test public void pktInPurgeBuffering() throws InterruptedException {
  this.pktInHandler.setMaxBufferTime(1);
  this.pktInHandler.setPacketCountPurge(2);
  PacketReceived pkt=createPacket();
  assertEquals(this.pktInHandler.getBufferSize(),0);
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  assertEquals(this.pktInHandler.getBufferSize(),1);
  Thread.sleep(10);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  assertEquals(this.pktInHandler.getBufferSize(),1);
}",0.9995728321230244
132102,"@Test public void pktInBuffering(){
  this.pktInHandler.setMaxBufferTime(10000000);
  this.pktInHandler.setPacketCountPurge(1000);
  PacketReceived pkt=createPacket();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
}","@Test public void pktInBuffering(){
  this.pktInHandler.setMaxBufferTime(10000);
  this.pktInHandler.setPacketCountPurge(1000);
  PacketReceived pkt=createPacket();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
}",0.9980756895445798
132103,"@Test public void pktInBufferingTimeout() throws InterruptedException {
  this.pktInHandler.setMaxBufferTime(1);
  this.pktInHandler.setPacketCountPurge(1000);
  PacketReceived pkt=createPacket();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  Thread.sleep(1);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
}","@Test public void pktInBufferingTimeout() throws InterruptedException {
  this.pktInHandler.setMaxBufferTime(1);
  this.pktInHandler.setPacketCountPurge(1000);
  PacketReceived pkt=createPacket();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
  Thread.sleep(10);
  resetFlowProgrammerMock();
  this.pktInHandler.onPacketReceived(pkt);
  verify(this.flowProgrammerMock,times(2)).setFlowRspId(anyLong());
  verify(this.flowProgrammerMock,times(2)).configurePathMapperAclFlow(anyString(),anyString(),anyString(),anyShort());
  verify(this.flowProgrammerMock,times(1)).compareClassificationTableCookie((FlowCookie)anyObject());
  verifyNoMoreInteractions(this.flowProgrammerMock);
}",0.999508116084604
132104,"/** 
 * delete flows for service function classifier The function returns true if successful. The function returns false if unsuccessful.
 * @param scf service function classifier
 * @return          delete result
 */
public boolean deletedServiceFunctionClassifier(ServiceFunctionClassifier scf){
  if (scf == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  LOG.debug(""String_Node_Str"",scf.getName(),scf.getAccessList(),scf.getSclServiceFunctionForwarder());
  Acl acl=SfcProviderAclAPI.readAccessList(scf.getAccessList());
  if (acl == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  String aclName=acl.getAclName();
  if (aclName == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  AccessListEntries accessListEntries=acl.getAccessListEntries();
  if (accessListEntries == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  List<Ace> acesList=accessListEntries.getAce();
  if (acesList == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  List<SclServiceFunctionForwarder> sfflist=scf.getSclServiceFunctionForwarder();
  if (sfflist == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  for (  SclServiceFunctionForwarder sclsff : sfflist) {
    for (    Ace ace : acesList) {
      String ruleName=ace.getRuleName();
      if (ruleName == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      Actions actions=ace.getActions();
      if (ruleName == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      Actions1 a1=actions.getAugmentation(Actions1.class);
      if (a1 == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      AclRenderedServicePath path=(AclRenderedServicePath)a1.getSfcAction();
      if (path == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      RspName rspName=new RspName(path.getRenderedServicePath());
      SfcNshHeader nsh=SfcNshHeader.getSfcNshHeader(rspName);
      SffName sffName=new SffName(sclsff.getName());
      ServiceFunctionForwarder sff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(sffName);
      if (sff == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      SffOvsBridgeAugmentation ovsSff=sff.getAugmentation(SffOvsBridgeAugmentation.class);
      if (ovsSff == null) {
        LOG.debug(""String_Node_Str"");
        continue;
      }
      String nodeName=SfcOvsUtil.getOpenFlowNodeIdForSff(sff);
      if (nodeName == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      StringBuffer key=new StringBuffer();
      key.append(scf.getName()).append(aclName).append(ruleName).append(""String_Node_Str"");
      SfcScfOfUtils.deleteClassifierFlow(nodeName,key.toString());
      RspName reverseRspName=null;
      if (path.getRenderedServicePath().endsWith(""String_Node_Str"")) {
        reverseRspName=new RspName(path.getRenderedServicePath().replaceFirst(""String_Node_Str"",""String_Node_Str""));
      }
 else {
        reverseRspName=new RspName(path.getRenderedServicePath() + ""String_Node_Str"");
      }
      SfcNshHeader reverseNsh=SfcNshHeader.getSfcNshHeader(reverseRspName);
      if (reverseNsh == null) {
        LOG.debug(""String_Node_Str"");
      }
 else {
        key=new StringBuffer();
        key.append(scf.getName()).append(aclName).append(ruleName).append(""String_Node_Str"");
        if (!SfcScfOfUtils.deleteClassifierFlow(nodeName,key.toString())) {
          LOG.error(""String_Node_Str"");
        }
        SffName lastSffName=reverseNsh.getSffName();
        if (lastSffName != null && !reverseNsh.getSffName().equals(sffName)) {
          ServiceFunctionForwarder lastSff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(lastSffName);
          String lastNodeName=SfcOvsUtil.getOpenFlowNodeIdForSff(lastSff);
          if (lastNodeName == null) {
            LOG.error(""String_Node_Str"");
          }
 else {
            LOG.debug(""String_Node_Str"",lastNodeName);
          }
          key=new StringBuffer();
          key.append(scf.getName()).append(aclName).append(ruleName).append(""String_Node_Str"");
          if (!SfcScfOfUtils.deleteClassifierFlow(lastNodeName,key.toString())) {
            LOG.error(""String_Node_Str"");
          }
        }
      }
    }
  }
  return true;
}","/** 
 * delete flows for service function classifier The function returns true if successful. The function returns false if unsuccessful.
 * @param scf service function classifier
 * @return          delete result
 */
public boolean deletedServiceFunctionClassifier(ServiceFunctionClassifier scf){
  if (scf == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  LOG.debug(""String_Node_Str"",scf.getName(),scf.getAccessList(),scf.getSclServiceFunctionForwarder());
  Acl acl=SfcProviderAclAPI.readAccessList(scf.getAccessList());
  if (acl == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  String aclName=acl.getAclName();
  if (aclName == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  AccessListEntries accessListEntries=acl.getAccessListEntries();
  if (accessListEntries == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  List<Ace> acesList=accessListEntries.getAce();
  if (acesList == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  List<SclServiceFunctionForwarder> sfflist=scf.getSclServiceFunctionForwarder();
  if (sfflist == null) {
    LOG.error(""String_Node_Str"");
    return false;
  }
  for (  SclServiceFunctionForwarder sclsff : sfflist) {
    for (    Ace ace : acesList) {
      String ruleName=ace.getRuleName();
      if (ruleName == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      Actions actions=ace.getActions();
      if (actions == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      Actions1 a1=actions.getAugmentation(Actions1.class);
      if (a1 == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      AclRenderedServicePath path=(AclRenderedServicePath)a1.getSfcAction();
      if (path == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      RspName rspName=new RspName(path.getRenderedServicePath());
      SfcNshHeader nsh=SfcNshHeader.getSfcNshHeader(rspName);
      SffName sffName=new SffName(sclsff.getName());
      ServiceFunctionForwarder sff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(sffName);
      if (sff == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      SffOvsBridgeAugmentation ovsSff=sff.getAugmentation(SffOvsBridgeAugmentation.class);
      if (ovsSff == null) {
        LOG.debug(""String_Node_Str"");
        continue;
      }
      String nodeName=SfcOvsUtil.getOpenFlowNodeIdForSff(sff);
      if (nodeName == null) {
        LOG.error(""String_Node_Str"");
        continue;
      }
      StringBuffer key=new StringBuffer();
      key.append(scf.getName()).append(aclName).append(ruleName).append(""String_Node_Str"");
      SfcScfOfUtils.deleteClassifierFlow(nodeName,key.toString());
      RspName reverseRspName=null;
      if (path.getRenderedServicePath().endsWith(""String_Node_Str"")) {
        reverseRspName=new RspName(path.getRenderedServicePath().replaceFirst(""String_Node_Str"",""String_Node_Str""));
      }
 else {
        reverseRspName=new RspName(path.getRenderedServicePath() + ""String_Node_Str"");
      }
      SfcNshHeader reverseNsh=SfcNshHeader.getSfcNshHeader(reverseRspName);
      if (reverseNsh == null) {
        LOG.debug(""String_Node_Str"");
      }
 else {
        key=new StringBuffer();
        key.append(scf.getName()).append(aclName).append(ruleName).append(""String_Node_Str"");
        if (!SfcScfOfUtils.deleteClassifierFlow(nodeName,key.toString())) {
          LOG.error(""String_Node_Str"");
        }
        SffName lastSffName=reverseNsh.getSffName();
        if (lastSffName != null && !reverseNsh.getSffName().equals(sffName)) {
          ServiceFunctionForwarder lastSff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarder(lastSffName);
          String lastNodeName=SfcOvsUtil.getOpenFlowNodeIdForSff(lastSff);
          if (lastNodeName == null) {
            LOG.error(""String_Node_Str"");
          }
 else {
            LOG.debug(""String_Node_Str"",lastNodeName);
          }
          key=new StringBuffer();
          key.append(scf.getName()).append(aclName).append(ruleName).append(""String_Node_Str"");
          if (!SfcScfOfUtils.deleteClassifierFlow(lastNodeName,key.toString())) {
            LOG.error(""String_Node_Str"");
          }
        }
      }
    }
  }
  return true;
}",0.9982668977469672
132105,"@Override public java.lang.AutoCloseable createInstance(){
  final OpendaylightSfc opendaylightSfc=new OpendaylightSfc();
  DataBroker dataBrokerService=getDataBrokerDependency();
  opendaylightSfc.setDataProvider(dataBrokerService);
  BindingAwareBroker broker=getBindingRegistryDependency();
  opendaylightSfc.setBroker(broker);
  final SfcProviderRpc sfcProviderRpc=new SfcProviderRpc();
  SfcProviderSffEntryDataListener sfcProviderSffEntryDataListener=new SfcProviderSffEntryDataListener();
  final ListenerRegistration<DataChangeListener> sffDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFF_ENTRY_IID,sfcProviderSffEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfEntryDataListener sfcProviderSfEntryDataListener=new SfcProviderSfEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SF_ENTRY_IID,sfcProviderSfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderScfEntryDataListener sfcProviderScfEntryDataListener=new SfcProviderScfEntryDataListener();
  final ListenerRegistration<DataChangeListener> scfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SCF_ENTRY_IID,sfcProviderScfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionService> sfRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionChainService> sfcRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionChainService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<RenderedServicePathService> rspRpcRegistration=getRpcRegistryDependency().addRpcImplementation(RenderedServicePathService.class,sfcProviderRpc);
final class AutoCloseableSfc implements AutoCloseable {
    @Override public void close(){
      sfEntryDataChangeListenerRegistration.close();
      scfEntryDataChangeListenerRegistration.close();
      sffDataChangeListenerRegistration.close();
      sfRpcRegistration.close();
      sfcRpcRegistration.close();
      rspRpcRegistration.close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  AutoCloseable ret=new AutoCloseableSfc();
  LOG.info(""String_Node_Str"",ret);
  return ret;
}","@Override public java.lang.AutoCloseable createInstance(){
  final OpendaylightSfc opendaylightSfc=new OpendaylightSfc();
  DataBroker dataBrokerService=getDataBrokerDependency();
  opendaylightSfc.setDataProvider(dataBrokerService);
  BindingAwareBroker broker=getBindingRegistryDependency();
  opendaylightSfc.setBroker(broker);
  final SfcProviderRpc sfcProviderRpc=new SfcProviderRpc();
  SfcProviderSffEntryDataListener sfcProviderSffEntryDataListener=new SfcProviderSffEntryDataListener();
  final ListenerRegistration<DataChangeListener> sffDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFF_ENTRY_IID,sfcProviderSffEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfEntryDataListener sfcProviderSfEntryDataListener=new SfcProviderSfEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SF_ENTRY_IID,sfcProviderSfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderScfEntryDataListener sfcProviderScfEntryDataListener=new SfcProviderScfEntryDataListener();
  final ListenerRegistration<DataChangeListener> scfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SCF_ENTRY_IID,sfcProviderScfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfstEntryDataListener sfcProviderSfstEntryDataListener=new SfcProviderSfstEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfstEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFST_ENTRY_IID,sfcProviderSfstEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionService> sfRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionChainService> sfcRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionChainService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<RenderedServicePathService> rspRpcRegistration=getRpcRegistryDependency().addRpcImplementation(RenderedServicePathService.class,sfcProviderRpc);
final class AutoCloseableSfc implements AutoCloseable {
    @Override public void close(){
      sfEntryDataChangeListenerRegistration.close();
      scfEntryDataChangeListenerRegistration.close();
      sffDataChangeListenerRegistration.close();
      sfstEntryDataChangeListenerRegistration.close();
      sfRpcRegistration.close();
      sfcRpcRegistration.close();
      rspRpcRegistration.close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  AutoCloseable ret=new AutoCloseableSfc();
  LOG.info(""String_Node_Str"",ret);
  return ret;
}",0.9264730854134828
132106,"@Override public void close(){
  sfEntryDataChangeListenerRegistration.close();
  scfEntryDataChangeListenerRegistration.close();
  sffDataChangeListenerRegistration.close();
  sfRpcRegistration.close();
  sfcRpcRegistration.close();
  rspRpcRegistration.close();
  try {
    opendaylightSfc.close();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
  }
  LOG.info(""String_Node_Str"",this);
}","@Override public void close(){
  sfEntryDataChangeListenerRegistration.close();
  scfEntryDataChangeListenerRegistration.close();
  sffDataChangeListenerRegistration.close();
  sfstEntryDataChangeListenerRegistration.close();
  sfRpcRegistration.close();
  sfcRpcRegistration.close();
  rspRpcRegistration.close();
  try {
    opendaylightSfc.close();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
  }
  LOG.info(""String_Node_Str"",this);
}",0.9474768280123584
132107,"/** 
 * This method is called whenever there is change in a SF Schedule Type. Before doing any changes it takes a global lock in order to ensure it is the only writer.
 * @param change
 */
@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  if (SfcConcurrencyAPI.getLock()) {
    try {
      Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
        if (entry.getValue() instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          LOG.debug(""String_Node_Str"",origServiceFunctionSchedulerType.getType(),origServiceFunctionSchedulerType.getName());
        }
      }
      Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
        if (entry.getValue() instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType createdServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          Object[] serviceFunctionSchedulerTypeObj={createdServiceFunctionSchedulerType};
          Class[] serviceFunctionSchedulerTypeClass={ServiceFunctionSchedulerType.class};
          Future future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getPut(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
          try {
            LOG.debug(""String_Node_Str"",future.get());
          }
 catch (          InterruptedException e) {
            LOG.warn(""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
      }
      Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
      for (      InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
        DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
        if (dataObject instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)dataObject;
          Object[] serviceFunctionSchedulerTypeObj={origServiceFunctionSchedulerType};
          Class[] serviceFunctionSchedulerTypeClass={ServiceFunctionSchedulerType.class};
          Future future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getDelete(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
          try {
            LOG.debug(""String_Node_Str"",future.get());
          }
 catch (          InterruptedException e) {
            LOG.warn(""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
      }
      Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
        if ((entry.getValue() instanceof ServiceFunctionSchedulerType) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
          DataObject dataObject=dataOriginalDataObject.get(entry.getKey());
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)dataObject;
          Object[] serviceFunctionSchedulerTypeObj={origServiceFunctionSchedulerType};
          Class[] serviceFunctionSchedulerTypeClass={ServiceFunctionSchedulerType.class};
          ServiceFunctionSchedulerType updatedServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          if (!updatedServiceFunctionSchedulerType.isEnabled() == true) {
            Object[] serviceFunctionSchedulerTypesObj={};
            Class[] serviceFunctionSchedulerTypesClass={};
            Future future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getReadAll(serviceFunctionSchedulerTypesObj,serviceFunctionSchedulerTypesClass));
            try {
              LOG.debug(""String_Node_Str"",future.get());
              ServiceFunctionSchedulerTypes serviceFunctionSchedulerTypes=(ServiceFunctionSchedulerTypes)future.get();
              List<ServiceFunctionSchedulerType> sfScheduleTypeList=serviceFunctionSchedulerTypes.getServiceFunctionSchedulerType();
              for (              ServiceFunctionSchedulerType sfst : sfScheduleTypeList) {
                if (sfst.isEnabled() == true) {
                  break;
                }
              }
            }
 catch (            InterruptedException e) {
              LOG.warn(""String_Node_Str"",e);
            }
catch (            ExecutionException e) {
              LOG.warn(""String_Node_Str"",e);
            }
            future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getDelete(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
            try {
              LOG.debug(""String_Node_Str"",future.get());
            }
 catch (            InterruptedException e) {
              LOG.warn(""String_Node_Str"",e);
            }
catch (            ExecutionException e) {
              LOG.warn(""String_Node_Str"",e);
            }
            serviceFunctionSchedulerTypeObj[0]=updatedServiceFunctionSchedulerType;
            serviceFunctionSchedulerTypeClass[0]=ServiceFunctionSchedulerType.class;
            future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getPut(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
            try {
              LOG.debug(""String_Node_Str"",future.get());
            }
 catch (            InterruptedException e) {
              LOG.warn(""String_Node_Str"",e);
            }
catch (            ExecutionException e) {
              LOG.warn(""String_Node_Str"",e);
            }
          }
        }
      }
    }
  finally {
      SfcConcurrencyAPI.releaseLock();
    }
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  printTraceStop(LOG);
}","/** 
 * This method is called whenever there is change in a SF Schedule Type. Before doing any changes it takes a global lock in order to ensure it is the only writer.
 * @param change
 */
@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  if (SfcConcurrencyAPI.getLock()) {
    try {
      Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
        if (entry.getValue() instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          LOG.debug(""String_Node_Str"",origServiceFunctionSchedulerType.getType(),origServiceFunctionSchedulerType.getName());
        }
      }
      Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
        if (entry.getValue() instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType createdServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          LOG.debug(""String_Node_Str"",createdServiceFunctionSchedulerType.getType(),createdServiceFunctionSchedulerType.getName());
          try {
            if (createdServiceFunctionSchedulerType.isEnabled() == true) {
              isCreateTrue=true;
              Object[] sfstObj={};
              Class[] sfstClass={};
              SfcProviderScheduleTypeAPI sfcProviderScheduleTypeAPI=SfcProviderScheduleTypeAPI.getReadAll(sfstObj,sfstClass);
              Future future=odlSfc.getExecutor().submit(sfcProviderScheduleTypeAPI);
              if (future.get() != null) {
                ServiceFunctionSchedulerTypes serviceFunctionSchedulerTypes=(ServiceFunctionSchedulerTypes)future.get();
                List<ServiceFunctionSchedulerType> sfScheduleTypeList=serviceFunctionSchedulerTypes.getServiceFunctionSchedulerType();
                for (                ServiceFunctionSchedulerType sfst : sfScheduleTypeList) {
                  if (sfst.isEnabled() == true) {
                    if (!(sfst.getType().equals(createdServiceFunctionSchedulerType.getType()))) {
                      ServiceFunctionSchedulerType sfstUpdate=new ServiceFunctionSchedulerTypeBuilder().setName(sfst.getName()).setType(sfst.getType()).setEnabled(false).build();
                      Object[] sfstObjUpdate={sfstUpdate};
                      Class[] sfstClassUpdate={ServiceFunctionSchedulerType.class};
                      future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getPut(sfstObjUpdate,sfstClassUpdate));
                      break;
                    }
                  }
                }
              }
            }
          }
 catch (          InterruptedException e) {
            LOG.warn(""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
      }
      Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
      for (      InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
        DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
        if (dataObject instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)dataObject;
          LOG.debug(""String_Node_Str"",origServiceFunctionSchedulerType.getType(),origServiceFunctionSchedulerType.getName());
        }
      }
      Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
        if ((entry.getValue() instanceof ServiceFunctionSchedulerType) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
          ServiceFunctionSchedulerType updatedServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          LOG.debug(""String_Node_Str"",updatedServiceFunctionSchedulerType.getType(),updatedServiceFunctionSchedulerType.getName());
          try {
            if (isCreateTrue == false) {
              if (updatedServiceFunctionSchedulerType.isEnabled() == true) {
                Object[] sfstObj={};
                Class[] sfstClass={};
                SfcProviderScheduleTypeAPI sfcProviderScheduleTypeAPI=SfcProviderScheduleTypeAPI.getReadAll(sfstObj,sfstClass);
                Future future=odlSfc.getExecutor().submit(sfcProviderScheduleTypeAPI);
                if (future.get() != null) {
                  ServiceFunctionSchedulerTypes serviceFunctionSchedulerTypes=(ServiceFunctionSchedulerTypes)future.get();
                  List<ServiceFunctionSchedulerType> sfScheduleTypeList=serviceFunctionSchedulerTypes.getServiceFunctionSchedulerType();
                  for (                  ServiceFunctionSchedulerType sfst : sfScheduleTypeList) {
                    if (sfst.isEnabled() == true) {
                      if (!(sfst.getType().equals(updatedServiceFunctionSchedulerType.getType()))) {
                        ServiceFunctionSchedulerType sfstUpdate=new ServiceFunctionSchedulerTypeBuilder().setName(sfst.getName()).setType(sfst.getType()).setEnabled(false).build();
                        Object[] serviceFunctionSchedulerTypeObj={sfstUpdate};
                        Class[] serviceFunctionSchedulerTypeClass={ServiceFunctionSchedulerType.class};
                        future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getPut(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
                        break;
                      }
                    }
                  }
                }
              }
            }
 else             if (isCreateTrue == true) {
              isCreateTrue=false;
            }
          }
 catch (          InterruptedException e) {
            LOG.warn(""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
      }
    }
  finally {
      SfcConcurrencyAPI.releaseLock();
    }
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  printTraceStop(LOG);
}",0.326040597109233
132108,"@Override public java.lang.AutoCloseable createInstance(){
  final OpendaylightSfc opendaylightSfc=OpendaylightSfc.getOpendaylightSfcObj();
  final SbRestSfEntryDataListener sbRestSfEntryDataListener=new SbRestSfEntryDataListener(opendaylightSfc);
  final SbRestSfgEntryDataListener sbRestSfgEntryDataListener=new SbRestSfgEntryDataListener(opendaylightSfc);
  final SbRestSffEntryDataListener sbRestSffEntryDataListener=new SbRestSffEntryDataListener(opendaylightSfc);
  final SbRestRspEntryDataListener sbRestRspEntryDataListener=new SbRestRspEntryDataListener(opendaylightSfc);
  final SbRestAclEntryDataListener sbRestAclEntryDataListener=new SbRestAclEntryDataListener(opendaylightSfc);
  final SbRestScfEntryDataListener sbRestScfEntryDataListener=new SbRestScfEntryDataListener(opendaylightSfc);
  final SbRestKeepAliveSocket sbRestKeepAliveSocket=new SbRestKeepAliveSocket();
  opendaylightSfc.getExecutor().execute(sbRestKeepAliveSocket);
final class AutoCloseableSfcSbRest implements AutoCloseable {
    @Override public void close(){
      sbRestSfEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSfgEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSffEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestRspEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestAclEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestScfEntryDataListener.getDataChangeListenerRegistration().close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  return new AutoCloseableSfcSbRest();
}","@Override public java.lang.AutoCloseable createInstance(){
  final OpendaylightSfc opendaylightSfc=OpendaylightSfc.getOpendaylightSfcObj();
  final SbRestSfEntryDataListener sbRestSfEntryDataListener=new SbRestSfEntryDataListener(opendaylightSfc);
  final SbRestSfgEntryDataListener sbRestSfgEntryDataListener=new SbRestSfgEntryDataListener(opendaylightSfc);
  final SbRestSffEntryDataListener sbRestSffEntryDataListener=new SbRestSffEntryDataListener(opendaylightSfc);
  final SbRestRspEntryDataListener sbRestRspEntryDataListener=new SbRestRspEntryDataListener(opendaylightSfc);
  final SbRestAclEntryDataListener sbRestAclEntryDataListener=new SbRestAclEntryDataListener(opendaylightSfc);
  final SbRestScfEntryDataListener sbRestScfEntryDataListener=new SbRestScfEntryDataListener(opendaylightSfc);
  final SbRestSfstEntryDataListener sbRestSfstEntryDataListener=new SbRestSfstEntryDataListener(opendaylightSfc);
  final SbRestKeepAliveSocket sbRestKeepAliveSocket=new SbRestKeepAliveSocket();
  opendaylightSfc.getExecutor().execute(sbRestKeepAliveSocket);
final class AutoCloseableSfcSbRest implements AutoCloseable {
    @Override public void close(){
      sbRestSfEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSfgEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSffEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestRspEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestAclEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestScfEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSfstEntryDataListener.getDataChangeListenerRegistration().close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  return new AutoCloseableSfcSbRest();
}",0.9481600859521891
132109,"@Override public void close(){
  sbRestSfEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSfgEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSffEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestRspEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestAclEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestScfEntryDataListener.getDataChangeListenerRegistration().close();
  try {
    opendaylightSfc.close();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",this);
  }
  LOG.info(""String_Node_Str"",this);
}","@Override public void close(){
  sbRestSfEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSfgEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSffEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestRspEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestAclEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestScfEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSfstEntryDataListener.getDataChangeListenerRegistration().close();
  try {
    opendaylightSfc.close();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",this);
  }
  LOG.info(""String_Node_Str"",this);
}",0.9454545454545454
132110,"/** 
 * Builds a complete service Function Schedule Types Object
 * @return ServiceFunctionSchedulerTypes object
 */
public void build_service_function_scheduler_types() throws Exception {
  AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> dataChangeEvent=Mockito.mock(AsyncDataChangeEvent.class);
  Map<InstanceIdentifier<?>,DataObject> createdData=new HashMap<InstanceIdentifier<?>,DataObject>();
  Set<InstanceIdentifier<?>> removedPaths=new HashSet<InstanceIdentifier<?>>();
  Map<InstanceIdentifier<?>,DataObject> updatedData=new HashMap<InstanceIdentifier<?>,DataObject>();
  Map<InstanceIdentifier<?>,DataObject> originalData=new HashMap<InstanceIdentifier<?>,DataObject>();
  String[] SFST_NAMES={""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  Class[] schedulerTypes={Random.class,RoundRobin.class,LoadBalance.class};
  Boolean enabledStatus=true;
  for (int i=0; i < SFST_NAMES.length; i++) {
    ServiceFunctionSchedulerTypeKey key=new ServiceFunctionSchedulerTypeKey(schedulerTypes[i]);
    ServiceFunctionSchedulerTypeBuilder sfstBuilder=new ServiceFunctionSchedulerTypeBuilder();
    sfstBuilder.setName(SFST_NAMES[i]).setKey(key).setType(schedulerTypes[i]).setEnabled(enabledStatus);
    ServiceFunctionSchedulerType serviceFunctionSchedulerType=sfstBuilder.build();
    assertTrue(SfcProviderScheduleTypeAPI.putServiceFunctionScheduleTypeExecutor(serviceFunctionSchedulerType));
    InstanceIdentifier<ServiceFunctionSchedulerType> sfstEntryIID=InstanceIdentifier.builder(ServiceFunctionSchedulerTypes.class).child(ServiceFunctionSchedulerType.class,serviceFunctionSchedulerType.getKey()).build();
    createdData.put(sfstEntryIID,serviceFunctionSchedulerType);
  }
  when(dataChangeEvent.getUpdatedData()).thenReturn(updatedData);
  when(dataChangeEvent.getOriginalData()).thenReturn(originalData);
  when(dataChangeEvent.getCreatedData()).thenReturn(createdData);
  when(dataChangeEvent.getRemovedPaths()).thenReturn(removedPaths);
  sfstEntryDataListener.onDataChanged(dataChangeEvent);
  Thread.sleep(500);
}","/** 
 * Builds a complete service Function Schedule Types Object
 * @return ServiceFunctionSchedulerTypes object
 */
public void build_service_function_scheduler_types() throws Exception {
  String[] SFST_NAMES={""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  Class[] schedulerTypes={Random.class,RoundRobin.class,LoadBalance.class};
  Boolean enabledStatus=true;
  for (int i=0; i < SFST_NAMES.length; i++) {
    AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> dataChangeEvent=Mockito.mock(AsyncDataChangeEvent.class);
    Map<InstanceIdentifier<?>,DataObject> createdData=new HashMap<InstanceIdentifier<?>,DataObject>();
    Set<InstanceIdentifier<?>> removedPaths=new HashSet<InstanceIdentifier<?>>();
    Map<InstanceIdentifier<?>,DataObject> updatedData=new HashMap<InstanceIdentifier<?>,DataObject>();
    Map<InstanceIdentifier<?>,DataObject> originalData=new HashMap<InstanceIdentifier<?>,DataObject>();
    ServiceFunctionSchedulerTypeKey key=new ServiceFunctionSchedulerTypeKey(schedulerTypes[i]);
    ServiceFunctionSchedulerTypeBuilder sfstBuilder=new ServiceFunctionSchedulerTypeBuilder();
    sfstBuilder.setName(SFST_NAMES[i]).setKey(key).setType(schedulerTypes[i]).setEnabled(enabledStatus);
    ServiceFunctionSchedulerType serviceFunctionSchedulerType=sfstBuilder.build();
    assertTrue(SfcProviderScheduleTypeAPI.putServiceFunctionScheduleTypeExecutor(serviceFunctionSchedulerType));
    InstanceIdentifier<ServiceFunctionSchedulerType> sfstEntryIID=InstanceIdentifier.builder(ServiceFunctionSchedulerTypes.class).child(ServiceFunctionSchedulerType.class,serviceFunctionSchedulerType.getKey()).build();
    createdData.put(sfstEntryIID,serviceFunctionSchedulerType);
    when(dataChangeEvent.getUpdatedData()).thenReturn(updatedData);
    when(dataChangeEvent.getOriginalData()).thenReturn(originalData);
    when(dataChangeEvent.getCreatedData()).thenReturn(createdData);
    when(dataChangeEvent.getRemovedPaths()).thenReturn(removedPaths);
    sfstEntryDataListener.onDataChanged(dataChangeEvent);
    Thread.sleep(500);
  }
}",0.7508515815085158
132111,"@Override public java.lang.AutoCloseable createInstance(){
  final OpendaylightSfc opendaylightSfc=new OpendaylightSfc();
  DataBroker dataBrokerService=getDataBrokerDependency();
  opendaylightSfc.setDataProvider(dataBrokerService);
  BindingAwareBroker broker=getBindingRegistryDependency();
  opendaylightSfc.setBroker(broker);
  final SfcProviderRpc sfcProviderRpc=new SfcProviderRpc();
  SfcProviderSffEntryDataListener sfcProviderSffEntryDataListener=new SfcProviderSffEntryDataListener();
  final ListenerRegistration<DataChangeListener> sffDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFF_ENTRY_IID,sfcProviderSffEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfEntryDataListener sfcProviderSfEntryDataListener=new SfcProviderSfEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SF_ENTRY_IID,sfcProviderSfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderScfEntryDataListener sfcProviderScfEntryDataListener=new SfcProviderScfEntryDataListener();
  final ListenerRegistration<DataChangeListener> scfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SCF_ENTRY_IID,sfcProviderScfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionService> sfRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionChainService> sfcRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionChainService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<RenderedServicePathService> rspRpcRegistration=getRpcRegistryDependency().addRpcImplementation(RenderedServicePathService.class,sfcProviderRpc);
final class AutoCloseableSfc implements AutoCloseable {
    @Override public void close(){
      sfEntryDataChangeListenerRegistration.close();
      scfEntryDataChangeListenerRegistration.close();
      sffDataChangeListenerRegistration.close();
      sfRpcRegistration.close();
      sfcRpcRegistration.close();
      rspRpcRegistration.close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  AutoCloseable ret=new AutoCloseableSfc();
  LOG.info(""String_Node_Str"",ret);
  return ret;
}","@Override public java.lang.AutoCloseable createInstance(){
  final OpendaylightSfc opendaylightSfc=new OpendaylightSfc();
  DataBroker dataBrokerService=getDataBrokerDependency();
  opendaylightSfc.setDataProvider(dataBrokerService);
  BindingAwareBroker broker=getBindingRegistryDependency();
  opendaylightSfc.setBroker(broker);
  final SfcProviderRpc sfcProviderRpc=new SfcProviderRpc();
  SfcProviderSffEntryDataListener sfcProviderSffEntryDataListener=new SfcProviderSffEntryDataListener();
  final ListenerRegistration<DataChangeListener> sffDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFF_ENTRY_IID,sfcProviderSffEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfEntryDataListener sfcProviderSfEntryDataListener=new SfcProviderSfEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SF_ENTRY_IID,sfcProviderSfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderScfEntryDataListener sfcProviderScfEntryDataListener=new SfcProviderScfEntryDataListener();
  final ListenerRegistration<DataChangeListener> scfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SCF_ENTRY_IID,sfcProviderScfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfstEntryDataListener sfcProviderSfstEntryDataListener=new SfcProviderSfstEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfstEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFST_ENTRY_IID,sfcProviderSfstEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionService> sfRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionChainService> sfcRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionChainService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<RenderedServicePathService> rspRpcRegistration=getRpcRegistryDependency().addRpcImplementation(RenderedServicePathService.class,sfcProviderRpc);
final class AutoCloseableSfc implements AutoCloseable {
    @Override public void close(){
      sfEntryDataChangeListenerRegistration.close();
      scfEntryDataChangeListenerRegistration.close();
      sffDataChangeListenerRegistration.close();
      sfstEntryDataChangeListenerRegistration.close();
      sfRpcRegistration.close();
      sfcRpcRegistration.close();
      rspRpcRegistration.close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  AutoCloseable ret=new AutoCloseableSfc();
  LOG.info(""String_Node_Str"",ret);
  return ret;
}",0.9264730854134828
132112,"@Override public void close(){
  sfEntryDataChangeListenerRegistration.close();
  scfEntryDataChangeListenerRegistration.close();
  sffDataChangeListenerRegistration.close();
  sfRpcRegistration.close();
  sfcRpcRegistration.close();
  rspRpcRegistration.close();
  try {
    opendaylightSfc.close();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
  }
  LOG.info(""String_Node_Str"",this);
}","@Override public void close(){
  sfEntryDataChangeListenerRegistration.close();
  scfEntryDataChangeListenerRegistration.close();
  sffDataChangeListenerRegistration.close();
  sfstEntryDataChangeListenerRegistration.close();
  sfRpcRegistration.close();
  sfcRpcRegistration.close();
  rspRpcRegistration.close();
  try {
    opendaylightSfc.close();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
  }
  LOG.info(""String_Node_Str"",this);
}",0.9474768280123584
132113,"/** 
 * This method is called whenever there is change in a SF Schedule Type. Before doing any changes it takes a global lock in order to ensure it is the only writer.
 * @param change
 */
@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  if (SfcConcurrencyAPI.getLock()) {
    try {
      Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
        if (entry.getValue() instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          LOG.debug(""String_Node_Str"",origServiceFunctionSchedulerType.getType(),origServiceFunctionSchedulerType.getName());
        }
      }
      Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
        if (entry.getValue() instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType createdServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          Object[] serviceFunctionSchedulerTypeObj={createdServiceFunctionSchedulerType};
          Class[] serviceFunctionSchedulerTypeClass={ServiceFunctionSchedulerType.class};
          Future future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getPut(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
          try {
            LOG.debug(""String_Node_Str"",future.get());
          }
 catch (          InterruptedException e) {
            LOG.warn(""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
      }
      Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
      for (      InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
        DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
        if (dataObject instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)dataObject;
          Object[] serviceFunctionSchedulerTypeObj={origServiceFunctionSchedulerType};
          Class[] serviceFunctionSchedulerTypeClass={ServiceFunctionSchedulerType.class};
          Future future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getDelete(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
          try {
            LOG.debug(""String_Node_Str"",future.get());
          }
 catch (          InterruptedException e) {
            LOG.warn(""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
      }
      Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
        if ((entry.getValue() instanceof ServiceFunctionSchedulerType) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
          DataObject dataObject=dataOriginalDataObject.get(entry.getKey());
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)dataObject;
          Object[] serviceFunctionSchedulerTypeObj={origServiceFunctionSchedulerType};
          Class[] serviceFunctionSchedulerTypeClass={ServiceFunctionSchedulerType.class};
          ServiceFunctionSchedulerType updatedServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          if (!updatedServiceFunctionSchedulerType.isEnabled() == true) {
            Object[] serviceFunctionSchedulerTypesObj={};
            Class[] serviceFunctionSchedulerTypesClass={};
            Future future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getReadAll(serviceFunctionSchedulerTypesObj,serviceFunctionSchedulerTypesClass));
            try {
              LOG.debug(""String_Node_Str"",future.get());
              ServiceFunctionSchedulerTypes serviceFunctionSchedulerTypes=(ServiceFunctionSchedulerTypes)future.get();
              List<ServiceFunctionSchedulerType> sfScheduleTypeList=serviceFunctionSchedulerTypes.getServiceFunctionSchedulerType();
              for (              ServiceFunctionSchedulerType sfst : sfScheduleTypeList) {
                if (sfst.isEnabled() == true) {
                  break;
                }
              }
            }
 catch (            InterruptedException e) {
              LOG.warn(""String_Node_Str"",e);
            }
catch (            ExecutionException e) {
              LOG.warn(""String_Node_Str"",e);
            }
            future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getDelete(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
            try {
              LOG.debug(""String_Node_Str"",future.get());
            }
 catch (            InterruptedException e) {
              LOG.warn(""String_Node_Str"",e);
            }
catch (            ExecutionException e) {
              LOG.warn(""String_Node_Str"",e);
            }
            serviceFunctionSchedulerTypeObj[0]=updatedServiceFunctionSchedulerType;
            serviceFunctionSchedulerTypeClass[0]=ServiceFunctionSchedulerType.class;
            future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getPut(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
            try {
              LOG.debug(""String_Node_Str"",future.get());
            }
 catch (            InterruptedException e) {
              LOG.warn(""String_Node_Str"",e);
            }
catch (            ExecutionException e) {
              LOG.warn(""String_Node_Str"",e);
            }
          }
        }
      }
    }
  finally {
      SfcConcurrencyAPI.releaseLock();
    }
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  printTraceStop(LOG);
}","/** 
 * This method is called whenever there is change in a SF Schedule Type. Before doing any changes it takes a global lock in order to ensure it is the only writer.
 * @param change
 */
@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  if (SfcConcurrencyAPI.getLock()) {
    try {
      Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
        if (entry.getValue() instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          LOG.debug(""String_Node_Str"",origServiceFunctionSchedulerType.getType(),origServiceFunctionSchedulerType.getName());
        }
      }
      Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
        if (entry.getValue() instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType createdServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          LOG.debug(""String_Node_Str"",createdServiceFunctionSchedulerType.getType(),createdServiceFunctionSchedulerType.getName());
          try {
            if (createdServiceFunctionSchedulerType.isEnabled() == true) {
              isCreateTrue=true;
              Object[] sfstObj={};
              Class[] sfstClass={};
              SfcProviderScheduleTypeAPI sfcProviderScheduleTypeAPI=SfcProviderScheduleTypeAPI.getReadAll(sfstObj,sfstClass);
              Future future=odlSfc.getExecutor().submit(sfcProviderScheduleTypeAPI);
              if (future.get() != null) {
                ServiceFunctionSchedulerTypes serviceFunctionSchedulerTypes=(ServiceFunctionSchedulerTypes)future.get();
                List<ServiceFunctionSchedulerType> sfScheduleTypeList=serviceFunctionSchedulerTypes.getServiceFunctionSchedulerType();
                for (                ServiceFunctionSchedulerType sfst : sfScheduleTypeList) {
                  if (sfst.isEnabled() == true) {
                    if (!(sfst.getType().equals(createdServiceFunctionSchedulerType.getType()))) {
                      ServiceFunctionSchedulerType sfstUpdate=new ServiceFunctionSchedulerTypeBuilder().setName(sfst.getName()).setType(sfst.getType()).setEnabled(false).build();
                      Object[] sfstObjUpdate={sfstUpdate};
                      Class[] sfstClassUpdate={ServiceFunctionSchedulerType.class};
                      future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getPut(sfstObjUpdate,sfstClassUpdate));
                      break;
                    }
                  }
                }
              }
            }
          }
 catch (          InterruptedException e) {
            LOG.warn(""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
      }
      Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
      for (      InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
        DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
        if (dataObject instanceof ServiceFunctionSchedulerType) {
          ServiceFunctionSchedulerType origServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)dataObject;
          LOG.debug(""String_Node_Str"",origServiceFunctionSchedulerType.getType(),origServiceFunctionSchedulerType.getName());
        }
      }
      Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
      for (      Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
        if ((entry.getValue() instanceof ServiceFunctionSchedulerType) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
          ServiceFunctionSchedulerType updatedServiceFunctionSchedulerType=(ServiceFunctionSchedulerType)entry.getValue();
          LOG.debug(""String_Node_Str"",updatedServiceFunctionSchedulerType.getType(),updatedServiceFunctionSchedulerType.getName());
          try {
            if (isCreateTrue == false) {
              if (updatedServiceFunctionSchedulerType.isEnabled() == true) {
                Object[] sfstObj={};
                Class[] sfstClass={};
                SfcProviderScheduleTypeAPI sfcProviderScheduleTypeAPI=SfcProviderScheduleTypeAPI.getReadAll(sfstObj,sfstClass);
                Future future=odlSfc.getExecutor().submit(sfcProviderScheduleTypeAPI);
                if (future.get() != null) {
                  ServiceFunctionSchedulerTypes serviceFunctionSchedulerTypes=(ServiceFunctionSchedulerTypes)future.get();
                  List<ServiceFunctionSchedulerType> sfScheduleTypeList=serviceFunctionSchedulerTypes.getServiceFunctionSchedulerType();
                  for (                  ServiceFunctionSchedulerType sfst : sfScheduleTypeList) {
                    if (sfst.isEnabled() == true) {
                      if (!(sfst.getType().equals(updatedServiceFunctionSchedulerType.getType()))) {
                        ServiceFunctionSchedulerType sfstUpdate=new ServiceFunctionSchedulerTypeBuilder().setName(sfst.getName()).setType(sfst.getType()).setEnabled(false).build();
                        Object[] serviceFunctionSchedulerTypeObj={sfstUpdate};
                        Class[] serviceFunctionSchedulerTypeClass={ServiceFunctionSchedulerType.class};
                        future=odlSfc.getExecutor().submit(SfcProviderScheduleTypeAPI.getPut(serviceFunctionSchedulerTypeObj,serviceFunctionSchedulerTypeClass));
                        break;
                      }
                    }
                  }
                }
              }
            }
 else             if (isCreateTrue == true) {
              isCreateTrue=false;
            }
          }
 catch (          InterruptedException e) {
            LOG.warn(""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
      }
    }
  finally {
      SfcConcurrencyAPI.releaseLock();
    }
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  printTraceStop(LOG);
}",0.326040597109233
132114,"@Override public java.lang.AutoCloseable createInstance(){
  final OpendaylightSfc opendaylightSfc=OpendaylightSfc.getOpendaylightSfcObj();
  final SbRestSfEntryDataListener sbRestSfEntryDataListener=new SbRestSfEntryDataListener(opendaylightSfc);
  final SbRestSfgEntryDataListener sbRestSfgEntryDataListener=new SbRestSfgEntryDataListener(opendaylightSfc);
  final SbRestSffEntryDataListener sbRestSffEntryDataListener=new SbRestSffEntryDataListener(opendaylightSfc);
  final SbRestRspEntryDataListener sbRestRspEntryDataListener=new SbRestRspEntryDataListener(opendaylightSfc);
  final SbRestAclEntryDataListener sbRestAclEntryDataListener=new SbRestAclEntryDataListener(opendaylightSfc);
  final SbRestScfEntryDataListener sbRestScfEntryDataListener=new SbRestScfEntryDataListener(opendaylightSfc);
  final SbRestKeepAliveSocket sbRestKeepAliveSocket=new SbRestKeepAliveSocket();
  opendaylightSfc.getExecutor().execute(sbRestKeepAliveSocket);
final class AutoCloseableSfcSbRest implements AutoCloseable {
    @Override public void close(){
      sbRestSfEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSfgEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSffEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestRspEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestAclEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestScfEntryDataListener.getDataChangeListenerRegistration().close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  return new AutoCloseableSfcSbRest();
}","@Override public java.lang.AutoCloseable createInstance(){
  final OpendaylightSfc opendaylightSfc=OpendaylightSfc.getOpendaylightSfcObj();
  final SbRestSfEntryDataListener sbRestSfEntryDataListener=new SbRestSfEntryDataListener(opendaylightSfc);
  final SbRestSfgEntryDataListener sbRestSfgEntryDataListener=new SbRestSfgEntryDataListener(opendaylightSfc);
  final SbRestSffEntryDataListener sbRestSffEntryDataListener=new SbRestSffEntryDataListener(opendaylightSfc);
  final SbRestRspEntryDataListener sbRestRspEntryDataListener=new SbRestRspEntryDataListener(opendaylightSfc);
  final SbRestAclEntryDataListener sbRestAclEntryDataListener=new SbRestAclEntryDataListener(opendaylightSfc);
  final SbRestScfEntryDataListener sbRestScfEntryDataListener=new SbRestScfEntryDataListener(opendaylightSfc);
  final SbRestSfstEntryDataListener sbRestSfstEntryDataListener=new SbRestSfstEntryDataListener(opendaylightSfc);
  final SbRestKeepAliveSocket sbRestKeepAliveSocket=new SbRestKeepAliveSocket();
  opendaylightSfc.getExecutor().execute(sbRestKeepAliveSocket);
final class AutoCloseableSfcSbRest implements AutoCloseable {
    @Override public void close(){
      sbRestSfEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSfgEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSffEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestRspEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestAclEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestScfEntryDataListener.getDataChangeListenerRegistration().close();
      sbRestSfstEntryDataListener.getDataChangeListenerRegistration().close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  return new AutoCloseableSfcSbRest();
}",0.9481600859521891
132115,"@Override public void close(){
  sbRestSfEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSfgEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSffEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestRspEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestAclEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestScfEntryDataListener.getDataChangeListenerRegistration().close();
  try {
    opendaylightSfc.close();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",this);
  }
  LOG.info(""String_Node_Str"",this);
}","@Override public void close(){
  sbRestSfEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSfgEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSffEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestRspEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestAclEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestScfEntryDataListener.getDataChangeListenerRegistration().close();
  sbRestSfstEntryDataListener.getDataChangeListenerRegistration().close();
  try {
    opendaylightSfc.close();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",this);
  }
  LOG.info(""String_Node_Str"",this);
}",0.9454545454545454
132116,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node originalNode=(Node)entry.getValue();
      LOG.debug(""String_Node_Str"",originalNode.toString());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node createdNode=(Node)entry.getValue();
      LOG.debug(""String_Node_Str"",createdNode.toString());
      ServiceFunctionForwarder serviceFunctionForwarder=SfcOvsToSffMappingAPI.buildServiceFunctionForwarderFromNode(createdNode);
      if ((serviceFunctionForwarder != null) && SfcProviderServiceForwarderAPI.putServiceFunctionForwarderExecutor(serviceFunctionForwarder)) {
        LOG.info(""String_Node_Str"",serviceFunctionForwarder.getName());
      }
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedObject.entrySet()) {
    if ((entry.getValue() instanceof Node) && (!dataCreatedObject.containsKey(entry.getKey()))) {
      Node updatedNode=(Node)entry.getValue();
      LOG.debug(""String_Node_Str"",updatedNode.toString());
      ServiceFunctionForwarder serviceFunctionForwarder=SfcOvsToSffMappingAPI.buildServiceFunctionForwarderFromNode(updatedNode);
      if ((serviceFunctionForwarder != null) && SfcProviderServiceForwarderAPI.updateServiceFunctionForwarderExecutor(serviceFunctionForwarder)) {
        LOG.info(""String_Node_Str"",serviceFunctionForwarder.getName());
      }
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof Node) {
      Node deletedNode=(Node)dataObject;
      LOG.debug(""String_Node_Str"",deletedNode.toString());
      String sffName=SfcOvsToSffMappingAPI.getServiceForwarderNameFromNode(deletedNode);
      if (SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarderExecutor(sffName)) {
        LOG.info(""String_Node_Str"",sffName);
      }
    }
 else     if (dataObject instanceof OvsdbBridgeAugmentation) {
      OvsdbBridgeAugmentation deletedBridge=(OvsdbBridgeAugmentation)dataObject;
      LOG.error(""String_Node_Str"",deletedBridge.toString());
      KeyedInstanceIdentifier keyedInstanceIdentifier=(KeyedInstanceIdentifier)instanceIdentifier.firstIdentifierOf(Node.class);
      if (keyedInstanceIdentifier != null) {
        NodeKey nodeKey=(NodeKey)keyedInstanceIdentifier.getKey();
        String nodeId=nodeKey.getNodeId().getValue();
        if (SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarderExecutor(nodeId)) {
          LOG.info(""String_Node_Str"",nodeId);
        }
      }
    }
 else     if (dataObject instanceof OvsdbTerminationPointAugmentation) {
      OvsdbTerminationPointAugmentation deletedTerminationPoint=(OvsdbTerminationPointAugmentation)dataObject;
      LOG.error(""String_Node_Str"",deletedTerminationPoint.toString());
      KeyedInstanceIdentifier keyedInstanceIdentifier=(KeyedInstanceIdentifier)instanceIdentifier.firstIdentifierOf(Node.class);
      if (keyedInstanceIdentifier != null) {
        NodeKey nodeKey=(NodeKey)keyedInstanceIdentifier.getKey();
        String nodeId=nodeKey.getNodeId().getValue();
        if (SfcProviderServiceForwarderAPI.deleteSffDataPlaneLocatorExecutor(nodeId,deletedTerminationPoint.getName())) {
          LOG.info(""String_Node_Str"",deletedTerminationPoint.getName());
        }
      }
    }
  }
  printTraceStop(LOG);
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node originalNode=(Node)entry.getValue();
      LOG.debug(""String_Node_Str"",originalNode.toString());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node createdNode=(Node)entry.getValue();
      LOG.debug(""String_Node_Str"",createdNode.toString());
      OvsdbNodeAugmentation ovsdbNodeAugmentation=createdNode.getAugmentation(OvsdbNodeAugmentation.class);
      if (ovsdbNodeAugmentation != null) {
        final ConnectionInfo connectionInfo=ovsdbNodeAugmentation.getConnectionInfo();
        if (connectionInfo != null) {
          CheckedFuture<Optional<ServiceFunctionForwarders>,ReadFailedException> exitsingSffs=readServiceFunctionForwarders();
          Futures.addCallback(exitsingSffs,new FutureCallback<Optional<ServiceFunctionForwarders>>(){
            @Override public void onSuccess(            Optional<ServiceFunctionForwarders> optionalSffs){
              if (optionalSffs.isPresent()) {
                ServiceFunctionForwarder sff=findSffByIp(optionalSffs.get(),connectionInfo.getRemoteIp());
                if (sff != null) {
                  SfcOvsSffEntryDataListener.addOvsdbAugmentations(sff,opendaylightSfc.getExecutor());
                }
              }
            }
            @Override public void onFailure(            Throwable t){
              LOG.error(""String_Node_Str"");
            }
          }
);
        }
      }
    }
  }
  printTraceStop(LOG);
}",0.3147088522383036
132117,"public SfcOvsNodeDataListener(OpendaylightSfc opendaylightSfc){
  setOpendaylightSfc(opendaylightSfc);
  setDataBroker(opendaylightSfc.getDataProvider());
  setInstanceIdentifier(OVSDB_NODE_AUGMENTATION_INSTANCE_IDENTIFIER);
  setDataStoreType(LogicalDatastoreType.OPERATIONAL);
}","public SfcOvsNodeDataListener(OpendaylightSfc opendaylightSfc){
  setOpendaylightSfc(opendaylightSfc);
  setDataBroker(opendaylightSfc.getDataProvider());
  setInstanceIdentifier(OVSDB_NODE_AUGMENTATION_INSTANCE_IDENTIFIER);
  setDataStoreType(LogicalDatastoreType.OPERATIONAL);
  registerAsDataChangeListener(DataBroker.DataChangeScope.BASE);
}",0.896
132118,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
      LOG.debug(""String_Node_Str"",serviceFunctionForwarder.toString());
      OvsdbBridgeAugmentation ovsdbBridge=SfcSffToOvsMappingAPI.buildOvsdbBridgeAugmentation(serviceFunctionForwarder,opendaylightSfc.getExecutor());
      if (ovsdbBridge != null) {
        SfcOvsUtil.putOvsdbBridge(ovsdbBridge,opendaylightSfc.getExecutor());
        SfcOvsUtil.putOvsdbTerminationPoints(ovsdbBridge,serviceFunctionForwarder.getSffDataPlaneLocator(),opendaylightSfc.getExecutor());
      }
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunctionForwarder) && (!dataCreatedObject.containsKey(entry.getKey()))) {
      ServiceFunctionForwarder updatedServiceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
      LOG.debug(""String_Node_Str"",updatedServiceFunctionForwarder.toString());
      OvsdbBridgeAugmentation ovsdbBridge=SfcSffToOvsMappingAPI.buildOvsdbBridgeAugmentation(updatedServiceFunctionForwarder,opendaylightSfc.getExecutor());
      if (ovsdbBridge != null) {
        SfcOvsUtil.putOvsdbBridge(ovsdbBridge,opendaylightSfc.getExecutor());
        SfcOvsUtil.putOvsdbTerminationPoints(ovsdbBridge,updatedServiceFunctionForwarder.getSffDataPlaneLocator(),opendaylightSfc.getExecutor());
      }
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder deletedServiceFunctionForwarder=(ServiceFunctionForwarder)dataObject;
      LOG.debug(""String_Node_Str"",deletedServiceFunctionForwarder.toString());
      KeyedInstanceIdentifier keyedInstanceIdentifier=(KeyedInstanceIdentifier)instanceIdentifier.firstIdentifierOf(ServiceFunctionForwarder.class);
      if (keyedInstanceIdentifier != null) {
        ServiceFunctionForwarderKey sffKey=(ServiceFunctionForwarderKey)keyedInstanceIdentifier.getKey();
        String sffName=sffKey.getName();
        SfcOvsUtil.deleteOvsdbNode(SfcOvsUtil.buildOvsdbNodeIID(sffName),opendaylightSfc.getExecutor());
      }
    }
 else     if (dataObject instanceof SffDataPlaneLocator) {
      SffDataPlaneLocator sffDataPlaneLocator=(SffDataPlaneLocator)dataObject;
      LOG.debug(""String_Node_Str"",sffDataPlaneLocator.getName());
      KeyedInstanceIdentifier keyedInstanceIdentifier=(KeyedInstanceIdentifier)instanceIdentifier.firstIdentifierOf(ServiceFunctionForwarder.class);
      if (keyedInstanceIdentifier != null) {
        ServiceFunctionForwarderKey sffKey=(ServiceFunctionForwarderKey)keyedInstanceIdentifier.getKey();
        String sffName=sffKey.getName();
        SfcOvsUtil.deleteOvsdbTerminationPoint(SfcOvsUtil.buildOvsdbTerminationPointIID(sffName,sffDataPlaneLocator.getName()),opendaylightSfc.getExecutor());
      }
    }
  }
  printTraceStop(LOG);
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
      LOG.debug(""String_Node_Str"",serviceFunctionForwarder.toString());
      addOvsdbAugmentations(serviceFunctionForwarder,opendaylightSfc.getExecutor());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunctionForwarder) && (!dataCreatedObject.containsKey(entry.getKey()))) {
      ServiceFunctionForwarder updatedServiceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
      LOG.debug(""String_Node_Str"",updatedServiceFunctionForwarder.toString());
      addOvsdbAugmentations(updatedServiceFunctionForwarder,opendaylightSfc.getExecutor());
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder deletedServiceFunctionForwarder=(ServiceFunctionForwarder)dataObject;
      LOG.debug(""String_Node_Str"",deletedServiceFunctionForwarder.toString());
      KeyedInstanceIdentifier keyedInstanceIdentifier=(KeyedInstanceIdentifier)instanceIdentifier.firstIdentifierOf(ServiceFunctionForwarder.class);
      if (keyedInstanceIdentifier != null) {
        ServiceFunctionForwarderKey sffKey=(ServiceFunctionForwarderKey)keyedInstanceIdentifier.getKey();
        String sffName=sffKey.getName();
        SfcOvsUtil.deleteOvsdbNode(SfcOvsUtil.buildOvsdbNodeIID(sffName),opendaylightSfc.getExecutor());
      }
    }
 else     if (dataObject instanceof SffDataPlaneLocator) {
      SffDataPlaneLocator sffDataPlaneLocator=(SffDataPlaneLocator)dataObject;
      LOG.debug(""String_Node_Str"",sffDataPlaneLocator.getName());
      KeyedInstanceIdentifier keyedInstanceIdentifier=(KeyedInstanceIdentifier)instanceIdentifier.firstIdentifierOf(ServiceFunctionForwarder.class);
      if (keyedInstanceIdentifier != null) {
        ServiceFunctionForwarderKey sffKey=(ServiceFunctionForwarderKey)keyedInstanceIdentifier.getKey();
        String sffName=sffKey.getName();
        SfcOvsUtil.deleteOvsdbTerminationPoint(SfcOvsUtil.buildOvsdbTerminationPointIID(sffName,sffDataPlaneLocator.getName()),opendaylightSfc.getExecutor());
      }
    }
  }
  printTraceStop(LOG);
}",0.8960941034534761
132119,"@Before public void before() throws ExecutionException, InterruptedException {
  DataBroker dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  scheduler=new SfcServiceFunctionRoundRobinSchedulerAPI();
  executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
  Thread.sleep(1000);
}","@Before public void before() throws InterruptedException, IllegalAccessException {
  DataBroker dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
  Thread.sleep(1000);
  Whitebox.getField(SfcServiceFunctionRoundRobinSchedulerAPI.class,""String_Node_Str"").set(HashMap.class,new HashMap<>());
}",0.6794871794871795
132120,"@Test public void testServiceFunctionRoundRobinScheduler(){
  List<String> result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,createServiceFunctionPath());
  List<String> serviceFunctions=new ArrayList<>();
  serviceFunctions.add(SF_NAME + 1);
  serviceFunctions.add(SF_NAME + 2);
  serviceFunctions.add(SF_NAME + 3);
  assertNotNull(""String_Node_Str"",result);
  assertTrue(""String_Node_Str"",result.containsAll(serviceFunctions));
}","@Test public void testServiceFunctionRoundRobinScheduler(){
  SfcServiceFunctionRoundRobinSchedulerAPI scheduler=new SfcServiceFunctionRoundRobinSchedulerAPI();
  List<String> result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,createServiceFunctionPath());
  List<String> serviceFunctions=new ArrayList<>();
  serviceFunctions.add(SF_NAME + 1);
  serviceFunctions.add(SF_NAME + 2);
  serviceFunctions.add(SF_NAME + 3);
  assertNotNull(""String_Node_Str"",result);
  assertTrue(""String_Node_Str"",result.containsAll(serviceFunctions));
}",0.9002961500493584
132121,"@Test public void testServiceFunctionRoundRobinScheduler1(){
  ServiceFunctionPathBuilder serviceFunctionPathBuilder=new ServiceFunctionPathBuilder();
  List<String> result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,serviceFunctionPathBuilder.build());
  assertNull(""String_Node_Str"",result);
  boolean transactionSuccessful=writeTypes();
  assertTrue(""String_Node_Str"",transactionSuccessful);
  result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,serviceFunctionPathBuilder.build());
  List<String> serviceFunctionTypes=new ArrayList<>();
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  assertNotNull(""String_Node_Str"",result);
  assertTrue(""String_Node_Str"",result.containsAll(serviceFunctionTypes));
}","@Test public void testServiceFunctionRoundRobinScheduler1(){
  SfcServiceFunctionRoundRobinSchedulerAPI scheduler=new SfcServiceFunctionRoundRobinSchedulerAPI();
  ServiceFunctionPathBuilder serviceFunctionPathBuilder=new ServiceFunctionPathBuilder();
  List<String> result;
  boolean transactionSuccessful=writeTypes();
  assertTrue(""String_Node_Str"",transactionSuccessful);
  result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,serviceFunctionPathBuilder.build());
  List<String> serviceFunctionTypes=new ArrayList<>();
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  assertNotNull(""String_Node_Str"",result);
  assertTrue(""String_Node_Str"",result.containsAll(serviceFunctionTypes));
}",0.5423728813559322
132122,"@Before public void before() throws ExecutionException, InterruptedException {
  DataBroker dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  int maxTries=10;
  ServiceFunctionType serviceFunctionType;
  List<SftServiceFunctionName> sftServiceFunctionNameList;
  boolean emptyFlag=true;
  while (maxTries > 0) {
    emptyFlag=true;
    executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
    Thread.sleep(1000);
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Firewall.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Dpi.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Napt44.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    maxTries--;
    if (emptyFlag) {
      break;
    }
  }
  LOG.debug(""String_Node_Str"",10 - maxTries,emptyFlag ? ""String_Node_Str"" : ""String_Node_Str"");
  String sfcName=""String_Node_Str"";
  List<SfcServiceFunction> sfcServiceFunctionList=new ArrayList<>();
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Firewall.class).build());
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Dpi.class).build());
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Napt44.class).build());
  sfChain=new ServiceFunctionChainBuilder().setName(sfcName).setKey(new ServiceFunctionChainKey(sfcName)).setSfcServiceFunction(sfcServiceFunctionList).build();
  ServiceFunctionPathBuilder serviceFunctionPathBuilder=new ServiceFunctionPathBuilder();
  serviceFunctionPathBuilder.setKey(new ServiceFunctionPathKey(""String_Node_Str""));
  serviceFunctionPathBuilder.setPathId(1l);
  serviceFunctionPathBuilder.setServiceChainName(sfcName);
  List<ServicePathHop> sphs=new ArrayList<>();
  serviceFunctionPathBuilder.setServicePathHop(sphs);
  sfPath=serviceFunctionPathBuilder.build();
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),555),""String_Node_Str"",VxlanGpe.class));
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),666),""String_Node_Str"",VxlanGpe.class));
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),777),""String_Node_Str"",VxlanGpe.class));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  ServiceFunctionsBuilder sfsBuilder=new ServiceFunctionsBuilder();
  sfsBuilder.setServiceFunction(sfList);
  executor.submit(SfcProviderServiceFunctionAPI.getPutAll(new Object[]{sfsBuilder.build()},new Class[]{ServiceFunctions.class})).get();
  Thread.sleep(1000);
  for (  ServiceFunction serviceFunction : sfList) {
    SfcProviderServiceTypeAPI.createServiceFunctionTypeEntryExecutor(serviceFunction);
  }
  Object[] sfcParameters={sfChain};
  Class[] sfcParameterTypes={ServiceFunctionChain.class};
  executor.submit(SfcProviderServiceChainAPI.getPut(sfcParameters,sfcParameterTypes)).get();
  Thread.sleep(1000);
}","@Before public void before() throws ExecutionException, InterruptedException, IllegalAccessException {
  DataBroker dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  int maxTries=10;
  ServiceFunctionType serviceFunctionType;
  List<SftServiceFunctionName> sftServiceFunctionNameList;
  boolean emptyFlag=true;
  while (maxTries > 0) {
    emptyFlag=true;
    executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
    Thread.sleep(1000);
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Firewall.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Dpi.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Napt44.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    maxTries--;
    if (emptyFlag) {
      break;
    }
  }
  Whitebox.getField(SfcServiceFunctionRoundRobinSchedulerAPI.class,""String_Node_Str"").set(HashMap.class,new HashMap<>());
  LOG.debug(""String_Node_Str"",10 - maxTries,emptyFlag ? ""String_Node_Str"" : ""String_Node_Str"");
  String sfcName=""String_Node_Str"";
  List<SfcServiceFunction> sfcServiceFunctionList=new ArrayList<>();
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Firewall.class).build());
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Dpi.class).build());
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Napt44.class).build());
  sfChain=new ServiceFunctionChainBuilder().setName(sfcName).setKey(new ServiceFunctionChainKey(sfcName)).setSfcServiceFunction(sfcServiceFunctionList).build();
  ServiceFunctionPathBuilder serviceFunctionPathBuilder=new ServiceFunctionPathBuilder();
  serviceFunctionPathBuilder.setKey(new ServiceFunctionPathKey(""String_Node_Str""));
  serviceFunctionPathBuilder.setPathId(1l);
  serviceFunctionPathBuilder.setServiceChainName(sfcName);
  List<ServicePathHop> sphs=new ArrayList<>();
  serviceFunctionPathBuilder.setServicePathHop(sphs);
  sfPath=serviceFunctionPathBuilder.build();
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),555),""String_Node_Str"",VxlanGpe.class));
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),666),""String_Node_Str"",VxlanGpe.class));
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),777),""String_Node_Str"",VxlanGpe.class));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  ServiceFunctionsBuilder sfsBuilder=new ServiceFunctionsBuilder();
  sfsBuilder.setServiceFunction(sfList);
  executor.submit(SfcProviderServiceFunctionAPI.getPutAll(new Object[]{sfsBuilder.build()},new Class[]{ServiceFunctions.class})).get();
  Thread.sleep(1000);
  for (  ServiceFunction serviceFunction : sfList) {
    SfcProviderServiceTypeAPI.createServiceFunctionTypeEntryExecutor(serviceFunction);
  }
  Object[] sfcParameters={sfChain};
  Class[] sfcParameterTypes={ServiceFunctionChain.class};
  executor.submit(SfcProviderServiceChainAPI.getPut(sfcParameters,sfcParameterTypes)).get();
  Thread.sleep(1000);
}",0.9879835390946502
132123,"public static RemoveMappingInput buildRemoveMappingInput(LispAddressContainer eid){
  RemoveMappingInputBuilder rmib=new RemoveMappingInputBuilder();
  rmib.setLispAddressContainer(eid);
  return rmib.build();
}","public static RemoveMappingInput buildRemoveMappingInput(LispAddressContainer eid,int mask){
  RemoveMappingInputBuilder rmib=new RemoveMappingInputBuilder();
  rmib.setLispAddressContainer(eid);
  rmib.setMaskLength((short)mask);
  return rmib.build();
}",0.8755364806866953
132124,"private boolean removeLispMapping(LispAddressContainer eid){
  Preconditions.checkNotNull(eid,""String_Node_Str"");
  try {
    LOG.trace(""String_Node_Str"",eid);
    Future<RpcResult<Void>> result=lfmService.removeMapping(LispUtil.buildRemoveMappingInput(eid));
    result.get().getResult();
    return true;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",eid);
  }
  return false;
}","private boolean removeLispMapping(LispAddressContainer eid){
  Preconditions.checkNotNull(eid,""String_Node_Str"");
  try {
    LOG.trace(""String_Node_Str"",eid);
    Future<RpcResult<Void>> result=lfmService.removeMapping(LispUtil.buildRemoveMappingInput(eid,0));
    result.get().getResult();
    return true;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",eid,e);
  }
  return false;
}",0.9949494949494948
132125,"public static RemoveMappingInput buildRemoveMappingInput(LispAddressContainer eid){
  RemoveMappingInputBuilder rmib=new RemoveMappingInputBuilder();
  rmib.setLispAddressContainer(eid);
  return rmib.build();
}","public static RemoveMappingInput buildRemoveMappingInput(LispAddressContainer eid,int mask){
  RemoveMappingInputBuilder rmib=new RemoveMappingInputBuilder();
  rmib.setLispAddressContainer(eid);
  rmib.setMaskLength((short)mask);
  return rmib.build();
}",0.8755364806866953
132126,"private boolean removeLispMapping(LispAddressContainer eid){
  Preconditions.checkNotNull(eid,""String_Node_Str"");
  try {
    LOG.trace(""String_Node_Str"",eid);
    Future<RpcResult<Void>> result=lfmService.removeMapping(LispUtil.buildRemoveMappingInput(eid));
    result.get().getResult();
    return true;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",eid);
  }
  return false;
}","private boolean removeLispMapping(LispAddressContainer eid){
  Preconditions.checkNotNull(eid,""String_Node_Str"");
  try {
    LOG.trace(""String_Node_Str"",eid);
    Future<RpcResult<Void>> result=lfmService.removeMapping(LispUtil.buildRemoveMappingInput(eid,0));
    result.get().getResult();
    return true;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",eid,e);
  }
  return false;
}",0.9949494949494948
132127,"@Before public void before() throws ExecutionException, InterruptedException {
  DataBroker dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  scheduler=new SfcServiceFunctionRoundRobinSchedulerAPI();
  executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
  Thread.sleep(1000);
}","@Before public void before() throws InterruptedException, IllegalAccessException {
  DataBroker dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
  Thread.sleep(1000);
  Whitebox.getField(SfcServiceFunctionRoundRobinSchedulerAPI.class,""String_Node_Str"").set(HashMap.class,new HashMap<>());
}",0.6794871794871795
132128,"@Test public void testServiceFunctionRoundRobinScheduler(){
  List<String> result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,createServiceFunctionPath());
  List<String> serviceFunctions=new ArrayList<>();
  serviceFunctions.add(SF_NAME + 1);
  serviceFunctions.add(SF_NAME + 2);
  serviceFunctions.add(SF_NAME + 3);
  assertNotNull(""String_Node_Str"",result);
  assertTrue(""String_Node_Str"",result.containsAll(serviceFunctions));
}","@Test public void testServiceFunctionRoundRobinScheduler(){
  SfcServiceFunctionRoundRobinSchedulerAPI scheduler=new SfcServiceFunctionRoundRobinSchedulerAPI();
  List<String> result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,createServiceFunctionPath());
  List<String> serviceFunctions=new ArrayList<>();
  serviceFunctions.add(SF_NAME + 1);
  serviceFunctions.add(SF_NAME + 2);
  serviceFunctions.add(SF_NAME + 3);
  assertNotNull(""String_Node_Str"",result);
  assertTrue(""String_Node_Str"",result.containsAll(serviceFunctions));
}",0.9002961500493584
132129,"@Test public void testServiceFunctionRoundRobinScheduler1(){
  ServiceFunctionPathBuilder serviceFunctionPathBuilder=new ServiceFunctionPathBuilder();
  List<String> result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,serviceFunctionPathBuilder.build());
  assertNull(""String_Node_Str"",result);
  boolean transactionSuccessful=writeTypes();
  assertTrue(""String_Node_Str"",transactionSuccessful);
  result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,serviceFunctionPathBuilder.build());
  List<String> serviceFunctionTypes=new ArrayList<>();
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  assertNotNull(""String_Node_Str"",result);
  assertTrue(""String_Node_Str"",result.containsAll(serviceFunctionTypes));
}","@Test public void testServiceFunctionRoundRobinScheduler1(){
  SfcServiceFunctionRoundRobinSchedulerAPI scheduler=new SfcServiceFunctionRoundRobinSchedulerAPI();
  ServiceFunctionPathBuilder serviceFunctionPathBuilder=new ServiceFunctionPathBuilder();
  List<String> result;
  boolean transactionSuccessful=writeTypes();
  assertTrue(""String_Node_Str"",transactionSuccessful);
  result=scheduler.scheduleServiceFunctions(createServiceFunctionChain(),255,serviceFunctionPathBuilder.build());
  List<String> serviceFunctionTypes=new ArrayList<>();
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  serviceFunctionTypes.add(SF_NAME + ""String_Node_Str"");
  assertNotNull(""String_Node_Str"",result);
  assertTrue(""String_Node_Str"",result.containsAll(serviceFunctionTypes));
}",0.5423728813559322
132130,"@Before public void before() throws ExecutionException, InterruptedException {
  DataBroker dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  int maxTries=10;
  ServiceFunctionType serviceFunctionType;
  List<SftServiceFunctionName> sftServiceFunctionNameList;
  boolean emptyFlag=true;
  while (maxTries > 0) {
    emptyFlag=true;
    executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
    Thread.sleep(1000);
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Firewall.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Dpi.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Napt44.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    maxTries--;
    if (emptyFlag) {
      break;
    }
  }
  LOG.debug(""String_Node_Str"",10 - maxTries,emptyFlag ? ""String_Node_Str"" : ""String_Node_Str"");
  String sfcName=""String_Node_Str"";
  List<SfcServiceFunction> sfcServiceFunctionList=new ArrayList<>();
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Firewall.class).build());
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Dpi.class).build());
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Napt44.class).build());
  sfChain=new ServiceFunctionChainBuilder().setName(sfcName).setKey(new ServiceFunctionChainKey(sfcName)).setSfcServiceFunction(sfcServiceFunctionList).build();
  ServiceFunctionPathBuilder serviceFunctionPathBuilder=new ServiceFunctionPathBuilder();
  serviceFunctionPathBuilder.setKey(new ServiceFunctionPathKey(""String_Node_Str""));
  serviceFunctionPathBuilder.setPathId(1l);
  serviceFunctionPathBuilder.setServiceChainName(sfcName);
  List<ServicePathHop> sphs=new ArrayList<>();
  serviceFunctionPathBuilder.setServicePathHop(sphs);
  sfPath=serviceFunctionPathBuilder.build();
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),555),""String_Node_Str"",VxlanGpe.class));
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),666),""String_Node_Str"",VxlanGpe.class));
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),777),""String_Node_Str"",VxlanGpe.class));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  ServiceFunctionsBuilder sfsBuilder=new ServiceFunctionsBuilder();
  sfsBuilder.setServiceFunction(sfList);
  executor.submit(SfcProviderServiceFunctionAPI.getPutAll(new Object[]{sfsBuilder.build()},new Class[]{ServiceFunctions.class})).get();
  Thread.sleep(1000);
  for (  ServiceFunction serviceFunction : sfList) {
    SfcProviderServiceTypeAPI.createServiceFunctionTypeEntryExecutor(serviceFunction);
  }
  Object[] sfcParameters={sfChain};
  Class[] sfcParameterTypes={ServiceFunctionChain.class};
  executor.submit(SfcProviderServiceChainAPI.getPut(sfcParameters,sfcParameterTypes)).get();
  Thread.sleep(1000);
}","@Before public void before() throws ExecutionException, InterruptedException, IllegalAccessException {
  DataBroker dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  int maxTries=10;
  ServiceFunctionType serviceFunctionType;
  List<SftServiceFunctionName> sftServiceFunctionNameList;
  boolean emptyFlag=true;
  while (maxTries > 0) {
    emptyFlag=true;
    executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
    executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
    Thread.sleep(1000);
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Firewall.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Dpi.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    serviceFunctionType=SfcProviderServiceTypeAPI.readServiceFunctionTypeExecutor(Napt44.class);
    if (serviceFunctionType != null) {
      sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
      if (sftServiceFunctionNameList.size() != 0) {
        emptyFlag=false;
      }
    }
    maxTries--;
    if (emptyFlag) {
      break;
    }
  }
  Whitebox.getField(SfcServiceFunctionRoundRobinSchedulerAPI.class,""String_Node_Str"").set(HashMap.class,new HashMap<>());
  LOG.debug(""String_Node_Str"",10 - maxTries,emptyFlag ? ""String_Node_Str"" : ""String_Node_Str"");
  String sfcName=""String_Node_Str"";
  List<SfcServiceFunction> sfcServiceFunctionList=new ArrayList<>();
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Firewall.class).build());
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Dpi.class).build());
  sfcServiceFunctionList.add(new SfcServiceFunctionBuilder().setName(""String_Node_Str"").setKey(new SfcServiceFunctionKey(""String_Node_Str"")).setType(Napt44.class).build());
  sfChain=new ServiceFunctionChainBuilder().setName(sfcName).setKey(new ServiceFunctionChainKey(sfcName)).setSfcServiceFunction(sfcServiceFunctionList).build();
  ServiceFunctionPathBuilder serviceFunctionPathBuilder=new ServiceFunctionPathBuilder();
  serviceFunctionPathBuilder.setKey(new ServiceFunctionPathKey(""String_Node_Str""));
  serviceFunctionPathBuilder.setPathId(1l);
  serviceFunctionPathBuilder.setServiceChainName(sfcName);
  List<ServicePathHop> sphs=new ArrayList<>();
  serviceFunctionPathBuilder.setServicePathHop(sphs);
  sfPath=serviceFunctionPathBuilder.build();
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),555),""String_Node_Str"",VxlanGpe.class));
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),666),""String_Node_Str"",VxlanGpe.class));
  sfDPLList.add(SimpleTestEntityBuilder.buildSfDataPlaneLocator(""String_Node_Str"",SimpleTestEntityBuilder.buildLocatorTypeIp(new IpAddress(new Ipv4Address(""String_Node_Str"")),777),""String_Node_Str"",VxlanGpe.class));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Firewall.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Dpi.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(0),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(1),Boolean.FALSE));
  sfList.add(SimpleTestEntityBuilder.buildServiceFunction(""String_Node_Str"",Napt44.class,new IpAddress(new Ipv4Address(""String_Node_Str"")),sfDPLList.get(2),Boolean.FALSE));
  ServiceFunctionsBuilder sfsBuilder=new ServiceFunctionsBuilder();
  sfsBuilder.setServiceFunction(sfList);
  executor.submit(SfcProviderServiceFunctionAPI.getPutAll(new Object[]{sfsBuilder.build()},new Class[]{ServiceFunctions.class})).get();
  Thread.sleep(1000);
  for (  ServiceFunction serviceFunction : sfList) {
    SfcProviderServiceTypeAPI.createServiceFunctionTypeEntryExecutor(serviceFunction);
  }
  Object[] sfcParameters={sfChain};
  Class[] sfcParameterTypes={ServiceFunctionChain.class};
  executor.submit(SfcProviderServiceChainAPI.getPut(sfcParameters,sfcParameterTypes)).get();
  Thread.sleep(1000);
}",0.9879835390946502
132131,"@Override public Future<RpcResult<Void>> putServiceFunctionChains(PutServiceFunctionChainsInput input){
  printTraceStart(LOG);
  ServiceFunctionChainsBuilder builder=new ServiceFunctionChainsBuilder();
  builder=builder.setServiceFunctionChain(input.getServiceFunctionChain());
  ServiceFunctionChains sfcs=builder.build();
  if (dataBroker != null) {
    WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
    writeTx.merge(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFC_IID,sfcs,true);
    writeTx.commit();
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}","@Override public Future<RpcResult<Void>> putServiceFunctionChains(PutServiceFunctionChainsInput input){
  printTraceStart(LOG);
  ServiceFunctionChainsBuilder serviceFunctionChainsBuilder=new ServiceFunctionChainsBuilder();
  serviceFunctionChainsBuilder=serviceFunctionChainsBuilder.setServiceFunctionChain(input.getServiceFunctionChain());
  ServiceFunctionChains sfcs=serviceFunctionChainsBuilder.build();
  if (!SfcDataStoreAPI.writeMergeTransactionAPI(OpendaylightSfc.SFC_IID,sfcs,LogicalDatastoreType.CONFIGURATION)) {
    LOG.error(""String_Node_Str"",input.getServiceFunctionChain().toString());
  }
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}",0.4838709677419355
132132,"private ServiceFunctionChain findServiceFunctionChain(String name){
  ServiceFunctionChainKey key=new ServiceFunctionChainKey(name);
  InstanceIdentifier<ServiceFunctionChain> iid=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,key).toInstance();
  ReadOnlyTransaction readTx=dataBroker.newReadOnlyTransaction();
  Optional<ServiceFunctionChain> dataObject=null;
  try {
    dataObject=readTx.read(LogicalDatastoreType.CONFIGURATION,iid).get();
    if (dataObject != null) {
      return dataObject.get();
    }
 else {
      LOG.error(""String_Node_Str"");
      return null;
    }
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.error(""String_Node_Str"",e.getMessage());
    return null;
  }
}","@SuppressWarnings(""String_Node_Str"") private ServiceFunctionChain findServiceFunctionChain(String name){
  ServiceFunctionChainKey key=new ServiceFunctionChainKey(name);
  InstanceIdentifier<ServiceFunctionChain> serviceFunctionChainInstanceIdentifier=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,key).build();
  ServiceFunctionChain serviceFunctionChain=SfcDataStoreAPI.readTransactionAPI(serviceFunctionChainInstanceIdentifier,LogicalDatastoreType.CONFIGURATION);
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",name);
  }
  return serviceFunctionChain;
}",0.5527272727272727
132133,"/** 
 * This methong reads all the necessary information for the first hop of a Rendered Service Path by ServiceFunctionTypeIdentity list. <p>
 * @param input RPC input including a ServiceFunctionTypeIdentity list
 * @return RPC output including a renderedServicePathFirstHop.
 */
@Override public Future<RpcResult<ReadRspFirstHopBySftListOutput>> readRspFirstHopBySftList(ReadRspFirstHopBySftListInput input){
  RenderedServicePathFirstHop renderedServicePathFirstHop=null;
  renderedServicePathFirstHop=SfcProviderRenderedPathAPI.readRspFirstHopBySftList(input.getSfst(),input.getSftList());
  ReadRspFirstHopBySftListOutput readRspFirstHopBySftListOutput=null;
  if (renderedServicePathFirstHop != null) {
    ReadRspFirstHopBySftListOutputBuilder readRspFirstHopBySftListOutputBuilder=new ReadRspFirstHopBySftListOutputBuilder();
    readRspFirstHopBySftListOutputBuilder.setRenderedServicePathFirstHop(renderedServicePathFirstHop);
    readRspFirstHopBySftListOutput=readRspFirstHopBySftListOutputBuilder.build();
  }
  RpcResultBuilder<ReadRspFirstHopBySftListOutput> rpcResultBuilder=RpcResultBuilder.success(readRspFirstHopBySftListOutput);
  return Futures.immediateFuture(rpcResultBuilder.build());
}","/** 
 * This method reads all the necessary information for the first hop of a Rendered Service Path by ServiceFunctionTypeIdentity list. <p>
 * @param input RPC input including a ServiceFunctionTypeIdentity list
 * @return RPC output including a renderedServicePathFirstHop.
 */
@Override public Future<RpcResult<ReadRspFirstHopBySftListOutput>> readRspFirstHopBySftList(ReadRspFirstHopBySftListInput input){
  RenderedServicePathFirstHop renderedServicePathFirstHop=null;
  renderedServicePathFirstHop=SfcProviderRenderedPathAPI.readRspFirstHopBySftList(input.getSfst(),input.getSftList());
  ReadRspFirstHopBySftListOutput readRspFirstHopBySftListOutput=null;
  if (renderedServicePathFirstHop != null) {
    ReadRspFirstHopBySftListOutputBuilder readRspFirstHopBySftListOutputBuilder=new ReadRspFirstHopBySftListOutputBuilder();
    readRspFirstHopBySftListOutputBuilder.setRenderedServicePathFirstHop(renderedServicePathFirstHop);
    readRspFirstHopBySftListOutput=readRspFirstHopBySftListOutputBuilder.build();
  }
  RpcResultBuilder<ReadRspFirstHopBySftListOutput> rpcResultBuilder=RpcResultBuilder.success(readRspFirstHopBySftListOutput);
  return Futures.immediateFuture(rpcResultBuilder.build());
}",0.9987598181066556
132134,"/** 
 * This method is used to retrieve a Service Function Type from the DataStore <p>
 * @param serviceFunctionType Service Function Type abstract class
 * @return Service Function Type Object which contains a list of SF of this type
 */
protected ServiceFunctionType readServiceFunctionType(Class<? extends ServiceFunctionTypeIdentity> serviceFunctionType){
  printTraceStart(LOG);
  ServiceFunctionType sft=null;
  InstanceIdentifier<ServiceFunctionType> sftIID;
  ServiceFunctionTypeKey serviceFunctionTypeKey=new ServiceFunctionTypeKey(serviceFunctionType);
  sftIID=InstanceIdentifier.builder(ServiceFunctionTypes.class).child(ServiceFunctionType.class,serviceFunctionTypeKey).build();
  ReadOnlyTransaction readTx=dataBroker.newReadOnlyTransaction();
  Optional<ServiceFunctionType> serviceFunctionTypeOptional=null;
  try {
    serviceFunctionTypeOptional=readTx.read(LogicalDatastoreType.CONFIGURATION,sftIID).get();
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",serviceFunctionType);
  }
  if (serviceFunctionTypeOptional != null && serviceFunctionTypeOptional.isPresent()) {
    sft=serviceFunctionTypeOptional.get();
  }
  printTraceStop(LOG);
  return sft;
}","/** 
 * This method is used to retrieve a Service Function Type from the DataStore <p>
 * @param serviceFunctionType Service Function Type abstract class
 * @return Service Function Type Object which contains a list of SF of this type
 */
protected ServiceFunctionType readServiceFunctionType(Class<? extends ServiceFunctionTypeIdentity> serviceFunctionType){
  printTraceStart(LOG);
  ServiceFunctionType sft=null;
  InstanceIdentifier<ServiceFunctionType> sftIID;
  ServiceFunctionTypeKey serviceFunctionTypeKey=new ServiceFunctionTypeKey(serviceFunctionType);
  sftIID=InstanceIdentifier.builder(ServiceFunctionTypes.class).child(ServiceFunctionType.class,serviceFunctionTypeKey).build();
  sft=SfcDataStoreAPI.readTransactionAPI(sftIID,LogicalDatastoreType.CONFIGURATION);
  if (sft == null) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",serviceFunctionType);
  }
  printTraceStop(LOG);
  return sft;
}",0.7981395348837209
132135,"@Override public Future<RpcResult<Void>> putServiceFunctionChains(PutServiceFunctionChainsInput input){
  printTraceStart(LOG);
  ServiceFunctionChainsBuilder builder=new ServiceFunctionChainsBuilder();
  builder=builder.setServiceFunctionChain(input.getServiceFunctionChain());
  ServiceFunctionChains sfcs=builder.build();
  if (dataBroker != null) {
    WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
    writeTx.merge(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFC_IID,sfcs,true);
    writeTx.commit();
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}","@Override public Future<RpcResult<Void>> putServiceFunctionChains(PutServiceFunctionChainsInput input){
  printTraceStart(LOG);
  ServiceFunctionChainsBuilder serviceFunctionChainsBuilder=new ServiceFunctionChainsBuilder();
  serviceFunctionChainsBuilder=serviceFunctionChainsBuilder.setServiceFunctionChain(input.getServiceFunctionChain());
  ServiceFunctionChains sfcs=serviceFunctionChainsBuilder.build();
  if (!SfcDataStoreAPI.writeMergeTransactionAPI(OpendaylightSfc.SFC_IID,sfcs,LogicalDatastoreType.CONFIGURATION)) {
    LOG.error(""String_Node_Str"",input.getServiceFunctionChain().toString());
  }
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}",0.4838709677419355
132136,"private ServiceFunctionChain findServiceFunctionChain(String name){
  ServiceFunctionChainKey key=new ServiceFunctionChainKey(name);
  InstanceIdentifier<ServiceFunctionChain> iid=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,key).toInstance();
  ReadOnlyTransaction readTx=dataBroker.newReadOnlyTransaction();
  Optional<ServiceFunctionChain> dataObject=null;
  try {
    dataObject=readTx.read(LogicalDatastoreType.CONFIGURATION,iid).get();
    if (dataObject != null) {
      return dataObject.get();
    }
 else {
      LOG.error(""String_Node_Str"");
      return null;
    }
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.error(""String_Node_Str"",e.getMessage());
    return null;
  }
}","@SuppressWarnings(""String_Node_Str"") private ServiceFunctionChain findServiceFunctionChain(String name){
  ServiceFunctionChainKey key=new ServiceFunctionChainKey(name);
  InstanceIdentifier<ServiceFunctionChain> serviceFunctionChainInstanceIdentifier=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,key).build();
  ServiceFunctionChain serviceFunctionChain=SfcDataStoreAPI.readTransactionAPI(serviceFunctionChainInstanceIdentifier,LogicalDatastoreType.CONFIGURATION);
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",name);
  }
  return serviceFunctionChain;
}",0.5527272727272727
132137,"/** 
 * This methong reads all the necessary information for the first hop of a Rendered Service Path by ServiceFunctionTypeIdentity list. <p>
 * @param input RPC input including a ServiceFunctionTypeIdentity list
 * @return RPC output including a renderedServicePathFirstHop.
 */
@Override public Future<RpcResult<ReadRspFirstHopBySftListOutput>> readRspFirstHopBySftList(ReadRspFirstHopBySftListInput input){
  RenderedServicePathFirstHop renderedServicePathFirstHop=null;
  renderedServicePathFirstHop=SfcProviderRenderedPathAPI.readRspFirstHopBySftList(input.getSfst(),input.getSftList());
  ReadRspFirstHopBySftListOutput readRspFirstHopBySftListOutput=null;
  if (renderedServicePathFirstHop != null) {
    ReadRspFirstHopBySftListOutputBuilder readRspFirstHopBySftListOutputBuilder=new ReadRspFirstHopBySftListOutputBuilder();
    readRspFirstHopBySftListOutputBuilder.setRenderedServicePathFirstHop(renderedServicePathFirstHop);
    readRspFirstHopBySftListOutput=readRspFirstHopBySftListOutputBuilder.build();
  }
  RpcResultBuilder<ReadRspFirstHopBySftListOutput> rpcResultBuilder=RpcResultBuilder.success(readRspFirstHopBySftListOutput);
  return Futures.immediateFuture(rpcResultBuilder.build());
}","/** 
 * This method reads all the necessary information for the first hop of a Rendered Service Path by ServiceFunctionTypeIdentity list. <p>
 * @param input RPC input including a ServiceFunctionTypeIdentity list
 * @return RPC output including a renderedServicePathFirstHop.
 */
@Override public Future<RpcResult<ReadRspFirstHopBySftListOutput>> readRspFirstHopBySftList(ReadRspFirstHopBySftListInput input){
  RenderedServicePathFirstHop renderedServicePathFirstHop=null;
  renderedServicePathFirstHop=SfcProviderRenderedPathAPI.readRspFirstHopBySftList(input.getSfst(),input.getSftList());
  ReadRspFirstHopBySftListOutput readRspFirstHopBySftListOutput=null;
  if (renderedServicePathFirstHop != null) {
    ReadRspFirstHopBySftListOutputBuilder readRspFirstHopBySftListOutputBuilder=new ReadRspFirstHopBySftListOutputBuilder();
    readRspFirstHopBySftListOutputBuilder.setRenderedServicePathFirstHop(renderedServicePathFirstHop);
    readRspFirstHopBySftListOutput=readRspFirstHopBySftListOutputBuilder.build();
  }
  RpcResultBuilder<ReadRspFirstHopBySftListOutput> rpcResultBuilder=RpcResultBuilder.success(readRspFirstHopBySftListOutput);
  return Futures.immediateFuture(rpcResultBuilder.build());
}",0.9987598181066556
132138,"/** 
 * This method is used to retrieve a Service Function Type from the DataStore <p>
 * @param serviceFunctionType Service Function Type abstract class
 * @return Service Function Type Object which contains a list of SF of this type
 */
protected ServiceFunctionType readServiceFunctionType(Class<? extends ServiceFunctionTypeIdentity> serviceFunctionType){
  printTraceStart(LOG);
  ServiceFunctionType sft=null;
  InstanceIdentifier<ServiceFunctionType> sftIID;
  ServiceFunctionTypeKey serviceFunctionTypeKey=new ServiceFunctionTypeKey(serviceFunctionType);
  sftIID=InstanceIdentifier.builder(ServiceFunctionTypes.class).child(ServiceFunctionType.class,serviceFunctionTypeKey).build();
  ReadOnlyTransaction readTx=dataBroker.newReadOnlyTransaction();
  Optional<ServiceFunctionType> serviceFunctionTypeOptional=null;
  try {
    serviceFunctionTypeOptional=readTx.read(LogicalDatastoreType.CONFIGURATION,sftIID).get();
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",serviceFunctionType);
  }
  if (serviceFunctionTypeOptional != null && serviceFunctionTypeOptional.isPresent()) {
    sft=serviceFunctionTypeOptional.get();
  }
  printTraceStop(LOG);
  return sft;
}","/** 
 * This method is used to retrieve a Service Function Type from the DataStore <p>
 * @param serviceFunctionType Service Function Type abstract class
 * @return Service Function Type Object which contains a list of SF of this type
 */
protected ServiceFunctionType readServiceFunctionType(Class<? extends ServiceFunctionTypeIdentity> serviceFunctionType){
  printTraceStart(LOG);
  ServiceFunctionType sft=null;
  InstanceIdentifier<ServiceFunctionType> sftIID;
  ServiceFunctionTypeKey serviceFunctionTypeKey=new ServiceFunctionTypeKey(serviceFunctionType);
  sftIID=InstanceIdentifier.builder(ServiceFunctionTypes.class).child(ServiceFunctionType.class,serviceFunctionTypeKey).build();
  sft=SfcDataStoreAPI.readTransactionAPI(sftIID,LogicalDatastoreType.CONFIGURATION);
  if (sft == null) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",serviceFunctionType);
  }
  printTraceStop(LOG);
  return sft;
}",0.7981395348837209
132139,"public void processRenderedServicePath(RenderedServicePath rsp,boolean addFlow){
  this.addFlow=addFlow;
  sfcL2ProviderUtils.addRsp(rsp.getPathId());
  String prevSffName=SffGraph.INGRESS;
  SffGraph sffGraph=new SffGraph();
  try {
    Iterator<RenderedServicePathHop> servicePathHopIter=rsp.getRenderedServicePathHop().iterator();
    String sfName=null;
    String sfgName=null;
    while (servicePathHopIter.hasNext()) {
      RenderedServicePathHop rspHop=servicePathHopIter.next();
      String curSffName=rspHop.getServiceFunctionForwarder();
      sfName=rspHop.getServiceFunctionName();
      sfgName=rspHop.getServiceFunctionGroupName();
      LOG.info(""String_Node_Str"",rsp.getPathId(),rspHop.getHopNumber());
      sffGraph.addGraphEntry(prevSffName,curSffName,sfName,sfgName,rsp.getPathId(),rspHop.getServiceIndex());
      prevSffName=curSffName;
    }
    sffGraph.addGraphEntry(prevSffName,SffGraph.EGRESS,sfName,sfgName,rsp.getPathId(),(short)0);
    processSffDpls(sffGraph,rsp.getTransportType().getName());
    setRspTransports(sffGraph,rsp.getTransportType(),rsp.getPathId());
    Iterator<SffGraph.SffGraphEntry> sffGraphIter=sffGraph.getGraphEntryIterator();
    while (sffGraphIter.hasNext()) {
      SffGraph.SffGraphEntry entry=sffGraphIter.next();
      LOG.debug(""String_Node_Str"",entry);
      if (!entry.getDstSff().equals(SffGraph.EGRESS)) {
        initializeSff(entry.getDstSff(),entry.getPathId());
      }
      configureSffIngress(entry,sffGraph);
      if (entry.getSf() != null) {
        configureSffEgress(entry,sffGraph);
      }
 else       if (entry.getSfg() != null) {
        configureSffEgressForGroup(entry,sffGraph);
      }
    }
  }
 catch (  RuntimeException e) {
    LOG.error(""String_Node_Str"",e.getMessage(),e);
  }
  sfcL2ProviderUtils.removeRsp(rsp.getPathId());
}","public void processRenderedServicePath(RenderedServicePath rsp,boolean addFlow){
  this.addFlow=addFlow;
  sfcL2ProviderUtils.addRsp(rsp.getPathId());
  String prevSffName=SffGraph.INGRESS;
  SffGraph sffGraph=new SffGraph();
  try {
    Iterator<RenderedServicePathHop> servicePathHopIter=rsp.getRenderedServicePathHop().iterator();
    String sfName=null;
    String sfgName=null;
    short lastServiceIndex=rsp.getStartingIndex();
    while (servicePathHopIter.hasNext()) {
      RenderedServicePathHop rspHop=servicePathHopIter.next();
      String curSffName=rspHop.getServiceFunctionForwarder();
      sfName=rspHop.getServiceFunctionName();
      sfgName=rspHop.getServiceFunctionGroupName();
      LOG.info(""String_Node_Str"",rsp.getPathId(),rspHop.getHopNumber());
      sffGraph.addGraphEntry(prevSffName,curSffName,sfName,sfgName,rsp.getPathId(),rspHop.getServiceIndex());
      lastServiceIndex=rspHop.getServiceIndex();
      prevSffName=curSffName;
    }
    sffGraph.addGraphEntry(prevSffName,SffGraph.EGRESS,sfName,sfgName,rsp.getPathId(),(short)(lastServiceIndex - 1));
    processSffDpls(sffGraph,rsp.getTransportType().getName());
    setRspTransports(sffGraph,rsp.getTransportType(),rsp.getPathId());
    Iterator<SffGraph.SffGraphEntry> sffGraphIter=sffGraph.getGraphEntryIterator();
    while (sffGraphIter.hasNext()) {
      SffGraph.SffGraphEntry entry=sffGraphIter.next();
      LOG.debug(""String_Node_Str"",entry);
      if (!entry.getDstSff().equals(SffGraph.EGRESS)) {
        initializeSff(entry.getDstSff(),entry.getPathId());
      }
      configureSffIngress(entry,sffGraph);
      if (entry.getSf() != null) {
        configureSffEgress(entry,sffGraph);
      }
 else       if (entry.getSfg() != null) {
        configureSffEgressForGroup(entry,sffGraph);
      }
    }
  }
 catch (  RuntimeException e) {
    LOG.error(""String_Node_Str"",e.getMessage(),e);
  }
  sfcL2ProviderUtils.removeRsp(rsp.getPathId());
}",0.967313313845336
132140,"private boolean setSffRemainingHopDataPlaneLocator(final ServiceFunctionForwarder sff,final String rspTransport,SffDataPlaneLocator alreadySetSffDpl,boolean ingressDplSet,final long pathId,SffGraph sffGraph){
  List<SffDataPlaneLocator> sffDplList=sff.getSffDataPlaneLocator();
  if (sffDplList.size() == 1) {
    return true;
  }
  for (  SffDataPlaneLocator sffDpl : sffDplList) {
    LOG.debug(""String_Node_Str"",sffDpl.getName(),sffDpl.getDataPlaneLocator().getTransport().getName());
    if (sffDpl.getName().equals(alreadySetSffDpl.getName())) {
      continue;
    }
    if (sffDpl.getDataPlaneLocator().getTransport().getName().equals(rspTransport)) {
      if (ingressDplSet) {
        sffGraph.setSffEgressDpl(sff.getName(),pathId,sffDpl.getName());
      }
 else {
        sffGraph.setSffIngressDpl(sff.getName(),pathId,sffDpl.getName());
      }
      return true;
    }
  }
  return false;
}","private boolean setSffRemainingHopDataPlaneLocator(final ServiceFunctionForwarder sff,final String rspTransport,SffDataPlaneLocator alreadySetSffDpl,boolean ingressDplSet,final long pathId,SffGraph sffGraph){
  List<SffDataPlaneLocator> sffDplList=sff.getSffDataPlaneLocator();
  if (sffDplList.size() == 1) {
    sffGraph.setSffIngressDpl(sff.getName(),pathId,sffDplList.get(0).getName());
    sffGraph.setSffEgressDpl(sff.getName(),pathId,sffDplList.get(0).getName());
    return true;
  }
  for (  SffDataPlaneLocator sffDpl : sffDplList) {
    LOG.debug(""String_Node_Str"",sffDpl.getName(),sffDpl.getDataPlaneLocator().getTransport().getName());
    if (sffDpl.getName().equals(alreadySetSffDpl.getName())) {
      continue;
    }
    if (sffDpl.getDataPlaneLocator().getTransport().getName().equals(rspTransport)) {
      if (ingressDplSet) {
        sffGraph.setSffEgressDpl(sff.getName(),pathId,sffDpl.getName());
      }
 else {
        sffGraph.setSffIngressDpl(sff.getName(),pathId,sffDpl.getName());
      }
      return true;
    }
  }
  return false;
}",0.4890696492119979
132141,"public RenderedServicePath createRenderedServicePath(ServiceFunctionPath sfp,List<ServiceFunction> sfList,List<ServiceFunctionForwarder> sffList){
  String rspName=RSP_NAME_PREFIX + String.valueOf(RSP_NAME_INDEX++);
  RenderedServicePathBuilder rspBuilder=new RenderedServicePathBuilder();
  rspBuilder.setKey(new RenderedServicePathKey(rspName));
  rspBuilder.setName(rspName);
  rspBuilder.setParentServiceFunctionPath(sfp.getName());
  rspBuilder.setPathId(RSP_PATHID_INDEX++);
  rspBuilder.setTransportType(sfp.getTransportType());
  short index=0;
  short serviceIndex=255;
  List<RenderedServicePathHop> rspHopList=new ArrayList<RenderedServicePathHop>();
  for (  ServiceFunction sf : sfList) {
    ServiceFunctionForwarder sff=sffList.get(index);
    RenderedServicePathHopBuilder rspHopBuilder=new RenderedServicePathHopBuilder();
    rspHopBuilder.setKey(new RenderedServicePathHopKey(index));
    rspHopBuilder.setServiceFunctionForwarder(sff.getName());
    rspHopBuilder.setServiceFunctionName(sf.getName());
    rspHopBuilder.setServiceFunctionForwarderLocator(sff.getSffDataPlaneLocator().get(0).getName());
    rspHopBuilder.setServiceIndex(serviceIndex);
    rspHopBuilder.setHopNumber(index);
    rspHopList.add(rspHopBuilder.build());
    --serviceIndex;
    ++index;
  }
  rspBuilder.setRenderedServicePathHop(rspHopList);
  return rspBuilder.build();
}","public RenderedServicePath createRenderedServicePath(ServiceFunctionPath sfp,List<ServiceFunction> sfList,List<ServiceFunctionForwarder> sffList){
  String rspName=RSP_NAME_PREFIX + String.valueOf(RSP_NAME_INDEX++);
  RenderedServicePathBuilder rspBuilder=new RenderedServicePathBuilder();
  rspBuilder.setKey(new RenderedServicePathKey(rspName));
  rspBuilder.setStartingIndex(new Short((short)255));
  rspBuilder.setName(rspName);
  rspBuilder.setParentServiceFunctionPath(sfp.getName());
  rspBuilder.setPathId(RSP_PATHID_INDEX++);
  rspBuilder.setTransportType(sfp.getTransportType());
  short index=0;
  short serviceIndex=255;
  List<RenderedServicePathHop> rspHopList=new ArrayList<RenderedServicePathHop>();
  for (  ServiceFunction sf : sfList) {
    ServiceFunctionForwarder sff=sffList.get(index);
    RenderedServicePathHopBuilder rspHopBuilder=new RenderedServicePathHopBuilder();
    rspHopBuilder.setKey(new RenderedServicePathHopKey(index));
    rspHopBuilder.setServiceFunctionForwarder(sff.getName());
    rspHopBuilder.setServiceFunctionName(sf.getName());
    rspHopBuilder.setServiceFunctionForwarderLocator(sff.getSffDataPlaneLocator().get(0).getName());
    rspHopBuilder.setServiceIndex(serviceIndex);
    rspHopBuilder.setHopNumber(index);
    rspHopList.add(rspHopBuilder.build());
    --serviceIndex;
    ++index;
  }
  rspBuilder.setRenderedServicePathHop(rspHopList);
  return rspBuilder.build();
}",0.9807142857142858
132142,"public void processRenderedServicePath(RenderedServicePath rsp,boolean addFlow){
  this.addFlow=addFlow;
  sfcL2ProviderUtils.addRsp(rsp.getPathId());
  String prevSffName=SffGraph.INGRESS;
  SffGraph sffGraph=new SffGraph();
  try {
    Iterator<RenderedServicePathHop> servicePathHopIter=rsp.getRenderedServicePathHop().iterator();
    String sfName=null;
    String sfgName=null;
    while (servicePathHopIter.hasNext()) {
      RenderedServicePathHop rspHop=servicePathHopIter.next();
      String curSffName=rspHop.getServiceFunctionForwarder();
      sfName=rspHop.getServiceFunctionName();
      sfgName=rspHop.getServiceFunctionGroupName();
      LOG.info(""String_Node_Str"",rsp.getPathId(),rspHop.getHopNumber());
      sffGraph.addGraphEntry(prevSffName,curSffName,sfName,sfgName,rsp.getPathId(),rspHop.getServiceIndex());
      prevSffName=curSffName;
    }
    sffGraph.addGraphEntry(prevSffName,SffGraph.EGRESS,sfName,sfgName,rsp.getPathId(),(short)0);
    processSffDpls(sffGraph,rsp.getTransportType().getName());
    setRspTransports(sffGraph,rsp.getTransportType(),rsp.getPathId());
    Iterator<SffGraph.SffGraphEntry> sffGraphIter=sffGraph.getGraphEntryIterator();
    while (sffGraphIter.hasNext()) {
      SffGraph.SffGraphEntry entry=sffGraphIter.next();
      LOG.debug(""String_Node_Str"",entry);
      if (!entry.getDstSff().equals(SffGraph.EGRESS)) {
        initializeSff(entry.getDstSff(),entry.getPathId());
      }
      configureSffIngress(entry,sffGraph);
      if (entry.getSf() != null) {
        configureSffEgress(entry,sffGraph);
      }
 else       if (entry.getSfg() != null) {
        configureSffEgressForGroup(entry,sffGraph);
      }
    }
  }
 catch (  RuntimeException e) {
    LOG.error(""String_Node_Str"",e.getMessage(),e);
  }
  sfcL2ProviderUtils.removeRsp(rsp.getPathId());
}","public void processRenderedServicePath(RenderedServicePath rsp,boolean addFlow){
  this.addFlow=addFlow;
  sfcL2ProviderUtils.addRsp(rsp.getPathId());
  String prevSffName=SffGraph.INGRESS;
  SffGraph sffGraph=new SffGraph();
  try {
    Iterator<RenderedServicePathHop> servicePathHopIter=rsp.getRenderedServicePathHop().iterator();
    String sfName=null;
    String sfgName=null;
    short lastServiceIndex=rsp.getStartingIndex();
    while (servicePathHopIter.hasNext()) {
      RenderedServicePathHop rspHop=servicePathHopIter.next();
      String curSffName=rspHop.getServiceFunctionForwarder();
      sfName=rspHop.getServiceFunctionName();
      sfgName=rspHop.getServiceFunctionGroupName();
      LOG.info(""String_Node_Str"",rsp.getPathId(),rspHop.getHopNumber());
      sffGraph.addGraphEntry(prevSffName,curSffName,sfName,sfgName,rsp.getPathId(),rspHop.getServiceIndex());
      lastServiceIndex=rspHop.getServiceIndex();
      prevSffName=curSffName;
    }
    sffGraph.addGraphEntry(prevSffName,SffGraph.EGRESS,sfName,sfgName,rsp.getPathId(),(short)(lastServiceIndex - 1));
    processSffDpls(sffGraph,rsp.getTransportType().getName());
    setRspTransports(sffGraph,rsp.getTransportType(),rsp.getPathId());
    Iterator<SffGraph.SffGraphEntry> sffGraphIter=sffGraph.getGraphEntryIterator();
    while (sffGraphIter.hasNext()) {
      SffGraph.SffGraphEntry entry=sffGraphIter.next();
      LOG.debug(""String_Node_Str"",entry);
      if (!entry.getDstSff().equals(SffGraph.EGRESS)) {
        initializeSff(entry.getDstSff(),entry.getPathId());
      }
      configureSffIngress(entry,sffGraph);
      if (entry.getSf() != null) {
        configureSffEgress(entry,sffGraph);
      }
 else       if (entry.getSfg() != null) {
        configureSffEgressForGroup(entry,sffGraph);
      }
    }
  }
 catch (  RuntimeException e) {
    LOG.error(""String_Node_Str"",e.getMessage(),e);
  }
  sfcL2ProviderUtils.removeRsp(rsp.getPathId());
}",0.967313313845336
132143,"private boolean setSffRemainingHopDataPlaneLocator(final ServiceFunctionForwarder sff,final String rspTransport,SffDataPlaneLocator alreadySetSffDpl,boolean ingressDplSet,final long pathId,SffGraph sffGraph){
  List<SffDataPlaneLocator> sffDplList=sff.getSffDataPlaneLocator();
  if (sffDplList.size() == 1) {
    return true;
  }
  for (  SffDataPlaneLocator sffDpl : sffDplList) {
    LOG.debug(""String_Node_Str"",sffDpl.getName(),sffDpl.getDataPlaneLocator().getTransport().getName());
    if (sffDpl.getName().equals(alreadySetSffDpl.getName())) {
      continue;
    }
    if (sffDpl.getDataPlaneLocator().getTransport().getName().equals(rspTransport)) {
      if (ingressDplSet) {
        sffGraph.setSffEgressDpl(sff.getName(),pathId,sffDpl.getName());
      }
 else {
        sffGraph.setSffIngressDpl(sff.getName(),pathId,sffDpl.getName());
      }
      return true;
    }
  }
  return false;
}","private boolean setSffRemainingHopDataPlaneLocator(final ServiceFunctionForwarder sff,final String rspTransport,SffDataPlaneLocator alreadySetSffDpl,boolean ingressDplSet,final long pathId,SffGraph sffGraph){
  List<SffDataPlaneLocator> sffDplList=sff.getSffDataPlaneLocator();
  if (sffDplList.size() == 1) {
    sffGraph.setSffIngressDpl(sff.getName(),pathId,sffDplList.get(0).getName());
    sffGraph.setSffEgressDpl(sff.getName(),pathId,sffDplList.get(0).getName());
    return true;
  }
  for (  SffDataPlaneLocator sffDpl : sffDplList) {
    LOG.debug(""String_Node_Str"",sffDpl.getName(),sffDpl.getDataPlaneLocator().getTransport().getName());
    if (sffDpl.getName().equals(alreadySetSffDpl.getName())) {
      continue;
    }
    if (sffDpl.getDataPlaneLocator().getTransport().getName().equals(rspTransport)) {
      if (ingressDplSet) {
        sffGraph.setSffEgressDpl(sff.getName(),pathId,sffDpl.getName());
      }
 else {
        sffGraph.setSffIngressDpl(sff.getName(),pathId,sffDpl.getName());
      }
      return true;
    }
  }
  return false;
}",0.4890696492119979
132144,"public RenderedServicePath createRenderedServicePath(ServiceFunctionPath sfp,List<ServiceFunction> sfList,List<ServiceFunctionForwarder> sffList){
  String rspName=RSP_NAME_PREFIX + String.valueOf(RSP_NAME_INDEX++);
  RenderedServicePathBuilder rspBuilder=new RenderedServicePathBuilder();
  rspBuilder.setKey(new RenderedServicePathKey(rspName));
  rspBuilder.setName(rspName);
  rspBuilder.setParentServiceFunctionPath(sfp.getName());
  rspBuilder.setPathId(RSP_PATHID_INDEX++);
  rspBuilder.setTransportType(sfp.getTransportType());
  short index=0;
  short serviceIndex=255;
  List<RenderedServicePathHop> rspHopList=new ArrayList<RenderedServicePathHop>();
  for (  ServiceFunction sf : sfList) {
    ServiceFunctionForwarder sff=sffList.get(index);
    RenderedServicePathHopBuilder rspHopBuilder=new RenderedServicePathHopBuilder();
    rspHopBuilder.setKey(new RenderedServicePathHopKey(index));
    rspHopBuilder.setServiceFunctionForwarder(sff.getName());
    rspHopBuilder.setServiceFunctionName(sf.getName());
    rspHopBuilder.setServiceFunctionForwarderLocator(sff.getSffDataPlaneLocator().get(0).getName());
    rspHopBuilder.setServiceIndex(serviceIndex);
    rspHopBuilder.setHopNumber(index);
    rspHopList.add(rspHopBuilder.build());
    --serviceIndex;
    ++index;
  }
  rspBuilder.setRenderedServicePathHop(rspHopList);
  return rspBuilder.build();
}","public RenderedServicePath createRenderedServicePath(ServiceFunctionPath sfp,List<ServiceFunction> sfList,List<ServiceFunctionForwarder> sffList){
  String rspName=RSP_NAME_PREFIX + String.valueOf(RSP_NAME_INDEX++);
  RenderedServicePathBuilder rspBuilder=new RenderedServicePathBuilder();
  rspBuilder.setKey(new RenderedServicePathKey(rspName));
  rspBuilder.setStartingIndex(new Short((short)255));
  rspBuilder.setName(rspName);
  rspBuilder.setParentServiceFunctionPath(sfp.getName());
  rspBuilder.setPathId(RSP_PATHID_INDEX++);
  rspBuilder.setTransportType(sfp.getTransportType());
  short index=0;
  short serviceIndex=255;
  List<RenderedServicePathHop> rspHopList=new ArrayList<RenderedServicePathHop>();
  for (  ServiceFunction sf : sfList) {
    ServiceFunctionForwarder sff=sffList.get(index);
    RenderedServicePathHopBuilder rspHopBuilder=new RenderedServicePathHopBuilder();
    rspHopBuilder.setKey(new RenderedServicePathHopKey(index));
    rspHopBuilder.setServiceFunctionForwarder(sff.getName());
    rspHopBuilder.setServiceFunctionName(sf.getName());
    rspHopBuilder.setServiceFunctionForwarderLocator(sff.getSffDataPlaneLocator().get(0).getName());
    rspHopBuilder.setServiceIndex(serviceIndex);
    rspHopBuilder.setHopNumber(index);
    rspHopList.add(rspHopBuilder.build());
    --serviceIndex;
    ++index;
  }
  rspBuilder.setRenderedServicePathHop(rspHopList);
  return rspBuilder.build();
}",0.9807142857142858
132145,"/** 
 * Create a Rendered Path and all the associated operational state based on the given rendered service path <p>
 * @param serviceFunctionPath RSP Object
 * @return Nothing.
 */
protected RenderedServicePath createRenderedServicePathEntry(ServiceFunctionPath serviceFunctionPath,CreateRenderedPathInput createRenderedPathInput){
  printTraceStart(LOG);
  long pathId;
  int serviceIndex;
  RenderedServicePath ret=null;
  ServiceFunctionChain serviceFunctionChain;
  String serviceFunctionChainName=serviceFunctionPath.getServiceChainName();
  serviceFunctionChain=serviceFunctionChainName != null ? SfcProviderServiceChainAPI.readServiceFunctionChain(serviceFunctionChainName) : null;
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",serviceFunctionPath.getName());
    return ret;
  }
  RenderedServicePathBuilder renderedServicePathBuilder=new RenderedServicePathBuilder();
  serviceIndex=MAX_STARTING_INDEX;
  List<String> sfgNameList=getSfgNameList(serviceFunctionChain);
  List<String> sfNameList=scheduler.scheduleServiceFuntions(serviceFunctionChain,serviceIndex);
  if (sfNameList == null && sfgNameList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  List<RenderedServicePathHop> renderedServicePathHopArrayList=createRenderedServicePathHopList(sfNameList,sfgNameList,serviceIndex);
  if (renderedServicePathHopArrayList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  pathId=(serviceFunctionPath.getPathId() != null) ? serviceFunctionPath.getPathId() : numCreatedPathIncrementGet();
  renderedServicePathBuilder.setRenderedServicePathHop(renderedServicePathHopArrayList);
  if (createRenderedPathInput.getName() == null || createRenderedPathInput.getName().isEmpty()) {
    renderedServicePathBuilder.setName(serviceFunctionPath.getName() + ""String_Node_Str"" + pathId);
  }
 else {
    renderedServicePathBuilder.setName(createRenderedPathInput.getName());
  }
  renderedServicePathBuilder.setPathId(pathId);
  renderedServicePathBuilder.setStartingIndex((short)MAX_STARTING_INDEX);
  renderedServicePathBuilder.setServiceChainName(serviceFunctionChainName);
  renderedServicePathBuilder.setParentServiceFunctionPath(serviceFunctionPath.getName());
  if (serviceFunctionPath.getTransportType() == null) {
    renderedServicePathBuilder.setTransportType(VxlanGpe.class);
  }
 else {
    renderedServicePathBuilder.setTransportType(serviceFunctionPath.getTransportType());
  }
  RenderedServicePathKey renderedServicePathKey=new RenderedServicePathKey(renderedServicePathBuilder.getName());
  InstanceIdentifier<RenderedServicePath> rspIID;
  rspIID=InstanceIdentifier.builder(RenderedServicePaths.class).child(RenderedServicePath.class,renderedServicePathKey).build();
  RenderedServicePath renderedServicePath=renderedServicePathBuilder.build();
  if (SfcDataStoreAPI.writeMergeTransactionAPI(rspIID,renderedServicePath,LogicalDatastoreType.OPERATIONAL)) {
    ret=renderedServicePath;
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],serviceFunctionPath.getName());
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * Create a Rendered Path and all the associated operational state based on the given rendered service path <p>
 * @param serviceFunctionPath RSP Object
 * @return Nothing.
 */
protected RenderedServicePath createRenderedServicePathEntry(ServiceFunctionPath serviceFunctionPath,CreateRenderedPathInput createRenderedPathInput){
  printTraceStart(LOG);
  long pathId;
  int serviceIndex;
  RenderedServicePath ret=null;
  ServiceFunctionChain serviceFunctionChain;
  String serviceFunctionChainName=serviceFunctionPath.getServiceChainName();
  serviceFunctionChain=serviceFunctionChainName != null ? SfcProviderServiceChainAPI.readServiceFunctionChain(serviceFunctionChainName) : null;
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",serviceFunctionPath.getName());
    return ret;
  }
  RenderedServicePathBuilder renderedServicePathBuilder=new RenderedServicePathBuilder();
  serviceIndex=MAX_STARTING_INDEX;
  List<String> sfgNameList=getSfgNameList(serviceFunctionChain);
  List<String> sfNameList=scheduler.scheduleServiceFunctions(serviceFunctionChain,serviceIndex);
  if (sfNameList == null && sfgNameList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  List<RenderedServicePathHop> renderedServicePathHopArrayList=createRenderedServicePathHopList(sfNameList,sfgNameList,serviceIndex);
  if (renderedServicePathHopArrayList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  pathId=(serviceFunctionPath.getPathId() != null) ? serviceFunctionPath.getPathId() : numCreatedPathIncrementGet();
  renderedServicePathBuilder.setRenderedServicePathHop(renderedServicePathHopArrayList);
  if (createRenderedPathInput.getName() == null || createRenderedPathInput.getName().isEmpty()) {
    renderedServicePathBuilder.setName(serviceFunctionPath.getName() + ""String_Node_Str"" + pathId);
  }
 else {
    renderedServicePathBuilder.setName(createRenderedPathInput.getName());
  }
  renderedServicePathBuilder.setPathId(pathId);
  renderedServicePathBuilder.setStartingIndex((short)MAX_STARTING_INDEX);
  renderedServicePathBuilder.setServiceChainName(serviceFunctionChainName);
  renderedServicePathBuilder.setParentServiceFunctionPath(serviceFunctionPath.getName());
  if (serviceFunctionPath.getTransportType() == null) {
    renderedServicePathBuilder.setTransportType(VxlanGpe.class);
  }
 else {
    renderedServicePathBuilder.setTransportType(serviceFunctionPath.getTransportType());
  }
  RenderedServicePathKey renderedServicePathKey=new RenderedServicePathKey(renderedServicePathBuilder.getName());
  InstanceIdentifier<RenderedServicePath> rspIID;
  rspIID=InstanceIdentifier.builder(RenderedServicePaths.class).child(RenderedServicePath.class,renderedServicePathKey).build();
  RenderedServicePath renderedServicePath=renderedServicePathBuilder.build();
  if (SfcDataStoreAPI.writeMergeTransactionAPI(rspIID,renderedServicePath,LogicalDatastoreType.OPERATIONAL)) {
    ret=renderedServicePath;
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],serviceFunctionPath.getName());
  }
  printTraceStop(LOG);
  return ret;
}",0.9998397692677456
132146,"SfcProviderRenderedPathAPI(Object[] params,Class[] paramsTypes,String m){
  super(params,paramsTypes,m);
  initServiceFuntionScheduler();
}","SfcProviderRenderedPathAPI(Object[] params,Class[] paramsTypes,String m){
  super(params,paramsTypes,m);
  initServiceFunctionScheduler();
}",0.996415770609319
132147,"private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType){
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  String sftServiceFunctionName=sftServiceFunctionNameList.get(0).getName();
  ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(sftServiceFunctionName);
  if (!SfcProviderServiceFunctionAPI.putServiceFunctionMonitorExecutor(serviceFunction)) {
    LOG.error(""String_Node_Str"",serviceFunction.getName());
  }
  if (SfcProviderServiceFunctionAPI.readServiceFunctionDescriptionMonitorExecutor(sftServiceFunctionName) != null) {
    java.lang.Long preCPUUtilization=SfcProviderServiceFunctionAPI.readServiceFunctionDescriptionMonitorExecutor(sftServiceFunctionName).getMonitoringInfo().getResourceUtilization().getCPUUtilization();
    for (    SftServiceFunctionName curSftServiceFunctionName : sftServiceFunctionNameList) {
      java.lang.Long curCPUUtilization=SfcProviderServiceFunctionAPI.readServiceFunctionDescriptionMonitorExecutor(curSftServiceFunctionName.getName()).getMonitoringInfo().getResourceUtilization().getCPUUtilization();
      if (preCPUUtilization > curCPUUtilization) {
        preCPUUtilization=curCPUUtilization;
        sftServiceFunctionName=curSftServiceFunctionName.getName();
      }
    }
  }
  return sftServiceFunctionName;
}","private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType){
  boolean ret=false;
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  String sfName=null;
  String sftServiceFunctionName=null;
  java.lang.Long preCPUUtilization=java.lang.Long.MAX_VALUE;
  for (  SftServiceFunctionName curSftServiceFunctionName : sftServiceFunctionNameList) {
    sfName=curSftServiceFunctionName.getName();
    ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(sfName);
    if (serviceFunction == null) {
      LOG.error(""String_Node_Str"",sfName);
      continue;
    }
    ret=SfcProviderServiceFunctionAPI.putServiceFunctionMonitorExecutor(serviceFunction);
    if (ret == false) {
      LOG.error(""String_Node_Str"",sfName);
    }
    SfcSfDescMon sfcSfDescMon=SfcProviderServiceFunctionAPI.readServiceFunctionDescriptionMonitorExecutor(sfName);
    if (sfcSfDescMon == null) {
      sftServiceFunctionName=sfName;
      LOG.error(""String_Node_Str"",sfName);
      break;
    }
    java.lang.Long curCPUUtilization=sfcSfDescMon.getMonitoringInfo().getResourceUtilization().getCPUUtilization();
    if (preCPUUtilization > curCPUUtilization) {
      preCPUUtilization=curCPUUtilization;
      sftServiceFunctionName=sfName;
    }
  }
  if (sftServiceFunctionName == null) {
    LOG.error(""String_Node_Str"",serviceFunctionType.getType().getSimpleName());
  }
  return sftServiceFunctionName;
}",0.4833333333333333
132148,"private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType){
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  Random rad=new Random();
  return sftServiceFunctionNameList.get(rad.nextInt(sftServiceFunctionNameList.size())).getName();
}","private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType){
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  int maxTries=sftServiceFunctionNameList.size();
  Random rad=new Random();
  ServiceFunction serviceFunction=null;
  String serviceFunctionName=null;
  int start=rad.nextInt(sftServiceFunctionNameList.size());
  while (maxTries > 0) {
    serviceFunctionName=sftServiceFunctionNameList.get(start).getName();
    serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(serviceFunctionName);
    if (serviceFunction != null) {
      break;
    }
 else {
      LOG.debug(""String_Node_Str"",serviceFunctionName);
      maxTries--;
      serviceFunctionName=null;
      start=(start + 1) % sftServiceFunctionNameList.size();
    }
  }
  if (serviceFunctionName == null) {
    LOG.error(""String_Node_Str"",serviceFunctionType.getType().getSimpleName());
  }
  return serviceFunctionName;
}",0.3981623277182236
132149,"/** 
 * This method finds out name of the Service Function closest to Service Function preSfName per serviceFunctionType. <p>
 * @param serviceFunctionType Type of Service Function to find
 * @param preSfName Name of previous Service Function in Service Function Path
 * @param sfcProviderGraph Topology graph comprised of all the SFs and SFFs
 * @return String Name of the Service Function with type serviceFunctionType
 */
private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType,String preSfName,SfcProviderGraph sfcProviderGraph){
  String sfcProviderTopologyNodeName=null;
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  if (sftServiceFunctionNameList.size() == 0) {
    LOG.debug(""String_Node_Str"",serviceFunctionType);
    return null;
  }
  if (preSfName == null) {
    Random rad=new Random();
    sfcProviderTopologyNodeName=sftServiceFunctionNameList.get(rad.nextInt(sftServiceFunctionNameList.size())).getName();
    LOG.debug(""String_Node_Str"",sfcProviderTopologyNodeName);
    return sfcProviderTopologyNodeName;
  }
  SfcProviderTopologyNode preSfcProviderTopologyNode=sfcProviderGraph.getNode(preSfName);
  if (preSfcProviderTopologyNode == null) {
    LOG.debug(""String_Node_Str"",preSfName);
    return null;
  }
  int minLength=Integer.MAX_VALUE;
  int length=0;
  sfcProviderTopologyNodeName=null;
  for (  SftServiceFunctionName sftServiceFunctionName : sftServiceFunctionNameList) {
    String curSfName=sftServiceFunctionName.getName();
    List<SfcProviderTopologyNode> sfcProviderTopologyNodeList=sfcProviderGraph.getShortestPath(preSfName,curSfName);
    length=sfcProviderTopologyNodeList.size();
    if (length <= 1) {
      LOG.debug(""String_Node_Str"",preSfName,curSfName);
      continue;
    }
    if (minLength > length) {
      minLength=length;
      sfcProviderTopologyNodeName=curSfName;
    }
  }
  if (sfcProviderTopologyNodeName == null) {
    LOG.debug(""String_Node_Str"",preSfName);
  }
  return sfcProviderTopologyNodeName;
}","/** 
 * This method finds out name of the Service Function closest to Service Function preSfName per serviceFunctionType. <p>
 * @param serviceFunctionType Type of Service Function to find
 * @param preSfName Name of previous Service Function in Service Function Path
 * @param sfcProviderGraph Topology graph comprised of all the SFs and SFFs
 * @return String Name of the Service Function with type serviceFunctionType
 */
private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType,String preSfName,SfcProviderGraph sfcProviderGraph){
  String sfcProviderTopologyNodeName=null;
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  int maxTries=sftServiceFunctionNameList.size();
  if (sftServiceFunctionNameList.size() == 0) {
    LOG.debug(""String_Node_Str"",serviceFunctionType);
    return null;
  }
  if (preSfName == null) {
    Random rad=new Random();
    int start=rad.nextInt(sftServiceFunctionNameList.size());
    SfcProviderTopologyNode firstHopNode=null;
    while (maxTries > 0) {
      sfcProviderTopologyNodeName=sftServiceFunctionNameList.get(start).getName();
      firstHopNode=sfcProviderGraph.getNode(sfcProviderTopologyNodeName);
      if (firstHopNode != null) {
        break;
      }
 else {
        LOG.debug(""String_Node_Str"",sfcProviderTopologyNodeName);
        sfcProviderTopologyNodeName=null;
        start=(start + 1) % sftServiceFunctionNameList.size();
        maxTries--;
      }
    }
    LOG.debug(""String_Node_Str"",sfcProviderTopologyNodeName);
    return sfcProviderTopologyNodeName;
  }
  SfcProviderTopologyNode preSfcProviderTopologyNode=sfcProviderGraph.getNode(preSfName);
  if (preSfcProviderTopologyNode == null) {
    LOG.debug(""String_Node_Str"",preSfName);
    return null;
  }
  int minLength=Integer.MAX_VALUE;
  int length=0;
  sfcProviderTopologyNodeName=null;
  for (  SftServiceFunctionName sftServiceFunctionName : sftServiceFunctionNameList) {
    String curSfName=sftServiceFunctionName.getName();
    SfcProviderTopologyNode curSfcProviderTopologyNode=sfcProviderGraph.getNode(curSfName);
    if (curSfcProviderTopologyNode == null) {
      continue;
    }
    List<SfcProviderTopologyNode> sfcProviderTopologyNodeList=sfcProviderGraph.getShortestPath(preSfName,curSfName);
    length=sfcProviderTopologyNodeList.size();
    if (length <= 1) {
      LOG.debug(""String_Node_Str"",preSfName,curSfName);
      continue;
    }
    if (minLength > length) {
      minLength=length;
      sfcProviderTopologyNodeName=curSfName;
    }
  }
  if (sfcProviderTopologyNodeName == null) {
    LOG.debug(""String_Node_Str"",preSfName);
  }
  return sfcProviderTopologyNodeName;
}",0.8258580753842915
132150,"@Before public void before() throws ExecutionException, InterruptedException {
  dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
  final String[] sfNames={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final IpAddress[] ipMgmtAddress=new IpAddress[sfNames.length];
  final IpAddress[] locatorIpAddress=new IpAddress[sfNames.length];
  SfDataPlaneLocator[] sfDataPlaneLocator=new SfDataPlaneLocator[sfNames.length];
  ServiceFunctionKey[] key=new ServiceFunctionKey[sfNames.length];
  for (int i=0; i < sfNames.length; i++) {
    ipMgmtAddress[i]=new IpAddress(new Ipv4Address(IP_MGMT_ADDRESS[0]));
    locatorIpAddress[i]=new IpAddress(new Ipv4Address(LOCATOR_IP_ADDRESS[0]));
    PortNumber portNumber=new PortNumber(PORT[i]);
    key[i]=new ServiceFunctionKey(sfNames[i]);
    IpBuilder ipBuilder=new IpBuilder();
    ipBuilder.setIp(locatorIpAddress[i]).setPort(portNumber);
    SfDataPlaneLocatorBuilder locatorBuilder=new SfDataPlaneLocatorBuilder();
    locatorBuilder.setName(LOCATOR_IP_ADDRESS[i]).setLocatorType(ipBuilder.build()).setServiceFunctionForwarder(SFF_NAMES[i]);
    sfDataPlaneLocator[i]=locatorBuilder.build();
    ServiceFunctionBuilder sfBuilder=new ServiceFunctionBuilder();
    List<SfDataPlaneLocator> dataPlaneLocatorList=new ArrayList<>();
    dataPlaneLocatorList.add(sfDataPlaneLocator[i]);
    sfBuilder.setName(sfNames[i]).setKey(key[i]).setType(sfTypes[i]).setIpMgmtAddress(ipMgmtAddress[i]).setSfDataPlaneLocator(dataPlaneLocatorList);
    sfList.add(sfBuilder.build());
  }
  ServiceFunctionsBuilder sfsBuilder=new ServiceFunctionsBuilder();
  sfsBuilder.setServiceFunction(sfList);
  executor.submit(SfcProviderServiceFunctionAPI.getPutAll(new Object[]{sfsBuilder.build()},new Class[]{ServiceFunctions.class})).get();
  for (  ServiceFunction serviceFunction : sfList) {
    boolean ret=SfcProviderServiceTypeAPI.createServiceFunctionTypeEntryExecutor(serviceFunction);
    LOG.debug(""String_Node_Str"",serviceFunction.getName());
    assertTrue(""String_Node_Str"",ret);
  }
  for (int i=0; i < SFF_NAMES.length; i++) {
    List<ConnectedSffDictionary> sffDictionaryList=new ArrayList<>();
    for (int j=0; j < 2; j++) {
      ConnectedSffDictionaryBuilder sffDictionaryEntryBuilder=new ConnectedSffDictionaryBuilder();
      ConnectedSffDictionary sffDictEntry=sffDictionaryEntryBuilder.setName(TO_SFF_NAMES[i][j]).build();
      sffDictionaryList.add(sffDictEntry);
    }
    List<ServiceFunctionDictionary> sfDictionaryList=new ArrayList<>();
    ServiceFunction serviceFunction=sfList.get(i);
    SfDataPlaneLocator sfDPLocator=serviceFunction.getSfDataPlaneLocator().get(0);
    SffSfDataPlaneLocatorBuilder sffSfDataPlaneLocatorBuilder=new SffSfDataPlaneLocatorBuilder(sfDPLocator);
    SffSfDataPlaneLocator sffSfDataPlaneLocator=sffSfDataPlaneLocatorBuilder.build();
    ServiceFunctionDictionaryBuilder dictionaryEntryBuilder=new ServiceFunctionDictionaryBuilder();
    dictionaryEntryBuilder.setName(serviceFunction.getName()).setKey(new ServiceFunctionDictionaryKey(serviceFunction.getName())).setType(serviceFunction.getType()).setSffSfDataPlaneLocator(sffSfDataPlaneLocator).setFailmode(Open.class).setSffInterfaces(null);
    ServiceFunctionDictionary sfDictEntry=dictionaryEntryBuilder.build();
    sfDictionaryList.add(sfDictEntry);
    List<SffDataPlaneLocator> locatorList=new ArrayList<>();
    IpBuilder ipBuilder=new IpBuilder();
    ipBuilder.setIp(new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[i]))).setPort(new PortNumber(PORT[i]));
    DataPlaneLocatorBuilder sffLocatorBuilder=new DataPlaneLocatorBuilder();
    sffLocatorBuilder.setLocatorType(ipBuilder.build()).setTransport(VxlanGpe.class);
    SffDataPlaneLocatorBuilder locatorBuilder=new SffDataPlaneLocatorBuilder();
    locatorBuilder.setName(SFF_LOCATOR_IP[i]).setKey(new SffDataPlaneLocatorKey(SFF_LOCATOR_IP[i])).setDataPlaneLocator(sffLocatorBuilder.build());
    locatorList.add(locatorBuilder.build());
    ServiceFunctionForwarderBuilder sffBuilder=new ServiceFunctionForwarderBuilder();
    sffBuilder.setName(SFF_NAMES[i]).setKey(new ServiceFunctionForwarderKey(SFF_NAMES[i])).setSffDataPlaneLocator(locatorList).setServiceFunctionDictionary(sfDictionaryList).setConnectedSffDictionary(sffDictionaryList).setServiceNode(null);
    ServiceFunctionForwarder sff=sffBuilder.build();
    executor.submit(SfcProviderServiceForwarderAPI.getPut(new Object[]{sff},new Class[]{ServiceFunctionForwarder.class})).get();
  }
}","@Before public void before() throws ExecutionException, InterruptedException {
  dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
  Thread.sleep(1000);
  final String[] sfNames={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final IpAddress[] ipMgmtAddress=new IpAddress[sfNames.length];
  final IpAddress[] locatorIpAddress=new IpAddress[sfNames.length];
  SfDataPlaneLocator[] sfDataPlaneLocator=new SfDataPlaneLocator[sfNames.length];
  ServiceFunctionKey[] key=new ServiceFunctionKey[sfNames.length];
  for (int i=0; i < sfNames.length; i++) {
    ipMgmtAddress[i]=new IpAddress(new Ipv4Address(IP_MGMT_ADDRESS[0]));
    locatorIpAddress[i]=new IpAddress(new Ipv4Address(LOCATOR_IP_ADDRESS[0]));
    PortNumber portNumber=new PortNumber(PORT[i]);
    key[i]=new ServiceFunctionKey(sfNames[i]);
    IpBuilder ipBuilder=new IpBuilder();
    ipBuilder.setIp(locatorIpAddress[i]).setPort(portNumber);
    SfDataPlaneLocatorBuilder locatorBuilder=new SfDataPlaneLocatorBuilder();
    locatorBuilder.setName(LOCATOR_IP_ADDRESS[i]).setLocatorType(ipBuilder.build()).setServiceFunctionForwarder(SFF_NAMES[i]);
    sfDataPlaneLocator[i]=locatorBuilder.build();
    ServiceFunctionBuilder sfBuilder=new ServiceFunctionBuilder();
    List<SfDataPlaneLocator> dataPlaneLocatorList=new ArrayList<>();
    dataPlaneLocatorList.add(sfDataPlaneLocator[i]);
    sfBuilder.setName(sfNames[i]).setKey(key[i]).setType(sfTypes[i]).setIpMgmtAddress(ipMgmtAddress[i]).setSfDataPlaneLocator(dataPlaneLocatorList);
    sfList.add(sfBuilder.build());
  }
  ServiceFunctionsBuilder sfsBuilder=new ServiceFunctionsBuilder();
  sfsBuilder.setServiceFunction(sfList);
  executor.submit(SfcProviderServiceFunctionAPI.getPutAll(new Object[]{sfsBuilder.build()},new Class[]{ServiceFunctions.class})).get();
  Thread.sleep(1000);
  for (  ServiceFunction serviceFunction : sfList) {
    boolean ret=SfcProviderServiceTypeAPI.createServiceFunctionTypeEntryExecutor(serviceFunction);
    LOG.debug(""String_Node_Str"",serviceFunction.getName());
    assertTrue(""String_Node_Str"",ret);
  }
  for (int i=0; i < SFF_NAMES.length; i++) {
    List<ConnectedSffDictionary> sffDictionaryList=new ArrayList<>();
    for (int j=0; j < 2; j++) {
      ConnectedSffDictionaryBuilder sffDictionaryEntryBuilder=new ConnectedSffDictionaryBuilder();
      ConnectedSffDictionary sffDictEntry=sffDictionaryEntryBuilder.setName(TO_SFF_NAMES[i][j]).build();
      sffDictionaryList.add(sffDictEntry);
    }
    List<ServiceFunctionDictionary> sfDictionaryList=new ArrayList<>();
    ServiceFunction serviceFunction=sfList.get(i);
    SfDataPlaneLocator sfDPLocator=serviceFunction.getSfDataPlaneLocator().get(0);
    SffSfDataPlaneLocatorBuilder sffSfDataPlaneLocatorBuilder=new SffSfDataPlaneLocatorBuilder(sfDPLocator);
    SffSfDataPlaneLocator sffSfDataPlaneLocator=sffSfDataPlaneLocatorBuilder.build();
    ServiceFunctionDictionaryBuilder dictionaryEntryBuilder=new ServiceFunctionDictionaryBuilder();
    dictionaryEntryBuilder.setName(serviceFunction.getName()).setKey(new ServiceFunctionDictionaryKey(serviceFunction.getName())).setType(serviceFunction.getType()).setSffSfDataPlaneLocator(sffSfDataPlaneLocator).setFailmode(Open.class).setSffInterfaces(null);
    ServiceFunctionDictionary sfDictEntry=dictionaryEntryBuilder.build();
    sfDictionaryList.add(sfDictEntry);
    List<SffDataPlaneLocator> locatorList=new ArrayList<>();
    IpBuilder ipBuilder=new IpBuilder();
    ipBuilder.setIp(new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[i]))).setPort(new PortNumber(PORT[i]));
    DataPlaneLocatorBuilder sffLocatorBuilder=new DataPlaneLocatorBuilder();
    sffLocatorBuilder.setLocatorType(ipBuilder.build()).setTransport(VxlanGpe.class);
    SffDataPlaneLocatorBuilder locatorBuilder=new SffDataPlaneLocatorBuilder();
    locatorBuilder.setName(SFF_LOCATOR_IP[i]).setKey(new SffDataPlaneLocatorKey(SFF_LOCATOR_IP[i])).setDataPlaneLocator(sffLocatorBuilder.build());
    locatorList.add(locatorBuilder.build());
    ServiceFunctionForwarderBuilder sffBuilder=new ServiceFunctionForwarderBuilder();
    sffBuilder.setName(SFF_NAMES[i]).setKey(new ServiceFunctionForwarderKey(SFF_NAMES[i])).setSffDataPlaneLocator(locatorList).setServiceFunctionDictionary(sfDictionaryList).setConnectedSffDictionary(sffDictionaryList).setServiceNode(null);
    ServiceFunctionForwarder sff=sffBuilder.build();
    executor.submit(SfcProviderServiceForwarderAPI.getPut(new Object[]{sff},new Class[]{ServiceFunctionForwarder.class})).get();
  }
  Thread.sleep(1000);
}",0.99346016646849
132151,"@Test public void testReadRspFirstHopBySftList() throws ExecutionException, InterruptedException {
  List<Class<? extends ServiceFunctionTypeIdentity>> sftList=new ArrayList<Class<? extends ServiceFunctionTypeIdentity>>();
  sftList.add(Firewall.class);
  sftList.add(Dpi.class);
  sftList.add(Napt44.class);
  sftList.add(HttpHeaderEnrichment.class);
  sftList.add(Qos.class);
  assertEquals(""String_Node_Str"",sftList.size(),5);
  RenderedServicePathFirstHop firstHop=SfcProviderRenderedPathAPI.readRspFirstHopBySftList(sftList);
  assertNotNull(""String_Node_Str"",firstHop);
  LOG.debug(""String_Node_Str"",firstHop.getIp().toString(),firstHop.getPort());
  assertEquals(""String_Node_Str"",firstHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[0])));
  assertEquals(""String_Node_Str"",firstHop.getPort(),new PortNumber(PORT[0]));
}","@Test public void testReadRspFirstHopBySftList() throws ExecutionException, InterruptedException {
  List<Class<? extends ServiceFunctionTypeIdentity>> sftList=new ArrayList<Class<? extends ServiceFunctionTypeIdentity>>();
  sftList.add(Firewall.class);
  sftList.add(Dpi.class);
  sftList.add(Napt44.class);
  sftList.add(HttpHeaderEnrichment.class);
  sftList.add(Qos.class);
  assertEquals(""String_Node_Str"",sftList.size(),5);
  RenderedServicePathFirstHop firstHop=null;
  try {
    firstHop=SfcProviderRenderedPathAPI.readRspFirstHopBySftList(sftList);
  }
 catch (  NullPointerException e) {
    e.printStackTrace();
  }
  assertNotNull(""String_Node_Str"",firstHop);
  LOG.debug(""String_Node_Str"",firstHop.getIp().toString(),firstHop.getPort());
  assertEquals(""String_Node_Str"",firstHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[0])));
  assertEquals(""String_Node_Str"",firstHop.getPort(),new PortNumber(PORT[0]));
}",0.9457013574660632
132152,"@Test public void testReadRenderedServicePathFirstHop() throws ExecutionException, InterruptedException {
  boolean ret;
  String sfcName=""String_Node_Str"";
  String[] sfNames={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  ServiceFunctionChainKey key=new ServiceFunctionChainKey(sfcName);
  List<SfcServiceFunction> sfcServiceFunctionList=new ArrayList<>();
  for (int i=0; i < sfNames.length; i++) {
    SfcServiceFunctionBuilder sfcSfBuilder=new SfcServiceFunctionBuilder();
    SfcServiceFunction sfcServiceFunction=sfcSfBuilder.setName(sfNames[i]).setKey(new SfcServiceFunctionKey(sfNames[i])).setType(sfTypes[i]).build();
    sfcServiceFunctionList.add(sfcServiceFunction);
  }
  ServiceFunctionChainBuilder sfcBuilder=new ServiceFunctionChainBuilder();
  sfcBuilder.setName(sfcName).setKey(key).setSfcServiceFunction(sfcServiceFunctionList).setSymmetric(true);
  Object[] parameters={sfcBuilder.build()};
  Class[] parameterTypes={ServiceFunctionChain.class};
  executor.submit(SfcProviderServiceChainAPI.getPut(parameters,parameterTypes)).get();
  Object[] parameters2={sfcName};
  Class[] parameterTypes2={String.class};
  Object result=executor.submit(SfcProviderServiceChainAPI.getRead(parameters2,parameterTypes2)).get();
  ServiceFunctionChain sfc2=(ServiceFunctionChain)result;
  assertNotNull(""String_Node_Str"",sfc2);
  assertEquals(""String_Node_Str"",sfc2.getSfcServiceFunction(),sfcServiceFunctionList);
  String pathName=""String_Node_Str"";
  ServiceFunctionPathBuilder pathBuilder=new ServiceFunctionPathBuilder();
  pathBuilder.setName(pathName).setServiceChainName(sfcName).setSymmetric(true);
  ServiceFunctionPath serviceFunctionPath=pathBuilder.build();
  assertNotNull(""String_Node_Str"",serviceFunctionPath);
  ret=SfcProviderServicePathAPI.putServiceFunctionPathExecutor(serviceFunctionPath);
  assertTrue(""String_Node_Str"",ret);
  RenderedServicePath renderedServicePath=null;
  RenderedServicePath revRenderedServicePath=null;
  CreateRenderedPathInputBuilder createRenderedPathInputBuilder=new CreateRenderedPathInputBuilder();
  createRenderedPathInputBuilder.setSymmetric(serviceFunctionPath.isSymmetric());
  renderedServicePath=SfcProviderRenderedPathAPI.createRenderedServicePathAndState(serviceFunctionPath,createRenderedPathInputBuilder.build());
  assertNotNull(""String_Node_Str"",renderedServicePath);
  revRenderedServicePath=SfcProviderRenderedPathAPI.createSymmetricRenderedServicePathAndState(renderedServicePath);
  assertNotNull(""String_Node_Str"",revRenderedServicePath);
  RenderedServicePathFirstHop firstHop;
  RenderedServicePathFirstHop lastHop;
  firstHop=SfcProviderRenderedPathAPI.readRenderedServicePathFirstHop(renderedServicePath.getName());
  assertNotNull(""String_Node_Str"",firstHop);
  lastHop=SfcProviderRenderedPathAPI.readRenderedServicePathFirstHop(revRenderedServicePath.getName());
  assertNotNull(""String_Node_Str"",lastHop);
  LOG.debug(""String_Node_Str"",firstHop.getIp().toString(),firstHop.getPort());
  LOG.debug(""String_Node_Str"",lastHop.getIp().toString(),lastHop.getPort());
  assertEquals(""String_Node_Str"",firstHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[0])));
  assertEquals(""String_Node_Str"",firstHop.getPort(),new PortNumber(PORT[0]));
  assertEquals(""String_Node_Str"",lastHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[SFF_LOCATOR_IP.length - 1])));
  assertEquals(""String_Node_Str"",lastHop.getPort(),new PortNumber(PORT[PORT.length - 1]));
  SfcProviderRenderedPathAPI.deleteRenderedServicePathExecutor(renderedServicePath.getName());
  SfcProviderRenderedPathAPI.deleteRenderedServicePathExecutor(revRenderedServicePath.getName());
}","@Test public void testReadRenderedServicePathFirstHop() throws ExecutionException, InterruptedException {
  boolean ret;
  String sfcName=""String_Node_Str"";
  String[] sfNames={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  ServiceFunctionChainKey key=new ServiceFunctionChainKey(sfcName);
  List<SfcServiceFunction> sfcServiceFunctionList=new ArrayList<>();
  for (int i=0; i < sfNames.length; i++) {
    SfcServiceFunctionBuilder sfcSfBuilder=new SfcServiceFunctionBuilder();
    SfcServiceFunction sfcServiceFunction=sfcSfBuilder.setName(sfNames[i]).setKey(new SfcServiceFunctionKey(sfNames[i])).setType(sfTypes[i]).build();
    sfcServiceFunctionList.add(sfcServiceFunction);
  }
  ServiceFunctionChainBuilder sfcBuilder=new ServiceFunctionChainBuilder();
  sfcBuilder.setName(sfcName).setKey(key).setSfcServiceFunction(sfcServiceFunctionList).setSymmetric(true);
  Object[] parameters={sfcBuilder.build()};
  Class[] parameterTypes={ServiceFunctionChain.class};
  executor.submit(SfcProviderServiceChainAPI.getPut(parameters,parameterTypes)).get();
  Object[] parameters2={sfcName};
  Class[] parameterTypes2={String.class};
  Object result=executor.submit(SfcProviderServiceChainAPI.getRead(parameters2,parameterTypes2)).get();
  ServiceFunctionChain sfc2=(ServiceFunctionChain)result;
  assertNotNull(""String_Node_Str"",sfc2);
  assertEquals(""String_Node_Str"",sfc2.getSfcServiceFunction(),sfcServiceFunctionList);
  String pathName=""String_Node_Str"";
  ServiceFunctionPathBuilder pathBuilder=new ServiceFunctionPathBuilder();
  pathBuilder.setName(pathName).setServiceChainName(sfcName).setSymmetric(true);
  ServiceFunctionPath serviceFunctionPath=pathBuilder.build();
  assertNotNull(""String_Node_Str"",serviceFunctionPath);
  ret=SfcProviderServicePathAPI.putServiceFunctionPathExecutor(serviceFunctionPath);
  assertTrue(""String_Node_Str"",ret);
  RenderedServicePath renderedServicePath=null;
  RenderedServicePath revRenderedServicePath=null;
  CreateRenderedPathInputBuilder createRenderedPathInputBuilder=new CreateRenderedPathInputBuilder();
  createRenderedPathInputBuilder.setSymmetric(serviceFunctionPath.isSymmetric());
  try {
    renderedServicePath=SfcProviderRenderedPathAPI.createRenderedServicePathAndState(serviceFunctionPath,createRenderedPathInputBuilder.build());
  }
 catch (  NullPointerException e) {
    e.printStackTrace();
  }
  assertNotNull(""String_Node_Str"",renderedServicePath);
  try {
    revRenderedServicePath=SfcProviderRenderedPathAPI.createSymmetricRenderedServicePathAndState(renderedServicePath);
  }
 catch (  NullPointerException e) {
    e.printStackTrace();
  }
  assertNotNull(""String_Node_Str"",revRenderedServicePath);
  RenderedServicePathFirstHop firstHop;
  RenderedServicePathFirstHop lastHop;
  firstHop=SfcProviderRenderedPathAPI.readRenderedServicePathFirstHop(renderedServicePath.getName());
  assertNotNull(""String_Node_Str"",firstHop);
  lastHop=SfcProviderRenderedPathAPI.readRenderedServicePathFirstHop(revRenderedServicePath.getName());
  assertNotNull(""String_Node_Str"",lastHop);
  LOG.debug(""String_Node_Str"",firstHop.getIp().toString(),firstHop.getPort());
  LOG.debug(""String_Node_Str"",lastHop.getIp().toString(),lastHop.getPort());
  assertEquals(""String_Node_Str"",firstHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[0])));
  assertEquals(""String_Node_Str"",firstHop.getPort(),new PortNumber(PORT[0]));
  assertEquals(""String_Node_Str"",lastHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[SFF_LOCATOR_IP.length - 1])));
  assertEquals(""String_Node_Str"",lastHop.getPort(),new PortNumber(PORT[PORT.length - 1]));
  SfcProviderRenderedPathAPI.deleteRenderedServicePathExecutor(renderedServicePath.getName());
  SfcProviderRenderedPathAPI.deleteRenderedServicePathExecutor(revRenderedServicePath.getName());
}",0.9789837722798616
132153,"/** 
 * Create a Rendered Path and all the associated operational state based on the given rendered service path <p>
 * @param serviceFunctionPath RSP Object
 * @return Nothing.
 */
protected RenderedServicePath createRenderedServicePathEntry(ServiceFunctionPath serviceFunctionPath,CreateRenderedPathInput createRenderedPathInput){
  printTraceStart(LOG);
  long pathId;
  int serviceIndex;
  RenderedServicePath ret=null;
  ServiceFunctionChain serviceFunctionChain;
  String serviceFunctionChainName=serviceFunctionPath.getServiceChainName();
  serviceFunctionChain=serviceFunctionChainName != null ? SfcProviderServiceChainAPI.readServiceFunctionChain(serviceFunctionChainName) : null;
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",serviceFunctionPath.getName());
    return ret;
  }
  RenderedServicePathBuilder renderedServicePathBuilder=new RenderedServicePathBuilder();
  serviceIndex=MAX_STARTING_INDEX;
  List<String> sfgNameList=getSfgNameList(serviceFunctionChain);
  List<String> sfNameList=scheduler.scheduleServiceFuntions(serviceFunctionChain,serviceIndex);
  if (sfNameList == null && sfgNameList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  List<RenderedServicePathHop> renderedServicePathHopArrayList=createRenderedServicePathHopList(sfNameList,sfgNameList,serviceIndex);
  if (renderedServicePathHopArrayList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  pathId=(serviceFunctionPath.getPathId() != null) ? serviceFunctionPath.getPathId() : numCreatedPathIncrementGet();
  renderedServicePathBuilder.setRenderedServicePathHop(renderedServicePathHopArrayList);
  if (createRenderedPathInput.getName() == null || createRenderedPathInput.getName().isEmpty()) {
    renderedServicePathBuilder.setName(serviceFunctionPath.getName() + ""String_Node_Str"" + pathId);
  }
 else {
    renderedServicePathBuilder.setName(createRenderedPathInput.getName());
  }
  renderedServicePathBuilder.setPathId(pathId);
  renderedServicePathBuilder.setStartingIndex((short)MAX_STARTING_INDEX);
  renderedServicePathBuilder.setServiceChainName(serviceFunctionChainName);
  renderedServicePathBuilder.setParentServiceFunctionPath(serviceFunctionPath.getName());
  if (serviceFunctionPath.getTransportType() == null) {
    renderedServicePathBuilder.setTransportType(VxlanGpe.class);
  }
 else {
    renderedServicePathBuilder.setTransportType(serviceFunctionPath.getTransportType());
  }
  RenderedServicePathKey renderedServicePathKey=new RenderedServicePathKey(renderedServicePathBuilder.getName());
  InstanceIdentifier<RenderedServicePath> rspIID;
  rspIID=InstanceIdentifier.builder(RenderedServicePaths.class).child(RenderedServicePath.class,renderedServicePathKey).build();
  RenderedServicePath renderedServicePath=renderedServicePathBuilder.build();
  if (SfcDataStoreAPI.writeMergeTransactionAPI(rspIID,renderedServicePath,LogicalDatastoreType.OPERATIONAL)) {
    ret=renderedServicePath;
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],serviceFunctionPath.getName());
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * Create a Rendered Path and all the associated operational state based on the given rendered service path <p>
 * @param serviceFunctionPath RSP Object
 * @return Nothing.
 */
protected RenderedServicePath createRenderedServicePathEntry(ServiceFunctionPath serviceFunctionPath,CreateRenderedPathInput createRenderedPathInput){
  printTraceStart(LOG);
  long pathId;
  int serviceIndex;
  RenderedServicePath ret=null;
  ServiceFunctionChain serviceFunctionChain;
  String serviceFunctionChainName=serviceFunctionPath.getServiceChainName();
  serviceFunctionChain=serviceFunctionChainName != null ? SfcProviderServiceChainAPI.readServiceFunctionChain(serviceFunctionChainName) : null;
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",serviceFunctionPath.getName());
    return ret;
  }
  RenderedServicePathBuilder renderedServicePathBuilder=new RenderedServicePathBuilder();
  serviceIndex=MAX_STARTING_INDEX;
  List<String> sfgNameList=getSfgNameList(serviceFunctionChain);
  List<String> sfNameList=scheduler.scheduleServiceFunctions(serviceFunctionChain,serviceIndex);
  if (sfNameList == null && sfgNameList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  List<RenderedServicePathHop> renderedServicePathHopArrayList=createRenderedServicePathHopList(sfNameList,sfgNameList,serviceIndex);
  if (renderedServicePathHopArrayList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  pathId=(serviceFunctionPath.getPathId() != null) ? serviceFunctionPath.getPathId() : numCreatedPathIncrementGet();
  renderedServicePathBuilder.setRenderedServicePathHop(renderedServicePathHopArrayList);
  if (createRenderedPathInput.getName() == null || createRenderedPathInput.getName().isEmpty()) {
    renderedServicePathBuilder.setName(serviceFunctionPath.getName() + ""String_Node_Str"" + pathId);
  }
 else {
    renderedServicePathBuilder.setName(createRenderedPathInput.getName());
  }
  renderedServicePathBuilder.setPathId(pathId);
  renderedServicePathBuilder.setStartingIndex((short)MAX_STARTING_INDEX);
  renderedServicePathBuilder.setServiceChainName(serviceFunctionChainName);
  renderedServicePathBuilder.setParentServiceFunctionPath(serviceFunctionPath.getName());
  if (serviceFunctionPath.getTransportType() == null) {
    renderedServicePathBuilder.setTransportType(VxlanGpe.class);
  }
 else {
    renderedServicePathBuilder.setTransportType(serviceFunctionPath.getTransportType());
  }
  RenderedServicePathKey renderedServicePathKey=new RenderedServicePathKey(renderedServicePathBuilder.getName());
  InstanceIdentifier<RenderedServicePath> rspIID;
  rspIID=InstanceIdentifier.builder(RenderedServicePaths.class).child(RenderedServicePath.class,renderedServicePathKey).build();
  RenderedServicePath renderedServicePath=renderedServicePathBuilder.build();
  if (SfcDataStoreAPI.writeMergeTransactionAPI(rspIID,renderedServicePath,LogicalDatastoreType.OPERATIONAL)) {
    ret=renderedServicePath;
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],serviceFunctionPath.getName());
  }
  printTraceStop(LOG);
  return ret;
}",0.9998397692677456
132154,"SfcProviderRenderedPathAPI(Object[] params,Class[] paramsTypes,String m){
  super(params,paramsTypes,m);
  initServiceFuntionScheduler();
}","SfcProviderRenderedPathAPI(Object[] params,Class[] paramsTypes,String m){
  super(params,paramsTypes,m);
  initServiceFunctionScheduler();
}",0.996415770609319
132155,"private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType){
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  String sftServiceFunctionName=sftServiceFunctionNameList.get(0).getName();
  ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(sftServiceFunctionName);
  if (!SfcProviderServiceFunctionAPI.putServiceFunctionMonitorExecutor(serviceFunction)) {
    LOG.error(""String_Node_Str"",serviceFunction.getName());
  }
  if (SfcProviderServiceFunctionAPI.readServiceFunctionDescriptionMonitorExecutor(sftServiceFunctionName) != null) {
    java.lang.Long preCPUUtilization=SfcProviderServiceFunctionAPI.readServiceFunctionDescriptionMonitorExecutor(sftServiceFunctionName).getMonitoringInfo().getResourceUtilization().getCPUUtilization();
    for (    SftServiceFunctionName curSftServiceFunctionName : sftServiceFunctionNameList) {
      java.lang.Long curCPUUtilization=SfcProviderServiceFunctionAPI.readServiceFunctionDescriptionMonitorExecutor(curSftServiceFunctionName.getName()).getMonitoringInfo().getResourceUtilization().getCPUUtilization();
      if (preCPUUtilization > curCPUUtilization) {
        preCPUUtilization=curCPUUtilization;
        sftServiceFunctionName=curSftServiceFunctionName.getName();
      }
    }
  }
  return sftServiceFunctionName;
}","private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType){
  boolean ret=false;
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  String sfName=null;
  String sftServiceFunctionName=null;
  java.lang.Long preCPUUtilization=java.lang.Long.MAX_VALUE;
  for (  SftServiceFunctionName curSftServiceFunctionName : sftServiceFunctionNameList) {
    sfName=curSftServiceFunctionName.getName();
    ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(sfName);
    if (serviceFunction == null) {
      LOG.error(""String_Node_Str"",sfName);
      continue;
    }
    ret=SfcProviderServiceFunctionAPI.putServiceFunctionMonitorExecutor(serviceFunction);
    if (ret == false) {
      LOG.error(""String_Node_Str"",sfName);
    }
    SfcSfDescMon sfcSfDescMon=SfcProviderServiceFunctionAPI.readServiceFunctionDescriptionMonitorExecutor(sfName);
    if (sfcSfDescMon == null) {
      sftServiceFunctionName=sfName;
      LOG.error(""String_Node_Str"",sfName);
      break;
    }
    java.lang.Long curCPUUtilization=sfcSfDescMon.getMonitoringInfo().getResourceUtilization().getCPUUtilization();
    if (preCPUUtilization > curCPUUtilization) {
      preCPUUtilization=curCPUUtilization;
      sftServiceFunctionName=sfName;
    }
  }
  if (sftServiceFunctionName == null) {
    LOG.error(""String_Node_Str"",serviceFunctionType.getType().getSimpleName());
  }
  return sftServiceFunctionName;
}",0.4833333333333333
132156,"private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType){
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  Random rad=new Random();
  return sftServiceFunctionNameList.get(rad.nextInt(sftServiceFunctionNameList.size())).getName();
}","private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType){
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  int maxTries=sftServiceFunctionNameList.size();
  Random rad=new Random();
  ServiceFunction serviceFunction=null;
  String serviceFunctionName=null;
  int start=rad.nextInt(sftServiceFunctionNameList.size());
  while (maxTries > 0) {
    serviceFunctionName=sftServiceFunctionNameList.get(start).getName();
    serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(serviceFunctionName);
    if (serviceFunction != null) {
      break;
    }
 else {
      LOG.debug(""String_Node_Str"",serviceFunctionName);
      maxTries--;
      serviceFunctionName=null;
      start=(start + 1) % sftServiceFunctionNameList.size();
    }
  }
  if (serviceFunctionName == null) {
    LOG.error(""String_Node_Str"",serviceFunctionType.getType().getSimpleName());
  }
  return serviceFunctionName;
}",0.3981623277182236
132157,"/** 
 * This method finds out name of the Service Function closest to Service Function preSfName per serviceFunctionType. <p>
 * @param serviceFunctionType Type of Service Function to find
 * @param preSfName Name of previous Service Function in Service Function Path
 * @param sfcProviderGraph Topology graph comprised of all the SFs and SFFs
 * @return String Name of the Service Function with type serviceFunctionType
 */
private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType,String preSfName,SfcProviderGraph sfcProviderGraph){
  String sfcProviderTopologyNodeName=null;
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  if (sftServiceFunctionNameList.size() == 0) {
    LOG.debug(""String_Node_Str"",serviceFunctionType);
    return null;
  }
  if (preSfName == null) {
    Random rad=new Random();
    sfcProviderTopologyNodeName=sftServiceFunctionNameList.get(rad.nextInt(sftServiceFunctionNameList.size())).getName();
    LOG.debug(""String_Node_Str"",sfcProviderTopologyNodeName);
    return sfcProviderTopologyNodeName;
  }
  SfcProviderTopologyNode preSfcProviderTopologyNode=sfcProviderGraph.getNode(preSfName);
  if (preSfcProviderTopologyNode == null) {
    LOG.debug(""String_Node_Str"",preSfName);
    return null;
  }
  int minLength=Integer.MAX_VALUE;
  int length=0;
  sfcProviderTopologyNodeName=null;
  for (  SftServiceFunctionName sftServiceFunctionName : sftServiceFunctionNameList) {
    String curSfName=sftServiceFunctionName.getName();
    List<SfcProviderTopologyNode> sfcProviderTopologyNodeList=sfcProviderGraph.getShortestPath(preSfName,curSfName);
    length=sfcProviderTopologyNodeList.size();
    if (length <= 1) {
      LOG.debug(""String_Node_Str"",preSfName,curSfName);
      continue;
    }
    if (minLength > length) {
      minLength=length;
      sfcProviderTopologyNodeName=curSfName;
    }
  }
  if (sfcProviderTopologyNodeName == null) {
    LOG.debug(""String_Node_Str"",preSfName);
  }
  return sfcProviderTopologyNodeName;
}","/** 
 * This method finds out name of the Service Function closest to Service Function preSfName per serviceFunctionType. <p>
 * @param serviceFunctionType Type of Service Function to find
 * @param preSfName Name of previous Service Function in Service Function Path
 * @param sfcProviderGraph Topology graph comprised of all the SFs and SFFs
 * @return String Name of the Service Function with type serviceFunctionType
 */
private String getServiceFunctionByType(ServiceFunctionType serviceFunctionType,String preSfName,SfcProviderGraph sfcProviderGraph){
  String sfcProviderTopologyNodeName=null;
  List<SftServiceFunctionName> sftServiceFunctionNameList=serviceFunctionType.getSftServiceFunctionName();
  int maxTries=sftServiceFunctionNameList.size();
  if (sftServiceFunctionNameList.size() == 0) {
    LOG.debug(""String_Node_Str"",serviceFunctionType);
    return null;
  }
  if (preSfName == null) {
    Random rad=new Random();
    int start=rad.nextInt(sftServiceFunctionNameList.size());
    SfcProviderTopologyNode firstHopNode=null;
    while (maxTries > 0) {
      sfcProviderTopologyNodeName=sftServiceFunctionNameList.get(start).getName();
      firstHopNode=sfcProviderGraph.getNode(sfcProviderTopologyNodeName);
      if (firstHopNode != null) {
        break;
      }
 else {
        LOG.debug(""String_Node_Str"",sfcProviderTopologyNodeName);
        sfcProviderTopologyNodeName=null;
        start=(start + 1) % sftServiceFunctionNameList.size();
        maxTries--;
      }
    }
    LOG.debug(""String_Node_Str"",sfcProviderTopologyNodeName);
    return sfcProviderTopologyNodeName;
  }
  SfcProviderTopologyNode preSfcProviderTopologyNode=sfcProviderGraph.getNode(preSfName);
  if (preSfcProviderTopologyNode == null) {
    LOG.debug(""String_Node_Str"",preSfName);
    return null;
  }
  int minLength=Integer.MAX_VALUE;
  int length=0;
  sfcProviderTopologyNodeName=null;
  for (  SftServiceFunctionName sftServiceFunctionName : sftServiceFunctionNameList) {
    String curSfName=sftServiceFunctionName.getName();
    SfcProviderTopologyNode curSfcProviderTopologyNode=sfcProviderGraph.getNode(curSfName);
    if (curSfcProviderTopologyNode == null) {
      continue;
    }
    List<SfcProviderTopologyNode> sfcProviderTopologyNodeList=sfcProviderGraph.getShortestPath(preSfName,curSfName);
    length=sfcProviderTopologyNodeList.size();
    if (length <= 1) {
      LOG.debug(""String_Node_Str"",preSfName,curSfName);
      continue;
    }
    if (minLength > length) {
      minLength=length;
      sfcProviderTopologyNodeName=curSfName;
    }
  }
  if (sfcProviderTopologyNodeName == null) {
    LOG.debug(""String_Node_Str"",preSfName);
  }
  return sfcProviderTopologyNodeName;
}",0.8258580753842915
132158,"@Before public void before() throws ExecutionException, InterruptedException {
  dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
  final String[] sfNames={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final IpAddress[] ipMgmtAddress=new IpAddress[sfNames.length];
  final IpAddress[] locatorIpAddress=new IpAddress[sfNames.length];
  SfDataPlaneLocator[] sfDataPlaneLocator=new SfDataPlaneLocator[sfNames.length];
  ServiceFunctionKey[] key=new ServiceFunctionKey[sfNames.length];
  for (int i=0; i < sfNames.length; i++) {
    ipMgmtAddress[i]=new IpAddress(new Ipv4Address(IP_MGMT_ADDRESS[0]));
    locatorIpAddress[i]=new IpAddress(new Ipv4Address(LOCATOR_IP_ADDRESS[0]));
    PortNumber portNumber=new PortNumber(PORT[i]);
    key[i]=new ServiceFunctionKey(sfNames[i]);
    IpBuilder ipBuilder=new IpBuilder();
    ipBuilder.setIp(locatorIpAddress[i]).setPort(portNumber);
    SfDataPlaneLocatorBuilder locatorBuilder=new SfDataPlaneLocatorBuilder();
    locatorBuilder.setName(LOCATOR_IP_ADDRESS[i]).setLocatorType(ipBuilder.build()).setServiceFunctionForwarder(SFF_NAMES[i]);
    sfDataPlaneLocator[i]=locatorBuilder.build();
    ServiceFunctionBuilder sfBuilder=new ServiceFunctionBuilder();
    List<SfDataPlaneLocator> dataPlaneLocatorList=new ArrayList<>();
    dataPlaneLocatorList.add(sfDataPlaneLocator[i]);
    sfBuilder.setName(sfNames[i]).setKey(key[i]).setType(sfTypes[i]).setIpMgmtAddress(ipMgmtAddress[i]).setSfDataPlaneLocator(dataPlaneLocatorList);
    sfList.add(sfBuilder.build());
  }
  ServiceFunctionsBuilder sfsBuilder=new ServiceFunctionsBuilder();
  sfsBuilder.setServiceFunction(sfList);
  executor.submit(SfcProviderServiceFunctionAPI.getPutAll(new Object[]{sfsBuilder.build()},new Class[]{ServiceFunctions.class})).get();
  for (  ServiceFunction serviceFunction : sfList) {
    boolean ret=SfcProviderServiceTypeAPI.createServiceFunctionTypeEntryExecutor(serviceFunction);
    LOG.debug(""String_Node_Str"",serviceFunction.getName());
    assertTrue(""String_Node_Str"",ret);
  }
  for (int i=0; i < SFF_NAMES.length; i++) {
    List<ConnectedSffDictionary> sffDictionaryList=new ArrayList<>();
    for (int j=0; j < 2; j++) {
      ConnectedSffDictionaryBuilder sffDictionaryEntryBuilder=new ConnectedSffDictionaryBuilder();
      ConnectedSffDictionary sffDictEntry=sffDictionaryEntryBuilder.setName(TO_SFF_NAMES[i][j]).build();
      sffDictionaryList.add(sffDictEntry);
    }
    List<ServiceFunctionDictionary> sfDictionaryList=new ArrayList<>();
    ServiceFunction serviceFunction=sfList.get(i);
    SfDataPlaneLocator sfDPLocator=serviceFunction.getSfDataPlaneLocator().get(0);
    SffSfDataPlaneLocatorBuilder sffSfDataPlaneLocatorBuilder=new SffSfDataPlaneLocatorBuilder(sfDPLocator);
    SffSfDataPlaneLocator sffSfDataPlaneLocator=sffSfDataPlaneLocatorBuilder.build();
    ServiceFunctionDictionaryBuilder dictionaryEntryBuilder=new ServiceFunctionDictionaryBuilder();
    dictionaryEntryBuilder.setName(serviceFunction.getName()).setKey(new ServiceFunctionDictionaryKey(serviceFunction.getName())).setType(serviceFunction.getType()).setSffSfDataPlaneLocator(sffSfDataPlaneLocator).setFailmode(Open.class).setSffInterfaces(null);
    ServiceFunctionDictionary sfDictEntry=dictionaryEntryBuilder.build();
    sfDictionaryList.add(sfDictEntry);
    List<SffDataPlaneLocator> locatorList=new ArrayList<>();
    IpBuilder ipBuilder=new IpBuilder();
    ipBuilder.setIp(new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[i]))).setPort(new PortNumber(PORT[i]));
    DataPlaneLocatorBuilder sffLocatorBuilder=new DataPlaneLocatorBuilder();
    sffLocatorBuilder.setLocatorType(ipBuilder.build()).setTransport(VxlanGpe.class);
    SffDataPlaneLocatorBuilder locatorBuilder=new SffDataPlaneLocatorBuilder();
    locatorBuilder.setName(SFF_LOCATOR_IP[i]).setKey(new SffDataPlaneLocatorKey(SFF_LOCATOR_IP[i])).setDataPlaneLocator(sffLocatorBuilder.build());
    locatorList.add(locatorBuilder.build());
    ServiceFunctionForwarderBuilder sffBuilder=new ServiceFunctionForwarderBuilder();
    sffBuilder.setName(SFF_NAMES[i]).setKey(new ServiceFunctionForwarderKey(SFF_NAMES[i])).setSffDataPlaneLocator(locatorList).setServiceFunctionDictionary(sfDictionaryList).setConnectedSffDictionary(sffDictionaryList).setServiceNode(null);
    ServiceFunctionForwarder sff=sffBuilder.build();
    executor.submit(SfcProviderServiceForwarderAPI.getPut(new Object[]{sff},new Class[]{ServiceFunctionForwarder.class})).get();
  }
}","@Before public void before() throws ExecutionException, InterruptedException {
  dataBroker=getDataBroker();
  opendaylightSfc.setDataProvider(dataBroker);
  executor=opendaylightSfc.getExecutor();
  executor.submit(SfcProviderServiceFunctionAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceForwarderAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceTypeAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServiceChainAPI.getDeleteAll(new Object[]{},new Class[]{}));
  executor.submit(SfcProviderServicePathAPI.getDeleteAll(new Object[]{},new Class[]{}));
  Thread.sleep(1000);
  final String[] sfNames={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final IpAddress[] ipMgmtAddress=new IpAddress[sfNames.length];
  final IpAddress[] locatorIpAddress=new IpAddress[sfNames.length];
  SfDataPlaneLocator[] sfDataPlaneLocator=new SfDataPlaneLocator[sfNames.length];
  ServiceFunctionKey[] key=new ServiceFunctionKey[sfNames.length];
  for (int i=0; i < sfNames.length; i++) {
    ipMgmtAddress[i]=new IpAddress(new Ipv4Address(IP_MGMT_ADDRESS[0]));
    locatorIpAddress[i]=new IpAddress(new Ipv4Address(LOCATOR_IP_ADDRESS[0]));
    PortNumber portNumber=new PortNumber(PORT[i]);
    key[i]=new ServiceFunctionKey(sfNames[i]);
    IpBuilder ipBuilder=new IpBuilder();
    ipBuilder.setIp(locatorIpAddress[i]).setPort(portNumber);
    SfDataPlaneLocatorBuilder locatorBuilder=new SfDataPlaneLocatorBuilder();
    locatorBuilder.setName(LOCATOR_IP_ADDRESS[i]).setLocatorType(ipBuilder.build()).setServiceFunctionForwarder(SFF_NAMES[i]);
    sfDataPlaneLocator[i]=locatorBuilder.build();
    ServiceFunctionBuilder sfBuilder=new ServiceFunctionBuilder();
    List<SfDataPlaneLocator> dataPlaneLocatorList=new ArrayList<>();
    dataPlaneLocatorList.add(sfDataPlaneLocator[i]);
    sfBuilder.setName(sfNames[i]).setKey(key[i]).setType(sfTypes[i]).setIpMgmtAddress(ipMgmtAddress[i]).setSfDataPlaneLocator(dataPlaneLocatorList);
    sfList.add(sfBuilder.build());
  }
  ServiceFunctionsBuilder sfsBuilder=new ServiceFunctionsBuilder();
  sfsBuilder.setServiceFunction(sfList);
  executor.submit(SfcProviderServiceFunctionAPI.getPutAll(new Object[]{sfsBuilder.build()},new Class[]{ServiceFunctions.class})).get();
  Thread.sleep(1000);
  for (  ServiceFunction serviceFunction : sfList) {
    boolean ret=SfcProviderServiceTypeAPI.createServiceFunctionTypeEntryExecutor(serviceFunction);
    LOG.debug(""String_Node_Str"",serviceFunction.getName());
    assertTrue(""String_Node_Str"",ret);
  }
  for (int i=0; i < SFF_NAMES.length; i++) {
    List<ConnectedSffDictionary> sffDictionaryList=new ArrayList<>();
    for (int j=0; j < 2; j++) {
      ConnectedSffDictionaryBuilder sffDictionaryEntryBuilder=new ConnectedSffDictionaryBuilder();
      ConnectedSffDictionary sffDictEntry=sffDictionaryEntryBuilder.setName(TO_SFF_NAMES[i][j]).build();
      sffDictionaryList.add(sffDictEntry);
    }
    List<ServiceFunctionDictionary> sfDictionaryList=new ArrayList<>();
    ServiceFunction serviceFunction=sfList.get(i);
    SfDataPlaneLocator sfDPLocator=serviceFunction.getSfDataPlaneLocator().get(0);
    SffSfDataPlaneLocatorBuilder sffSfDataPlaneLocatorBuilder=new SffSfDataPlaneLocatorBuilder(sfDPLocator);
    SffSfDataPlaneLocator sffSfDataPlaneLocator=sffSfDataPlaneLocatorBuilder.build();
    ServiceFunctionDictionaryBuilder dictionaryEntryBuilder=new ServiceFunctionDictionaryBuilder();
    dictionaryEntryBuilder.setName(serviceFunction.getName()).setKey(new ServiceFunctionDictionaryKey(serviceFunction.getName())).setType(serviceFunction.getType()).setSffSfDataPlaneLocator(sffSfDataPlaneLocator).setFailmode(Open.class).setSffInterfaces(null);
    ServiceFunctionDictionary sfDictEntry=dictionaryEntryBuilder.build();
    sfDictionaryList.add(sfDictEntry);
    List<SffDataPlaneLocator> locatorList=new ArrayList<>();
    IpBuilder ipBuilder=new IpBuilder();
    ipBuilder.setIp(new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[i]))).setPort(new PortNumber(PORT[i]));
    DataPlaneLocatorBuilder sffLocatorBuilder=new DataPlaneLocatorBuilder();
    sffLocatorBuilder.setLocatorType(ipBuilder.build()).setTransport(VxlanGpe.class);
    SffDataPlaneLocatorBuilder locatorBuilder=new SffDataPlaneLocatorBuilder();
    locatorBuilder.setName(SFF_LOCATOR_IP[i]).setKey(new SffDataPlaneLocatorKey(SFF_LOCATOR_IP[i])).setDataPlaneLocator(sffLocatorBuilder.build());
    locatorList.add(locatorBuilder.build());
    ServiceFunctionForwarderBuilder sffBuilder=new ServiceFunctionForwarderBuilder();
    sffBuilder.setName(SFF_NAMES[i]).setKey(new ServiceFunctionForwarderKey(SFF_NAMES[i])).setSffDataPlaneLocator(locatorList).setServiceFunctionDictionary(sfDictionaryList).setConnectedSffDictionary(sffDictionaryList).setServiceNode(null);
    ServiceFunctionForwarder sff=sffBuilder.build();
    executor.submit(SfcProviderServiceForwarderAPI.getPut(new Object[]{sff},new Class[]{ServiceFunctionForwarder.class})).get();
  }
  Thread.sleep(1000);
}",0.99346016646849
132159,"@Test public void testReadRspFirstHopBySftList() throws ExecutionException, InterruptedException {
  List<Class<? extends ServiceFunctionTypeIdentity>> sftList=new ArrayList<Class<? extends ServiceFunctionTypeIdentity>>();
  sftList.add(Firewall.class);
  sftList.add(Dpi.class);
  sftList.add(Napt44.class);
  sftList.add(HttpHeaderEnrichment.class);
  sftList.add(Qos.class);
  assertEquals(""String_Node_Str"",sftList.size(),5);
  RenderedServicePathFirstHop firstHop=SfcProviderRenderedPathAPI.readRspFirstHopBySftList(sftList);
  assertNotNull(""String_Node_Str"",firstHop);
  LOG.debug(""String_Node_Str"",firstHop.getIp().toString(),firstHop.getPort());
  assertEquals(""String_Node_Str"",firstHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[0])));
  assertEquals(""String_Node_Str"",firstHop.getPort(),new PortNumber(PORT[0]));
}","@Test public void testReadRspFirstHopBySftList() throws ExecutionException, InterruptedException {
  List<Class<? extends ServiceFunctionTypeIdentity>> sftList=new ArrayList<Class<? extends ServiceFunctionTypeIdentity>>();
  sftList.add(Firewall.class);
  sftList.add(Dpi.class);
  sftList.add(Napt44.class);
  sftList.add(HttpHeaderEnrichment.class);
  sftList.add(Qos.class);
  assertEquals(""String_Node_Str"",sftList.size(),5);
  RenderedServicePathFirstHop firstHop=null;
  try {
    firstHop=SfcProviderRenderedPathAPI.readRspFirstHopBySftList(sftList);
  }
 catch (  NullPointerException e) {
    e.printStackTrace();
  }
  assertNotNull(""String_Node_Str"",firstHop);
  LOG.debug(""String_Node_Str"",firstHop.getIp().toString(),firstHop.getPort());
  assertEquals(""String_Node_Str"",firstHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[0])));
  assertEquals(""String_Node_Str"",firstHop.getPort(),new PortNumber(PORT[0]));
}",0.9457013574660632
132160,"@Test public void testReadRenderedServicePathFirstHop() throws ExecutionException, InterruptedException {
  boolean ret;
  String sfcName=""String_Node_Str"";
  String[] sfNames={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  ServiceFunctionChainKey key=new ServiceFunctionChainKey(sfcName);
  List<SfcServiceFunction> sfcServiceFunctionList=new ArrayList<>();
  for (int i=0; i < sfNames.length; i++) {
    SfcServiceFunctionBuilder sfcSfBuilder=new SfcServiceFunctionBuilder();
    SfcServiceFunction sfcServiceFunction=sfcSfBuilder.setName(sfNames[i]).setKey(new SfcServiceFunctionKey(sfNames[i])).setType(sfTypes[i]).build();
    sfcServiceFunctionList.add(sfcServiceFunction);
  }
  ServiceFunctionChainBuilder sfcBuilder=new ServiceFunctionChainBuilder();
  sfcBuilder.setName(sfcName).setKey(key).setSfcServiceFunction(sfcServiceFunctionList).setSymmetric(true);
  Object[] parameters={sfcBuilder.build()};
  Class[] parameterTypes={ServiceFunctionChain.class};
  executor.submit(SfcProviderServiceChainAPI.getPut(parameters,parameterTypes)).get();
  Object[] parameters2={sfcName};
  Class[] parameterTypes2={String.class};
  Object result=executor.submit(SfcProviderServiceChainAPI.getRead(parameters2,parameterTypes2)).get();
  ServiceFunctionChain sfc2=(ServiceFunctionChain)result;
  assertNotNull(""String_Node_Str"",sfc2);
  assertEquals(""String_Node_Str"",sfc2.getSfcServiceFunction(),sfcServiceFunctionList);
  String pathName=""String_Node_Str"";
  ServiceFunctionPathBuilder pathBuilder=new ServiceFunctionPathBuilder();
  pathBuilder.setName(pathName).setServiceChainName(sfcName).setSymmetric(true);
  ServiceFunctionPath serviceFunctionPath=pathBuilder.build();
  assertNotNull(""String_Node_Str"",serviceFunctionPath);
  ret=SfcProviderServicePathAPI.putServiceFunctionPathExecutor(serviceFunctionPath);
  assertTrue(""String_Node_Str"",ret);
  RenderedServicePath renderedServicePath=null;
  RenderedServicePath revRenderedServicePath=null;
  CreateRenderedPathInputBuilder createRenderedPathInputBuilder=new CreateRenderedPathInputBuilder();
  createRenderedPathInputBuilder.setSymmetric(serviceFunctionPath.isSymmetric());
  renderedServicePath=SfcProviderRenderedPathAPI.createRenderedServicePathAndState(serviceFunctionPath,createRenderedPathInputBuilder.build());
  assertNotNull(""String_Node_Str"",renderedServicePath);
  revRenderedServicePath=SfcProviderRenderedPathAPI.createSymmetricRenderedServicePathAndState(renderedServicePath);
  assertNotNull(""String_Node_Str"",revRenderedServicePath);
  RenderedServicePathFirstHop firstHop;
  RenderedServicePathFirstHop lastHop;
  firstHop=SfcProviderRenderedPathAPI.readRenderedServicePathFirstHop(renderedServicePath.getName());
  assertNotNull(""String_Node_Str"",firstHop);
  lastHop=SfcProviderRenderedPathAPI.readRenderedServicePathFirstHop(revRenderedServicePath.getName());
  assertNotNull(""String_Node_Str"",lastHop);
  LOG.debug(""String_Node_Str"",firstHop.getIp().toString(),firstHop.getPort());
  LOG.debug(""String_Node_Str"",lastHop.getIp().toString(),lastHop.getPort());
  assertEquals(""String_Node_Str"",firstHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[0])));
  assertEquals(""String_Node_Str"",firstHop.getPort(),new PortNumber(PORT[0]));
  assertEquals(""String_Node_Str"",lastHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[SFF_LOCATOR_IP.length - 1])));
  assertEquals(""String_Node_Str"",lastHop.getPort(),new PortNumber(PORT[PORT.length - 1]));
  SfcProviderRenderedPathAPI.deleteRenderedServicePathExecutor(renderedServicePath.getName());
  SfcProviderRenderedPathAPI.deleteRenderedServicePathExecutor(revRenderedServicePath.getName());
}","@Test public void testReadRenderedServicePathFirstHop() throws ExecutionException, InterruptedException {
  boolean ret;
  String sfcName=""String_Node_Str"";
  String[] sfNames={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  ServiceFunctionChainKey key=new ServiceFunctionChainKey(sfcName);
  List<SfcServiceFunction> sfcServiceFunctionList=new ArrayList<>();
  for (int i=0; i < sfNames.length; i++) {
    SfcServiceFunctionBuilder sfcSfBuilder=new SfcServiceFunctionBuilder();
    SfcServiceFunction sfcServiceFunction=sfcSfBuilder.setName(sfNames[i]).setKey(new SfcServiceFunctionKey(sfNames[i])).setType(sfTypes[i]).build();
    sfcServiceFunctionList.add(sfcServiceFunction);
  }
  ServiceFunctionChainBuilder sfcBuilder=new ServiceFunctionChainBuilder();
  sfcBuilder.setName(sfcName).setKey(key).setSfcServiceFunction(sfcServiceFunctionList).setSymmetric(true);
  Object[] parameters={sfcBuilder.build()};
  Class[] parameterTypes={ServiceFunctionChain.class};
  executor.submit(SfcProviderServiceChainAPI.getPut(parameters,parameterTypes)).get();
  Object[] parameters2={sfcName};
  Class[] parameterTypes2={String.class};
  Object result=executor.submit(SfcProviderServiceChainAPI.getRead(parameters2,parameterTypes2)).get();
  ServiceFunctionChain sfc2=(ServiceFunctionChain)result;
  assertNotNull(""String_Node_Str"",sfc2);
  assertEquals(""String_Node_Str"",sfc2.getSfcServiceFunction(),sfcServiceFunctionList);
  String pathName=""String_Node_Str"";
  ServiceFunctionPathBuilder pathBuilder=new ServiceFunctionPathBuilder();
  pathBuilder.setName(pathName).setServiceChainName(sfcName).setSymmetric(true);
  ServiceFunctionPath serviceFunctionPath=pathBuilder.build();
  assertNotNull(""String_Node_Str"",serviceFunctionPath);
  ret=SfcProviderServicePathAPI.putServiceFunctionPathExecutor(serviceFunctionPath);
  assertTrue(""String_Node_Str"",ret);
  RenderedServicePath renderedServicePath=null;
  RenderedServicePath revRenderedServicePath=null;
  CreateRenderedPathInputBuilder createRenderedPathInputBuilder=new CreateRenderedPathInputBuilder();
  createRenderedPathInputBuilder.setSymmetric(serviceFunctionPath.isSymmetric());
  try {
    renderedServicePath=SfcProviderRenderedPathAPI.createRenderedServicePathAndState(serviceFunctionPath,createRenderedPathInputBuilder.build());
  }
 catch (  NullPointerException e) {
    e.printStackTrace();
  }
  assertNotNull(""String_Node_Str"",renderedServicePath);
  try {
    revRenderedServicePath=SfcProviderRenderedPathAPI.createSymmetricRenderedServicePathAndState(renderedServicePath);
  }
 catch (  NullPointerException e) {
    e.printStackTrace();
  }
  assertNotNull(""String_Node_Str"",revRenderedServicePath);
  RenderedServicePathFirstHop firstHop;
  RenderedServicePathFirstHop lastHop;
  firstHop=SfcProviderRenderedPathAPI.readRenderedServicePathFirstHop(renderedServicePath.getName());
  assertNotNull(""String_Node_Str"",firstHop);
  lastHop=SfcProviderRenderedPathAPI.readRenderedServicePathFirstHop(revRenderedServicePath.getName());
  assertNotNull(""String_Node_Str"",lastHop);
  LOG.debug(""String_Node_Str"",firstHop.getIp().toString(),firstHop.getPort());
  LOG.debug(""String_Node_Str"",lastHop.getIp().toString(),lastHop.getPort());
  assertEquals(""String_Node_Str"",firstHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[0])));
  assertEquals(""String_Node_Str"",firstHop.getPort(),new PortNumber(PORT[0]));
  assertEquals(""String_Node_Str"",lastHop.getIp(),new IpAddress(new Ipv4Address(SFF_LOCATOR_IP[SFF_LOCATOR_IP.length - 1])));
  assertEquals(""String_Node_Str"",lastHop.getPort(),new PortNumber(PORT[PORT.length - 1]));
  SfcProviderRenderedPathAPI.deleteRenderedServicePathExecutor(renderedServicePath.getName());
  SfcProviderRenderedPathAPI.deleteRenderedServicePathExecutor(revRenderedServicePath.getName());
}",0.9789837722798616
132161,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node node=(Node)entry.getValue();
      String nodeName=node.getNodeId().getValue();
      LOG.debug(""String_Node_Str"",node.toString());
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    Node node=null;
    if (entry.getKey().getTargetType() == NetconfNode.class) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeId.getValue());
      NetconfNode nnode=(NetconfNode)entry.getValue();
      NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
      if (csts == NetconfNodeFields.ConnectionStatus.Connected) {
        List<String> capabilities=nnode.getAvailableCapabilities().getAvailableCapability();
        LOG.info(""String_Node_Str"",capabilities);
      }
      if (SfcProviderServiceForwarderAPI.putServiceFunctionForwarderExecutor(SfcNetconfServiceForwarderAPI.buildServiceForwarderFromNetonf(nodeName,nnode))) {
        LOG.info(""String_Node_Str"",nodeName);
      }
 else {
        LOG.error(""String_Node_Str"",nodeName);
      }
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if (entry.getKey().getTargetType() == NetconfNode.class) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeId.getValue());
      NetconfNode nnode=(NetconfNode)entry.getValue();
      NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
      if (csts == NetconfNodeFields.ConnectionStatus.Connected) {
        List<String> capabilities=nnode.getAvailableCapabilities().getAvailableCapability();
        LOG.info(""String_Node_Str"",capabilities);
      }
    }
  }
  printTraceStop(LOG);
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node node=(Node)entry.getValue();
      String nodeName=node.getNodeId().getValue();
      LOG.debug(""String_Node_Str"",node.toString());
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    Node node=null;
    if (entry.getKey().getTargetType() == NetconfNode.class) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeId.getValue());
      NetconfNode nnode=(NetconfNode)entry.getValue();
      NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
      if (csts == NetconfNodeFields.ConnectionStatus.Connected) {
        List<String> capabilities=nnode.getAvailableCapabilities().getAvailableCapability();
        LOG.info(""String_Node_Str"",capabilities);
      }
      if (SfcProviderServiceForwarderAPI.putServiceFunctionForwarderExecutor(SfcNetconfServiceForwarderAPI.buildServiceForwarderFromNetonf(nodeName,nnode))) {
        LOG.info(""String_Node_Str"",nodeName);
      }
 else {
        LOG.error(""String_Node_Str"",nodeName);
      }
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if ((entry.getKey().getTargetType() == NetconfNode.class) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeId.getValue());
      NetconfNode nnode=(NetconfNode)entry.getValue();
      NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
      if (csts == NetconfNodeFields.ConnectionStatus.Connected) {
        if (SfcProviderServiceForwarderAPI.putServiceFunctionForwarderExecutor(SfcNetconfServiceForwarderAPI.buildServiceForwarderFromNetonf(nodeName,nnode))) {
          LOG.info(""String_Node_Str"",nodeName);
        }
 else {
          LOG.error(""String_Node_Str"",nodeName);
        }
      }
    }
  }
  printTraceStop(LOG);
}",0.9051221093581154
132162,"/** 
 * Create a Rendered Path and all the associated operational state based on the given rendered service path <p>
 * @param serviceFunctionPath RSP Object
 * @return Nothing.
 */
protected RenderedServicePath createRenderedServicePathEntry(ServiceFunctionPath serviceFunctionPath,CreateRenderedPathInput createRenderedPathInput){
  printTraceStart(LOG);
  long pathId;
  int serviceIndex;
  RenderedServicePath ret=null;
  ServiceFunctionChain serviceFunctionChain;
  String serviceFunctionChainName=serviceFunctionPath.getServiceChainName();
  serviceFunctionChain=serviceFunctionChainName != null ? SfcProviderServiceChainAPI.readServiceFunctionChain(serviceFunctionChainName) : null;
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",serviceFunctionPath.getName());
    return ret;
  }
  RenderedServicePathBuilder renderedServicePathBuilder=new RenderedServicePathBuilder();
  serviceIndex=MAX_STARTING_INDEX;
  List<String> sfNameList=scheduler.scheduleServiceFuntions(serviceFunctionChain,serviceIndex);
  if (sfNameList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  List<RenderedServicePathHop> renderedServicePathHopArrayList=createRenderedServicePathHopList(sfNameList,serviceIndex);
  if (renderedServicePathHopArrayList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  pathId=(serviceFunctionPath.getPathId() != null) ? serviceFunctionPath.getPathId() : numCreatedPathIncrementGet();
  renderedServicePathBuilder.setRenderedServicePathHop(renderedServicePathHopArrayList);
  if (createRenderedPathInput.getName() == null || createRenderedPathInput.getName().isEmpty()) {
    renderedServicePathBuilder.setName(serviceFunctionChainName + ""String_Node_Str"" + pathId);
  }
 else {
    renderedServicePathBuilder.setName(createRenderedPathInput.getName());
  }
  renderedServicePathBuilder.setPathId(pathId);
  renderedServicePathBuilder.setStartingIndex((short)MAX_STARTING_INDEX);
  renderedServicePathBuilder.setServiceChainName(serviceFunctionChainName);
  renderedServicePathBuilder.setParentServiceFunctionPath(serviceFunctionPath.getName());
  RenderedServicePathKey renderedServicePathKey=new RenderedServicePathKey(renderedServicePathBuilder.getName());
  InstanceIdentifier<RenderedServicePath> rspIID;
  rspIID=InstanceIdentifier.builder(RenderedServicePaths.class).child(RenderedServicePath.class,renderedServicePathKey).build();
  RenderedServicePath renderedServicePath=renderedServicePathBuilder.build();
  if (SfcDataStoreAPI.writeMergeTransactionAPI(rspIID,renderedServicePath,LogicalDatastoreType.OPERATIONAL)) {
    ret=renderedServicePath;
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],serviceFunctionPath.getName());
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * Create a Rendered Path and all the associated operational state based on the given rendered service path <p>
 * @param serviceFunctionPath RSP Object
 * @return Nothing.
 */
protected RenderedServicePath createRenderedServicePathEntry(ServiceFunctionPath serviceFunctionPath,CreateRenderedPathInput createRenderedPathInput){
  printTraceStart(LOG);
  long pathId;
  int serviceIndex;
  RenderedServicePath ret=null;
  ServiceFunctionChain serviceFunctionChain;
  String serviceFunctionChainName=serviceFunctionPath.getServiceChainName();
  serviceFunctionChain=serviceFunctionChainName != null ? SfcProviderServiceChainAPI.readServiceFunctionChain(serviceFunctionChainName) : null;
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",serviceFunctionPath.getName());
    return ret;
  }
  RenderedServicePathBuilder renderedServicePathBuilder=new RenderedServicePathBuilder();
  serviceIndex=MAX_STARTING_INDEX;
  List<String> sfNameList=scheduler.scheduleServiceFuntions(serviceFunctionChain,serviceIndex);
  if (sfNameList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  List<RenderedServicePathHop> renderedServicePathHopArrayList=createRenderedServicePathHopList(sfNameList,serviceIndex);
  if (renderedServicePathHopArrayList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  pathId=(serviceFunctionPath.getPathId() != null) ? serviceFunctionPath.getPathId() : numCreatedPathIncrementGet();
  renderedServicePathBuilder.setRenderedServicePathHop(renderedServicePathHopArrayList);
  if (createRenderedPathInput.getName() == null || createRenderedPathInput.getName().isEmpty()) {
    renderedServicePathBuilder.setName(serviceFunctionPath.getName() + ""String_Node_Str"" + pathId);
  }
 else {
    renderedServicePathBuilder.setName(createRenderedPathInput.getName());
  }
  renderedServicePathBuilder.setPathId(pathId);
  renderedServicePathBuilder.setStartingIndex((short)MAX_STARTING_INDEX);
  renderedServicePathBuilder.setServiceChainName(serviceFunctionChainName);
  renderedServicePathBuilder.setParentServiceFunctionPath(serviceFunctionPath.getName());
  RenderedServicePathKey renderedServicePathKey=new RenderedServicePathKey(renderedServicePathBuilder.getName());
  InstanceIdentifier<RenderedServicePath> rspIID;
  rspIID=InstanceIdentifier.builder(RenderedServicePaths.class).child(RenderedServicePath.class,renderedServicePathKey).build();
  RenderedServicePath renderedServicePath=renderedServicePathBuilder.build();
  if (SfcDataStoreAPI.writeMergeTransactionAPI(rspIID,renderedServicePath,LogicalDatastoreType.OPERATIONAL)) {
    ret=renderedServicePath;
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],serviceFunctionPath.getName());
  }
  printTraceStop(LOG);
  return ret;
}",0.9973132724341752
132163,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node node=(Node)entry.getValue();
      String nodeName=node.getNodeId().getValue();
      LOG.debug(""String_Node_Str"",node.toString());
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    Node node=null;
    if (entry.getKey().getTargetType() == NetconfNode.class) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeId.getValue());
      NetconfNode nnode=(NetconfNode)entry.getValue();
      NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
      if (csts == NetconfNodeFields.ConnectionStatus.Connected) {
        List<String> capabilities=nnode.getAvailableCapabilities().getAvailableCapability();
        LOG.info(""String_Node_Str"",capabilities);
      }
      if (SfcProviderServiceForwarderAPI.putServiceFunctionForwarderExecutor(SfcNetconfServiceForwarderAPI.buildServiceForwarderFromNetonf(nodeName,nnode))) {
        LOG.info(""String_Node_Str"",nodeName);
      }
 else {
        LOG.error(""String_Node_Str"",nodeName);
      }
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if (entry.getKey().getTargetType() == NetconfNode.class) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeId.getValue());
      NetconfNode nnode=(NetconfNode)entry.getValue();
      NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
      if (csts == NetconfNodeFields.ConnectionStatus.Connected) {
        List<String> capabilities=nnode.getAvailableCapabilities().getAvailableCapability();
        LOG.info(""String_Node_Str"",capabilities);
      }
    }
  }
  printTraceStop(LOG);
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof Node) {
      Node node=(Node)entry.getValue();
      String nodeName=node.getNodeId().getValue();
      LOG.debug(""String_Node_Str"",node.toString());
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getCreatedData().entrySet()) {
    Node node=null;
    if (entry.getKey().getTargetType() == NetconfNode.class) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeId.getValue());
      NetconfNode nnode=(NetconfNode)entry.getValue();
      NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
      if (csts == NetconfNodeFields.ConnectionStatus.Connected) {
        List<String> capabilities=nnode.getAvailableCapabilities().getAvailableCapability();
        LOG.info(""String_Node_Str"",capabilities);
      }
      if (SfcProviderServiceForwarderAPI.putServiceFunctionForwarderExecutor(SfcNetconfServiceForwarderAPI.buildServiceForwarderFromNetonf(nodeName,nnode))) {
        LOG.info(""String_Node_Str"",nodeName);
      }
 else {
        LOG.error(""String_Node_Str"",nodeName);
      }
    }
  }
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : change.getUpdatedData().entrySet()) {
    if ((entry.getKey().getTargetType() == NetconfNode.class) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      NodeId nodeId=getNodeId(entry);
      String nodeName=nodeId.getValue();
      LOG.info(""String_Node_Str"",nodeId.getValue());
      NetconfNode nnode=(NetconfNode)entry.getValue();
      NetconfNodeFields.ConnectionStatus csts=nnode.getConnectionStatus();
      if (csts == NetconfNodeFields.ConnectionStatus.Connected) {
        if (SfcProviderServiceForwarderAPI.putServiceFunctionForwarderExecutor(SfcNetconfServiceForwarderAPI.buildServiceForwarderFromNetonf(nodeName,nnode))) {
          LOG.info(""String_Node_Str"",nodeName);
        }
 else {
          LOG.error(""String_Node_Str"",nodeName);
        }
      }
    }
  }
  printTraceStop(LOG);
}",0.9051221093581154
132164,"/** 
 * Create a Rendered Path and all the associated operational state based on the given rendered service path <p>
 * @param serviceFunctionPath RSP Object
 * @return Nothing.
 */
protected RenderedServicePath createRenderedServicePathEntry(ServiceFunctionPath serviceFunctionPath,CreateRenderedPathInput createRenderedPathInput){
  printTraceStart(LOG);
  long pathId;
  int serviceIndex;
  RenderedServicePath ret=null;
  ServiceFunctionChain serviceFunctionChain;
  String serviceFunctionChainName=serviceFunctionPath.getServiceChainName();
  serviceFunctionChain=serviceFunctionChainName != null ? SfcProviderServiceChainAPI.readServiceFunctionChain(serviceFunctionChainName) : null;
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",serviceFunctionPath.getName());
    return ret;
  }
  RenderedServicePathBuilder renderedServicePathBuilder=new RenderedServicePathBuilder();
  serviceIndex=MAX_STARTING_INDEX;
  List<String> sfNameList=scheduler.scheduleServiceFuntions(serviceFunctionChain,serviceIndex);
  if (sfNameList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  List<RenderedServicePathHop> renderedServicePathHopArrayList=createRenderedServicePathHopList(sfNameList,serviceIndex);
  if (renderedServicePathHopArrayList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  pathId=(serviceFunctionPath.getPathId() != null) ? serviceFunctionPath.getPathId() : numCreatedPathIncrementGet();
  renderedServicePathBuilder.setRenderedServicePathHop(renderedServicePathHopArrayList);
  if (createRenderedPathInput.getName() == null || createRenderedPathInput.getName().isEmpty()) {
    renderedServicePathBuilder.setName(serviceFunctionChainName + ""String_Node_Str"" + pathId);
  }
 else {
    renderedServicePathBuilder.setName(createRenderedPathInput.getName());
  }
  renderedServicePathBuilder.setPathId(pathId);
  renderedServicePathBuilder.setStartingIndex((short)MAX_STARTING_INDEX);
  renderedServicePathBuilder.setServiceChainName(serviceFunctionChainName);
  renderedServicePathBuilder.setParentServiceFunctionPath(serviceFunctionPath.getName());
  RenderedServicePathKey renderedServicePathKey=new RenderedServicePathKey(renderedServicePathBuilder.getName());
  InstanceIdentifier<RenderedServicePath> rspIID;
  rspIID=InstanceIdentifier.builder(RenderedServicePaths.class).child(RenderedServicePath.class,renderedServicePathKey).build();
  RenderedServicePath renderedServicePath=renderedServicePathBuilder.build();
  if (SfcDataStoreAPI.writeMergeTransactionAPI(rspIID,renderedServicePath,LogicalDatastoreType.OPERATIONAL)) {
    ret=renderedServicePath;
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],serviceFunctionPath.getName());
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * Create a Rendered Path and all the associated operational state based on the given rendered service path <p>
 * @param serviceFunctionPath RSP Object
 * @return Nothing.
 */
protected RenderedServicePath createRenderedServicePathEntry(ServiceFunctionPath serviceFunctionPath,CreateRenderedPathInput createRenderedPathInput){
  printTraceStart(LOG);
  long pathId;
  int serviceIndex;
  RenderedServicePath ret=null;
  ServiceFunctionChain serviceFunctionChain;
  String serviceFunctionChainName=serviceFunctionPath.getServiceChainName();
  serviceFunctionChain=serviceFunctionChainName != null ? SfcProviderServiceChainAPI.readServiceFunctionChain(serviceFunctionChainName) : null;
  if (serviceFunctionChain == null) {
    LOG.error(""String_Node_Str"",serviceFunctionPath.getName());
    return ret;
  }
  RenderedServicePathBuilder renderedServicePathBuilder=new RenderedServicePathBuilder();
  serviceIndex=MAX_STARTING_INDEX;
  List<String> sfNameList=scheduler.scheduleServiceFuntions(serviceFunctionChain,serviceIndex);
  if (sfNameList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  List<RenderedServicePathHop> renderedServicePathHopArrayList=createRenderedServicePathHopList(sfNameList,serviceIndex);
  if (renderedServicePathHopArrayList == null) {
    LOG.warn(""String_Node_Str"");
    return null;
  }
  pathId=(serviceFunctionPath.getPathId() != null) ? serviceFunctionPath.getPathId() : numCreatedPathIncrementGet();
  renderedServicePathBuilder.setRenderedServicePathHop(renderedServicePathHopArrayList);
  if (createRenderedPathInput.getName() == null || createRenderedPathInput.getName().isEmpty()) {
    renderedServicePathBuilder.setName(serviceFunctionPath.getName() + ""String_Node_Str"" + pathId);
  }
 else {
    renderedServicePathBuilder.setName(createRenderedPathInput.getName());
  }
  renderedServicePathBuilder.setPathId(pathId);
  renderedServicePathBuilder.setStartingIndex((short)MAX_STARTING_INDEX);
  renderedServicePathBuilder.setServiceChainName(serviceFunctionChainName);
  renderedServicePathBuilder.setParentServiceFunctionPath(serviceFunctionPath.getName());
  RenderedServicePathKey renderedServicePathKey=new RenderedServicePathKey(renderedServicePathBuilder.getName());
  InstanceIdentifier<RenderedServicePath> rspIID;
  rspIID=InstanceIdentifier.builder(RenderedServicePaths.class).child(RenderedServicePath.class,renderedServicePathKey).build();
  RenderedServicePath renderedServicePath=renderedServicePathBuilder.build();
  if (SfcDataStoreAPI.writeMergeTransactionAPI(rspIID,renderedServicePath,LogicalDatastoreType.OPERATIONAL)) {
    ret=renderedServicePath;
  }
 else {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],serviceFunctionPath.getName());
  }
  printTraceStop(LOG);
  return ret;
}",0.9973132724341752
132165,"public SfcL2AclDataListener(DataBroker dataBroker,SfcL2FlowProgrammer sfcL2FlowProgrammer){
  setDataBroker(dataBroker);
  setIID(OpendaylightSfc.SFP_ENTRY_IID);
  registerAsDataChangeListener();
  this.sfcL2FlowProgrammer=sfcL2FlowProgrammer;
}","public SfcL2AclDataListener(DataBroker dataBroker,SfcL2FlowProgrammer sfcL2FlowProgrammer){
  setDataBroker(dataBroker);
  setIID(OpendaylightSfc.ACL_ENTRY_IID);
  registerAsDataChangeListener();
  this.sfcL2FlowProgrammer=sfcL2FlowProgrammer;
}",0.9877551020408164
132166,"private void writeFlowToConfig(FlowBuilder flow){
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setId(new NodeId(sffNodeName));
  nodeBuilder.setKey(new NodeKey(nodeBuilder.getId()));
  InstanceIdentifier<Flow> flowIID=InstanceIdentifier.builder(Nodes.class).child(Node.class,nodeBuilder.getKey()).augmentation(FlowCapableNode.class).child(Table.class,new TableKey(flow.getTableId())).child(Flow.class,flow.getKey()).build();
  if (!SfcDataStoreAPI.writeMergeTransactionAPI(flowIID,flow.build(),LogicalDatastoreType.OPERATIONAL)) {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sffNodeName);
  }
}","private void writeFlowToConfig(FlowBuilder flow){
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setId(new NodeId(sffNodeName));
  nodeBuilder.setKey(new NodeKey(nodeBuilder.getId()));
  InstanceIdentifier<Flow> flowInstanceId=InstanceIdentifier.builder(Nodes.class).child(Node.class,nodeBuilder.getKey()).augmentation(FlowCapableNode.class).child(Table.class,new TableKey(flow.getTableId())).child(Flow.class,flow.getKey()).build();
  LOG.info(""String_Node_Str"",sffNodeName,flow.getTableId());
  LOG.debug(""String_Node_Str"",flowInstanceId);
  if (!SfcDataStoreAPI.writeMergeTransactionAPI(flowInstanceId,flow.build(),LogicalDatastoreType.CONFIGURATION)) {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sffNodeName);
  }
}",0.7542857142857143
132167,"private void removeFlowFromConfig(FlowBuilder flow){
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setId(new NodeId(sffNodeName));
  nodeBuilder.setKey(new NodeKey(nodeBuilder.getId()));
  InstanceIdentifier<Flow> flowIID=InstanceIdentifier.builder(Nodes.class).child(Node.class,nodeBuilder.getKey()).augmentation(FlowCapableNode.class).child(Table.class,new TableKey(flow.getTableId())).child(Flow.class,flow.getKey()).build();
  if (!SfcDataStoreAPI.deleteTransactionAPI(flowIID,LogicalDatastoreType.OPERATIONAL)) {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sffNodeName);
  }
}","private void removeFlowFromConfig(FlowBuilder flow){
  NodeBuilder nodeBuilder=new NodeBuilder();
  nodeBuilder.setId(new NodeId(sffNodeName));
  nodeBuilder.setKey(new NodeKey(nodeBuilder.getId()));
  InstanceIdentifier<Flow> flowInstanceId=InstanceIdentifier.builder(Nodes.class).child(Node.class,nodeBuilder.getKey()).augmentation(FlowCapableNode.class).child(Table.class,new TableKey(flow.getTableId())).child(Flow.class,flow.getKey()).build();
  if (!SfcDataStoreAPI.deleteTransactionAPI(flowInstanceId,LogicalDatastoreType.OPERATIONAL)) {
    LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sffNodeName);
  }
}",0.98256735340729
132168,"private void configureSffFlows(RenderedServicePath rsp,boolean isAddFlow){
  RenderedServicePathHop servicePathHopPrev=null;
  Long sfpId=0L;
  sfpId=rsp.getPathId();
  Iterator<RenderedServicePathHop> servicePathHopIter=rsp.getRenderedServicePathHop().iterator();
  String curSFFName=null;
  RenderedServicePathHop servicePathHopCur=null;
  while (servicePathHopIter.hasNext()) {
    servicePathHopCur=servicePathHopIter.next();
    curSFFName=servicePathHopCur.getServiceFunctionForwarder();
    this.sfcL2FlowProgrammer.setNodeInfo(curSFFName);
    this.sfcL2FlowProgrammer.configureIngressTransportFlow(isAddFlow);
    this.sfcL2FlowProgrammer.configureSffNextHopDefaultFlow(isAddFlow);
    ServiceFunctionForwarder curSff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarderExecutor(curSFFName);
    List<SffDataPlaneLocator> curSffDataPlanelocatorList=curSff.getSffDataPlaneLocator();
    for (    SffDataPlaneLocator curSffDataPlanelocator : curSffDataPlanelocatorList) {
      LocatorType curSFFLocatorType=curSffDataPlanelocator.getDataPlaneLocator().getLocatorType();
      if (curSFFLocatorType.getImplementedInterface().equals(Mac.class)) {
        int curSFFVlan=((MacAddressLocator)curSffDataPlanelocator.getDataPlaneLocator().getLocatorType()).getVlanId();
        this.sfcL2FlowProgrammer.configureIngressFlow(curSFFVlan,isAddFlow);
      }
    }
    ServiceFunction curSF=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(servicePathHopCur.getServiceFunctionName());
    List<SfDataPlaneLocator> curSfDataPlaneLocatorList=curSF.getSfDataPlaneLocator();
    for (    SfDataPlaneLocator curSfDataPlanelocator : curSfDataPlaneLocatorList) {
      LocatorType curSFLocatorType=curSfDataPlanelocator.getLocatorType();
      if (curSFLocatorType.getImplementedInterface().equals(Mac.class)) {
        int curSFVlan=((MacAddressLocator)curSfDataPlanelocator.getLocatorType()).getVlanId();
        this.sfcL2FlowProgrammer.configureIngressFlow(curSFVlan,isAddFlow);
      }
    }
    if (servicePathHopPrev == null) {
      installDefaultNextHopEntry(rsp.getPathId(),servicePathHopCur,isAddFlow);
    }
 else {
      String prevSFSrcMac=null;
      ServiceFunction prevSF;
      ServiceFunctionForwarder prevSFF;
      List<SfDataPlaneLocator> prevSFDataPlaneLocatorList;
      prevSFF=SfcProviderServiceForwarderAPI.readServiceFunctionForwarderExecutor(servicePathHopPrev.getServiceFunctionForwarder());
      prevSF=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(servicePathHopPrev.getServiceFunctionName());
      prevSFDataPlaneLocatorList=prevSF.getSfDataPlaneLocator();
      if (servicePathHopPrev.getServiceFunctionForwarder().equals(curSFFName)) {
        for (        SfDataPlaneLocator curSfDataPlanelocator : curSfDataPlaneLocatorList) {
          LocatorType curSFLocatorType=curSfDataPlanelocator.getLocatorType();
          if (curSFLocatorType.getImplementedInterface().equals(Mac.class)) {
            MacAddressLocator macAddressLocator;
            macAddressLocator=(MacAddressLocator)curSFLocatorType;
            String dstMac=macAddressLocator.getMac().getValue();
            int dstVlan=macAddressLocator.getVlanId();
            for (            SfDataPlaneLocator prevSFDataPlanelocator : prevSFDataPlaneLocatorList) {
              LocatorType prevSFLocatorType=prevSFDataPlanelocator.getLocatorType();
              if (prevSFLocatorType.getImplementedInterface().equals(Mac.class)) {
                prevSFSrcMac=((MacAddressLocator)prevSFLocatorType).getMac().getValue();
                this.sfcL2FlowProgrammer.configureNextHopFlow(rsp.getPathId(),prevSFSrcMac,dstMac,dstVlan,isAddFlow);
              }
            }
            break;
          }
        }
      }
 else {
        for (        SfDataPlaneLocator curSfDataPlanelocator : curSfDataPlaneLocatorList) {
          LocatorType curSFLocatorType=curSfDataPlanelocator.getLocatorType();
          if (curSFLocatorType.getImplementedInterface().equals(Mac.class)) {
            MacAddressLocator macAddressLocator;
            macAddressLocator=(MacAddressLocator)curSFLocatorType;
            String dstMac=macAddressLocator.getMac().getValue();
            int dstVlan=macAddressLocator.getVlanId();
            for (            SffDataPlaneLocator curSffDataPlanelocator : curSffDataPlanelocatorList) {
              String srcMacSff=((MacAddressLocator)curSffDataPlanelocator.getDataPlaneLocator().getLocatorType()).getMac().getValue();
              this.sfcL2FlowProgrammer.configureNextHopFlow(rsp.getPathId(),srcMacSff,dstMac,dstVlan,isAddFlow);
            }
            break;
          }
        }
        List<SffDataPlaneLocator> prevSFFDataPlanelocatorList=prevSFF.getSffDataPlaneLocator();
        for (        SffDataPlaneLocator prevSFFDataPlanelocator : prevSFFDataPlanelocatorList) {
          LocatorType curSFFLocatorType=prevSFFDataPlanelocator.getDataPlaneLocator().getLocatorType();
          if (curSFFLocatorType.getImplementedInterface().equals(Mac.class)) {
            String dstMacSff=((MacAddressLocator)prevSFFDataPlanelocator.getDataPlaneLocator().getLocatorType()).getMac().getValue();
            int dstVlanSff=((MacAddressLocator)prevSFFDataPlanelocator.getDataPlaneLocator().getLocatorType()).getVlanId();
            this.sfcL2FlowProgrammer.setNodeInfo(servicePathHopPrev.getServiceFunctionForwarder());
            for (            SfDataPlaneLocator prevSFDataPlanelocator : prevSFDataPlaneLocatorList) {
              LocatorType prevSFLocatorType=prevSFDataPlanelocator.getLocatorType();
              if (prevSFLocatorType.getImplementedInterface().equals(Mac.class)) {
                prevSFSrcMac=((MacAddressLocator)prevSFLocatorType).getMac().getValue();
                this.sfcL2FlowProgrammer.configureNextHopFlow(rsp.getPathId(),prevSFSrcMac,dstMacSff,dstVlanSff,isAddFlow);
              }
            }
            break;
          }
        }
      }
    }
    servicePathHopPrev=servicePathHopCur;
    servicePathHopCur=null;
  }
}","private void configureSffFlows(RenderedServicePath rsp,boolean isAddFlow){
  RenderedServicePathHop servicePathHopPrev=null;
  Long sfpId=0L;
  sfpId=rsp.getPathId();
  Iterator<RenderedServicePathHop> servicePathHopIter=rsp.getRenderedServicePathHop().iterator();
  String curSFFName=null;
  RenderedServicePathHop servicePathHopCur=null;
  while (servicePathHopIter.hasNext()) {
    servicePathHopCur=servicePathHopIter.next();
    curSFFName=servicePathHopCur.getServiceFunctionForwarder();
    LOG.info(""String_Node_Str"",servicePathHopCur.getHopNumber(),servicePathHopCur.getServiceFunctionName(),curSFFName);
    this.sfcL2FlowProgrammer.setNodeInfo(curSFFName);
    this.sfcL2FlowProgrammer.configureIngressTransportFlow(isAddFlow);
    this.sfcL2FlowProgrammer.configureSffNextHopDefaultFlow(isAddFlow);
    ServiceFunctionForwarder curSff=SfcProviderServiceForwarderAPI.readServiceFunctionForwarderExecutor(curSFFName);
    List<SffDataPlaneLocator> curSffDataPlanelocatorList=curSff.getSffDataPlaneLocator();
    for (    SffDataPlaneLocator curSffDataPlanelocator : curSffDataPlanelocatorList) {
      LocatorType curSFFLocatorType=curSffDataPlanelocator.getDataPlaneLocator().getLocatorType();
      if (curSFFLocatorType.getImplementedInterface().equals(Mac.class)) {
        int curSFFVlan=((MacAddressLocator)curSffDataPlanelocator.getDataPlaneLocator().getLocatorType()).getVlanId();
        this.sfcL2FlowProgrammer.configureIngressFlow(curSFFVlan,isAddFlow);
      }
    }
    ServiceFunction curSF=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(servicePathHopCur.getServiceFunctionName());
    List<SfDataPlaneLocator> curSfDataPlaneLocatorList=curSF.getSfDataPlaneLocator();
    for (    SfDataPlaneLocator curSfDataPlanelocator : curSfDataPlaneLocatorList) {
      LocatorType curSFLocatorType=curSfDataPlanelocator.getLocatorType();
      if (curSFLocatorType.getImplementedInterface().equals(Mac.class)) {
        int curSFVlan=((MacAddressLocator)curSfDataPlanelocator.getLocatorType()).getVlanId();
        this.sfcL2FlowProgrammer.configureIngressFlow(curSFVlan,isAddFlow);
      }
    }
    if (servicePathHopPrev == null) {
      installDefaultNextHopEntry(rsp.getPathId(),servicePathHopCur,isAddFlow);
    }
 else {
      String prevSFSrcMac=null;
      ServiceFunction prevSF;
      ServiceFunctionForwarder prevSFF;
      List<SfDataPlaneLocator> prevSFDataPlaneLocatorList;
      prevSFF=SfcProviderServiceForwarderAPI.readServiceFunctionForwarderExecutor(servicePathHopPrev.getServiceFunctionForwarder());
      prevSF=SfcProviderServiceFunctionAPI.readServiceFunctionExecutor(servicePathHopPrev.getServiceFunctionName());
      prevSFDataPlaneLocatorList=prevSF.getSfDataPlaneLocator();
      if (servicePathHopPrev.getServiceFunctionForwarder().equals(curSFFName)) {
        for (        SfDataPlaneLocator curSfDataPlanelocator : curSfDataPlaneLocatorList) {
          LocatorType curSFLocatorType=curSfDataPlanelocator.getLocatorType();
          if (curSFLocatorType.getImplementedInterface().equals(Mac.class)) {
            MacAddressLocator macAddressLocator;
            macAddressLocator=(MacAddressLocator)curSFLocatorType;
            String dstMac=macAddressLocator.getMac().getValue();
            int dstVlan=macAddressLocator.getVlanId();
            for (            SfDataPlaneLocator prevSFDataPlanelocator : prevSFDataPlaneLocatorList) {
              LocatorType prevSFLocatorType=prevSFDataPlanelocator.getLocatorType();
              if (prevSFLocatorType.getImplementedInterface().equals(Mac.class)) {
                prevSFSrcMac=((MacAddressLocator)prevSFLocatorType).getMac().getValue();
                this.sfcL2FlowProgrammer.configureNextHopFlow(rsp.getPathId(),prevSFSrcMac,dstMac,dstVlan,isAddFlow);
              }
            }
            break;
          }
        }
      }
 else {
        for (        SfDataPlaneLocator curSfDataPlanelocator : curSfDataPlaneLocatorList) {
          LocatorType curSFLocatorType=curSfDataPlanelocator.getLocatorType();
          if (curSFLocatorType.getImplementedInterface().equals(Mac.class)) {
            MacAddressLocator macAddressLocator;
            macAddressLocator=(MacAddressLocator)curSFLocatorType;
            String dstMac=macAddressLocator.getMac().getValue();
            int dstVlan=macAddressLocator.getVlanId();
            for (            SffDataPlaneLocator curSffDataPlanelocator : curSffDataPlanelocatorList) {
              String srcMacSff=((MacAddressLocator)curSffDataPlanelocator.getDataPlaneLocator().getLocatorType()).getMac().getValue();
              this.sfcL2FlowProgrammer.configureNextHopFlow(rsp.getPathId(),srcMacSff,dstMac,dstVlan,isAddFlow);
            }
            break;
          }
        }
        List<SffDataPlaneLocator> prevSFFDataPlanelocatorList=prevSFF.getSffDataPlaneLocator();
        for (        SffDataPlaneLocator prevSFFDataPlanelocator : prevSFFDataPlanelocatorList) {
          LocatorType curSFFLocatorType=prevSFFDataPlanelocator.getDataPlaneLocator().getLocatorType();
          if (curSFFLocatorType.getImplementedInterface().equals(Mac.class)) {
            String dstMacSff=((MacAddressLocator)prevSFFDataPlanelocator.getDataPlaneLocator().getLocatorType()).getMac().getValue();
            int dstVlanSff=((MacAddressLocator)prevSFFDataPlanelocator.getDataPlaneLocator().getLocatorType()).getVlanId();
            this.sfcL2FlowProgrammer.setNodeInfo(servicePathHopPrev.getServiceFunctionForwarder());
            for (            SfDataPlaneLocator prevSFDataPlanelocator : prevSFDataPlaneLocatorList) {
              LocatorType prevSFLocatorType=prevSFDataPlanelocator.getLocatorType();
              if (prevSFLocatorType.getImplementedInterface().equals(Mac.class)) {
                prevSFSrcMac=((MacAddressLocator)prevSFLocatorType).getMac().getValue();
                this.sfcL2FlowProgrammer.configureNextHopFlow(rsp.getPathId(),prevSFSrcMac,dstMacSff,dstVlanSff,isAddFlow);
              }
            }
            break;
          }
        }
      }
    }
    servicePathHopPrev=servicePathHopCur;
    servicePathHopCur=null;
  }
}",0.9901234567901236
132169,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataCreatedConfigurationObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof RenderedServicePath) {
      RenderedServicePath createdRenderedServicePath=(RenderedServicePath)entry.getValue();
      configureSffFlows(createdRenderedServicePath,true);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof RenderedServicePath && (!(dataCreatedConfigurationObject.containsKey(entry.getKey()))))) {
      RenderedServicePath updatedRenderedServicePath=(RenderedServicePath)entry.getValue();
      configureSffFlows(updatedRenderedServicePath,true);
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=change.getOriginalData().get(instanceIdentifier);
    if (dataObject instanceof RenderedServicePath) {
      configureSffFlows((RenderedServicePath)dataObject,false);
    }
  }
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataCreatedConfigurationObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof RenderedServicePath) {
      LOG.info(""String_Node_Str"",((RenderedServicePath)entry.getValue()).getName());
      RenderedServicePath createdRenderedServicePath=(RenderedServicePath)entry.getValue();
      configureSffFlows(createdRenderedServicePath,true);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof RenderedServicePath && (!(dataCreatedConfigurationObject.containsKey(entry.getKey()))))) {
      RenderedServicePath updatedRenderedServicePath=(RenderedServicePath)entry.getValue();
      configureSffFlows(updatedRenderedServicePath,true);
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=change.getOriginalData().get(instanceIdentifier);
    if (dataObject instanceof RenderedServicePath) {
      configureSffFlows((RenderedServicePath)dataObject,false);
    }
  }
}",0.9702067998597969
132170,"@Override public java.lang.AutoCloseable createInstance(){
  DataBroker dataBroker=getDataBrokerDependency();
  RpcProviderRegistry rpcProvider=getRpcRegistryDependency();
  final OpenflowSfcRenderer openflowSfcRenderer=new OpenflowSfcRenderer(dataBroker,rpcProvider);
  OpenflowSfcFlowProgrammer.createFlowProgrammer(dataBroker);
  return new AutoCloseable(){
    @Override public void close() throws Exception {
      openflowSfcRenderer.close();
    }
  }
;
}","@Override public java.lang.AutoCloseable createInstance(){
  DataBroker dataBroker=getDataBrokerDependency();
  RpcProviderRegistry rpcProvider=getRpcRegistryDependency();
  final OpenflowSfcRenderer openflowSfcRenderer=new OpenflowSfcRenderer(dataBroker,rpcProvider);
  OpenflowSfcFlowProgrammer.createFlowProgrammer();
  return new AutoCloseable(){
    @Override public void close() throws Exception {
      openflowSfcRenderer.close();
    }
  }
;
}",0.9890590809628008
132171,"public void setIID(InstanceIdentifier<?> IID){
  iID=IID;
}","public void setIID(InstanceIdentifier<?> iID){
  this.iID=iID;
}",0.926829268292683
132172,"@Override public java.lang.AutoCloseable createInstance(){
  SfcProviderLogbackLoader.loadSfcLogbackConfiguration();
  final OpendaylightSfc opendaylightSfc=new OpendaylightSfc();
  DataBroker dataBrokerService=getDataBrokerDependency();
  opendaylightSfc.setDataProvider(dataBrokerService);
  final SfcProviderRpc sfcProviderRpc=new SfcProviderRpc();
  SfcProviderSffEntryDataListener sfcProviderSffEntryDataListener=new SfcProviderSffEntryDataListener();
  final ListenerRegistration<DataChangeListener> sffDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sffIID,sfcProviderSffEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfEntryDataListener sfcProviderSfEntryDataListener=new SfcProviderSfEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sfEntryIID,sfcProviderSfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfcEntryDataListener sfcProviderSfcEntryDataListener=new SfcProviderSfcEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfcEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sfcEntryIID,sfcProviderSfcEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderScfEntryDataListener sfcProviderScfEntryDataListener=new SfcProviderScfEntryDataListener();
  final ListenerRegistration<DataChangeListener> scfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.scfEntryIID,sfcProviderScfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfpEntryDataListener sfcProviderSfpEntryDataListener=new SfcProviderSfpEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfpEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sfpEntryIID,sfcProviderSfpEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfpDataListener sfcProviderSfpDataListener=new SfcProviderSfpDataListener();
  final ListenerRegistration<DataChangeListener> sfpDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sfpIID,sfcProviderSfpDataListener,DataBroker.DataChangeScope.SUBTREE);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionService> sfRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionChainService> sfcRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionChainService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<ServiceNodeService> snRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceNodeService.class,sfcProviderRpc);
final class AutoCloseableSfc implements AutoCloseable {
    @Override public void close(){
      sfEntryDataChangeListenerRegistration.close();
      sfcEntryDataChangeListenerRegistration.close();
      scfEntryDataChangeListenerRegistration.close();
      sfpEntryDataChangeListenerRegistration.close();
      sffDataChangeListenerRegistration.close();
      sfRpcRegistration.close();
      sfcRpcRegistration.close();
      snRpcRegistration.close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  AutoCloseable ret=new AutoCloseableSfc();
  Object[] emptyObjArray={};
  Class[] emptyClassArray={};
  ScheduledExecutorService scheduledExecutorService=Executors.newScheduledThreadPool(1);
  scheduledExecutorService.schedule(SfcProviderBootstrapRestAPI.getPutBootstrapData(emptyObjArray,emptyClassArray),15,TimeUnit.SECONDS);
  scheduledExecutorService.shutdown();
  LOG.info(""String_Node_Str"",ret);
  return ret;
}","@Override public java.lang.AutoCloseable createInstance(){
  SfcProviderLogbackLoader.loadSfcLogbackConfiguration();
  final OpendaylightSfc opendaylightSfc=new OpendaylightSfc();
  DataBroker dataBrokerService=getDataBrokerDependency();
  opendaylightSfc.setDataProvider(dataBrokerService);
  final SfcProviderRpc sfcProviderRpc=new SfcProviderRpc();
  SfcProviderSffEntryDataListener sfcProviderSffEntryDataListener=new SfcProviderSffEntryDataListener();
  final ListenerRegistration<DataChangeListener> sffDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFF_IID,sfcProviderSffEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfEntryDataListener sfcProviderSfEntryDataListener=new SfcProviderSfEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SF_ENTRY_IID,sfcProviderSfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfcEntryDataListener sfcProviderSfcEntryDataListener=new SfcProviderSfcEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfcEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFC_ENTRY_IID,sfcProviderSfcEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderScfEntryDataListener sfcProviderScfEntryDataListener=new SfcProviderScfEntryDataListener();
  final ListenerRegistration<DataChangeListener> scfEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SCF_ENTRY_IID,sfcProviderScfEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfpEntryDataListener sfcProviderSfpEntryDataListener=new SfcProviderSfpEntryDataListener();
  final ListenerRegistration<DataChangeListener> sfpEntryDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFP_ENTRY_IID,sfcProviderSfpEntryDataListener,DataBroker.DataChangeScope.SUBTREE);
  SfcProviderSfpDataListener sfcProviderSfpDataListener=new SfcProviderSfpDataListener();
  final ListenerRegistration<DataChangeListener> sfpDataChangeListenerRegistration=dataBrokerService.registerDataChangeListener(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFP_IID,sfcProviderSfpDataListener,DataBroker.DataChangeScope.SUBTREE);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionService> sfRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<ServiceFunctionChainService> sfcRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceFunctionChainService.class,sfcProviderRpc);
  final BindingAwareBroker.RpcRegistration<ServiceNodeService> snRpcRegistration=getRpcRegistryDependency().addRpcImplementation(ServiceNodeService.class,sfcProviderRpc);
final class AutoCloseableSfc implements AutoCloseable {
    @Override public void close(){
      sfEntryDataChangeListenerRegistration.close();
      sfcEntryDataChangeListenerRegistration.close();
      scfEntryDataChangeListenerRegistration.close();
      sfpEntryDataChangeListenerRegistration.close();
      sffDataChangeListenerRegistration.close();
      sfRpcRegistration.close();
      sfcRpcRegistration.close();
      snRpcRegistration.close();
      try {
        opendaylightSfc.close();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.error(""String_Node_Str"" + ""String_Node_Str"",this);
      }
      LOG.info(""String_Node_Str"",this);
    }
  }
  AutoCloseable ret=new AutoCloseableSfc();
  Object[] emptyObjArray={};
  Class[] emptyClassArray={};
  ScheduledExecutorService scheduledExecutorService=Executors.newScheduledThreadPool(1);
  scheduledExecutorService.schedule(SfcProviderBootstrapRestAPI.getPutBootstrapData(emptyObjArray,emptyClassArray),15,TimeUnit.SECONDS);
  scheduledExecutorService.shutdown();
  LOG.info(""String_Node_Str"",ret);
  return ret;
}",0.9899449365573378
132173,"public OpendaylightSfc(){
  executor=Executors.newFixedThreadPool(40);
  opendaylightSfcObj=this;
}","public OpendaylightSfc(){
  executor=Executors.newFixedThreadPool(EXECUTOR_THREAD_POOL_SIZE);
  opendaylightSfcObj=this;
}",0.8778280542986425
132174,"/** 
 * Implemented from the AutoCloseable interface.
 */
@Override public void close() throws ExecutionException, InterruptedException {
  executor.shutdown();
  if (dataProvider != null) {
    final AsyncReadWriteTransaction t=dataProvider.newReadWriteTransaction();
    t.delete(LogicalDatastoreType.CONFIGURATION,sfEntryIID);
    t.delete(LogicalDatastoreType.CONFIGURATION,sfEntryIID);
    t.delete(LogicalDatastoreType.CONFIGURATION,sfEntryIID);
    t.delete(LogicalDatastoreType.CONFIGURATION,sfcEntryIID);
    t.delete(LogicalDatastoreType.CONFIGURATION,sfsIID);
    t.delete(LogicalDatastoreType.CONFIGURATION,snIID);
    t.delete(LogicalDatastoreType.CONFIGURATION,sffIID);
    t.delete(LogicalDatastoreType.CONFIGURATION,sfpIID);
    t.delete(LogicalDatastoreType.CONFIGURATION,sftIID);
    t.commit().get();
  }
}","/** 
 * Implemented from the AutoCloseable interface.
 */
@Override public void close() throws ExecutionException, InterruptedException {
  executor.shutdown();
  if (dataProvider != null) {
    final AsyncReadWriteTransaction t=dataProvider.newReadWriteTransaction();
    t.delete(LogicalDatastoreType.CONFIGURATION,SF_ENTRY_IID);
    t.delete(LogicalDatastoreType.CONFIGURATION,SF_ENTRY_IID);
    t.delete(LogicalDatastoreType.CONFIGURATION,SF_ENTRY_IID);
    t.delete(LogicalDatastoreType.CONFIGURATION,SFC_ENTRY_IID);
    t.delete(LogicalDatastoreType.CONFIGURATION,SFS_IID);
    t.delete(LogicalDatastoreType.CONFIGURATION,SN_IID);
    t.delete(LogicalDatastoreType.CONFIGURATION,SFF_IID);
    t.delete(LogicalDatastoreType.CONFIGURATION,SFP_IID);
    t.delete(LogicalDatastoreType.CONFIGURATION,SFT_IID);
    t.commit().get();
  }
}",0.9452796151533374
132175,"/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param renderedServicePath Service Function Path object
 */
public void putRenderedServicePath(RenderedServicePath renderedServicePath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI=null;
  String restURI;
  String sfpJSON=getRESTObj(getRenderedServicePathURI(renderedServicePath));
  List<RenderedServicePathHop> renderedServicePathHopList=renderedServicePath.getRenderedServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  RenderedServicePathHop renderedServicePathHop : renderedServicePathHopList) {
    String sffName=renderedServicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse putClientRemoteResponse=null;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        try {
          sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ renderedServicePath.getName();
          putClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sfpJSON);
          if (putClientRemoteResponse.getStatus() >= 300) {
            throw new UniformInterfaceException(sfpURI + ""String_Node_Str"" + HTTP_ERROR_MSG+ putClientRemoteResponse.getStatus(),putClientRemoteResponse);
          }
        }
 catch (        UniformInterfaceException e) {
          LOG.error(""String_Node_Str"",e.getMessage());
        }
catch (        ClientHandlerException e) {
          if (e.getCause() instanceof ConnectException) {
            LOG.error(""String_Node_Str"",sfpURI);
          }
 else {
            LOG.error(""String_Node_Str"",e.getMessage());
          }
        }
 finally {
          if (putClientRemoteResponse != null) {
            putClientRemoteResponse.close();
          }
        }
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  printTraceStop(LOG);
}","/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param renderedServicePath Service Function Path object
 */
public void putRenderedServicePath(RenderedServicePath renderedServicePath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI=null;
  String restURI;
  String sfpJSON=getRESTObj(getRenderedServicePathURI(renderedServicePath));
  List<RenderedServicePathHop> renderedServicePathHopList=renderedServicePath.getRenderedServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  RenderedServicePathHop renderedServicePathHop : renderedServicePathHopList) {
    String sffName=renderedServicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=ODL_SFC.getExecutor().submit(sfcProviderServiceForwarderAPI);
      ClientResponse putClientRemoteResponse=null;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        try {
          sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ renderedServicePath.getName();
          putClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sfpJSON);
          if (putClientRemoteResponse.getStatus() >= 300) {
            throw new UniformInterfaceException(sfpURI + ""String_Node_Str"" + HTTP_ERROR_MSG+ putClientRemoteResponse.getStatus(),putClientRemoteResponse);
          }
        }
 catch (        UniformInterfaceException e) {
          LOG.error(""String_Node_Str"",e.getMessage());
        }
catch (        ClientHandlerException e) {
          if (e.getCause() instanceof ConnectException) {
            LOG.error(""String_Node_Str"",sfpURI);
          }
 else {
            LOG.error(""String_Node_Str"",e.getMessage());
          }
        }
 finally {
          if (putClientRemoteResponse != null) {
            putClientRemoteResponse.close();
          }
        }
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  printTraceStop(LOG);
}",0.9957918898240244
132176,"/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param renderedServicePath Service Function Path object
 */
public void deleteServiceFunctionPath(RenderedServicePath renderedServicePath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  List<RenderedServicePathHop> renderedServicePathHopList=renderedServicePath.getRenderedServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  RenderedServicePathHop renderedServicePathHop : renderedServicePathHopList) {
    String sffName=renderedServicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse deleteClientRemoteResponse=null;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ renderedServicePath.getName();
        try {
          deleteClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).delete(ClientResponse.class);
          if (deleteClientRemoteResponse.getStatus() >= 300) {
            throw new UniformInterfaceException(sfpURI + ""String_Node_Str"" + HTTP_ERROR_MSG+ deleteClientRemoteResponse.getStatus(),deleteClientRemoteResponse);
          }
        }
 catch (        UniformInterfaceException e) {
          LOG.error(""String_Node_Str"",e.getMessage());
        }
catch (        ClientHandlerException e) {
          if (e.getCause() instanceof ConnectException) {
            LOG.error(""String_Node_Str"",sfpURI);
          }
 else {
            LOG.error(""String_Node_Str"",e.getMessage());
          }
        }
 finally {
          if (deleteClientRemoteResponse != null) {
            deleteClientRemoteResponse.close();
          }
        }
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  printTraceStop(LOG);
}","/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param renderedServicePath Service Function Path object
 */
public void deleteServiceFunctionPath(RenderedServicePath renderedServicePath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  List<RenderedServicePathHop> renderedServicePathHopList=renderedServicePath.getRenderedServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  RenderedServicePathHop renderedServicePathHop : renderedServicePathHopList) {
    String sffName=renderedServicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=ODL_SFC.getExecutor().submit(sfcProviderServiceForwarderAPI);
      ClientResponse deleteClientRemoteResponse=null;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ renderedServicePath.getName();
        try {
          deleteClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).delete(ClientResponse.class);
          if (deleteClientRemoteResponse.getStatus() >= 300) {
            throw new UniformInterfaceException(sfpURI + ""String_Node_Str"" + HTTP_ERROR_MSG+ deleteClientRemoteResponse.getStatus(),deleteClientRemoteResponse);
          }
        }
 catch (        UniformInterfaceException e) {
          LOG.error(""String_Node_Str"",e.getMessage());
        }
catch (        ClientHandlerException e) {
          if (e.getCause() instanceof ConnectException) {
            LOG.error(""String_Node_Str"",sfpURI);
          }
 else {
            LOG.error(""String_Node_Str"",e.getMessage());
          }
        }
 finally {
          if (deleteClientRemoteResponse != null) {
            deleteClientRemoteResponse.close();
          }
        }
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  printTraceStop(LOG);
}",0.9956845821890936
132177,"@Override public Future<RpcResult<InstantiateServiceFunctionChainOutput>> instantiateServiceFunctionChain(InstantiateServiceFunctionChainInput input){
  if (dataBroker != null) {
    ServiceFunctionChain chain=findServiceFunctionChain(input.getName());
    if (chain != null) {
      List<SfcServiceFunction> sfRefList=chain.getSfcServiceFunction();
      LOG.debug(""String_Node_Str"" + sfRefList);
      if (sfRefList != null && sfRefList.size() > 0) {
        ServiceFunctionPathBuilder pathBuilder=new ServiceFunctionPathBuilder();
        List<ServicePathHop> instances=new ArrayList<>();
        Random rand=new Random();
        for (        SfcServiceFunction ref : sfRefList) {
          List<ServicePathHop> instanceList=findInstancesByType(ref.getType());
          LOG.debug(""String_Node_Str"" + instanceList);
          if (instanceList != null && instanceList.size() > 0) {
            instances.add(instanceList.get(rand.nextInt(instanceList.size())));
          }
 else {
            throw new IllegalStateException(""String_Node_Str"" + ref.getName() + ""String_Node_Str"");
          }
        }
        String pathName=input.getName() + ""String_Node_Str"" + java.lang.System.currentTimeMillis();
        ServiceFunctionPath path=pathBuilder.setName(pathName).setServicePathHop(instances).setServiceChainName(input.getName()).build();
        List<ServiceFunctionPath> list=new ArrayList<>();
        list.add(path);
        ServiceFunctionPaths paths=buildServiceFunctionPaths(list);
        WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
        writeTx.merge(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sfpIID,paths,true);
        writeTx.commit();
        InstantiateServiceFunctionChainOutputBuilder outputBuilder=new InstantiateServiceFunctionChainOutputBuilder();
        outputBuilder.setName(pathName);
        return Futures.immediateFuture(Rpcs.getRpcResult(true,outputBuilder.build(),Collections.<RpcError>emptySet()));
      }
 else {
        throw new IllegalStateException(""String_Node_Str"");
      }
    }
 else {
      throw new IllegalStateException(""String_Node_Str"" + input.getName() + ""String_Node_Str"");
    }
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  return Futures.immediateFuture(Rpcs.<InstantiateServiceFunctionChainOutput>getRpcResult(false,Collections.<RpcError>emptySet()));
}","@Override public Future<RpcResult<InstantiateServiceFunctionChainOutput>> instantiateServiceFunctionChain(InstantiateServiceFunctionChainInput input){
  if (dataBroker != null) {
    ServiceFunctionChain chain=findServiceFunctionChain(input.getName());
    if (chain != null) {
      List<SfcServiceFunction> sfRefList=chain.getSfcServiceFunction();
      LOG.debug(""String_Node_Str"" + sfRefList);
      if (sfRefList != null && sfRefList.size() > 0) {
        ServiceFunctionPathBuilder pathBuilder=new ServiceFunctionPathBuilder();
        List<ServicePathHop> instances=new ArrayList<>();
        Random rand=new Random();
        for (        SfcServiceFunction ref : sfRefList) {
          List<ServicePathHop> instanceList=findInstancesByType(ref.getType());
          LOG.debug(""String_Node_Str"" + instanceList);
          if (instanceList != null && instanceList.size() > 0) {
            instances.add(instanceList.get(rand.nextInt(instanceList.size())));
          }
 else {
            throw new IllegalStateException(""String_Node_Str"" + ref.getName() + ""String_Node_Str"");
          }
        }
        String pathName=input.getName() + ""String_Node_Str"" + java.lang.System.currentTimeMillis();
        ServiceFunctionPath path=pathBuilder.setName(pathName).setServicePathHop(instances).setServiceChainName(input.getName()).build();
        List<ServiceFunctionPath> list=new ArrayList<>();
        list.add(path);
        ServiceFunctionPaths paths=buildServiceFunctionPaths(list);
        WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
        writeTx.merge(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFP_IID,paths,true);
        writeTx.commit();
        InstantiateServiceFunctionChainOutputBuilder outputBuilder=new InstantiateServiceFunctionChainOutputBuilder();
        outputBuilder.setName(pathName);
        return Futures.immediateFuture(Rpcs.getRpcResult(true,outputBuilder.build(),Collections.<RpcError>emptySet()));
      }
 else {
        throw new IllegalStateException(""String_Node_Str"");
      }
    }
 else {
      throw new IllegalStateException(""String_Node_Str"" + input.getName() + ""String_Node_Str"");
    }
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  return Futures.immediateFuture(Rpcs.<InstantiateServiceFunctionChainOutput>getRpcResult(false,Collections.<RpcError>emptySet()));
}",0.998535258422264
132178,"@Override public Future<RpcResult<Void>> deleteServiceFunction(DeleteServiceFunctionInput input){
  printTraceStart(LOG);
  LOG.info(""String_Node_Str"" + input);
  if (dataBroker != null) {
    ReadOnlyTransaction readTx=dataBroker.newReadOnlyTransaction();
    Optional<ServiceFunctions> dataObject=null;
    try {
      dataObject=readTx.read(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sfsIID).get();
    }
 catch (    InterruptedException|ExecutionException e) {
      LOG.error(""String_Node_Str"");
      return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
    }
    if (dataObject instanceof ServiceFunctions) {
      ServiceFunctionKey sfkey=new ServiceFunctionKey(input.getName());
      InstanceIdentifier<ServiceFunction> sfIID;
      sfIID=InstanceIdentifier.builder(ServiceFunctions.class).child(ServiceFunction.class,sfkey).toInstance();
      WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
      writeTx.delete(LogicalDatastoreType.CONFIGURATION,sfIID);
      writeTx.commit();
    }
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  printTraceStop(LOG);
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}","@Override public Future<RpcResult<Void>> deleteServiceFunction(DeleteServiceFunctionInput input){
  printTraceStart(LOG);
  LOG.info(""String_Node_Str"" + input);
  if (dataBroker != null) {
    ReadOnlyTransaction readTx=dataBroker.newReadOnlyTransaction();
    Optional<ServiceFunctions> dataObject=null;
    try {
      dataObject=readTx.read(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFS_IID).get();
    }
 catch (    InterruptedException|ExecutionException e) {
      LOG.error(""String_Node_Str"");
      return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
    }
    if (dataObject instanceof ServiceFunctions) {
      ServiceFunctionKey sfkey=new ServiceFunctionKey(input.getName());
      InstanceIdentifier<ServiceFunction> sfIID;
      sfIID=InstanceIdentifier.builder(ServiceFunctions.class).child(ServiceFunction.class,sfkey).toInstance();
      WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
      writeTx.delete(LogicalDatastoreType.CONFIGURATION,sfIID);
      writeTx.commit();
    }
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  printTraceStop(LOG);
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}",0.9972581276929104
132179,"@Override public Future<RpcResult<Void>> putServiceFunctionChains(PutServiceFunctionChainsInput input){
  printTraceStart(LOG);
  ServiceFunctionChainsBuilder builder=new ServiceFunctionChainsBuilder();
  builder=builder.setServiceFunctionChain(input.getServiceFunctionChain());
  ServiceFunctionChains sfcs=builder.build();
  if (dataBroker != null) {
    WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
    writeTx.merge(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sfcIID,sfcs,true);
    writeTx.commit();
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}","@Override public Future<RpcResult<Void>> putServiceFunctionChains(PutServiceFunctionChainsInput input){
  printTraceStart(LOG);
  ServiceFunctionChainsBuilder builder=new ServiceFunctionChainsBuilder();
  builder=builder.setServiceFunctionChain(input.getServiceFunctionChain());
  ServiceFunctionChains sfcs=builder.build();
  if (dataBroker != null) {
    WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
    writeTx.merge(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFC_IID,sfcs,true);
    writeTx.commit();
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}",0.9951422623178348
132180,"@Override public Future<RpcResult<Void>> deleteAllServiceFunction(){
  printTraceStart(LOG);
  if (dataBroker != null) {
    WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
    writeTx.delete(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.sfsIID);
    writeTx.commit();
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  printTraceStop(LOG);
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}","@Override public Future<RpcResult<Void>> deleteAllServiceFunction(){
  printTraceStart(LOG);
  if (dataBroker != null) {
    WriteTransaction writeTx=dataBroker.newWriteOnlyTransaction();
    writeTx.delete(LogicalDatastoreType.CONFIGURATION,OpendaylightSfc.SFS_IID);
    writeTx.commit();
  }
 else {
    LOG.warn(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  }
  printTraceStop(LOG);
  return Futures.immediateFuture(Rpcs.<Void>getRpcResult(true,Collections.<RpcError>emptySet()));
}",0.9930348258706468
132181,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalConfigurationObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionClassifier) {
      ServiceFunctionClassifier originalServiceFunctionClassifier=(ServiceFunctionClassifier)entry.getValue();
      LOG.debug(""String_Node_Str"",originalServiceFunctionClassifier.getName());
      LOG.debug(""String_Node_Str"",originalServiceFunctionClassifier.getAccessList());
      LOG.debug(""String_Node_Str"",originalServiceFunctionClassifier.getRenderedServicePath());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionClassifier) {
      ServiceFunctionClassifier createdServiceFunctionClassifier=(ServiceFunctionClassifier)entry.getValue();
      LOG.debug(""String_Node_Str"",createdServiceFunctionClassifier.getName());
      String serviceFunctionPathName=null;
      serviceFunctionPathName=createdServiceFunctionClassifier.getRenderedServicePath();
      if (serviceFunctionPathName != null) {
        Object[] params={createdServiceFunctionClassifier.getAccessList(),createdServiceFunctionClassifier.getRenderedServicePath()};
        Class[] paramsTypes={String.class,String.class};
        odlSfc.executor.submit(SfcProviderAclAPI.getSetAclEntriesSfcAction(params,paramsTypes));
      }
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunctionClassifier) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      ServiceFunctionClassifier updatedServiceFunctionClassifier=(ServiceFunctionClassifier)entry.getValue();
      LOG.debug(""String_Node_Str"",updatedServiceFunctionClassifier.getName());
      ServiceFunctionClassifier originalServiceFunctionClassifier=(ServiceFunctionClassifier)dataOriginalConfigurationObject.get(entry.getKey());
      if (originalServiceFunctionClassifier != null && (!originalServiceFunctionClassifier.getAccessList().equalsIgnoreCase(updatedServiceFunctionClassifier.getAccessList()))) {
        Object[] params={originalServiceFunctionClassifier.getAccessList(),""String_Node_Str""};
        Class[] paramsTypes={String.class,String.class};
        odlSfc.executor.submit(SfcProviderAclAPI.getSetAclEntriesSfcAction(params,paramsTypes));
      }
      Object[] params={updatedServiceFunctionClassifier.getAccessList(),updatedServiceFunctionClassifier.getRenderedServicePath()};
      Class[] paramsTypes={String.class,String.class};
      odlSfc.executor.submit(SfcProviderAclAPI.getSetAclEntriesSfcAction(params,paramsTypes));
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalConfigurationObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionClassifier) {
      ServiceFunctionClassifier originalServiceFunctionClassifier=(ServiceFunctionClassifier)dataObject;
      Object[] params={originalServiceFunctionClassifier.getAccessList(),""String_Node_Str""};
      Class[] paramsTypes={String.class,String.class};
      odlSfc.executor.submit(SfcProviderAclAPI.getSetAclEntriesSfcAction(params,paramsTypes));
    }
  }
  printTraceStop(LOG);
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  Map<InstanceIdentifier<?>,DataObject> dataOriginalConfigurationObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionClassifier) {
      ServiceFunctionClassifier originalServiceFunctionClassifier=(ServiceFunctionClassifier)entry.getValue();
      LOG.debug(""String_Node_Str"",originalServiceFunctionClassifier.getName());
      LOG.debug(""String_Node_Str"",originalServiceFunctionClassifier.getAccessList());
      LOG.debug(""String_Node_Str"",originalServiceFunctionClassifier.getRenderedServicePath());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionClassifier) {
      ServiceFunctionClassifier createdServiceFunctionClassifier=(ServiceFunctionClassifier)entry.getValue();
      LOG.debug(""String_Node_Str"",createdServiceFunctionClassifier.getName());
      String serviceFunctionPathName=null;
      serviceFunctionPathName=createdServiceFunctionClassifier.getRenderedServicePath();
      if (serviceFunctionPathName != null) {
        Object[] params={createdServiceFunctionClassifier.getAccessList(),createdServiceFunctionClassifier.getRenderedServicePath()};
        Class[] paramsTypes={String.class,String.class};
        odlSfc.getExecutor().submit(SfcProviderAclAPI.getSetAclEntriesSfcAction(params,paramsTypes));
      }
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunctionClassifier) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      ServiceFunctionClassifier updatedServiceFunctionClassifier=(ServiceFunctionClassifier)entry.getValue();
      LOG.debug(""String_Node_Str"",updatedServiceFunctionClassifier.getName());
      ServiceFunctionClassifier originalServiceFunctionClassifier=(ServiceFunctionClassifier)dataOriginalConfigurationObject.get(entry.getKey());
      if (originalServiceFunctionClassifier != null && (!originalServiceFunctionClassifier.getAccessList().equalsIgnoreCase(updatedServiceFunctionClassifier.getAccessList()))) {
        Object[] params={originalServiceFunctionClassifier.getAccessList(),""String_Node_Str""};
        Class[] paramsTypes={String.class,String.class};
        odlSfc.getExecutor().submit(SfcProviderAclAPI.getSetAclEntriesSfcAction(params,paramsTypes));
      }
      Object[] params={updatedServiceFunctionClassifier.getAccessList(),updatedServiceFunctionClassifier.getRenderedServicePath()};
      Class[] paramsTypes={String.class,String.class};
      odlSfc.getExecutor().submit(SfcProviderAclAPI.getSetAclEntriesSfcAction(params,paramsTypes));
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalConfigurationObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionClassifier) {
      ServiceFunctionClassifier originalServiceFunctionClassifier=(ServiceFunctionClassifier)dataObject;
      Object[] params={originalServiceFunctionClassifier.getAccessList(),""String_Node_Str""};
      Class[] paramsTypes={String.class,String.class};
      odlSfc.getExecutor().submit(SfcProviderAclAPI.getSetAclEntriesSfcAction(params,paramsTypes));
    }
  }
  printTraceStop(LOG);
}",0.9962933545141648
132182,"/** 
 * This method is called whenever there is change in a SF. Before doing any changes it takes a global lock in order to ensure it is the only writer.
 * @param change
 */
@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  odlSfc.getLock();
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunction) {
      ServiceFunction originalServiceFunction=(ServiceFunction)entry.getValue();
      LOG.debug(""String_Node_Str"",originalServiceFunction.getType(),originalServiceFunction.getName());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunction) {
      ServiceFunction createdServiceFunction=(ServiceFunction)entry.getValue();
      Object[] serviceTypeObj={createdServiceFunction};
      Class[] serviceTypeClass={ServiceFunction.class};
      Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getCreateServiceFunctionTypeEntry(serviceTypeObj,serviceTypeClass));
      try {
        LOG.debug(""String_Node_Str"",future.get());
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
      LOG.debug(""String_Node_Str"",createdServiceFunction.getType(),createdServiceFunction.getName());
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunction) {
      ServiceFunction originalServiceFunction=(ServiceFunction)dataObject;
      Object[] serviceFunctionObj={originalServiceFunction};
      Class[] serviceFunctionClass={ServiceFunction.class};
      Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getDeleteServiceFunctionFromServiceType(serviceFunctionObj,serviceFunctionClass));
      try {
        LOG.debug(""String_Node_Str"",future.get());
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
      String sfName=originalServiceFunction.getName();
      List<SfServicePath> sfServicePathList=SfcProviderServiceFunctionAPI.readServiceFunctionStateExecutor(sfName);
      List<String> rspList=new ArrayList<>();
      if ((sfServicePathList != null) && (!sfServicePathList.isEmpty())) {
        if (SfcProviderServiceFunctionAPI.deleteServiceFunctionStateExecutor(sfName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sfName);
        }
        for (        SfServicePath sfServicePath : sfServicePathList) {
          String rspName=sfServicePath.getName();
          SfcProviderServiceForwarderAPI.deletePathFromServiceForwarderStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunction) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      DataObject dataObject=dataOriginalDataObject.get(entry.getKey());
      ServiceFunction originalServiceFunction=(ServiceFunction)dataObject;
      Object[] serviceFunctionObj={originalServiceFunction};
      Class[] serviceFunctionClass={ServiceFunction.class};
      ServiceFunction updatedServiceFunction=(ServiceFunction)entry.getValue();
      if (!updatedServiceFunction.getType().equals(originalServiceFunction.getType())) {
        Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getDeleteServiceFunctionFromServiceType(serviceFunctionObj,serviceFunctionClass));
        try {
          LOG.debug(""String_Node_Str"",future.get());
        }
 catch (        InterruptedException e) {
          LOG.warn(""String_Node_Str"",e);
        }
catch (        ExecutionException e) {
          LOG.warn(""String_Node_Str"",e);
        }
        serviceFunctionObj[0]=updatedServiceFunction;
        serviceFunctionClass[0]=ServiceFunction.class;
        odlSfc.executor.submit(SfcProviderServiceTypeAPI.getCreateServiceFunctionTypeEntry(serviceFunctionObj,serviceFunctionClass));
      }
      String sfName=originalServiceFunction.getName();
      List<SfServicePath> sfServicePathList=SfcProviderServiceFunctionAPI.readServiceFunctionStateExecutor(sfName);
      List<String> rspList=new ArrayList<>();
      if ((sfServicePathList != null) && (!sfServicePathList.isEmpty())) {
        if (SfcProviderServiceFunctionAPI.deleteServiceFunctionStateExecutor(sfName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sfName);
        }
        for (        SfServicePath sfServicePath : sfServicePathList) {
          String rspName=sfServicePath.getName();
          SfcProviderServiceForwarderAPI.deletePathFromServiceForwarderStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
    }
  }
  odlSfc.releaseLock();
  printTraceStop(LOG);
}","/** 
 * This method is called whenever there is change in a SF. Before doing any changes it takes a global lock in order to ensure it is the only writer.
 * @param change
 */
@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  odlSfc.getLock();
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunction) {
      ServiceFunction originalServiceFunction=(ServiceFunction)entry.getValue();
      LOG.debug(""String_Node_Str"",originalServiceFunction.getType(),originalServiceFunction.getName());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunction) {
      ServiceFunction createdServiceFunction=(ServiceFunction)entry.getValue();
      Object[] serviceTypeObj={createdServiceFunction};
      Class[] serviceTypeClass={ServiceFunction.class};
      Future future=odlSfc.getExecutor().submit(SfcProviderServiceTypeAPI.getCreateServiceFunctionTypeEntry(serviceTypeObj,serviceTypeClass));
      try {
        LOG.debug(""String_Node_Str"",future.get());
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
      LOG.debug(""String_Node_Str"",createdServiceFunction.getType(),createdServiceFunction.getName());
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunction) {
      ServiceFunction originalServiceFunction=(ServiceFunction)dataObject;
      Object[] serviceFunctionObj={originalServiceFunction};
      Class[] serviceFunctionClass={ServiceFunction.class};
      Future future=odlSfc.getExecutor().submit(SfcProviderServiceTypeAPI.getDeleteServiceFunctionFromServiceType(serviceFunctionObj,serviceFunctionClass));
      try {
        LOG.debug(""String_Node_Str"",future.get());
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
      String sfName=originalServiceFunction.getName();
      List<SfServicePath> sfServicePathList=SfcProviderServiceFunctionAPI.readServiceFunctionStateExecutor(sfName);
      List<String> rspList=new ArrayList<>();
      if ((sfServicePathList != null) && (!sfServicePathList.isEmpty())) {
        if (SfcProviderServiceFunctionAPI.deleteServiceFunctionStateExecutor(sfName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sfName);
        }
        for (        SfServicePath sfServicePath : sfServicePathList) {
          String rspName=sfServicePath.getName();
          SfcProviderServiceForwarderAPI.deletePathFromServiceForwarderStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunction) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      DataObject dataObject=dataOriginalDataObject.get(entry.getKey());
      ServiceFunction originalServiceFunction=(ServiceFunction)dataObject;
      Object[] serviceFunctionObj={originalServiceFunction};
      Class[] serviceFunctionClass={ServiceFunction.class};
      ServiceFunction updatedServiceFunction=(ServiceFunction)entry.getValue();
      if (!updatedServiceFunction.getType().equals(originalServiceFunction.getType())) {
        Future future=odlSfc.getExecutor().submit(SfcProviderServiceTypeAPI.getDeleteServiceFunctionFromServiceType(serviceFunctionObj,serviceFunctionClass));
        try {
          LOG.debug(""String_Node_Str"",future.get());
        }
 catch (        InterruptedException e) {
          LOG.warn(""String_Node_Str"",e);
        }
catch (        ExecutionException e) {
          LOG.warn(""String_Node_Str"",e);
        }
        serviceFunctionObj[0]=updatedServiceFunction;
        serviceFunctionClass[0]=ServiceFunction.class;
        odlSfc.getExecutor().submit(SfcProviderServiceTypeAPI.getCreateServiceFunctionTypeEntry(serviceFunctionObj,serviceFunctionClass));
      }
      String sfName=originalServiceFunction.getName();
      List<SfServicePath> sfServicePathList=SfcProviderServiceFunctionAPI.readServiceFunctionStateExecutor(sfName);
      List<String> rspList=new ArrayList<>();
      if ((sfServicePathList != null) && (!sfServicePathList.isEmpty())) {
        if (SfcProviderServiceFunctionAPI.deleteServiceFunctionStateExecutor(sfName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sfName);
        }
        for (        SfServicePath sfServicePath : sfServicePathList) {
          String rspName=sfServicePath.getName();
          SfcProviderServiceForwarderAPI.deletePathFromServiceForwarderStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
    }
  }
  odlSfc.releaseLock();
  printTraceStop(LOG);
}",0.9975575715282624
132183,"@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  odlSfc.getLock();
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder originalServiceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)dataObject;
      String sffName=serviceFunctionForwarder.getName();
      List<String> rspList=new ArrayList<>();
      List<SffServicePath> sffServicePathList=SfcProviderServiceForwarderAPI.readSffStateExecutor(sffName);
      if ((sffServicePathList != null) && !sffServicePathList.isEmpty()) {
        if (SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarderStateExecutor(sffName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sffName);
        }
        for (        SffServicePath sffServicePath : sffServicePathList) {
          String rspName=sffServicePath.getName();
          SfcProviderServiceFunctionAPI.deleteServicePathFromServiceFunctionStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
      Object[] serviceForwarderObj={serviceFunctionForwarder};
      Class[] serviceForwarderClass={ServiceFunctionForwarder.class};
      SfcProviderRestAPI sfcProviderRestAPI=SfcProviderRestAPI.getDeleteServiceFunctionForwarder(serviceForwarderObj,serviceForwarderClass);
      odlSfc.executor.submit(sfcProviderRestAPI);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder createdServiceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
      Object[] serviceForwarderObj={createdServiceFunctionForwarder};
      Class[] serviceForwarderClass={ServiceFunctionForwarder.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getCheckServiceForwarderAPI(serviceForwarderObj,serviceForwarderClass);
      odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunctionForwarder) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
      String sffName=serviceFunctionForwarder.getName();
      SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarderStateExecutor(sffName);
      List<String> rspList=new ArrayList<>();
      List<SffServicePath> sffServicePathList=SfcProviderServiceForwarderAPI.readSffStateExecutor(sffName);
      if ((sffServicePathList != null) && !sffServicePathList.isEmpty()) {
        for (        SffServicePath sffServicePath : sffServicePathList) {
          String rspName=sffServicePath.getName();
          SfcProviderServiceFunctionAPI.deleteServicePathFromServiceFunctionStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
      Object[] serviceForwarderObj={serviceFunctionForwarder};
      Class[] serviceForwarderClass={ServiceFunctionForwarder.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getCheckServiceForwarderAPI(serviceForwarderObj,serviceForwarderClass);
      odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
    }
  }
  odlSfc.releaseLock();
  printTraceStop(LOG);
}","@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  ODL_SFC.getLock();
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder originalServiceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)dataObject;
      String sffName=serviceFunctionForwarder.getName();
      List<String> rspList=new ArrayList<>();
      List<SffServicePath> sffServicePathList=SfcProviderServiceForwarderAPI.readSffStateExecutor(sffName);
      if ((sffServicePathList != null) && !sffServicePathList.isEmpty()) {
        if (SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarderStateExecutor(sffName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sffName);
        }
        for (        SffServicePath sffServicePath : sffServicePathList) {
          String rspName=sffServicePath.getName();
          SfcProviderServiceFunctionAPI.deleteServicePathFromServiceFunctionStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
      Object[] serviceForwarderObj={serviceFunctionForwarder};
      Class[] serviceForwarderClass={ServiceFunctionForwarder.class};
      SfcProviderRestAPI sfcProviderRestAPI=SfcProviderRestAPI.getDeleteServiceFunctionForwarder(serviceForwarderObj,serviceForwarderClass);
      ODL_SFC.getExecutor().submit(sfcProviderRestAPI);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunctionForwarder) {
      ServiceFunctionForwarder createdServiceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
      Object[] serviceForwarderObj={createdServiceFunctionForwarder};
      Class[] serviceForwarderClass={ServiceFunctionForwarder.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getCheckServiceForwarderAPI(serviceForwarderObj,serviceForwarderClass);
      ODL_SFC.getExecutor().submit(sfcProviderServiceForwarderAPI);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunctionForwarder) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)entry.getValue();
      String sffName=serviceFunctionForwarder.getName();
      SfcProviderServiceForwarderAPI.deleteServiceFunctionForwarderStateExecutor(sffName);
      List<String> rspList=new ArrayList<>();
      List<SffServicePath> sffServicePathList=SfcProviderServiceForwarderAPI.readSffStateExecutor(sffName);
      if ((sffServicePathList != null) && !sffServicePathList.isEmpty()) {
        for (        SffServicePath sffServicePath : sffServicePathList) {
          String rspName=sffServicePath.getName();
          SfcProviderServiceFunctionAPI.deleteServicePathFromServiceFunctionStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
      Object[] serviceForwarderObj={serviceFunctionForwarder};
      Class[] serviceForwarderClass={ServiceFunctionForwarder.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getCheckServiceForwarderAPI(serviceForwarderObj,serviceForwarderClass);
      ODL_SFC.getExecutor().submit(sfcProviderServiceForwarderAPI);
    }
  }
  ODL_SFC.releaseLock();
  printTraceStop(LOG);
}",0.9895880488909008
132184,"/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param renderedServicePath Service Function Path object
 */
public void putRenderedServicePath(RenderedServicePath renderedServicePath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  String sfpJSON=getRESTObj(getRenderedServicePathURI(renderedServicePath));
  List<RenderedServicePathHop> renderedServicePathHopList=renderedServicePath.getRenderedServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  RenderedServicePathHop renderedServicePathHop : renderedServicePathHopList) {
    String sffName=renderedServicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse putClientRemoteResponse;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ renderedServicePath.getName();
        putClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sfpJSON);
        if (putClientRemoteResponse.getStatus() >= 300) {
          throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
        }
        putClientRemoteResponse.close();
      }
 catch (      InterruptedException e) {
        e.printStackTrace();
      }
catch (      ExecutionException e) {
        e.printStackTrace();
      }
    }
  }
  printTraceStop(LOG);
}","/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param renderedServicePath Service Function Path object
 */
public void putRenderedServicePath(RenderedServicePath renderedServicePath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  String sfpJSON=getRESTObj(getRenderedServicePathURI(renderedServicePath));
  List<RenderedServicePathHop> renderedServicePathHopList=renderedServicePath.getRenderedServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  RenderedServicePathHop renderedServicePathHop : renderedServicePathHopList) {
    String sffName=renderedServicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse putClientRemoteResponse;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ renderedServicePath.getName();
        putClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sfpJSON);
        if (putClientRemoteResponse.getStatus() >= 300) {
          throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
        }
        putClientRemoteResponse.close();
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  printTraceStop(LOG);
}",0.9313160422670508
132185,"/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param renderedServicePath Service Function Path object
 */
public void deleteServiceFunctionPath(RenderedServicePath renderedServicePath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  List<RenderedServicePathHop> renderedServicePathHopList=renderedServicePath.getRenderedServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  RenderedServicePathHop renderedServicePathHop : renderedServicePathHopList) {
    String sffName=renderedServicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse deleteClientRemoteResponse;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ renderedServicePath.getName();
        deleteClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).delete(ClientResponse.class);
        if (deleteClientRemoteResponse.getStatus() >= 300) {
          throw new UniformInterfaceException(HTTP_ERROR_MSG + deleteClientRemoteResponse.getStatus(),deleteClientRemoteResponse);
        }
        deleteClientRemoteResponse.close();
      }
 catch (      InterruptedException e) {
        e.printStackTrace();
      }
catch (      ExecutionException e) {
        e.printStackTrace();
      }
    }
  }
  printTraceStop(LOG);
}","/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param renderedServicePath Service Function Path object
 */
public void deleteServiceFunctionPath(RenderedServicePath renderedServicePath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  List<RenderedServicePathHop> renderedServicePathHopList=renderedServicePath.getRenderedServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  RenderedServicePathHop renderedServicePathHop : renderedServicePathHopList) {
    String sffName=renderedServicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse deleteClientRemoteResponse;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ renderedServicePath.getName();
        deleteClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).delete(ClientResponse.class);
        if (deleteClientRemoteResponse.getStatus() >= 300) {
          throw new UniformInterfaceException(HTTP_ERROR_MSG + deleteClientRemoteResponse.getStatus(),deleteClientRemoteResponse);
        }
        deleteClientRemoteResponse.close();
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  printTraceStop(LOG);
}",0.9292429490351312
132186,"/** 
 * This method is called whenever there is change in a SF. Before doing any changes it takes a global lock in order to ensure it is the only writer.
 * @param change
 */
@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  odlSfc.getLock();
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunction) {
      ServiceFunction originalServiceFunction=(ServiceFunction)entry.getValue();
      LOG.debug(""String_Node_Str"",originalServiceFunction.getType(),originalServiceFunction.getName());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunction) {
      ServiceFunction createdServiceFunction=(ServiceFunction)entry.getValue();
      Object[] serviceTypeObj={createdServiceFunction};
      Class[] serviceTypeClass={ServiceFunction.class};
      Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getCreateServiceFunctionTypeEntry(serviceTypeObj,serviceTypeClass));
      try {
        LOG.debug(""String_Node_Str"",future.get());
      }
 catch (      InterruptedException e) {
        e.printStackTrace();
      }
catch (      ExecutionException e) {
        e.printStackTrace();
      }
      LOG.debug(""String_Node_Str"",createdServiceFunction.getType(),createdServiceFunction.getName());
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunction) {
      ServiceFunction originalServiceFunction=(ServiceFunction)dataObject;
      Object[] serviceFunctionObj={originalServiceFunction};
      Class[] serviceFunctionClass={ServiceFunction.class};
      Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getDeleteServiceFunctionFromServiceType(serviceFunctionObj,serviceFunctionClass));
      try {
        LOG.debug(""String_Node_Str"",future.get());
      }
 catch (      InterruptedException e) {
        e.printStackTrace();
      }
catch (      ExecutionException e) {
        e.printStackTrace();
      }
      String sfName=originalServiceFunction.getName();
      List<SfServicePath> sfServicePathList=SfcProviderServiceFunctionAPI.readServiceFunctionStateExecutor(sfName);
      List<String> rspList=new ArrayList<>();
      if ((sfServicePathList != null) && (!sfServicePathList.isEmpty())) {
        if (SfcProviderServiceFunctionAPI.deleteServiceFunctionStateExecutor(sfName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sfName);
        }
        for (        SfServicePath sfServicePath : sfServicePathList) {
          String rspName=sfServicePath.getName();
          SfcProviderServiceForwarderAPI.deletePathFromServiceForwarderStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunction) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      DataObject dataObject=dataOriginalDataObject.get(entry.getKey());
      ServiceFunction originalServiceFunction=(ServiceFunction)dataObject;
      Object[] serviceFunctionObj={originalServiceFunction};
      Class[] serviceFunctionClass={ServiceFunction.class};
      ServiceFunction updatedServiceFunction=(ServiceFunction)entry.getValue();
      if (!updatedServiceFunction.getType().equals(originalServiceFunction.getType())) {
        Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getDeleteServiceFunctionFromServiceType(serviceFunctionObj,serviceFunctionClass));
        try {
          LOG.debug(""String_Node_Str"",future.get());
        }
 catch (        InterruptedException e) {
          e.printStackTrace();
        }
catch (        ExecutionException e) {
          e.printStackTrace();
        }
        serviceFunctionObj[0]=updatedServiceFunction;
        serviceFunctionClass[0]=ServiceFunction.class;
        odlSfc.executor.submit(SfcProviderServiceTypeAPI.getCreateServiceFunctionTypeEntry(serviceFunctionObj,serviceFunctionClass));
      }
      String sfName=originalServiceFunction.getName();
      List<SfServicePath> sfServicePathList=SfcProviderServiceFunctionAPI.readServiceFunctionStateExecutor(sfName);
      List<String> rspList=new ArrayList<>();
      if ((sfServicePathList != null) && (!sfServicePathList.isEmpty())) {
        if (SfcProviderServiceFunctionAPI.deleteServiceFunctionStateExecutor(sfName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sfName);
        }
        for (        SfServicePath sfServicePath : sfServicePathList) {
          String rspName=sfServicePath.getName();
          SfcProviderServiceForwarderAPI.deletePathFromServiceForwarderStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
    }
  }
  odlSfc.releaseLock();
  printTraceStop(LOG);
}","/** 
 * This method is called whenever there is change in a SF. Before doing any changes it takes a global lock in order to ensure it is the only writer.
 * @param change
 */
@Override public void onDataChanged(final AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  printTraceStart(LOG);
  odlSfc.getLock();
  Map<InstanceIdentifier<?>,DataObject> dataOriginalDataObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalDataObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunction) {
      ServiceFunction originalServiceFunction=(ServiceFunction)entry.getValue();
      LOG.debug(""String_Node_Str"",originalServiceFunction.getType(),originalServiceFunction.getName());
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataCreatedObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedObject.entrySet()) {
    if (entry.getValue() instanceof ServiceFunction) {
      ServiceFunction createdServiceFunction=(ServiceFunction)entry.getValue();
      Object[] serviceTypeObj={createdServiceFunction};
      Class[] serviceTypeClass={ServiceFunction.class};
      Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getCreateServiceFunctionTypeEntry(serviceTypeObj,serviceTypeClass));
      try {
        LOG.debug(""String_Node_Str"",future.get());
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
      LOG.debug(""String_Node_Str"",createdServiceFunction.getType(),createdServiceFunction.getName());
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalDataObject.get(instanceIdentifier);
    if (dataObject instanceof ServiceFunction) {
      ServiceFunction originalServiceFunction=(ServiceFunction)dataObject;
      Object[] serviceFunctionObj={originalServiceFunction};
      Class[] serviceFunctionClass={ServiceFunction.class};
      Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getDeleteServiceFunctionFromServiceType(serviceFunctionObj,serviceFunctionClass));
      try {
        LOG.debug(""String_Node_Str"",future.get());
      }
 catch (      InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
catch (      ExecutionException e) {
        LOG.warn(""String_Node_Str"",e);
      }
      String sfName=originalServiceFunction.getName();
      List<SfServicePath> sfServicePathList=SfcProviderServiceFunctionAPI.readServiceFunctionStateExecutor(sfName);
      List<String> rspList=new ArrayList<>();
      if ((sfServicePathList != null) && (!sfServicePathList.isEmpty())) {
        if (SfcProviderServiceFunctionAPI.deleteServiceFunctionStateExecutor(sfName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sfName);
        }
        for (        SfServicePath sfServicePath : sfServicePathList) {
          String rspName=sfServicePath.getName();
          SfcProviderServiceForwarderAPI.deletePathFromServiceForwarderStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof ServiceFunction) && (!(dataCreatedObject.containsKey(entry.getKey())))) {
      DataObject dataObject=dataOriginalDataObject.get(entry.getKey());
      ServiceFunction originalServiceFunction=(ServiceFunction)dataObject;
      Object[] serviceFunctionObj={originalServiceFunction};
      Class[] serviceFunctionClass={ServiceFunction.class};
      ServiceFunction updatedServiceFunction=(ServiceFunction)entry.getValue();
      if (!updatedServiceFunction.getType().equals(originalServiceFunction.getType())) {
        Future future=odlSfc.executor.submit(SfcProviderServiceTypeAPI.getDeleteServiceFunctionFromServiceType(serviceFunctionObj,serviceFunctionClass));
        try {
          LOG.debug(""String_Node_Str"",future.get());
        }
 catch (        InterruptedException e) {
          LOG.warn(""String_Node_Str"",e);
        }
catch (        ExecutionException e) {
          LOG.warn(""String_Node_Str"",e);
        }
        serviceFunctionObj[0]=updatedServiceFunction;
        serviceFunctionClass[0]=ServiceFunction.class;
        odlSfc.executor.submit(SfcProviderServiceTypeAPI.getCreateServiceFunctionTypeEntry(serviceFunctionObj,serviceFunctionClass));
      }
      String sfName=originalServiceFunction.getName();
      List<SfServicePath> sfServicePathList=SfcProviderServiceFunctionAPI.readServiceFunctionStateExecutor(sfName);
      List<String> rspList=new ArrayList<>();
      if ((sfServicePathList != null) && (!sfServicePathList.isEmpty())) {
        if (SfcProviderServiceFunctionAPI.deleteServiceFunctionStateExecutor(sfName)) {
        }
 else {
          LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1],sfName);
        }
        for (        SfServicePath sfServicePath : sfServicePathList) {
          String rspName=sfServicePath.getName();
          SfcProviderServiceForwarderAPI.deletePathFromServiceForwarderStateExecutor(rspName);
          rspList.add(rspName);
        }
        SfcProviderServicePathAPI.deleteRenderedServicePathsExecutor(rspList);
      }
    }
  }
  odlSfc.releaseLock();
  printTraceStop(LOG);
}",0.9757554462403372
132187,"public static <U extends org.opendaylight.yangtools.yang.binding.DataObject>U readTransactionAPI(InstanceIdentifier<U> readIID,LogicalDatastoreType logicalDatastoreType){
  U ret=null;
  ReadOnlyTransaction readTx=odlSfc.getDataProvider().newReadOnlyTransaction();
  Optional<U> optionalDataObject;
  CheckedFuture<Optional<U>,ReadFailedException> submitFuture=readTx.read(logicalDatastoreType,readIID);
  try {
    optionalDataObject=submitFuture.checkedGet();
    if (optionalDataObject != null && optionalDataObject.isPresent()) {
      ret=optionalDataObject.get();
    }
 else {
      LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    }
  }
 catch (  ReadFailedException e) {
    e.printStackTrace();
  }
  return ret;
}","public static <U extends org.opendaylight.yangtools.yang.binding.DataObject>U readTransactionAPI(InstanceIdentifier<U> readIID,LogicalDatastoreType logicalDatastoreType){
  U ret=null;
  ReadOnlyTransaction readTx=odlSfc.getDataProvider().newReadOnlyTransaction();
  Optional<U> optionalDataObject;
  CheckedFuture<Optional<U>,ReadFailedException> submitFuture=readTx.read(logicalDatastoreType,readIID);
  try {
    optionalDataObject=submitFuture.checkedGet();
    if (optionalDataObject != null && optionalDataObject.isPresent()) {
      ret=optionalDataObject.get();
    }
 else {
      LOG.error(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    }
  }
 catch (  ReadFailedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  return ret;
}",0.9722222222222222
132188,"public SfcDataStoreCallback(){
  this.transaction_progress=true;
  try {
    semaphore.acquire();
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
}","public SfcDataStoreCallback(){
  this.transaction_progress=true;
  try {
    semaphore.acquire();
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
}",0.9132947976878611
132189,"public void getSemaphore(){
  try {
    semaphore.acquire();
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
  return;
}","public void getSemaphore(){
  try {
    semaphore.acquire();
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  return;
}",0.8972602739726028
132190,"/** 
 * This method reads the operational state for a service function. <p>
 * @param serviceFunctionName SF name
 * @return A ServiceFunctionState object that is a list of all paths usingthis service function, null otherwise
 */
public static ServiceFunctionChain readServiceFunctionChainExecutor(String serviceFunctionName){
  printTraceStart(LOG);
  ServiceFunctionChain ret=null;
  Object[] servicePathObj={serviceFunctionName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceChainAPI sfcProviderServiceChainAPI=SfcProviderServiceChainAPI.getRead(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceChainAPI);
  try {
    ret=(ServiceFunctionChain)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * This method reads the operational state for a service function. <p>
 * @param serviceFunctionName SF name
 * @return A ServiceFunctionState object that is a list of all paths usingthis service function, null otherwise
 */
public static ServiceFunctionChain readServiceFunctionChainExecutor(String serviceFunctionName){
  printTraceStart(LOG);
  ServiceFunctionChain ret=null;
  Object[] servicePathObj={serviceFunctionName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceChainAPI sfcProviderServiceChainAPI=SfcProviderServiceChainAPI.getRead(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceChainAPI);
  try {
    ret=(ServiceFunctionChain)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8616684266103485
132191,"/** 
 * This method deletes all RSPs used by the given SFF <p>
 * @param serviceFunctionForwarder SFF object
 * @return Nothing.
 */
@SuppressWarnings(""String_Node_Str"") public static boolean deletePathsUsedByServiceForwarderExecutor(ServiceFunctionForwarder serviceFunctionForwarder){
  printTraceStart(LOG);
  boolean ret=true;
  Object[] serviceForwarderObj={serviceFunctionForwarder};
  Class[] serviceForwarderClass={ServiceFunctionForwarder.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getDeletePathsUsedByServiceForwarder(serviceForwarderObj,serviceForwarderClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(boolean)future.get();
    LOG.info(""String_Node_Str"",ret);
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * This method deletes all RSPs used by the given SFF <p>
 * @param serviceFunctionForwarder SFF object
 * @return Nothing.
 */
@SuppressWarnings(""String_Node_Str"") public static boolean deletePathsUsedByServiceForwarderExecutor(ServiceFunctionForwarder serviceFunctionForwarder){
  printTraceStart(LOG);
  boolean ret=true;
  Object[] serviceForwarderObj={serviceFunctionForwarder};
  Class[] serviceForwarderClass={ServiceFunctionForwarder.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getDeletePathsUsedByServiceForwarder(serviceForwarderObj,serviceForwarderClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(boolean)future.get();
    LOG.info(""String_Node_Str"",ret);
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8632567849686847
132192,"/** 
 * Attaches the path name to the operational store of each SFF. <p>
 * @param sfpName SF name
 * @param sffName SFF name
 * @return Nothing.
 */
@SuppressWarnings(""String_Node_Str"") @SfcReflection public static boolean deletePathFromServiceForwarderStateExecutor(String sfpName,String sffName){
  printTraceStart(LOG);
  boolean ret=false;
  Object[] servicePathObj={sfpName,sffName};
  Class[] servicePathClass={String.class,String.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getDeletePathFromServiceForwarderState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(boolean)future.get();
    LOG.info(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * Attaches the path name to the operational store of each SFF. <p>
 * @param sfpName SF name
 * @param sffName SFF name
 * @return Nothing.
 */
@SuppressWarnings(""String_Node_Str"") @SfcReflection public static boolean deletePathFromServiceForwarderStateExecutor(String sfpName,String sffName){
  printTraceStart(LOG);
  boolean ret=false;
  Object[] servicePathObj={sfpName,sffName};
  Class[] servicePathClass={String.class,String.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getDeletePathFromServiceForwarderState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(boolean)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8574434508153603
132193,"/** 
 * Wrapper API to read the list of SFPs anchored by the given SFF name. It includes Executor creation and response management <p>
 * @param sffName SFF name
 * @return ServiceFunctionState.
 */
public static List<SffServicePath> readSffStateExecutor(String sffName){
  printTraceStart(LOG);
  List<SffServicePath> ret=null;
  Object[] serviceForwarderObj={sffName};
  Class[] serviceForwarderClass={String.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getReadSffState(serviceForwarderObj,serviceForwarderClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(List<SffServicePath>)future.get();
    LOG.info(""String_Node_Str"",ret);
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  return ret;
}","/** 
 * Wrapper API to read the list of SFPs anchored by the given SFF name. It includes Executor creation and response management <p>
 * @param sffName SFF name
 * @return ServiceFunctionState.
 */
public static List<SffServicePath> readSffStateExecutor(String sffName){
  printTraceStart(LOG);
  List<SffServicePath> ret=null;
  Object[] serviceForwarderObj={sffName};
  Class[] serviceForwarderClass={String.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getReadSffState(serviceForwarderObj,serviceForwarderClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(List<SffServicePath>)future.get();
    LOG.debug(""String_Node_Str"",ret);
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  return ret;
}",0.9455973079080202
132194,"/** 
 * This method deletes the operational state for the given SFF name. <p>
 * @param sffName SFF name
 * @return true if state was deletes, false otherwise
 */
public static boolean deleteServiceFunctionForwarderStateExecutor(String sffName){
  printTraceStart(LOG);
  boolean ret=false;
  Object[] servicePathObj={sffName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getDeleteServiceFunctionForwarderState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(boolean)future.get();
    LOG.info(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * This method deletes the operational state for the given SFF name. <p>
 * @param sffName SFF name
 * @return true if state was deletes, false otherwise
 */
public static boolean deleteServiceFunctionForwarderStateExecutor(String sffName){
  printTraceStart(LOG);
  boolean ret=false;
  Object[] servicePathObj={sffName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getDeleteServiceFunctionForwarderState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(boolean)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.845231296402056
132195,"/** 
 * We add the path name to the operational store of each SFF. <p>
 * @param pathName Service Function Path Object
 * @return Nothing.
 */
public static boolean addPathToServiceForwarderStateExecutor(String pathName){
  printTraceStart(LOG);
  boolean ret=false;
  Object[] servicePathObj={pathName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getAddPathToServiceForwarderState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(boolean)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * We add the path name to the operational store of each SFF. <p>
 * @param pathName Service Function Path Object
 * @return Nothing.
 */
public static boolean addPathToServiceForwarderStateExecutor(String pathName){
  printTraceStart(LOG);
  boolean ret=false;
  Object[] servicePathObj={pathName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getAddPathToServiceForwarderState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
  try {
    ret=(boolean)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8455188679245284
132196,"/** 
 * This method reads the operational state for a service function. <p>
 * @param serviceFunctionName SF name
 * @return A ServiceFunctionState object that is a list of all paths usingthis service function, null otherwise
 */
public static ServiceFunction readServiceFunctionExecutor(String serviceFunctionName){
  printTraceStart(LOG);
  ServiceFunction ret=null;
  Object[] servicePathObj={serviceFunctionName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getRead(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(ServiceFunction)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * This method reads the operational state for a service function. <p>
 * @param serviceFunctionName SF name
 * @return A ServiceFunctionState object that is a list of all paths usingthis service function, null otherwise
 */
public static ServiceFunction readServiceFunctionExecutor(String serviceFunctionName){
  printTraceStart(LOG);
  ServiceFunction ret=null;
  Object[] servicePathObj={serviceFunctionName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getRead(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(ServiceFunction)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8604898828541001
132197,"/** 
 * This method reads the operational state for a service function. <p>
 * @param serviceFunctionName SF name
 * @return A ServiceFunctionState object that is a list of all paths usingthis service function, null otherwise
 */
public static List<SfServicePath> readServiceFunctionStateExecutor(String serviceFunctionName){
  printTraceStart(LOG);
  List<SfServicePath> ret=null;
  Object[] servicePathObj={serviceFunctionName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getReadServiceFunctionState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(List<SfServicePath>)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * This method reads the operational state for a service function. <p>
 * @param serviceFunctionName SF name
 * @return A ServiceFunctionState object that is a list of all paths usingthis service function, null otherwise
 */
public static List<SfServicePath> readServiceFunctionStateExecutor(String serviceFunctionName){
  printTraceStart(LOG);
  List<SfServicePath> ret=null;
  Object[] servicePathObj={serviceFunctionName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getReadServiceFunctionState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(List<SfServicePath>)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8657786885245902
132198,"/** 
 * This method adds a SFP name to the corresponding SF operational state. <p>
 * @param pathName SFP name
 * @return true if SFP was added, false otherwise
 */
public static boolean addPathToServiceFunctionStateExecutor(String pathName){
  boolean ret=false;
  printTraceStart(LOG);
  Object[] servicePathObj={pathName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getAddPathToServiceFunctionState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(boolean)future.get();
    LOG.info(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * This method adds a SFP name to the corresponding SF operational state. <p>
 * @param pathName SFP name
 * @return true if SFP was added, false otherwise
 */
public static boolean addPathToServiceFunctionStateExecutor(String pathName){
  boolean ret=false;
  printTraceStart(LOG);
  Object[] servicePathObj={pathName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getAddPathToServiceFunctionState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(boolean)future.get();
    LOG.info(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8482039397450754
132199,"/** 
 * When a Service Path is deleted directly (not as a consequence of deleting a SF), we need to remove its reference from all the ServiceFunction states. <p>
 * @param sfpName RSP List
 * @return true if SF was deleted, false otherwise
 */
@SuppressWarnings(""String_Node_Str"") public static boolean deleteServicePathFromServiceFunctionStateExecutor(String sfpName){
  boolean ret=false;
  printTraceStart(LOG);
  Object[] servicePathObj={sfpName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getDeleteServicePathFromServiceFunctionState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(boolean)future.get();
    LOG.info(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * When a Service Path is deleted directly (not as a consequence of deleting a SF), we need to remove its reference from all the ServiceFunction states. <p>
 * @param sfpName RSP List
 * @return true if SF was deleted, false otherwise
 */
@SuppressWarnings(""String_Node_Str"") public static boolean deleteServicePathFromServiceFunctionStateExecutor(String sfpName){
  boolean ret=false;
  printTraceStart(LOG);
  Object[] servicePathObj={sfpName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getDeleteServicePathFromServiceFunctionState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(boolean)future.get();
    LOG.info(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8691308691308691
132200,"/** 
 * This method deletes the operational state for a service function. <p>
 * @param serviceFunctionName SF name
 * @return A ServiceFunctionState object that is a list of all paths usingthis service function, null otherwise
 */
public static boolean deleteServiceFunctionStateExecutor(String serviceFunctionName){
  printTraceStart(LOG);
  boolean ret=false;
  Object[] servicePathObj={serviceFunctionName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getDeleteServiceFunctionState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(boolean)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
  }
catch (  ExecutionException e) {
    e.printStackTrace();
  }
  printTraceStop(LOG);
  return ret;
}","/** 
 * This method deletes the operational state for a service function. <p>
 * @param serviceFunctionName SF name
 * @return A ServiceFunctionState object that is a list of all paths usingthis service function, null otherwise
 */
public static boolean deleteServiceFunctionStateExecutor(String serviceFunctionName){
  printTraceStart(LOG);
  boolean ret=false;
  Object[] servicePathObj={serviceFunctionName};
  Class[] servicePathClass={String.class};
  SfcProviderServiceFunctionAPI sfcProviderServiceFunctionAPI=SfcProviderServiceFunctionAPI.getDeleteServiceFunctionState(servicePathObj,servicePathClass);
  Future future=odlSfc.executor.submit(sfcProviderServiceFunctionAPI);
  try {
    ret=(boolean)future.get();
    LOG.debug(""String_Node_Str"",future.get());
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  printTraceStop(LOG);
  return ret;
}",0.8616684266103485
132201,"public OpendaylightSfc(){
  executor=Executors.newFixedThreadPool(6);
  opendaylightSfcObj=this;
}","public OpendaylightSfc(){
  executor=Executors.newFixedThreadPool(10);
  opendaylightSfcObj=this;
}",0.9847715736040608
132202,"@Override public void run(){
  if (methodName != null) {
    Class<?> c=this.getClass();
    Method method;
    try {
      method=c.getDeclaredMethod(methodName,parameterTypes);
      method.invoke(this,parameters);
    }
 catch (    IllegalAccessException|NoSuchMethodException|InvocationTargetException e) {
      LOG.error(""String_Node_Str"",methodName);
      return;
    }
catch (    UniformInterfaceException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
catch (    ClientHandlerException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
  }
}","@Override public void run(){
  if (methodName != null) {
    Class<?> c=this.getClass();
    Method method;
    try {
      method=c.getDeclaredMethod(methodName,parameterTypes);
      method.invoke(this,parameters);
    }
 catch (    IllegalAccessException|NoSuchMethodException e) {
      LOG.error(""String_Node_Str"",methodName);
      return;
    }
catch (    InvocationTargetException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
catch (    UniformInterfaceException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
catch (    ClientHandlerException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
  }
}",0.8514241724403387
132203,"/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param serviceFunctionPath Service Function Path object
 * @return Nothing
 */
public void putServiceFunctionPath(ServiceFunctionPath serviceFunctionPath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  String sfpJSON=getRESTObj(getServiceFunctionPathURI(serviceFunctionPath));
  List<ServicePathHop> servicePathHopList=serviceFunctionPath.getServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  ServicePathHop servicePathHop : servicePathHopList) {
    String sffName=servicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse putClientRemoteResponse;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().toString();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ serviceFunctionPath.getName();
        putClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sfpJSON);
        if (putClientRemoteResponse.getStatus() >= 300) {
          throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
        }
        putClientRemoteResponse.close();
      }
 catch (      InterruptedException e) {
        e.printStackTrace();
      }
catch (      ExecutionException e) {
        e.printStackTrace();
      }
    }
  }
  printTraceStop(LOG);
}","/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param serviceFunctionPath Service Function Path object
 * @return Nothing
 */
public void putServiceFunctionPath(ServiceFunctionPath serviceFunctionPath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  String sfpJSON=getRESTObj(getServiceFunctionPathURI(serviceFunctionPath));
  List<ServicePathHop> servicePathHopList=serviceFunctionPath.getServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  ServicePathHop servicePathHop : servicePathHopList) {
    String sffName=servicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse putClientRemoteResponse;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ serviceFunctionPath.getName();
        putClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sfpJSON);
        if (putClientRemoteResponse.getStatus() >= 300) {
          throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
        }
        putClientRemoteResponse.close();
      }
 catch (      InterruptedException e) {
        e.printStackTrace();
      }
catch (      ExecutionException e) {
        e.printStackTrace();
      }
    }
  }
  printTraceStop(LOG);
}",0.9961051606621228
132204,"public void putServiceFunctionForwarder(ServiceFunctionForwarder serviceFunctionForwarder){
  printTraceStart(LOG);
  JSONObject jsonObject=null;
  String sffURI;
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sffJSON=getRESTObj(getServiceFunctionForwarderURI(serviceFunctionForwarder));
  String restURI=serviceFunctionForwarder.getRestUri().toString();
  sffURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ serviceFunctionForwarder.getName();
  ClientResponse putClientRemoteResponse;
  putClientRemoteResponse=client.resource(sffURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sffJSON);
  if (putClientRemoteResponse.getStatus() >= 300) {
    throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
  }
  putClientRemoteResponse.close();
  printTraceStop(LOG);
}","public void putServiceFunctionForwarder(ServiceFunctionForwarder serviceFunctionForwarder){
  printTraceStart(LOG);
  JSONObject jsonObject=null;
  String sffURI;
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sffJSON=getRESTObj(getServiceFunctionForwarderURI(serviceFunctionForwarder));
  String restURI=serviceFunctionForwarder.getRestUri().getValue();
  sffURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ serviceFunctionForwarder.getName();
  ClientResponse putClientRemoteResponse;
  putClientRemoteResponse=client.resource(sffURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sffJSON);
  if (putClientRemoteResponse.getStatus() >= 300) {
    throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
  }
  putClientRemoteResponse.close();
  printTraceStop(LOG);
}",0.9913885898815932
132205,"@SuppressWarnings(""String_Node_Str"") public void deleteServicePathContainingFunction(ServiceFunction serviceFunction){
  printTraceStart(LOG);
  InstanceIdentifier<ServiceFunctionPath> sfpIID;
  ServiceFunctionState serviceFunctionState;
  ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(serviceFunction.getName());
  InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
  ReadOnlyTransaction readTx=odlSfc.getDataProvider().newReadOnlyTransaction();
  Optional<ServiceFunctionState> serviceFunctionStateObject=null;
  try {
    serviceFunctionStateObject=readTx.read(LogicalDatastoreType.OPERATIONAL,sfStateIID).get();
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.error(""String_Node_Str"");
    return;
  }
  if ((serviceFunctionStateObject != null) && (serviceFunctionStateObject.get() instanceof ServiceFunctionState)) {
    serviceFunctionState=serviceFunctionStateObject.get();
    List<String> sfServiceFunctionPathList=serviceFunctionState.getSfServiceFunctionPath();
    List<String> removedPaths=new ArrayList<>();
    for (    String pathName : sfServiceFunctionPathList) {
      ServiceFunctionPathKey serviceFunctionPathKey=new ServiceFunctionPathKey(pathName);
      sfpIID=InstanceIdentifier.builder(ServiceFunctionPaths.class).child(ServiceFunctionPath.class,serviceFunctionPathKey).build();
      WriteTransaction writeTx=odlSfc.getDataProvider().newWriteOnlyTransaction();
      writeTx.delete(LogicalDatastoreType.CONFIGURATION,sfpIID);
      writeTx.commit();
      removedPaths.add(pathName);
    }
    if (removedPaths.containsAll(sfServiceFunctionPathList)) {
      SfcProviderServiceFunctionAPI.deleteServiceFunctionState(serviceFunction.getName());
    }
 else {
      LOG.error(""String_Node_Str"",serviceFunction.getName());
    }
  }
 else {
    LOG.warn(""String_Node_Str"",serviceFunction.getName());
  }
  printTraceStop(LOG);
}","@SuppressWarnings(""String_Node_Str"") public void deleteServicePathContainingFunction(ServiceFunction serviceFunction){
  printTraceStart(LOG);
  InstanceIdentifier<ServiceFunctionPath> sfpIID;
  ServiceFunctionState serviceFunctionState;
  ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(serviceFunction.getName());
  InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
  ReadOnlyTransaction readTx=odlSfc.getDataProvider().newReadOnlyTransaction();
  Optional<ServiceFunctionState> serviceFunctionStateObject;
  try {
    serviceFunctionStateObject=readTx.read(LogicalDatastoreType.OPERATIONAL,sfStateIID).get();
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.error(""String_Node_Str"");
    return;
  }
  if ((serviceFunctionStateObject.isPresent()) && (serviceFunctionStateObject.get() instanceof ServiceFunctionState)) {
    serviceFunctionState=serviceFunctionStateObject.get();
    List<String> sfServiceFunctionPathList=serviceFunctionState.getSfServiceFunctionPath();
    List<String> removedPaths=new ArrayList<>();
    for (    String pathName : sfServiceFunctionPathList) {
      ServiceFunctionPathKey serviceFunctionPathKey=new ServiceFunctionPathKey(pathName);
      sfpIID=InstanceIdentifier.builder(ServiceFunctionPaths.class).child(ServiceFunctionPath.class,serviceFunctionPathKey).build();
      WriteTransaction writeTx=odlSfc.getDataProvider().newWriteOnlyTransaction();
      writeTx.delete(LogicalDatastoreType.CONFIGURATION,sfpIID);
      CheckedFuture<Void,TransactionCommitFailedException> submitFuture=writeTx.submit();
      Futures.addCallback(submitFuture,new SfcDataStoreCallback());
      removedPaths.add(pathName);
    }
    if (removedPaths.containsAll(sfServiceFunctionPathList)) {
      SfcProviderServiceFunctionAPI.deleteServiceFunctionState(serviceFunction.getName());
    }
 else {
      LOG.error(""String_Node_Str"",serviceFunction.getName());
    }
  }
 else {
    LOG.warn(""String_Node_Str"",serviceFunction.getName());
  }
  printTraceStop(LOG);
}",0.9605546258666028
132206,"public OpendaylightSfc(){
  executor=Executors.newFixedThreadPool(6);
  opendaylightSfcObj=this;
}","public OpendaylightSfc(){
  executor=Executors.newFixedThreadPool(10);
  opendaylightSfcObj=this;
}",0.9847715736040608
132207,"@Override public void run(){
  if (methodName != null) {
    Class<?> c=this.getClass();
    Method method;
    try {
      method=c.getDeclaredMethod(methodName,parameterTypes);
      method.invoke(this,parameters);
    }
 catch (    IllegalAccessException|NoSuchMethodException|InvocationTargetException e) {
      LOG.error(""String_Node_Str"",methodName);
      return;
    }
catch (    UniformInterfaceException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
catch (    ClientHandlerException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
  }
}","@Override public void run(){
  if (methodName != null) {
    Class<?> c=this.getClass();
    Method method;
    try {
      method=c.getDeclaredMethod(methodName,parameterTypes);
      method.invoke(this,parameters);
    }
 catch (    IllegalAccessException|NoSuchMethodException e) {
      LOG.error(""String_Node_Str"",methodName);
      return;
    }
catch (    InvocationTargetException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
catch (    UniformInterfaceException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
catch (    ClientHandlerException e) {
      LOG.error(""String_Node_Str"",e.getMessage());
      return;
    }
  }
}",0.8514241724403387
132208,"/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param serviceFunctionPath Service Function Path object
 * @return Nothing
 */
public void putServiceFunctionPath(ServiceFunctionPath serviceFunctionPath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  String sfpJSON=getRESTObj(getServiceFunctionPathURI(serviceFunctionPath));
  List<ServicePathHop> servicePathHopList=serviceFunctionPath.getServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  ServicePathHop servicePathHop : servicePathHopList) {
    String sffName=servicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse putClientRemoteResponse;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().toString();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ serviceFunctionPath.getName();
        putClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sfpJSON);
        if (putClientRemoteResponse.getStatus() >= 300) {
          throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
        }
        putClientRemoteResponse.close();
      }
 catch (      InterruptedException e) {
        e.printStackTrace();
      }
catch (      ExecutionException e) {
        e.printStackTrace();
      }
    }
  }
  printTraceStop(LOG);
}","/** 
 * Communicates SFP to REST URI found in SFF configuration Server. It sends SFP information to each SFF present in the service-hop list. <p>
 * @param serviceFunctionPath Service Function Path object
 * @return Nothing
 */
public void putServiceFunctionPath(ServiceFunctionPath serviceFunctionPath){
  printTraceStart(LOG);
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sfpURI;
  String restURI;
  String sfpJSON=getRESTObj(getServiceFunctionPathURI(serviceFunctionPath));
  List<ServicePathHop> servicePathHopList=serviceFunctionPath.getServicePathHop();
  Set<String> sffNameSet=new HashSet<>();
  for (  ServicePathHop servicePathHop : servicePathHopList) {
    String sffName=servicePathHop.getServiceFunctionForwarder();
    if (sffNameSet.add(sffName)) {
      Object[] serviceForwarderObj={sffName};
      Class[] serviceForwarderClass={String.class};
      SfcProviderServiceForwarderAPI sfcProviderServiceForwarderAPI=SfcProviderServiceForwarderAPI.getRead(serviceForwarderObj,serviceForwarderClass);
      Future<Object> future=odlSfc.executor.submit(sfcProviderServiceForwarderAPI);
      ClientResponse putClientRemoteResponse;
      try {
        ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)future.get();
        restURI=serviceFunctionForwarder.getRestUri().getValue();
        sfpURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ serviceFunctionPath.getName();
        putClientRemoteResponse=client.resource(sfpURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sfpJSON);
        if (putClientRemoteResponse.getStatus() >= 300) {
          throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
        }
        putClientRemoteResponse.close();
      }
 catch (      InterruptedException e) {
        e.printStackTrace();
      }
catch (      ExecutionException e) {
        e.printStackTrace();
      }
    }
  }
  printTraceStop(LOG);
}",0.9961051606621228
132209,"public void putServiceFunctionForwarder(ServiceFunctionForwarder serviceFunctionForwarder){
  printTraceStart(LOG);
  JSONObject jsonObject=null;
  String sffURI;
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sffJSON=getRESTObj(getServiceFunctionForwarderURI(serviceFunctionForwarder));
  String restURI=serviceFunctionForwarder.getRestUri().toString();
  sffURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ serviceFunctionForwarder.getName();
  ClientResponse putClientRemoteResponse;
  putClientRemoteResponse=client.resource(sffURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sffJSON);
  if (putClientRemoteResponse.getStatus() >= 300) {
    throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
  }
  putClientRemoteResponse.close();
  printTraceStop(LOG);
}","public void putServiceFunctionForwarder(ServiceFunctionForwarder serviceFunctionForwarder){
  printTraceStart(LOG);
  JSONObject jsonObject=null;
  String sffURI;
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  String sffJSON=getRESTObj(getServiceFunctionForwarderURI(serviceFunctionForwarder));
  String restURI=serviceFunctionForwarder.getRestUri().getValue();
  sffURI=restURI + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ serviceFunctionForwarder.getName();
  ClientResponse putClientRemoteResponse;
  putClientRemoteResponse=client.resource(sffURI).type(MediaType.APPLICATION_JSON_TYPE).put(ClientResponse.class,sffJSON);
  if (putClientRemoteResponse.getStatus() >= 300) {
    throw new UniformInterfaceException(HTTP_ERROR_MSG + putClientRemoteResponse.getStatus(),putClientRemoteResponse);
  }
  putClientRemoteResponse.close();
  printTraceStop(LOG);
}",0.9913885898815932
132210,"@SuppressWarnings(""String_Node_Str"") public void deleteServicePathContainingFunction(ServiceFunction serviceFunction){
  printTraceStart(LOG);
  InstanceIdentifier<ServiceFunctionPath> sfpIID;
  ServiceFunctionState serviceFunctionState;
  ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(serviceFunction.getName());
  InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
  ReadOnlyTransaction readTx=odlSfc.getDataProvider().newReadOnlyTransaction();
  Optional<ServiceFunctionState> serviceFunctionStateObject=null;
  try {
    serviceFunctionStateObject=readTx.read(LogicalDatastoreType.OPERATIONAL,sfStateIID).get();
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.error(""String_Node_Str"");
    return;
  }
  if ((serviceFunctionStateObject != null) && (serviceFunctionStateObject.get() instanceof ServiceFunctionState)) {
    serviceFunctionState=serviceFunctionStateObject.get();
    List<String> sfServiceFunctionPathList=serviceFunctionState.getSfServiceFunctionPath();
    List<String> removedPaths=new ArrayList<>();
    for (    String pathName : sfServiceFunctionPathList) {
      ServiceFunctionPathKey serviceFunctionPathKey=new ServiceFunctionPathKey(pathName);
      sfpIID=InstanceIdentifier.builder(ServiceFunctionPaths.class).child(ServiceFunctionPath.class,serviceFunctionPathKey).build();
      WriteTransaction writeTx=odlSfc.getDataProvider().newWriteOnlyTransaction();
      writeTx.delete(LogicalDatastoreType.CONFIGURATION,sfpIID);
      writeTx.commit();
      removedPaths.add(pathName);
    }
    if (removedPaths.containsAll(sfServiceFunctionPathList)) {
      SfcProviderServiceFunctionAPI.deleteServiceFunctionState(serviceFunction.getName());
    }
 else {
      LOG.error(""String_Node_Str"",serviceFunction.getName());
    }
  }
 else {
    LOG.warn(""String_Node_Str"",serviceFunction.getName());
  }
  printTraceStop(LOG);
}","@SuppressWarnings(""String_Node_Str"") public void deleteServicePathContainingFunction(ServiceFunction serviceFunction){
  printTraceStart(LOG);
  InstanceIdentifier<ServiceFunctionPath> sfpIID;
  ServiceFunctionState serviceFunctionState;
  ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(serviceFunction.getName());
  InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
  ReadOnlyTransaction readTx=odlSfc.getDataProvider().newReadOnlyTransaction();
  Optional<ServiceFunctionState> serviceFunctionStateObject;
  try {
    serviceFunctionStateObject=readTx.read(LogicalDatastoreType.OPERATIONAL,sfStateIID).get();
  }
 catch (  InterruptedException|ExecutionException e) {
    LOG.error(""String_Node_Str"");
    return;
  }
  if ((serviceFunctionStateObject.isPresent()) && (serviceFunctionStateObject.get() instanceof ServiceFunctionState)) {
    serviceFunctionState=serviceFunctionStateObject.get();
    List<String> sfServiceFunctionPathList=serviceFunctionState.getSfServiceFunctionPath();
    List<String> removedPaths=new ArrayList<>();
    for (    String pathName : sfServiceFunctionPathList) {
      ServiceFunctionPathKey serviceFunctionPathKey=new ServiceFunctionPathKey(pathName);
      sfpIID=InstanceIdentifier.builder(ServiceFunctionPaths.class).child(ServiceFunctionPath.class,serviceFunctionPathKey).build();
      WriteTransaction writeTx=odlSfc.getDataProvider().newWriteOnlyTransaction();
      writeTx.delete(LogicalDatastoreType.CONFIGURATION,sfpIID);
      CheckedFuture<Void,TransactionCommitFailedException> submitFuture=writeTx.submit();
      Futures.addCallback(submitFuture,new SfcDataStoreCallback());
      removedPaths.add(pathName);
    }
    if (removedPaths.containsAll(sfServiceFunctionPathList)) {
      SfcProviderServiceFunctionAPI.deleteServiceFunctionState(serviceFunction.getName());
    }
 else {
      LOG.error(""String_Node_Str"",serviceFunction.getName());
    }
  }
 else {
    LOG.warn(""String_Node_Str"",serviceFunction.getName());
  }
  printTraceStop(LOG);
}",0.9605546258666028
132211,"public void putBootstrapData(){
  SfcProviderConfig providerConfig=SfcProviderConfig.getInstance();
  if (!providerConfig.readConfigFile()) {
    return;
  }
  JSONObject jo=providerConfig.getJsonBootstrapObject();
  JSONArray files;
  try {
    final String CONFIG_FILES_DIR=jo.getString(""String_Node_Str"");
    final String CONFIG_DATA_URL=jo.getString(""String_Node_Str"");
    final String CONFIG_DATA_MIME_TYPE=jo.getString(""String_Node_Str"");
    files=jo.getJSONArray(""String_Node_Str"");
    ClientConfig clientConfig=new DefaultClientConfig();
    Client client=Client.create(clientConfig);
    if (files.length() > 0) {
      for (int i=0; i < files.length(); i++) {
        JSONObject o=files.getJSONObject(i);
        String json;
        String filename=o.getString(""String_Node_Str"");
        String urlpath=o.getString(""String_Node_Str"");
        try {
          byte[] encoded=Files.readAllBytes(Paths.get(CONFIG_FILES_DIR + filename));
          json=new String(encoded,StandardCharsets.UTF_8);
        }
 catch (        FileNotFoundException e) {
          LOG.error(""String_Node_Str"",filename);
          continue;
        }
catch (        IOException e) {
          e.printStackTrace();
          break;
        }
        try {
          new JSONObject(json);
          ClientResponse putClientResponse=client.resource(CONFIG_DATA_URL + urlpath).type(CONFIG_DATA_MIME_TYPE).put(ClientResponse.class,json);
          putClientResponse.close();
        }
 catch (        JSONException e) {
          LOG.error(""String_Node_Str"",filename);
        }
      }
    }
  }
 catch (  JSONException e) {
    e.printStackTrace();
  }
}","public void putBootstrapData(){
  SfcProviderConfig providerConfig=SfcProviderConfig.getInstance();
  if (!providerConfig.readConfigFile()) {
    return;
  }
  JSONObject jo=providerConfig.getJsonBootstrapObject();
  JSONArray files;
  try {
    final String CONFIG_FILES_DIR=jo.getString(""String_Node_Str"");
    final String CONFIG_DATA_URL=jo.getString(""String_Node_Str"");
    final String CONFIG_DATA_MIME_TYPE=jo.getString(""String_Node_Str"");
    final HTTPBasicAuthFilter basicAuthFilter=new HTTPBasicAuthFilter(""String_Node_Str"",""String_Node_Str"");
    files=jo.getJSONArray(""String_Node_Str"");
    ClientConfig clientConfig=new DefaultClientConfig();
    Client client=Client.create(clientConfig);
    client.addFilter(basicAuthFilter);
    if (files.length() > 0) {
      for (int i=0; i < files.length(); i++) {
        JSONObject o=files.getJSONObject(i);
        String json;
        String filename=o.getString(""String_Node_Str"");
        String urlpath=o.getString(""String_Node_Str"");
        try {
          byte[] encoded=Files.readAllBytes(Paths.get(CONFIG_FILES_DIR + filename));
          json=new String(encoded,StandardCharsets.UTF_8);
        }
 catch (        FileNotFoundException e) {
          LOG.error(""String_Node_Str"",filename);
          continue;
        }
catch (        IOException e) {
          e.printStackTrace();
          break;
        }
        try {
          new JSONObject(json);
          ClientResponse putClientResponse=client.resource(CONFIG_DATA_URL + urlpath).type(CONFIG_DATA_MIME_TYPE).put(ClientResponse.class,json);
          putClientResponse.close();
          if (putClientResponse.getStatus() != 200) {
            LOG.error(""String_Node_Str"",filename,putClientResponse.getStatus());
          }
        }
 catch (        JSONException e) {
          LOG.error(""String_Node_Str"",filename);
        }
      }
    }
  }
 catch (  JSONException e) {
    e.printStackTrace();
  }
}",0.9177852348993288
132212,"@Override public java.lang.AutoCloseable createInstance(){
  DataBroker dataBroker=getDataBrokerDependency();
  final OpenflowSfcRenderer openflowSfcRenderer=new OpenflowSfcRenderer(dataBroker);
  OpenflowSfcFlowProgrammer.createFlowProgrammer(dataBroker);
  return new AutoCloseable(){
    @Override public void close() throws Exception {
      openflowSfcRenderer.close();
    }
  }
;
}","@Override public java.lang.AutoCloseable createInstance(){
  DataBroker dataBroker=getDataBrokerDependency();
  RpcProviderRegistry rpcProvider=getRpcRegistryDependency();
  final OpenflowSfcRenderer openflowSfcRenderer=new OpenflowSfcRenderer(dataBroker,rpcProvider);
  OpenflowSfcFlowProgrammer.createFlowProgrammer(dataBroker);
  return new AutoCloseable(){
    @Override public void close() throws Exception {
      openflowSfcRenderer.close();
    }
  }
;
}",0.9129411764705884
132213,"@Override public void onDataChanged(AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataOriginalConfigurationObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof AccessListEntries) {
      AccessListEntries originalAccessListEntries=(AccessListEntries)entry.getValue();
      AclServiceFunctionPath action=(AclServiceFunctionPath)(SfcAction)originalAccessListEntries.getActions();
      Long pathId=1L;
      Matches matches;
      AceIp aceIp;
      AceIpv4 aceIpv4;
      matches=originalAccessListEntries.getMatches();
      aceIp=(AceIp)matches.getAceType();
      aceIpv4=(AceIpv4)aceIp.getAceIpVersion();
      String srcIpAddress=aceIpv4.getSourceIpv4Address().getValue();
      String dstIpAddress=aceIpv4.getDestinationIpv4Address().getValue();
      Short srcPort=aceIp.getSourcePortRange().getLowerPort().getValue().shortValue();
      Short dstPort=aceIp.getDestinationPortRange().getLowerPort().getValue().shortValue();
      byte protocol=aceIp.getIpProtocol().byteValue();
      OpenflowSfcFlowProgrammer.getInstance().writeClassificationFlow(srcIpAddress,(short)32,dstIpAddress,(short)32,srcPort,dstPort,protocol,pathId);
    }
  }
}","@Override public void onDataChanged(AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataOriginalConfigurationObject;
  dataOriginalConfigurationObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedConfigurationObject;
  dataCreatedConfigurationObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof AccessListEntries) {
      AccessListEntries createdAccessListEntries=(AccessListEntries)entry.getValue();
      configureAclFlows(createdAccessListEntries,true);
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalConfigurationObject.get(instanceIdentifier);
    if (dataObject instanceof AccessListEntries) {
      AccessListEntries removedAccessListEntries=(AccessListEntries)dataObject;
      configureAclFlows(removedAccessListEntries,false);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject;
  dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof AccessListEntries && (!(dataCreatedConfigurationObject.containsKey(entry.getKey()))))) {
      AccessListEntries updatedAccessListEntries=(AccessListEntries)entry.getValue();
      configureAclFlows(updatedAccessListEntries,true);
    }
  }
}",0.3258010118043845
132214,"@Override public java.lang.AutoCloseable createInstance(){
  DataBroker dataBroker=getDataBrokerDependency();
  final OpenflowSfcRenderer openflowSfcRenderer=new OpenflowSfcRenderer(dataBroker);
  OpenflowSfcFlowProgrammer.createFlowProgrammer(dataBroker);
  return new AutoCloseable(){
    @Override public void close() throws Exception {
      openflowSfcRenderer.close();
    }
  }
;
}","@Override public java.lang.AutoCloseable createInstance(){
  DataBroker dataBroker=getDataBrokerDependency();
  RpcProviderRegistry rpcProvider=getRpcRegistryDependency();
  final OpenflowSfcRenderer openflowSfcRenderer=new OpenflowSfcRenderer(dataBroker,rpcProvider);
  OpenflowSfcFlowProgrammer.createFlowProgrammer(dataBroker);
  return new AutoCloseable(){
    @Override public void close() throws Exception {
      openflowSfcRenderer.close();
    }
  }
;
}",0.9129411764705884
132215,"@Override public void onDataChanged(AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataOriginalConfigurationObject=change.getOriginalData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataOriginalConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof AccessListEntries) {
      AccessListEntries originalAccessListEntries=(AccessListEntries)entry.getValue();
      AclServiceFunctionPath action=(AclServiceFunctionPath)(SfcAction)originalAccessListEntries.getActions();
      Long pathId=1L;
      Matches matches;
      AceIp aceIp;
      AceIpv4 aceIpv4;
      matches=originalAccessListEntries.getMatches();
      aceIp=(AceIp)matches.getAceType();
      aceIpv4=(AceIpv4)aceIp.getAceIpVersion();
      String srcIpAddress=aceIpv4.getSourceIpv4Address().getValue();
      String dstIpAddress=aceIpv4.getDestinationIpv4Address().getValue();
      Short srcPort=aceIp.getSourcePortRange().getLowerPort().getValue().shortValue();
      Short dstPort=aceIp.getDestinationPortRange().getLowerPort().getValue().shortValue();
      byte protocol=aceIp.getIpProtocol().byteValue();
      OpenflowSfcFlowProgrammer.getInstance().writeClassificationFlow(srcIpAddress,(short)32,dstIpAddress,(short)32,srcPort,dstPort,protocol,pathId);
    }
  }
}","@Override public void onDataChanged(AsyncDataChangeEvent<InstanceIdentifier<?>,DataObject> change){
  Map<InstanceIdentifier<?>,DataObject> dataOriginalConfigurationObject;
  dataOriginalConfigurationObject=change.getOriginalData();
  Map<InstanceIdentifier<?>,DataObject> dataCreatedConfigurationObject;
  dataCreatedConfigurationObject=change.getCreatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataCreatedConfigurationObject.entrySet()) {
    if (entry.getValue() instanceof AccessListEntries) {
      AccessListEntries createdAccessListEntries=(AccessListEntries)entry.getValue();
      configureAclFlows(createdAccessListEntries,true);
    }
  }
  Set<InstanceIdentifier<?>> dataRemovedConfigurationIID=change.getRemovedPaths();
  for (  InstanceIdentifier instanceIdentifier : dataRemovedConfigurationIID) {
    DataObject dataObject=dataOriginalConfigurationObject.get(instanceIdentifier);
    if (dataObject instanceof AccessListEntries) {
      AccessListEntries removedAccessListEntries=(AccessListEntries)dataObject;
      configureAclFlows(removedAccessListEntries,false);
    }
  }
  Map<InstanceIdentifier<?>,DataObject> dataUpdatedConfigurationObject;
  dataUpdatedConfigurationObject=change.getUpdatedData();
  for (  Map.Entry<InstanceIdentifier<?>,DataObject> entry : dataUpdatedConfigurationObject.entrySet()) {
    if ((entry.getValue() instanceof AccessListEntries && (!(dataCreatedConfigurationObject.containsKey(entry.getKey()))))) {
      AccessListEntries updatedAccessListEntries=(AccessListEntries)entry.getValue();
      configureAclFlows(updatedAccessListEntries,true);
    }
  }
}",0.3258010118043845
132216,"public void putServiceFunctionForwarders(ServiceFunctionForwarders serviceFunctionForwarders){
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  ClientResponse getClientResponse=client.resource(""String_Node_Str"").accept(""String_Node_Str"").get(ClientResponse.class);
  String jsonOutput=getClientResponse.getEntity(String.class);
  getClientResponse.close();
  ClientResponse putClientResponse=client.resource(""String_Node_Str"").type(""String_Node_Str"").put(ClientResponse.class,jsonOutput);
  putClientResponse.close();
}","public void putServiceFunctionForwarders(ServiceFunctionForwarders serviceFunctionForwarders){
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  ClientResponse getClientResponse=client.resource(""String_Node_Str"").accept(""String_Node_Str"").get(ClientResponse.class);
  String jsonOutput=getClientResponse.getEntity(String.class);
  getClientResponse.close();
  ClientResponse putClientRemoteResponse=client.resource(""String_Node_Str"").type(""String_Node_Str"").put(ClientResponse.class,jsonOutput);
  putClientRemoteResponse.close();
  ClientResponse putClientLocalResponse=client.resource(""String_Node_Str"").type(""String_Node_Str"").put(ClientResponse.class,jsonOutput);
  putClientLocalResponse.close();
}",0.8620949510173324
132217,"public void putServiceFunctionPaths(ServiceFunctionPaths serviceFunctionPaths){
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  ClientResponse getClientResponse=client.resource(""String_Node_Str"").accept(""String_Node_Str"").get(ClientResponse.class);
  String jsonOutput=getClientResponse.getEntity(String.class);
  getClientResponse.close();
  ClientResponse putClientResponse=client.resource(""String_Node_Str"").type(""String_Node_Str"").put(ClientResponse.class,jsonOutput);
  putClientResponse.close();
}","public void putServiceFunctionPaths(ServiceFunctionPaths serviceFunctionPaths){
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  ClientResponse getClientResponse=client.resource(""String_Node_Str"").accept(""String_Node_Str"").get(ClientResponse.class);
  String jsonOutput=getClientResponse.getEntity(String.class);
  getClientResponse.close();
}",0.8310598111227702
132218,"public void updateServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  deleteServiceFunctionForwarder(serviceFunction);
  createServiceFunctionForwarder(serviceFunction);
}","public void updateServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  deleteServiceFunctionFromForwarder(serviceFunction);
  createServiceFunctionForwarder(serviceFunction);
}",0.9921875
132219,"public void deleteServiceFunctionFromForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void deleteServiceFunctionFromForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionDictionary> sffIID;
  ServiceFunctionDictionaryKey serviceFunctionDictionaryKey=new ServiceFunctionDictionaryKey(serviceFunction.getName());
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).child(ServiceFunctionDictionary.class,serviceFunctionDictionaryKey).build();
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.8850828729281768
132220,"private void deleteServicePathContainingFunction(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionPath> sfpIID;
  ServiceFunctionState serviceFunctionState;
  ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(serviceFunction.getName());
  InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
  DataObject dataSfcStateObject=odlSfc.dataProvider.readOperationalData(sfStateIID);
  if (dataSfcStateObject instanceof ServiceFunctionState) {
    serviceFunctionState=(ServiceFunctionState)dataSfcStateObject;
    List<String> sfServiceFunctionPathList=serviceFunctionState.getSfServiceFunctionPath();
    List<String> removedPaths=new ArrayList<>();
    for (    String pathName : sfServiceFunctionPathList) {
      ServiceFunctionPathKey serviceFunctionPathKey=new ServiceFunctionPathKey(pathName);
      sfpIID=InstanceIdentifier.builder(ServiceFunctionPaths.class).child(ServiceFunctionPath.class,serviceFunctionPathKey).build();
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.removeConfigurationData(sfpIID);
      try {
        t.commit().get();
        removedPaths.add(pathName);
      }
 catch (      InterruptedException|ExecutionException e) {
        LOG.error(""String_Node_Str"",pathName);
      }
    }
    sfServiceFunctionPathList.removeAll(removedPaths);
    ServiceFunctionStateBuilder serviceFunctionStateBuilder=new ServiceFunctionStateBuilder();
    serviceFunctionStateBuilder.setName(serviceFunction.getName());
    serviceFunctionStateBuilder.setSfServiceFunctionPath(sfServiceFunctionPathList);
    final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
    t.putOperationalData(sfStateIID,serviceFunctionStateBuilder.build());
    try {
      t.commit().get();
    }
 catch (    InterruptedException|ExecutionException e) {
      LOG.error(""String_Node_Str"",serviceFunction.getName());
    }
  }
 else {
    LOG.error(""String_Node_Str"",serviceFunction.getName());
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","private void deleteServicePathContainingFunction(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionPath> sfpIID;
  ServiceFunctionState serviceFunctionState;
  ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(serviceFunction.getName());
  InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
  DataObject dataSfcStateObject=odlSfc.dataProvider.readOperationalData(sfStateIID);
  if (dataSfcStateObject instanceof ServiceFunctionState) {
    serviceFunctionState=(ServiceFunctionState)dataSfcStateObject;
    List<String> sfServiceFunctionPathList=serviceFunctionState.getSfServiceFunctionPath();
    List<String> removedPaths=new ArrayList<>();
    for (    String pathName : sfServiceFunctionPathList) {
      ServiceFunctionPathKey serviceFunctionPathKey=new ServiceFunctionPathKey(pathName);
      sfpIID=InstanceIdentifier.builder(ServiceFunctionPaths.class).child(ServiceFunctionPath.class,serviceFunctionPathKey).build();
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.removeConfigurationData(sfpIID);
      try {
        t.commit().get();
        removedPaths.add(pathName);
      }
 catch (      InterruptedException|ExecutionException e) {
        LOG.error(""String_Node_Str"",pathName);
      }
    }
    if (removedPaths.containsAll(sfServiceFunctionPathList)) {
      SfcProviderServiceFunctionAPI.deleteServiceFunctionState(serviceFunction.getName());
    }
 else {
      LOG.error(""String_Node_Str"",serviceFunction.getName());
    }
  }
 else {
    LOG.warn(""String_Node_Str"",serviceFunction.getName());
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.836424957841484
132221,"public static ServiceFunctionChain readServiceFunctionChain(String serviceFunctionChainName){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionChain> sfcIID;
  ServiceFunctionChainKey serviceFunctionChainKey=new ServiceFunctionChainKey(serviceFunctionChainName);
  sfcIID=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,serviceFunctionChainKey).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sfcIID);
  if (dataObject instanceof ServiceFunctionChain) {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return (ServiceFunctionChain)dataObject;
  }
 else {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}","public static ServiceFunctionChain readServiceFunctionChain(String serviceFunctionChainName){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionChain> sfcIID;
  ServiceFunctionChainKey serviceFunctionChainKey=new ServiceFunctionChainKey(serviceFunctionChainName);
  sfcIID=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,serviceFunctionChainKey).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sfcIID);
  if (dataObject instanceof ServiceFunctionChain) {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return (ServiceFunctionChain)dataObject;
  }
 else {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}",0.9833846153846154
132222,"public static void addPathToServiceFunctionChain(ServiceFunctionChain serviceFunctionChain,ServiceFunctionPath serviceFunctionPath){
  ServiceFunctionChainStateKey serviceFunctionChainStateKey=new ServiceFunctionChainStateKey(serviceFunctionChain.getName());
  InstanceIdentifier<ServiceFunctionChainState> sfcoIID=InstanceIdentifier.builder(ServiceFunctionChainsState.class).child(ServiceFunctionChainState.class,serviceFunctionChainStateKey).build();
  ServiceFunctionChainStateBuilder serviceFunctionChainStateBuilder=new ServiceFunctionChainStateBuilder();
  ArrayList<String> sfcServiceFunctionPathArrayList=new ArrayList<>();
  sfcServiceFunctionPathArrayList.add(serviceFunctionPath.getName());
  serviceFunctionChainStateBuilder.setSfcServiceFunctionPath(sfcServiceFunctionPathArrayList);
  serviceFunctionChainStateBuilder.setName(serviceFunctionChain.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putOperationalData(sfcoIID,serviceFunctionChainStateBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
}","public static void addPathToServiceFunctionChain(ServiceFunctionChain serviceFunctionChain,ServiceFunctionPath serviceFunctionPath){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionChainStateKey serviceFunctionChainStateKey=new ServiceFunctionChainStateKey(serviceFunctionChain.getName());
  InstanceIdentifier<ServiceFunctionChainState> sfcoIID=InstanceIdentifier.builder(ServiceFunctionChainsState.class).child(ServiceFunctionChainState.class,serviceFunctionChainStateKey).build();
  ServiceFunctionChainStateBuilder serviceFunctionChainStateBuilder=new ServiceFunctionChainStateBuilder();
  ArrayList<String> sfcServiceFunctionPathArrayList=new ArrayList<>();
  sfcServiceFunctionPathArrayList.add(serviceFunctionPath.getName());
  serviceFunctionChainStateBuilder.setSfcServiceFunctionPath(sfcServiceFunctionPathArrayList);
  serviceFunctionChainStateBuilder.setName(serviceFunctionChain.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putOperationalData(sfcoIID,serviceFunctionChainStateBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9397884458909682
132223,"public static void removeServiceFunctionFromChain(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionChains serviceFunctionChains=getServiceFunctionChainsRef();
  if (serviceFunctionChains != null) {
    List<ServiceFunctionChain> serviceFunctionChainList=serviceFunctionChains.getServiceFunctionChain();
    InstanceIdentifier<SfcServiceFunction> sfIID;
    SfcServiceFunctionKey serviceFunctionKey;
    for (    ServiceFunctionChain serviceFunctionChain : serviceFunctionChainList) {
      serviceFunctionKey=new SfcServiceFunctionKey(serviceFunction.getName());
      sfIID=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,serviceFunctionChain.getKey()).child(SfcServiceFunction.class,serviceFunctionKey).build();
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.removeConfigurationData(sfIID);
      try {
        t.commit().get();
      }
 catch (      InterruptedException|ExecutionException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
 else {
    LOG.warn(""String_Node_Str"");
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public static void removeServiceFunctionFromChain(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionChains serviceFunctionChains=getServiceFunctionChainsRef();
  if (serviceFunctionChains != null) {
    List<ServiceFunctionChain> serviceFunctionChainList=serviceFunctionChains.getServiceFunctionChain();
    InstanceIdentifier<SfcServiceFunction> sfIID;
    SfcServiceFunctionKey serviceFunctionKey;
    for (    ServiceFunctionChain serviceFunctionChain : serviceFunctionChainList) {
      serviceFunctionKey=new SfcServiceFunctionKey(serviceFunction.getName());
      sfIID=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,serviceFunctionChain.getKey()).child(SfcServiceFunction.class,serviceFunctionKey).build();
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.removeConfigurationData(sfIID);
      try {
        t.commit().get();
      }
 catch (      InterruptedException|ExecutionException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
 else {
    LOG.warn(""String_Node_Str"");
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9927125506072876
132224,"public void addChainToChainState(ServiceFunctionChain serviceFunctionChain){
  ServiceFunctionChainStateKey serviceFunctionChainStateKey=new ServiceFunctionChainStateKey(serviceFunctionChain.getName());
  InstanceIdentifier<ServiceFunctionChainState> sfcoIID=InstanceIdentifier.builder(ServiceFunctionChainsState.class).child(ServiceFunctionChainState.class,serviceFunctionChainStateKey).build();
  ServiceFunctionChainStateBuilder serviceFunctionChainStateBuilder=new ServiceFunctionChainStateBuilder();
  serviceFunctionChainStateBuilder.setName(serviceFunctionChain.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putOperationalData(sfcoIID,serviceFunctionChainStateBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
}","public void addChainToChainState(ServiceFunctionChain serviceFunctionChain){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionChainStateKey serviceFunctionChainStateKey=new ServiceFunctionChainStateKey(serviceFunctionChain.getName());
  InstanceIdentifier<ServiceFunctionChainState> sfcoIID=InstanceIdentifier.builder(ServiceFunctionChainsState.class).child(ServiceFunctionChainState.class,serviceFunctionChainStateKey).build();
  ServiceFunctionChainStateBuilder serviceFunctionChainStateBuilder=new ServiceFunctionChainStateBuilder();
  serviceFunctionChainStateBuilder.setName(serviceFunctionChain.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putOperationalData(sfcoIID,serviceFunctionChainStateBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9210245464247598
132225,"public static ServiceFunctionChains getServiceFunctionChainsRef(){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionChains> sfcsIID;
  sfcsIID=InstanceIdentifier.builder(ServiceFunctionChains.class).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sfcsIID);
  if (dataObject instanceof ServiceFunctionChains) {
    ServiceFunctionChains serviceFunctionChains=(ServiceFunctionChains)dataObject;
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return serviceFunctionChains;
  }
 else {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}","public static ServiceFunctionChains getServiceFunctionChainsRef(){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionChains> sfcsIID;
  sfcsIID=InstanceIdentifier.builder(ServiceFunctionChains.class).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sfcsIID);
  if (dataObject instanceof ServiceFunctionChains) {
    ServiceFunctionChains serviceFunctionChains=(ServiceFunctionChains)dataObject;
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return serviceFunctionChains;
  }
 else {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}",0.980700500357398
132226,"public static void addPathIdtoServiceFunctionForwarder(ServiceFunctionPath serviceFunctionPath){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarders> sffsIID;
  sffsIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).build();
  ServiceFunctionForwardersBuilder serviceFunctionForwardersBuilder=new ServiceFunctionForwardersBuilder();
  ArrayList<ServiceFunctionForwarder> serviceFunctionForwarderList=new ArrayList<>();
  List<SfpServiceFunction> sfpServiceFunctionArrayList=serviceFunctionPath.getSfpServiceFunction();
  for (  SfpServiceFunction sfpServiceFunction : sfpServiceFunctionArrayList) {
    ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(sfpServiceFunction.getServiceFunctionForwarder());
    ServiceFunctionForwarder serviceFunctionForwarder=readServiceFunctionForwarder(sfpServiceFunction.getServiceFunctionForwarder());
    ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
    if (serviceFunctionForwarder != null) {
      serviceFunctionForwarderBuilder.setPathId(serviceFunctionPath.getPathId());
      serviceFunctionForwarderBuilder.setName(sfpServiceFunction.getServiceFunctionForwarder());
      serviceFunctionForwarderBuilder.setSffDataPlaneLocator(serviceFunctionForwarder.getSffDataPlaneLocator());
      serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionForwarder.getServiceFunctionDictionary());
      serviceFunctionForwarderBuilder.setKey(serviceFunctionForwarderKey);
    }
    serviceFunctionForwarderList.add(serviceFunctionForwarderBuilder.build());
  }
  serviceFunctionForwardersBuilder.setServiceFunctionForwarder(serviceFunctionForwarderList);
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putConfigurationData(sffsIID,serviceFunctionForwardersBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public static void addPathIdtoServiceFunctionForwarder(ServiceFunctionPath serviceFunctionPath){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarders> sffsIID;
  sffsIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).build();
  ServiceFunctionForwardersBuilder serviceFunctionForwardersBuilder=new ServiceFunctionForwardersBuilder();
  ArrayList<ServiceFunctionForwarder> serviceFunctionForwarderList=new ArrayList<>();
  List<SfpServiceFunction> sfpServiceFunctionArrayList=serviceFunctionPath.getSfpServiceFunction();
  for (  SfpServiceFunction sfpServiceFunction : sfpServiceFunctionArrayList) {
    ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(sfpServiceFunction.getServiceFunctionForwarder());
    ServiceFunctionForwarder serviceFunctionForwarder=readServiceFunctionForwarder(sfpServiceFunction.getServiceFunctionForwarder());
    ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
    if (serviceFunctionForwarder != null) {
      serviceFunctionForwarderBuilder.setPathId(serviceFunctionPath.getPathId());
      serviceFunctionForwarderBuilder.setName(sfpServiceFunction.getServiceFunctionForwarder());
      serviceFunctionForwarderBuilder.setSffDataPlaneLocator(serviceFunctionForwarder.getSffDataPlaneLocator());
      serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionForwarder.getServiceFunctionDictionary());
      serviceFunctionForwarderBuilder.setKey(serviceFunctionForwarderKey);
    }
    serviceFunctionForwarderList.add(serviceFunctionForwarderBuilder.build());
  }
  serviceFunctionForwardersBuilder.setServiceFunctionForwarder(serviceFunctionForwarderList);
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putConfigurationData(sffsIID,serviceFunctionForwardersBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9957943925233644
132227,"public void deleteServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void deleteServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9888198757763976
132228,"public static ServiceFunctionForwarder readServiceFunctionForwarder(String name){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(name);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sffIID);
  if (dataObject instanceof ServiceFunctionForwarder) {
    ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)dataObject;
    return serviceFunctionForwarder;
  }
 else {
    return null;
  }
}","public static ServiceFunctionForwarder readServiceFunctionForwarder(String name){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(name);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sffIID);
  if (dataObject instanceof ServiceFunctionForwarder) {
    ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)dataObject;
    return serviceFunctionForwarder;
  }
 else {
    return null;
  }
}",0.993963782696177
132229,"public void createServiceFunctionForwarders(ServiceFunctionChains serviceFunctionchains){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarders> sffIID;
  ServiceFunctionForwardersBuilder serviceFunctionForwardersBuilder=new ServiceFunctionForwardersBuilder();
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).build();
  ArrayList<ServiceFunctionForwarder> serviceFunctionForwarderList=new ArrayList<>();
  List<ServiceFunctionChain> serviceFunctionChainList=serviceFunctionchains.getServiceFunctionChain();
  for (  ServiceFunctionChain serviceFunctionChain : serviceFunctionChainList) {
    List<SfcServiceFunction> sfcServiceFunctionList=serviceFunctionChain.getSfcServiceFunction();
    for (    SfcServiceFunction sfcServiceFunction : sfcServiceFunctionList) {
      ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunction(sfcServiceFunction.getName());
      ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
      ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
      serviceFunctionForwarderBuilder.setName(serviceFunction.getServiceFunctionForwarder());
      serviceFunctionForwarderBuilder.setKey(serviceFunctionForwarderKey);
      ArrayList<ServiceFunctionDictionary> serviceFunctionDictionaryList=new ArrayList<>();
      ServiceFunctionDictionaryBuilder serviceFunctionDictionaryBuilder=new ServiceFunctionDictionaryBuilder();
      serviceFunctionDictionaryBuilder.setName(serviceFunction.getName()).setType(serviceFunction.getType()).setServiceFunctionForwarder(serviceFunction.getServiceFunctionForwarder()).setSfDataPlaneLocator(serviceFunction.getSfDataPlaneLocator());
      serviceFunctionDictionaryList.add(serviceFunctionDictionaryBuilder.build());
      serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionDictionaryList);
      serviceFunctionForwarderList.add(serviceFunctionForwarderBuilder.build());
      LOG.info(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.putConfigurationData(sffIID,serviceFunctionForwarderBuilder.build());
      try {
        t.commit().get();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void createServiceFunctionForwarders(ServiceFunctionChains serviceFunctionchains){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarders> sffIID;
  ServiceFunctionForwardersBuilder serviceFunctionForwardersBuilder=new ServiceFunctionForwardersBuilder();
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).build();
  ArrayList<ServiceFunctionForwarder> serviceFunctionForwarderList=new ArrayList<>();
  List<ServiceFunctionChain> serviceFunctionChainList=serviceFunctionchains.getServiceFunctionChain();
  for (  ServiceFunctionChain serviceFunctionChain : serviceFunctionChainList) {
    List<SfcServiceFunction> sfcServiceFunctionList=serviceFunctionChain.getSfcServiceFunction();
    for (    SfcServiceFunction sfcServiceFunction : sfcServiceFunctionList) {
      ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunction(sfcServiceFunction.getName());
      ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
      ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
      serviceFunctionForwarderBuilder.setName(serviceFunction.getServiceFunctionForwarder());
      serviceFunctionForwarderBuilder.setKey(serviceFunctionForwarderKey);
      ArrayList<ServiceFunctionDictionary> serviceFunctionDictionaryList=new ArrayList<>();
      ServiceFunctionDictionaryBuilder serviceFunctionDictionaryBuilder=new ServiceFunctionDictionaryBuilder();
      serviceFunctionDictionaryBuilder.setName(serviceFunction.getName()).setType(serviceFunction.getType()).setServiceFunctionForwarder(serviceFunction.getServiceFunctionForwarder()).setSfDataPlaneLocator(serviceFunction.getSfDataPlaneLocator());
      serviceFunctionDictionaryList.add(serviceFunctionDictionaryBuilder.build());
      serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionDictionaryList);
      serviceFunctionForwarderList.add(serviceFunctionForwarderBuilder.build());
      LOG.debug(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.putConfigurationData(sffIID,serviceFunctionForwarderBuilder.build());
      try {
        t.commit().get();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9948285769009768
132230,"public void updateServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  deleteServiceFunctionForwarder(serviceFunction);
  createServiceFunctionForwarder(serviceFunction);
}","public void updateServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  deleteServiceFunctionForwarder(serviceFunction);
  createServiceFunctionForwarder(serviceFunction);
}",0.9822485207100592
132231,"public void deleteServiceFunctionFromForwarder(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionDictionary> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  ServiceFunctionDictionaryKey serviceFunctionDictionaryKey=new ServiceFunctionDictionaryKey(serviceFunction.getName());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).child(ServiceFunctionDictionary.class,serviceFunctionDictionaryKey).build();
  LOG.info(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void deleteServiceFunctionFromForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionDictionary> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  ServiceFunctionDictionaryKey serviceFunctionDictionaryKey=new ServiceFunctionDictionaryKey(serviceFunction.getName());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).child(ServiceFunctionDictionary.class,serviceFunctionDictionaryKey).build();
  LOG.debug(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9877551020408164
132232,"public void createServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.setName(serviceFunction.getServiceFunctionForwarder());
  ArrayList<ServiceFunctionDictionary> serviceFunctionDictionaryList=new ArrayList<>();
  ServiceFunctionDictionaryBuilder serviceFunctionDictionaryBuilder=new ServiceFunctionDictionaryBuilder();
  serviceFunctionDictionaryBuilder.setName(serviceFunction.getName()).setType(serviceFunction.getType()).setServiceFunctionForwarder(serviceFunction.getServiceFunctionForwarder()).setSfDataPlaneLocator(serviceFunction.getSfDataPlaneLocator());
  serviceFunctionDictionaryList.add(serviceFunctionDictionaryBuilder.build());
  serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionDictionaryList);
  LOG.info(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putConfigurationData(sffIID,serviceFunctionForwarderBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void createServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.setName(serviceFunction.getServiceFunctionForwarder());
  ArrayList<ServiceFunctionDictionary> serviceFunctionDictionaryList=new ArrayList<>();
  ServiceFunctionDictionaryBuilder serviceFunctionDictionaryBuilder=new ServiceFunctionDictionaryBuilder();
  serviceFunctionDictionaryBuilder.setName(serviceFunction.getName()).setType(serviceFunction.getType()).setServiceFunctionForwarder(serviceFunction.getServiceFunctionForwarder()).setSfDataPlaneLocator(serviceFunction.getSfDataPlaneLocator());
  serviceFunctionDictionaryList.add(serviceFunctionDictionaryBuilder.build());
  serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionDictionaryList);
  LOG.debug(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putConfigurationData(sffIID,serviceFunctionForwarderBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9923055001424907
132233,"public static ServiceFunction readServiceFunction(String serviceFunctionName){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunction> sfIID;
  ServiceFunctionKey serviceFunctionKey=new ServiceFunctionKey(serviceFunctionName);
  sfIID=InstanceIdentifier.builder(ServiceFunctions.class).child(ServiceFunction.class,serviceFunctionKey).build();
  DataObject serviceFunctiondataObject=odlSfc.dataProvider.readConfigurationData(sfIID);
  if (serviceFunctiondataObject instanceof ServiceFunction) {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return (ServiceFunction)serviceFunctiondataObject;
  }
 else {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}","public static ServiceFunction readServiceFunction(String serviceFunctionName){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunction> sfIID;
  ServiceFunctionKey serviceFunctionKey=new ServiceFunctionKey(serviceFunctionName);
  sfIID=InstanceIdentifier.builder(ServiceFunctions.class).child(ServiceFunction.class,serviceFunctionKey).build();
  DataObject serviceFunctiondataObject=odlSfc.dataProvider.readConfigurationData(sfIID);
  if (serviceFunctiondataObject instanceof ServiceFunction) {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return (ServiceFunction)serviceFunctiondataObject;
  }
 else {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}",0.9829005699810006
132234,"public static ServiceFunctionChain readServiceFunctionChain(String serviceFunctionChainName){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionChain> sfcIID;
  ServiceFunctionChainKey serviceFunctionChainKey=new ServiceFunctionChainKey(serviceFunctionChainName);
  sfcIID=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,serviceFunctionChainKey).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sfcIID);
  if (dataObject instanceof ServiceFunctionChain) {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return (ServiceFunctionChain)dataObject;
  }
 else {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}","public static ServiceFunctionChain readServiceFunctionChain(String serviceFunctionChainName){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionChain> sfcIID;
  ServiceFunctionChainKey serviceFunctionChainKey=new ServiceFunctionChainKey(serviceFunctionChainName);
  sfcIID=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,serviceFunctionChainKey).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sfcIID);
  if (dataObject instanceof ServiceFunctionChain) {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return (ServiceFunctionChain)dataObject;
  }
 else {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}",0.9833846153846154
132235,"public static void addPathToServiceFunctionChain(ServiceFunctionChain serviceFunctionChain,ServiceFunctionPath serviceFunctionPath){
  ServiceFunctionChainStateKey serviceFunctionChainStateKey=new ServiceFunctionChainStateKey(serviceFunctionChain.getName());
  InstanceIdentifier<ServiceFunctionChainState> sfcoIID=InstanceIdentifier.builder(ServiceFunctionChainsState.class).child(ServiceFunctionChainState.class,serviceFunctionChainStateKey).build();
  ServiceFunctionChainStateBuilder serviceFunctionChainStateBuilder=new ServiceFunctionChainStateBuilder();
  ArrayList<String> sfcServiceFunctionPathArrayList=new ArrayList<>();
  sfcServiceFunctionPathArrayList.add(serviceFunctionPath.getName());
  serviceFunctionChainStateBuilder.setSfcServiceFunctionPath(sfcServiceFunctionPathArrayList);
  serviceFunctionChainStateBuilder.setName(serviceFunctionChain.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putOperationalData(sfcoIID,serviceFunctionChainStateBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
}","public static void addPathToServiceFunctionChain(ServiceFunctionChain serviceFunctionChain,ServiceFunctionPath serviceFunctionPath){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionChainStateKey serviceFunctionChainStateKey=new ServiceFunctionChainStateKey(serviceFunctionChain.getName());
  InstanceIdentifier<ServiceFunctionChainState> sfcoIID=InstanceIdentifier.builder(ServiceFunctionChainsState.class).child(ServiceFunctionChainState.class,serviceFunctionChainStateKey).build();
  ServiceFunctionChainStateBuilder serviceFunctionChainStateBuilder=new ServiceFunctionChainStateBuilder();
  ArrayList<String> sfcServiceFunctionPathArrayList=new ArrayList<>();
  sfcServiceFunctionPathArrayList.add(serviceFunctionPath.getName());
  serviceFunctionChainStateBuilder.setSfcServiceFunctionPath(sfcServiceFunctionPathArrayList);
  serviceFunctionChainStateBuilder.setName(serviceFunctionChain.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putOperationalData(sfcoIID,serviceFunctionChainStateBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9397884458909682
132236,"public static void removeServiceFunctionFromChain(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionChains serviceFunctionChains=getServiceFunctionChainsRef();
  if (serviceFunctionChains != null) {
    List<ServiceFunctionChain> serviceFunctionChainList=serviceFunctionChains.getServiceFunctionChain();
    InstanceIdentifier<SfcServiceFunction> sfIID;
    SfcServiceFunctionKey serviceFunctionKey;
    for (    ServiceFunctionChain serviceFunctionChain : serviceFunctionChainList) {
      serviceFunctionKey=new SfcServiceFunctionKey(serviceFunction.getName());
      sfIID=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,serviceFunctionChain.getKey()).child(SfcServiceFunction.class,serviceFunctionKey).build();
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.removeConfigurationData(sfIID);
      try {
        t.commit().get();
      }
 catch (      InterruptedException|ExecutionException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
 else {
    LOG.warn(""String_Node_Str"");
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public static void removeServiceFunctionFromChain(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionChains serviceFunctionChains=getServiceFunctionChainsRef();
  if (serviceFunctionChains != null) {
    List<ServiceFunctionChain> serviceFunctionChainList=serviceFunctionChains.getServiceFunctionChain();
    InstanceIdentifier<SfcServiceFunction> sfIID;
    SfcServiceFunctionKey serviceFunctionKey;
    for (    ServiceFunctionChain serviceFunctionChain : serviceFunctionChainList) {
      serviceFunctionKey=new SfcServiceFunctionKey(serviceFunction.getName());
      sfIID=InstanceIdentifier.builder(ServiceFunctionChains.class).child(ServiceFunctionChain.class,serviceFunctionChain.getKey()).child(SfcServiceFunction.class,serviceFunctionKey).build();
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.removeConfigurationData(sfIID);
      try {
        t.commit().get();
      }
 catch (      InterruptedException|ExecutionException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
 else {
    LOG.warn(""String_Node_Str"");
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9927125506072876
132237,"public void addChainToChainState(ServiceFunctionChain serviceFunctionChain){
  ServiceFunctionChainStateKey serviceFunctionChainStateKey=new ServiceFunctionChainStateKey(serviceFunctionChain.getName());
  InstanceIdentifier<ServiceFunctionChainState> sfcoIID=InstanceIdentifier.builder(ServiceFunctionChainsState.class).child(ServiceFunctionChainState.class,serviceFunctionChainStateKey).build();
  ServiceFunctionChainStateBuilder serviceFunctionChainStateBuilder=new ServiceFunctionChainStateBuilder();
  serviceFunctionChainStateBuilder.setName(serviceFunctionChain.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putOperationalData(sfcoIID,serviceFunctionChainStateBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
}","public void addChainToChainState(ServiceFunctionChain serviceFunctionChain){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionChainStateKey serviceFunctionChainStateKey=new ServiceFunctionChainStateKey(serviceFunctionChain.getName());
  InstanceIdentifier<ServiceFunctionChainState> sfcoIID=InstanceIdentifier.builder(ServiceFunctionChainsState.class).child(ServiceFunctionChainState.class,serviceFunctionChainStateKey).build();
  ServiceFunctionChainStateBuilder serviceFunctionChainStateBuilder=new ServiceFunctionChainStateBuilder();
  serviceFunctionChainStateBuilder.setName(serviceFunctionChain.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putOperationalData(sfcoIID,serviceFunctionChainStateBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9210245464247598
132238,"public static ServiceFunctionChains getServiceFunctionChainsRef(){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionChains> sfcsIID;
  sfcsIID=InstanceIdentifier.builder(ServiceFunctionChains.class).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sfcsIID);
  if (dataObject instanceof ServiceFunctionChains) {
    ServiceFunctionChains serviceFunctionChains=(ServiceFunctionChains)dataObject;
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return serviceFunctionChains;
  }
 else {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}","public static ServiceFunctionChains getServiceFunctionChainsRef(){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionChains> sfcsIID;
  sfcsIID=InstanceIdentifier.builder(ServiceFunctionChains.class).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sfcsIID);
  if (dataObject instanceof ServiceFunctionChains) {
    ServiceFunctionChains serviceFunctionChains=(ServiceFunctionChains)dataObject;
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return serviceFunctionChains;
  }
 else {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}",0.980700500357398
132239,"public static void addPathIdtoServiceFunctionForwarder(ServiceFunctionPath serviceFunctionPath){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarders> sffsIID;
  sffsIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).build();
  ServiceFunctionForwardersBuilder serviceFunctionForwardersBuilder=new ServiceFunctionForwardersBuilder();
  ArrayList<ServiceFunctionForwarder> serviceFunctionForwarderList=new ArrayList<>();
  List<SfpServiceFunction> sfpServiceFunctionArrayList=serviceFunctionPath.getSfpServiceFunction();
  for (  SfpServiceFunction sfpServiceFunction : sfpServiceFunctionArrayList) {
    ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(sfpServiceFunction.getServiceFunctionForwarder());
    ServiceFunctionForwarder serviceFunctionForwarder=readServiceFunctionForwarder(sfpServiceFunction.getServiceFunctionForwarder());
    ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
    if (serviceFunctionForwarder != null) {
      serviceFunctionForwarderBuilder.setPathId(serviceFunctionPath.getPathId());
      serviceFunctionForwarderBuilder.setName(sfpServiceFunction.getServiceFunctionForwarder());
      serviceFunctionForwarderBuilder.setSffDataPlaneLocator(serviceFunctionForwarder.getSffDataPlaneLocator());
      serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionForwarder.getServiceFunctionDictionary());
      serviceFunctionForwarderBuilder.setKey(serviceFunctionForwarderKey);
    }
    serviceFunctionForwarderList.add(serviceFunctionForwarderBuilder.build());
  }
  serviceFunctionForwardersBuilder.setServiceFunctionForwarder(serviceFunctionForwarderList);
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putConfigurationData(sffsIID,serviceFunctionForwardersBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public static void addPathIdtoServiceFunctionForwarder(ServiceFunctionPath serviceFunctionPath){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarders> sffsIID;
  sffsIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).build();
  ServiceFunctionForwardersBuilder serviceFunctionForwardersBuilder=new ServiceFunctionForwardersBuilder();
  ArrayList<ServiceFunctionForwarder> serviceFunctionForwarderList=new ArrayList<>();
  List<SfpServiceFunction> sfpServiceFunctionArrayList=serviceFunctionPath.getSfpServiceFunction();
  for (  SfpServiceFunction sfpServiceFunction : sfpServiceFunctionArrayList) {
    ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(sfpServiceFunction.getServiceFunctionForwarder());
    ServiceFunctionForwarder serviceFunctionForwarder=readServiceFunctionForwarder(sfpServiceFunction.getServiceFunctionForwarder());
    ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
    if (serviceFunctionForwarder != null) {
      serviceFunctionForwarderBuilder.setPathId(serviceFunctionPath.getPathId());
      serviceFunctionForwarderBuilder.setName(sfpServiceFunction.getServiceFunctionForwarder());
      serviceFunctionForwarderBuilder.setSffDataPlaneLocator(serviceFunctionForwarder.getSffDataPlaneLocator());
      serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionForwarder.getServiceFunctionDictionary());
      serviceFunctionForwarderBuilder.setKey(serviceFunctionForwarderKey);
    }
    serviceFunctionForwarderList.add(serviceFunctionForwarderBuilder.build());
  }
  serviceFunctionForwardersBuilder.setServiceFunctionForwarder(serviceFunctionForwarderList);
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putConfigurationData(sffsIID,serviceFunctionForwardersBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9957943925233644
132240,"public void deleteServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void deleteServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9888198757763976
132241,"public static ServiceFunctionForwarder readServiceFunctionForwarder(String name){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(name);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sffIID);
  if (dataObject instanceof ServiceFunctionForwarder) {
    ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)dataObject;
    return serviceFunctionForwarder;
  }
 else {
    return null;
  }
}","public static ServiceFunctionForwarder readServiceFunctionForwarder(String name){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(name);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  DataObject dataObject=odlSfc.dataProvider.readConfigurationData(sffIID);
  if (dataObject instanceof ServiceFunctionForwarder) {
    ServiceFunctionForwarder serviceFunctionForwarder=(ServiceFunctionForwarder)dataObject;
    return serviceFunctionForwarder;
  }
 else {
    return null;
  }
}",0.993963782696177
132242,"public void createServiceFunctionForwarders(ServiceFunctionChains serviceFunctionchains){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarders> sffIID;
  ServiceFunctionForwardersBuilder serviceFunctionForwardersBuilder=new ServiceFunctionForwardersBuilder();
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).build();
  ArrayList<ServiceFunctionForwarder> serviceFunctionForwarderList=new ArrayList<>();
  List<ServiceFunctionChain> serviceFunctionChainList=serviceFunctionchains.getServiceFunctionChain();
  for (  ServiceFunctionChain serviceFunctionChain : serviceFunctionChainList) {
    List<SfcServiceFunction> sfcServiceFunctionList=serviceFunctionChain.getSfcServiceFunction();
    for (    SfcServiceFunction sfcServiceFunction : sfcServiceFunctionList) {
      ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunction(sfcServiceFunction.getName());
      ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
      ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
      serviceFunctionForwarderBuilder.setName(serviceFunction.getServiceFunctionForwarder());
      serviceFunctionForwarderBuilder.setKey(serviceFunctionForwarderKey);
      ArrayList<ServiceFunctionDictionary> serviceFunctionDictionaryList=new ArrayList<>();
      ServiceFunctionDictionaryBuilder serviceFunctionDictionaryBuilder=new ServiceFunctionDictionaryBuilder();
      serviceFunctionDictionaryBuilder.setName(serviceFunction.getName()).setType(serviceFunction.getType()).setServiceFunctionForwarder(serviceFunction.getServiceFunctionForwarder()).setSfDataPlaneLocator(serviceFunction.getSfDataPlaneLocator());
      serviceFunctionDictionaryList.add(serviceFunctionDictionaryBuilder.build());
      serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionDictionaryList);
      serviceFunctionForwarderList.add(serviceFunctionForwarderBuilder.build());
      LOG.info(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.putConfigurationData(sffIID,serviceFunctionForwarderBuilder.build());
      try {
        t.commit().get();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void createServiceFunctionForwarders(ServiceFunctionChains serviceFunctionchains){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarders> sffIID;
  ServiceFunctionForwardersBuilder serviceFunctionForwardersBuilder=new ServiceFunctionForwardersBuilder();
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).build();
  ArrayList<ServiceFunctionForwarder> serviceFunctionForwarderList=new ArrayList<>();
  List<ServiceFunctionChain> serviceFunctionChainList=serviceFunctionchains.getServiceFunctionChain();
  for (  ServiceFunctionChain serviceFunctionChain : serviceFunctionChainList) {
    List<SfcServiceFunction> sfcServiceFunctionList=serviceFunctionChain.getSfcServiceFunction();
    for (    SfcServiceFunction sfcServiceFunction : sfcServiceFunctionList) {
      ServiceFunction serviceFunction=SfcProviderServiceFunctionAPI.readServiceFunction(sfcServiceFunction.getName());
      ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
      ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
      serviceFunctionForwarderBuilder.setName(serviceFunction.getServiceFunctionForwarder());
      serviceFunctionForwarderBuilder.setKey(serviceFunctionForwarderKey);
      ArrayList<ServiceFunctionDictionary> serviceFunctionDictionaryList=new ArrayList<>();
      ServiceFunctionDictionaryBuilder serviceFunctionDictionaryBuilder=new ServiceFunctionDictionaryBuilder();
      serviceFunctionDictionaryBuilder.setName(serviceFunction.getName()).setType(serviceFunction.getType()).setServiceFunctionForwarder(serviceFunction.getServiceFunctionForwarder()).setSfDataPlaneLocator(serviceFunction.getSfDataPlaneLocator());
      serviceFunctionDictionaryList.add(serviceFunctionDictionaryBuilder.build());
      serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionDictionaryList);
      serviceFunctionForwarderList.add(serviceFunctionForwarderBuilder.build());
      LOG.debug(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.putConfigurationData(sffIID,serviceFunctionForwarderBuilder.build());
      try {
        t.commit().get();
      }
 catch (      ExecutionException|InterruptedException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9948285769009768
132243,"public void updateServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  deleteServiceFunctionForwarder(serviceFunction);
  createServiceFunctionForwarder(serviceFunction);
}","public void updateServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  deleteServiceFunctionForwarder(serviceFunction);
  createServiceFunctionForwarder(serviceFunction);
}",0.9822485207100592
132244,"public void deleteServiceFunctionFromForwarder(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionDictionary> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  ServiceFunctionDictionaryKey serviceFunctionDictionaryKey=new ServiceFunctionDictionaryKey(serviceFunction.getName());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).child(ServiceFunctionDictionary.class,serviceFunctionDictionaryKey).build();
  LOG.info(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void deleteServiceFunctionFromForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionDictionary> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  ServiceFunctionDictionaryKey serviceFunctionDictionaryKey=new ServiceFunctionDictionaryKey(serviceFunction.getName());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).child(ServiceFunctionDictionary.class,serviceFunctionDictionaryKey).build();
  LOG.debug(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9877551020408164
132245,"public void createServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.setName(serviceFunction.getServiceFunctionForwarder());
  ArrayList<ServiceFunctionDictionary> serviceFunctionDictionaryList=new ArrayList<>();
  ServiceFunctionDictionaryBuilder serviceFunctionDictionaryBuilder=new ServiceFunctionDictionaryBuilder();
  serviceFunctionDictionaryBuilder.setName(serviceFunction.getName()).setType(serviceFunction.getType()).setServiceFunctionForwarder(serviceFunction.getServiceFunctionForwarder()).setSfDataPlaneLocator(serviceFunction.getSfDataPlaneLocator());
  serviceFunctionDictionaryList.add(serviceFunctionDictionaryBuilder.build());
  serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionDictionaryList);
  LOG.info(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putConfigurationData(sffIID,serviceFunctionForwarderBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void createServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  ServiceFunctionForwarderBuilder serviceFunctionForwarderBuilder=new ServiceFunctionForwarderBuilder();
  serviceFunctionForwarderBuilder.setName(serviceFunction.getServiceFunctionForwarder());
  ArrayList<ServiceFunctionDictionary> serviceFunctionDictionaryList=new ArrayList<>();
  ServiceFunctionDictionaryBuilder serviceFunctionDictionaryBuilder=new ServiceFunctionDictionaryBuilder();
  serviceFunctionDictionaryBuilder.setName(serviceFunction.getName()).setType(serviceFunction.getType()).setServiceFunctionForwarder(serviceFunction.getServiceFunctionForwarder()).setSfDataPlaneLocator(serviceFunction.getSfDataPlaneLocator());
  serviceFunctionDictionaryList.add(serviceFunctionDictionaryBuilder.build());
  serviceFunctionForwarderBuilder.setServiceFunctionDictionary(serviceFunctionDictionaryList);
  LOG.debug(""String_Node_Str"",serviceFunction.getServiceFunctionForwarder(),serviceFunction.getName());
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.putConfigurationData(sffIID,serviceFunctionForwarderBuilder.build());
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.9923055001424907
132246,"public static ServiceFunction readServiceFunction(String serviceFunctionName){
  LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunction> sfIID;
  ServiceFunctionKey serviceFunctionKey=new ServiceFunctionKey(serviceFunctionName);
  sfIID=InstanceIdentifier.builder(ServiceFunctions.class).child(ServiceFunction.class,serviceFunctionKey).build();
  DataObject serviceFunctiondataObject=odlSfc.dataProvider.readConfigurationData(sfIID);
  if (serviceFunctiondataObject instanceof ServiceFunction) {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return (ServiceFunction)serviceFunctiondataObject;
  }
 else {
    LOG.info(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}","public static ServiceFunction readServiceFunction(String serviceFunctionName){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunction> sfIID;
  ServiceFunctionKey serviceFunctionKey=new ServiceFunctionKey(serviceFunctionName);
  sfIID=InstanceIdentifier.builder(ServiceFunctions.class).child(ServiceFunction.class,serviceFunctionKey).build();
  DataObject serviceFunctiondataObject=odlSfc.dataProvider.readConfigurationData(sfIID);
  if (serviceFunctiondataObject instanceof ServiceFunction) {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return (ServiceFunction)serviceFunctiondataObject;
  }
 else {
    LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
    return null;
  }
}",0.9829005699810006
132247,"public void putServiceFunctionForwarders(ServiceFunctionForwarders serviceFunctionForwarders){
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  ClientResponse getClientResponse=client.resource(""String_Node_Str"").accept(""String_Node_Str"").get(ClientResponse.class);
  String jsonOutput=getClientResponse.getEntity(String.class);
  getClientResponse.close();
  ClientResponse putClientResponse=client.resource(""String_Node_Str"").type(""String_Node_Str"").put(ClientResponse.class,jsonOutput);
  putClientResponse.close();
}","public void putServiceFunctionForwarders(ServiceFunctionForwarders serviceFunctionForwarders){
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  ClientResponse getClientResponse=client.resource(""String_Node_Str"").accept(""String_Node_Str"").get(ClientResponse.class);
  String jsonOutput=getClientResponse.getEntity(String.class);
  getClientResponse.close();
  ClientResponse putClientRemoteResponse=client.resource(""String_Node_Str"").type(""String_Node_Str"").put(ClientResponse.class,jsonOutput);
  putClientRemoteResponse.close();
  ClientResponse putClientLocalResponse=client.resource(""String_Node_Str"").type(""String_Node_Str"").put(ClientResponse.class,jsonOutput);
  putClientLocalResponse.close();
}",0.8620949510173324
132248,"public void putServiceFunctionPaths(ServiceFunctionPaths serviceFunctionPaths){
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  ClientResponse getClientResponse=client.resource(""String_Node_Str"").accept(""String_Node_Str"").get(ClientResponse.class);
  String jsonOutput=getClientResponse.getEntity(String.class);
  getClientResponse.close();
  ClientResponse putClientResponse=client.resource(""String_Node_Str"").type(""String_Node_Str"").put(ClientResponse.class,jsonOutput);
  putClientResponse.close();
}","public void putServiceFunctionPaths(ServiceFunctionPaths serviceFunctionPaths){
  ClientConfig clientConfig=new DefaultClientConfig();
  Client client=Client.create(clientConfig);
  ClientResponse getClientResponse=client.resource(""String_Node_Str"").accept(""String_Node_Str"").get(ClientResponse.class);
  String jsonOutput=getClientResponse.getEntity(String.class);
  getClientResponse.close();
}",0.8310598111227702
132249,"public void updateServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  deleteServiceFunctionForwarder(serviceFunction);
  createServiceFunctionForwarder(serviceFunction);
}","public void updateServiceFunctionForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  deleteServiceFunctionFromForwarder(serviceFunction);
  createServiceFunctionForwarder(serviceFunction);
}",0.9921875
132250,"public void deleteServiceFunctionFromForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionForwarder> sffIID;
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).build();
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","public void deleteServiceFunctionFromForwarder(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionDictionary> sffIID;
  ServiceFunctionDictionaryKey serviceFunctionDictionaryKey=new ServiceFunctionDictionaryKey(serviceFunction.getName());
  ServiceFunctionForwarderKey serviceFunctionForwarderKey=new ServiceFunctionForwarderKey(serviceFunction.getServiceFunctionForwarder());
  sffIID=InstanceIdentifier.builder(ServiceFunctionForwarders.class).child(ServiceFunctionForwarder.class,serviceFunctionForwarderKey).child(ServiceFunctionDictionary.class,serviceFunctionDictionaryKey).build();
  final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
  t.removeConfigurationData(sffIID);
  try {
    t.commit().get();
  }
 catch (  ExecutionException|InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.8850828729281768
132251,"private void deleteServicePathContainingFunction(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionPath> sfpIID;
  ServiceFunctionState serviceFunctionState;
  ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(serviceFunction.getName());
  InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
  DataObject dataSfcStateObject=odlSfc.dataProvider.readOperationalData(sfStateIID);
  if (dataSfcStateObject instanceof ServiceFunctionState) {
    serviceFunctionState=(ServiceFunctionState)dataSfcStateObject;
    List<String> sfServiceFunctionPathList=serviceFunctionState.getSfServiceFunctionPath();
    List<String> removedPaths=new ArrayList<>();
    for (    String pathName : sfServiceFunctionPathList) {
      ServiceFunctionPathKey serviceFunctionPathKey=new ServiceFunctionPathKey(pathName);
      sfpIID=InstanceIdentifier.builder(ServiceFunctionPaths.class).child(ServiceFunctionPath.class,serviceFunctionPathKey).build();
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.removeConfigurationData(sfpIID);
      try {
        t.commit().get();
        removedPaths.add(pathName);
      }
 catch (      InterruptedException|ExecutionException e) {
        LOG.error(""String_Node_Str"",pathName);
      }
    }
    sfServiceFunctionPathList.removeAll(removedPaths);
    ServiceFunctionStateBuilder serviceFunctionStateBuilder=new ServiceFunctionStateBuilder();
    serviceFunctionStateBuilder.setName(serviceFunction.getName());
    serviceFunctionStateBuilder.setSfServiceFunctionPath(sfServiceFunctionPathList);
    final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
    t.putOperationalData(sfStateIID,serviceFunctionStateBuilder.build());
    try {
      t.commit().get();
    }
 catch (    InterruptedException|ExecutionException e) {
      LOG.error(""String_Node_Str"",serviceFunction.getName());
    }
  }
 else {
    LOG.error(""String_Node_Str"",serviceFunction.getName());
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}","private void deleteServicePathContainingFunction(ServiceFunction serviceFunction){
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
  InstanceIdentifier<ServiceFunctionPath> sfpIID;
  ServiceFunctionState serviceFunctionState;
  ServiceFunctionStateKey serviceFunctionStateKey=new ServiceFunctionStateKey(serviceFunction.getName());
  InstanceIdentifier<ServiceFunctionState> sfStateIID=InstanceIdentifier.builder(ServiceFunctionsState.class).child(ServiceFunctionState.class,serviceFunctionStateKey).build();
  DataObject dataSfcStateObject=odlSfc.dataProvider.readOperationalData(sfStateIID);
  if (dataSfcStateObject instanceof ServiceFunctionState) {
    serviceFunctionState=(ServiceFunctionState)dataSfcStateObject;
    List<String> sfServiceFunctionPathList=serviceFunctionState.getSfServiceFunctionPath();
    List<String> removedPaths=new ArrayList<>();
    for (    String pathName : sfServiceFunctionPathList) {
      ServiceFunctionPathKey serviceFunctionPathKey=new ServiceFunctionPathKey(pathName);
      sfpIID=InstanceIdentifier.builder(ServiceFunctionPaths.class).child(ServiceFunctionPath.class,serviceFunctionPathKey).build();
      final DataModificationTransaction t=odlSfc.dataProvider.beginTransaction();
      t.removeConfigurationData(sfpIID);
      try {
        t.commit().get();
        removedPaths.add(pathName);
      }
 catch (      InterruptedException|ExecutionException e) {
        LOG.error(""String_Node_Str"",pathName);
      }
    }
    if (removedPaths.containsAll(sfServiceFunctionPathList)) {
      SfcProviderServiceFunctionAPI.deleteServiceFunctionState(serviceFunction.getName());
    }
 else {
      LOG.error(""String_Node_Str"",serviceFunction.getName());
    }
  }
 else {
    LOG.warn(""String_Node_Str"",serviceFunction.getName());
  }
  LOG.debug(""String_Node_Str"",Thread.currentThread().getStackTrace()[1]);
}",0.836424957841484
132252,"@Override public View getView(int pos,View convertView,ViewGroup parent){
  if (convertView == null) {
    convertView=mInflater.inflate(R.layout.plugin_item,parent,false);
  }
  TextView tv=(TextView)convertView.findViewById(R.id.name);
  final PluginInfo info=mDatas.get(pos);
  tv.setText(info.applicationName);
  Button btn=(Button)convertView.findViewById(R.id.status);
  btn.setFocusable(false);
  if (info.isInstalled) {
    btn.setText(""String_Node_Str"");
    btn.setOnClickListener(new View.OnClickListener(){
      @Override public void onClick(      View arg0){
        PluginManager.getInstance().uninstallPlugin(mContext,info.applicationName);
        notifyDataSetChanged();
      }
    }
);
  }
 else {
    btn.setText(""String_Node_Str"");
    btn.setOnClickListener(new View.OnClickListener(){
      @Override public void onClick(      View arg0){
        PluginManager.getInstance().installPlugin(info,(InstallPluginListener)mContext);
        ;
      }
    }
);
  }
  return convertView;
}","@Override public View getView(int pos,View convertView,ViewGroup parent){
  if (convertView == null) {
    convertView=mInflater.inflate(R.layout.plugin_item,parent,false);
  }
  TextView tv=(TextView)convertView.findViewById(R.id.name);
  final PluginInfo info=mDatas.get(pos);
  tv.setText(info.applicationName);
  Button btn=(Button)convertView.findViewById(R.id.status);
  btn.setFocusable(false);
  if (info.isInstalled) {
    btn.setText(""String_Node_Str"");
    btn.setOnClickListener(new View.OnClickListener(){
      @Override public void onClick(      View arg0){
        PluginManager.getInstance().uninstallPlugin(mContext,info.packageName);
        notifyDataSetChanged();
      }
    }
);
  }
 else {
    btn.setText(""String_Node_Str"");
    btn.setOnClickListener(new View.OnClickListener(){
      @Override public void onClick(      View arg0){
        PluginManager.getInstance().installPlugin(info,(InstallPluginListener)mContext);
        ;
      }
    }
);
  }
  return convertView;
}",0.9910358565737052
132253,"@Override protected void onCreate(Bundle savedInstanceState){
  super.onCreate(savedInstanceState);
  setContentView(R.layout.activity_main);
  mListView=(ListView)findViewById(R.id.list);
  mListView.setOnItemClickListener(this);
  mDialog=new ProgressDialog(this);
  mDialog.setMessage(""String_Node_Str"");
  File dir=getFilesDir();
  dir=new File(dir,""String_Node_Str"");
  PluginManager.getInstance().init(this,getDir(""String_Node_Str"",Context.MODE_PRIVATE).getAbsolutePath(),dir.getAbsolutePath());
  ApkFile apk=new ApkFile();
  apk.apkName=""String_Node_Str"";
  apk.name=""String_Node_Str"";
  apk.nativeLibs.add(""String_Node_Str"");
  apks.add(apk);
  apk=new ApkFile();
  apk.apkName=""String_Node_Str"";
  apk.name=""String_Node_Str"";
  apk.nativeLibs.add(""String_Node_Str"");
  apks.add(apk);
}","@Override protected void onCreate(Bundle savedInstanceState){
  super.onCreate(savedInstanceState);
  setContentView(R.layout.activity_main);
  mListView=(ListView)findViewById(R.id.list);
  mListView.setOnItemClickListener(this);
  mDialog=new ProgressDialog(this);
  mDialog.setMessage(""String_Node_Str"");
  File dir=getFilesDir();
  dir=new File(dir,""String_Node_Str"");
  PluginManager.getInstance().init(this,getDir(""String_Node_Str"",Context.MODE_PRIVATE).getAbsolutePath(),dir.getAbsolutePath());
  ApkFile apk=new ApkFile();
  apk.apkName=""String_Node_Str"";
  apk.name=""String_Node_Str"";
  apk.nativeLibs.add(""String_Node_Str"");
  apk.version=1.0f;
  apks.add(apk);
  apk=new ApkFile();
  apk.apkName=""String_Node_Str"";
  apk.name=""String_Node_Str"";
  apk.nativeLibs.add(""String_Node_Str"");
  apk.version=1.0f;
  apks.add(apk);
  PluginManager.getInstance().copyApksFromAsset(apks,getAssets(),this);
}",0.9236192714453584
132254,"public static void main(String[] args) throws IOException, GeneralSecurityException {
  PipelineOptionsFactory.register(Options.class);
  Options options=PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);
  Options.Methods.validateOptions(options);
  StreamVariantsRequest prototype=CallSetNamesOptions.Methods.getRequestPrototype(options);
  OfflineAuth auth=GenomicsOptions.Methods.getGenomicsAuth(options);
  ImmutableSet<String> callSetIds=ImmutableSet.<String>builder().addAll(CallSetNamesOptions.Methods.getCallSetIds(options)).build();
  LOG.info(""String_Node_Str"" + ""String_Node_Str"" + callSetIds.size() + ""String_Node_Str""+ callSetIds);
  if (options.getIdentifyVariantsWithoutCalls()) {
    LOG.info(""String_Node_Str"");
  }
  List<StreamVariantsRequest> shardRequests=options.isAllReferences() ? ShardUtils.getVariantRequests(prototype,ShardUtils.SexChromosomeFilter.INCLUDE_XY,options.getBasesPerShard(),auth) : ShardUtils.getVariantRequests(prototype,options.getBasesPerShard(),options.getReferences());
  Pipeline p=Pipeline.create(options);
  PCollection<Variant> variants=p.begin().apply(Create.of(shardRequests)).apply(new VariantStreamer(auth,ShardBoundary.Requirement.STRICT,VARIANT_FIELDS)).apply(ParDo.of(new PrivateVariantsFilterFn(callSetIds,options.getIdentifyVariantsWithoutCalls())));
  variants.apply(ParDo.named(""String_Node_Str"").of(new DoFn<Variant,String>(){
    @Override public void processElement(    ProcessContext c){
      Variant v=c.element();
      c.output(Joiner.on(""String_Node_Str"").join(v.getId(),v.getReferenceName(),v.getStart(),v.getEnd(),v.getReferenceBases(),Joiner.on(""String_Node_Str"").join(v.getAlternateBasesList())));
    }
  }
)).apply(TextIO.Write.to(options.getOutput()));
  p.run();
}","public static void main(String[] args) throws IOException, GeneralSecurityException {
  PipelineOptionsFactory.register(Options.class);
  Options options=PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);
  Options.Methods.validateOptions(options);
  StreamVariantsRequest prototype=StreamVariantsRequest.newBuilder(CallSetNamesOptions.Methods.getRequestPrototype(options)).clearCallSetIds().build();
  OfflineAuth auth=GenomicsOptions.Methods.getGenomicsAuth(options);
  ImmutableSet<String> callSetIds=ImmutableSet.<String>builder().addAll(CallSetNamesOptions.Methods.getCallSetIds(options)).build();
  LOG.info(""String_Node_Str"" + ""String_Node_Str"" + callSetIds.size() + ""String_Node_Str""+ callSetIds);
  if (options.getIdentifyVariantsWithoutCalls()) {
    LOG.info(""String_Node_Str"");
  }
  List<StreamVariantsRequest> shardRequests=options.isAllReferences() ? ShardUtils.getVariantRequests(prototype,ShardUtils.SexChromosomeFilter.INCLUDE_XY,options.getBasesPerShard(),auth) : ShardUtils.getVariantRequests(prototype,options.getBasesPerShard(),options.getReferences());
  Pipeline p=Pipeline.create(options);
  PCollection<Variant> variants=p.begin().apply(Create.of(shardRequests)).apply(new VariantStreamer(auth,ShardBoundary.Requirement.STRICT,VARIANT_FIELDS)).apply(ParDo.of(new PrivateVariantsFilterFn(callSetIds,options.getIdentifyVariantsWithoutCalls())));
  variants.apply(ParDo.named(""String_Node_Str"").of(new DoFn<Variant,String>(){
    @Override public void processElement(    ProcessContext c){
      Variant v=c.element();
      c.output(Joiner.on(""String_Node_Str"").join(v.getId(),v.getReferenceName(),v.getStart(),v.getEnd(),v.getReferenceBases(),Joiner.on(""String_Node_Str"").join(v.getAlternateBasesList())));
    }
  }
)).apply(TextIO.Write.to(options.getOutput()));
  p.run();
}",0.9832869080779943
132255,"private void testBase(String[] ARGS,String[] expectedResult) throws Exception {
  IdentifyPrivateVariants.main(ARGS);
  List<String> results=Lists.newArrayList();
  for (  GcsPath path : helper.gcsUtil.expand(GcsPath.fromUri(outputPrefix + ""String_Node_Str""))) {
    BufferedReader reader=helper.openOutput(path.toString());
    for (String line=reader.readLine(); line != null; line=reader.readLine()) {
      results.add(line);
    }
  }
  assertThat(results,CoreMatchers.allOf(CoreMatchers.hasItems(expectedResult)));
}","private void testBase(String[] ARGS,String[] expectedResult) throws Exception {
  IdentifyPrivateVariants.main(ARGS);
  List<String> results=Lists.newArrayList();
  for (  GcsPath path : helper.gcsUtil.expand(GcsPath.fromUri(outputPrefix + ""String_Node_Str""))) {
    BufferedReader reader=helper.openOutput(path.toString());
    for (String line=reader.readLine(); line != null; line=reader.readLine()) {
      results.add(line);
    }
  }
  assertEquals(results.size(),expectedResult.length);
  assertThat(results,CoreMatchers.allOf(CoreMatchers.hasItems(expectedResult)));
}",0.9508196721311476
132256,"/** 
 * To compare how sharded reading works vs. plain HTSJDK sequential iteration, this method implements such iteration. This makes it easier to discover errors such as reads that are somehow skipped by a sharded approach.
 */
public static Iterable<Read> readSequentiallyForTesting(Objects storageClient,String storagePath,Contig contig,ReaderOptions options) throws IOException {
  Stopwatch timer=Stopwatch.createStarted();
  SamReader samReader=BAMIO.openBAM(storageClient,storagePath,options.getStringency());
  SAMRecordIterator iterator=samReader.queryOverlapping(contig.referenceName,(int)contig.start + 1,(int)contig.end + 1);
  List<Read> reads=new ArrayList<Read>();
  int recordsBeforeStart=0;
  int recordsAfterEnd=0;
  int mismatchedSequence=0;
  int recordsProcessed=0;
  Filter filter=setupFilter(options,contig.referenceName);
  while (iterator.hasNext()) {
    SAMRecord record=iterator.next();
    final boolean passesFilter=passesFilter(record,filter,contig.referenceName);
    if (!passesFilter) {
      mismatchedSequence++;
      continue;
    }
    if (record.getAlignmentStart() < contig.start) {
      recordsBeforeStart++;
      continue;
    }
    if (record.getAlignmentStart() > contig.end) {
      recordsAfterEnd++;
      continue;
    }
    reads.add(ReadUtils.makeRead(record));
    recordsProcessed++;
  }
  timer.stop();
  LOG.info(""String_Node_Str"" + recordsProcessed + ""String_Node_Str""+ timer+ ""String_Node_Str""+ (recordsProcessed * 1000) / timer.elapsed(TimeUnit.MILLISECONDS) + ""String_Node_Str"" + ""String_Node_Str"" + mismatchedSequence + ""String_Node_Str"" + recordsBeforeStart + ""String_Node_Str"" + recordsAfterEnd);
  return reads;
}","/** 
 * To compare how sharded reading works vs. plain HTSJDK sequential iteration, this method implements such iteration. This makes it easier to discover errors such as reads that are somehow skipped by a sharded approach.
 */
public static Iterable<Read> readSequentiallyForTesting(Objects storageClient,String storagePath,Contig contig,ReaderOptions options) throws IOException {
  Stopwatch timer=Stopwatch.createStarted();
  SamReader samReader=BAMIO.openBAM(storageClient,storagePath,options.getStringency());
  SAMRecordIterator iterator=samReader.queryOverlapping(contig.referenceName,(int)contig.start + 1,(int)contig.end);
  List<Read> reads=new ArrayList<Read>();
  int recordsBeforeStart=0;
  int recordsAfterEnd=0;
  int mismatchedSequence=0;
  int recordsProcessed=0;
  Filter filter=setupFilter(options,contig.referenceName);
  while (iterator.hasNext()) {
    SAMRecord record=iterator.next();
    final boolean passesFilter=passesFilter(record,filter,contig.referenceName);
    if (!passesFilter) {
      mismatchedSequence++;
      continue;
    }
    if (record.getAlignmentStart() < contig.start) {
      recordsBeforeStart++;
      continue;
    }
    if (record.getAlignmentStart() > contig.end) {
      recordsAfterEnd++;
      continue;
    }
    reads.add(ReadUtils.makeRead(record));
    recordsProcessed++;
  }
  timer.stop();
  LOG.info(""String_Node_Str"" + recordsProcessed + ""String_Node_Str""+ timer+ ""String_Node_Str""+ (recordsProcessed * 1000) / timer.elapsed(TimeUnit.MILLISECONDS) + ""String_Node_Str"" + ""String_Node_Str"" + mismatchedSequence + ""String_Node_Str"" + recordsBeforeStart + ""String_Node_Str"" + recordsAfterEnd);
  return reads;
}",0.9988066825775656
132257,"public Sharder(Objects storageClient,String filePath,Iterable<Contig> requestedContigs,ShardingPolicy shardingPolicy,SharderOutput output){
  super();
  this.storageClient=storageClient;
  this.filePath=filePath;
  this.shardingPolicy=shardingPolicy;
  this.requestedContigs=Iterables.transform(requestedContigs,new Function<Contig,Contig>(){
    @Override @Nullable public Contig apply(    @Nullable Contig arg0){
      return new Contig(arg0.referenceName,arg0.start + 1,arg0.end + 1);
    }
  }
);
  this.output=output;
}","public Sharder(Objects storageClient,String filePath,Iterable<Contig> requestedContigs,ShardingPolicy shardingPolicy,SharderOutput output){
  super();
  this.storageClient=storageClient;
  this.filePath=filePath;
  this.shardingPolicy=shardingPolicy;
  this.requestedContigs=Iterables.transform(requestedContigs,new Function<Contig,Contig>(){
    @Override @Nullable public Contig apply(    @Nullable Contig arg0){
      return new Contig(arg0.referenceName,arg0.start + 1,arg0.end);
    }
  }
);
  this.output=output;
}",0.9961685823754788
132258,"@Override @Nullable public Contig apply(@Nullable Contig arg0){
  return new Contig(arg0.referenceName,arg0.start + 1,arg0.end + 1);
}","@Override @Nullable public Contig apply(@Nullable Contig arg0){
  return new Contig(arg0.referenceName,arg0.start + 1,arg0.end);
}",0.9848484848484848
132259,"/** 
 * Run the VerifyBamId algorithm and output the resulting contamination estimate.
 */
public static void main(String[] args) throws GeneralSecurityException, IOException {
  PipelineOptionsFactory.register(VerifyBamIdOptions.class);
  options=PipelineOptionsFactory.fromArgs(args).withValidation().as(VerifyBamId.VerifyBamIdOptions.class);
  GenomicsDatasetOptions.Methods.validateOptions(options);
  auth=GenomicsOptions.Methods.getGenomicsAuth(options);
  p=Pipeline.create(options);
  DataflowWorkarounds.registerGenomicsCoders(p);
  DataflowWorkarounds.registerCoder(p,Read.class,SerializableCoder.of(Read.class));
  DataflowWorkarounds.registerCoder(p,Variant.class,SerializableCoder.of(Variant.class));
  DataflowWorkarounds.registerCoder(p,ReadBaseQuality.class,GenericJsonCoder.of(ReadBaseQuality.class));
  DataflowWorkarounds.registerCoder(p,AlleleFreq.class,GenericJsonCoder.of(AlleleFreq.class));
  DataflowWorkarounds.registerCoder(p,ReadCounts.class,GenericJsonCoder.of(ReadCounts.class));
  if (options.getInputDatasetId().isEmpty() && (options.getReadGroupSetIds().isEmpty() || options.getVariantSetIds().isEmpty())) {
    throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
  }
  List<String> rgsIds;
  List<String> vsIds;
  if (options.getInputDatasetId().isEmpty()) {
    rgsIds=Lists.newArrayList(options.getReadGroupSetIds().split(""String_Node_Str""));
    vsIds=Lists.newArrayList(options.getVariantSetIds().split(""String_Node_Str""));
  }
 else {
    rgsIds=ReadUtils.getReadGroupSetIds(options.getInputDatasetId(),auth);
    vsIds=VariantUtils.getVariantSetIds(options.getInputDatasetId(),auth);
  }
  List<Contig> contigs;
  String referenceSetId=checkReferenceSetIds(rgsIds);
  if (options.isAllReferences()) {
    contigs=getAllReferences(referenceSetId);
  }
 else {
    contigs=parseReferences(options.getReferences(),referenceSetId);
  }
  PCollection<Read> reads=getReadsFromAPI(rgsIds);
  PCollection<Variant> variants=getVariantsFromAPI(vsIds);
  PCollection<KV<Position,AlleleFreq>> refFreq=getFreq(variants,options.getMinFrequency());
  PCollection<KV<Position,ReadCounts>> readCountsTable=combineReads(reads,options.getSamplingFraction(),HASH_PREFIX,refFreq);
  PCollectionView<Map<Position,ReadCounts>> view=readCountsTable.apply(View.<Position,ReadCounts>asMap().withSingletonValues());
  PCollection<String> result=p.begin().apply(Create.of(""String_Node_Str"")).apply(ParDo.of(new Maximizer(view)).withSideInputs(view));
  result.apply(TextIO.Write.to(options.getOutput()).named(""String_Node_Str"").withoutSharding());
  p.run();
}","/** 
 * Run the VerifyBamId algorithm and output the resulting contamination estimate.
 */
public static void main(String[] args) throws GeneralSecurityException, IOException {
  PipelineOptionsFactory.register(VerifyBamIdOptions.class);
  options=PipelineOptionsFactory.fromArgs(args).withValidation().as(VerifyBamId.VerifyBamIdOptions.class);
  GenomicsDatasetOptions.Methods.validateOptions(options);
  auth=GenomicsOptions.Methods.getGenomicsAuth(options);
  p=Pipeline.create(options);
  DataflowWorkarounds.registerGenomicsCoders(p);
  DataflowWorkarounds.registerCoder(p,Read.class,SerializableCoder.of(Read.class));
  DataflowWorkarounds.registerCoder(p,Variant.class,SerializableCoder.of(Variant.class));
  DataflowWorkarounds.registerCoder(p,ReadBaseQuality.class,GenericJsonCoder.of(ReadBaseQuality.class));
  DataflowWorkarounds.registerCoder(p,AlleleFreq.class,GenericJsonCoder.of(AlleleFreq.class));
  DataflowWorkarounds.registerCoder(p,ReadCounts.class,GenericJsonCoder.of(ReadCounts.class));
  if (options.getInputDatasetId().isEmpty() && (options.getReadGroupSetIds().isEmpty() || options.getVariantSetIds().isEmpty())) {
    throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
  }
  List<String> rgsIds;
  List<String> vsIds;
  if (options.getInputDatasetId().isEmpty()) {
    rgsIds=Lists.newArrayList(options.getReadGroupSetIds().split(""String_Node_Str""));
    vsIds=Lists.newArrayList(options.getVariantSetIds().split(""String_Node_Str""));
  }
 else {
    rgsIds=ReadStreamer.getReadGroupSetIds(options.getInputDatasetId(),auth);
    vsIds=VariantStreamer.getVariantSetIds(options.getInputDatasetId(),auth);
  }
  List<Contig> contigs;
  String referenceSetId=checkReferenceSetIds(rgsIds);
  if (options.isAllReferences()) {
    contigs=getAllReferences(referenceSetId);
  }
 else {
    contigs=parseReferences(options.getReferences(),referenceSetId);
  }
  PCollection<Read> reads=getReadsFromAPI(rgsIds);
  PCollection<Variant> variants=getVariantsFromAPI(vsIds);
  PCollection<KV<Position,AlleleFreq>> refFreq=getFreq(variants,options.getMinFrequency());
  PCollection<KV<Position,ReadCounts>> readCountsTable=combineReads(reads,options.getSamplingFraction(),HASH_PREFIX,refFreq);
  PCollectionView<Map<Position,ReadCounts>> view=readCountsTable.apply(View.<Position,ReadCounts>asMap().withSingletonValues());
  PCollection<String> result=p.begin().apply(Create.of(""String_Node_Str"")).apply(ParDo.of(new Maximizer(view)).withSideInputs(view));
  result.apply(TextIO.Write.to(options.getOutput()).named(""String_Node_Str"").withoutSharding());
  p.run();
}",0.9949942241047364
132260,"@Override public void startBundle(DoFn<BAMShard,Read>.Context c) throws IOException {
  storage=GCSOptions.Methods.createStorageClient(c,auth);
}","@Override public void startBundle(DoFn<BAMShard,Read>.Context c) throws IOException {
  storage=Transport.newStorageClient(c.getPipelineOptions().as(GCSOptions.class)).build().objects();
}",0.7327327327327328
132261,"@Test public void openBAMTest() throws IOException {
  GCSOptions popts=PipelineOptionsFactory.create().as(GCSOptions.class);
  popts.setApiKey(API_KEY);
  final Storage.Objects storageClient=GCSOptions.Methods.createStorageClient(popts,GCSOptions.Methods.createGCSAuth(popts));
  SamReader samReader=BAMIO.openBAM(storageClient,TEST_BAM_FNAME,ValidationStringency.DEFAULT_STRINGENCY);
  SAMRecordIterator iterator=samReader.query(""String_Node_Str"",550000,560000,false);
  int readCount=0;
  while (iterator.hasNext()) {
    iterator.next();
    readCount++;
  }
  Assert.assertEquals(""String_Node_Str"",EXPECTED_UNMAPPED_READS_COUNT,readCount);
}","@Test public void openBAMTest() throws IOException {
  GCSOptions popts=PipelineOptionsFactory.create().as(GCSOptions.class);
  popts.setApiKey(API_KEY);
  final Storage.Objects storageClient=Transport.newStorageClient(popts).build().objects();
  SamReader samReader=BAMIO.openBAM(storageClient,TEST_BAM_FNAME,ValidationStringency.DEFAULT_STRINGENCY);
  SAMRecordIterator iterator=samReader.query(""String_Node_Str"",550000,560000,false);
  int readCount=0;
  while (iterator.hasNext()) {
    iterator.next();
    readCount++;
  }
  Assert.assertEquals(""String_Node_Str"",EXPECTED_UNMAPPED_READS_COUNT,readCount);
}",0.9252782193958664
132262,"private static PCollection<Read> getReadsFromAPI(){
  List<SearchReadsRequest> requests=getReadRequests(options);
  PCollection<SearchReadsRequest> readRequests=p.begin().apply(Create.of(requests));
  PCollection<Read> reads=readRequests.apply(ParDo.of(new ReadReader(auth,Paginator.ShardBoundary.OVERLAPS)).named(ReadReader.class.getSimpleName()));
  return reads;
}","private static PCollection<Read> getReadsFromAPI(){
  List<SearchReadsRequest> requests=getReadRequests(options);
  PCollection<SearchReadsRequest> readRequests=p.begin().apply(Create.of(requests));
  PCollection<Read> reads=readRequests.apply(ParDo.of(new ReadReader(auth,Paginator.ShardBoundary.STRICT)).named(ReadReader.class.getSimpleName()));
  return reads;
}",0.9808743169398908
132263,"private static PCollection<Read> getReadsFromBAMFile() throws IOException {
  LOG.info(""String_Node_Str"");
  final Iterable<Contig> contigs=Contig.parseContigsFromCommandLine(options.getReferences());
  if (options.getShardBAMReading()) {
    LOG.info(""String_Node_Str"" + options.getBAMFilePath());
    return ReadBAMTransform.getReadsFromBAMFilesSharded(p,auth,contigs,Collections.singletonList(options.getBAMFilePath()));
  }
 else {
    LOG.info(""String_Node_Str"" + options.getBAMFilePath());
    return p.apply(Create.of(Reader.readSequentiallyForTesting(GCSOptions.Methods.createStorageClient(options,auth),options.getBAMFilePath(),contigs.iterator().next())));
  }
}","private static PCollection<Read> getReadsFromBAMFile() throws IOException {
  LOG.info(""String_Node_Str"");
  final Iterable<Contig> contigs=Contig.parseContigsFromCommandLine(options.getReferences());
  if (options.isShardBAMReading()) {
    LOG.info(""String_Node_Str"" + options.getBAMFilePath());
    return ReadBAMTransform.getReadsFromBAMFilesSharded(p,auth,contigs,Collections.singletonList(options.getBAMFilePath()));
  }
 else {
    LOG.info(""String_Node_Str"" + options.getBAMFilePath());
    return p.apply(Create.of(Reader.readSequentiallyForTesting(GCSOptions.Methods.createStorageClient(options,auth),options.getBAMFilePath(),contigs.iterator().next())));
  }
}",0.996276991809382
132264,"public static void main(String[] args) throws GeneralSecurityException, IOException {
  PipelineOptionsFactory.register(CountReadsOptions.class);
  options=PipelineOptionsFactory.fromArgs(args).withValidation().as(CountReadsOptions.class);
  GenomicsDatasetOptions.Methods.validateOptions(options);
  auth=GenomicsOptions.Methods.getGenomicsAuth(options);
  p=Pipeline.create(options);
  DataflowWorkarounds.registerGenomicsCoders(p);
  String BAMFilePath=options.getBAMFilePath();
  if (!Strings.isNullOrEmpty(BAMFilePath)) {
    if (GCSURLExists(BAMFilePath)) {
      System.out.println(BAMFilePath + ""String_Node_Str"");
    }
 else {
      System.out.println(""String_Node_Str"" + BAMFilePath + ""String_Node_Str"");
      return;
    }
    if (options.getShardBAMReading()) {
      String BAMIndexPath=BAMFilePath + ""String_Node_Str"";
      if (GCSURLExists(BAMIndexPath)) {
        System.out.println(BAMIndexPath + ""String_Node_Str"");
      }
 else {
        System.out.println(""String_Node_Str"" + BAMIndexPath + ""String_Node_Str"");
        return;
      }
    }
  }
  System.out.println(""String_Node_Str"" + options.getOutput());
  PCollection<Read> reads=getReads();
  PCollection<Long> readCount=reads.apply(Count.<Read>globally());
  PCollection<String> readCountText=readCount.apply(ParDo.of(new DoFn<Long,String>(){
    @Override public void processElement(    DoFn<Long,String>.ProcessContext c) throws Exception {
      c.output(String.valueOf(c.element()));
    }
  }
).named(""String_Node_Str""));
  readCountText.apply(TextIO.Write.to(options.getOutput()).named(""String_Node_Str"").withoutSharding());
  p.run();
}","public static void main(String[] args) throws GeneralSecurityException, IOException {
  PipelineOptionsFactory.register(CountReadsOptions.class);
  options=PipelineOptionsFactory.fromArgs(args).withValidation().as(CountReadsOptions.class);
  GenomicsDatasetOptions.Methods.validateOptions(options);
  auth=GenomicsOptions.Methods.getGenomicsAuth(options);
  p=Pipeline.create(options);
  DataflowWorkarounds.registerGenomicsCoders(p);
  String BAMFilePath=options.getBAMFilePath();
  if (!Strings.isNullOrEmpty(BAMFilePath)) {
    if (GCSURLExists(BAMFilePath)) {
      System.out.println(BAMFilePath + ""String_Node_Str"");
    }
 else {
      System.out.println(""String_Node_Str"" + BAMFilePath + ""String_Node_Str"");
      return;
    }
    if (options.isShardBAMReading()) {
      String BAMIndexPath=BAMFilePath + ""String_Node_Str"";
      if (GCSURLExists(BAMIndexPath)) {
        System.out.println(BAMIndexPath + ""String_Node_Str"");
      }
 else {
        System.out.println(""String_Node_Str"" + BAMIndexPath + ""String_Node_Str"");
        return;
      }
    }
  }
  System.out.println(""String_Node_Str"" + options.getOutput());
  PCollection<Read> reads=getReads();
  PCollection<Long> readCount=reads.apply(Count.<Read>globally());
  PCollection<String> readCountText=readCount.apply(ParDo.of(new DoFn<Long,String>(){
    @Override public void processElement(    DoFn<Long,String>.ProcessContext c) throws Exception {
      c.output(String.valueOf(c.element()));
    }
  }
).named(""String_Node_Str""));
  readCountText.apply(TextIO.Write.to(options.getOutput()).named(""String_Node_Str"").withoutSharding());
  p.run();
}",0.9984591679506932
132265,"/** 
 * Appends chunks from another bin to the list and moved the end position.
 */
public void addBin(List<Chunk> chunksToAdd,int lastLocus){
}","/** 
 * Appends chunks from another bin to the list and moved the end position.
 */
public void addBin(List<Chunk> chunksToAdd,long lastLocus){
}",0.9826989619377162
132266,"/** 
 * Generates a final list of chunks, now that we know the exact bounding loci for this shard. We get all chunks overlapping this loci, and then ask the index for the chunks overlapping them. 
 */
public BAMShard finalize(BAMFileIndexImpl index,int lastLocus){
  if (lastLocus >= 0) {
    contig=new Contig(contig.referenceName,contig.start,lastLocus);
  }
  this.chunks=index.getChunksOverlapping(contig.referenceName,(int)contig.start,(int)contig.end);
  updateSpan();
  return this;
}","/** 
 * Generates a final list of chunks, now that we know the exact bounding loci for this shard. We get all chunks overlapping this loci, and then ask the index for the chunks overlapping them. 
 */
public BAMShard finalize(BAMFileIndexImpl index,int lastLocus){
  contig=new Contig(contig.referenceName,contig.start,lastLocus);
  this.chunks=index.getChunksOverlapping(contig.referenceName,(int)contig.start,(int)contig.end);
  updateSpan();
  return this;
}",0.8592436974789915
132267,"boolean isWrongSequence(SAMRecord record){
  return (procesingUnmapped && !record.getReadUnmappedFlag()) || (!procesingUnmapped && shard.contig.referenceName != null && !shard.contig.referenceName.isEmpty() && !shard.contig.referenceName.equals(record.getReferenceName()));
}","boolean isWrongSequence(SAMRecord record){
  return (procesingUnmapped && !record.getReadUnmappedFlag()) || (!procesingUnmapped && (record.getReadUnmappedFlag() || (shard.contig.referenceName != null && !shard.contig.referenceName.isEmpty() && !shard.contig.referenceName.equals(record.getReferenceName()))));
}",0.938566552901024
132268,"/** 
 * To compare how sharded reading works vs. plain HTSJDK sequential iteration, this method implements such iteration. This makes it easier to discover errors such as reads that are somehow skipped by a sharded approach.
 */
public static Iterable<Read> readSequentiallyForTesting(Objects storageClient,String storagePath,Contig contig) throws IOException {
  Stopwatch timer=Stopwatch.createStarted();
  SamReader samReader=BAMIO.openBAM(storageClient,storagePath);
  SAMRecordIterator iterator=samReader.queryOverlapping(contig.referenceName,(int)contig.start,(int)contig.end);
  List<Read> reads=new ArrayList<Read>();
  int recordsBeforeStart=0;
  int recordsAfterEnd=0;
  int mismatchedSequence=0;
  int recordsProcessed=0;
  while (iterator.hasNext()) {
    SAMRecord record=iterator.next();
    final boolean wrongSequence=!contig.referenceName.equals(record.getReferenceName());
    if (wrongSequence) {
      mismatchedSequence++;
      continue;
    }
    if (record.getAlignmentStart() < contig.start) {
      recordsBeforeStart++;
      continue;
    }
    if (record.getAlignmentStart() > contig.end) {
      recordsAfterEnd++;
      continue;
    }
    reads.add(ReadConverter.makeRead(record));
    recordsProcessed++;
  }
  timer.stop();
  LOG.info(""String_Node_Str"" + recordsProcessed + ""String_Node_Str""+ timer+ ""String_Node_Str""+ (recordsProcessed * 1000) / timer.elapsed(TimeUnit.MILLISECONDS) + ""String_Node_Str"" + ""String_Node_Str"" + mismatchedSequence + ""String_Node_Str"" + recordsBeforeStart + ""String_Node_Str"" + recordsAfterEnd);
  return reads;
}","/** 
 * To compare how sharded reading works vs. plain HTSJDK sequential iteration, this method implements such iteration. This makes it easier to discover errors such as reads that are somehow skipped by a sharded approach.
 */
public static Iterable<Read> readSequentiallyForTesting(Objects storageClient,String storagePath,Contig contig) throws IOException {
  Stopwatch timer=Stopwatch.createStarted();
  SamReader samReader=BAMIO.openBAM(storageClient,storagePath);
  SAMRecordIterator iterator=samReader.queryOverlapping(contig.referenceName,(int)contig.start + 1,(int)contig.end + 1);
  List<Read> reads=new ArrayList<Read>();
  int recordsBeforeStart=0;
  int recordsAfterEnd=0;
  int mismatchedSequence=0;
  int recordsProcessed=0;
  while (iterator.hasNext()) {
    SAMRecord record=iterator.next();
    final boolean wrongSequence=!contig.referenceName.equals(record.getReferenceName()) || (!contig.referenceName.equals(""String_Node_Str"") && record.getReadUnmappedFlag());
    if (wrongSequence) {
      mismatchedSequence++;
      continue;
    }
    if (record.getAlignmentStart() < contig.start) {
      recordsBeforeStart++;
      continue;
    }
    if (record.getAlignmentStart() >= contig.end) {
      recordsAfterEnd++;
      continue;
    }
    reads.add(ReadConverter.makeRead(record));
    recordsProcessed++;
  }
  timer.stop();
  LOG.info(""String_Node_Str"" + recordsProcessed + ""String_Node_Str""+ timer+ ""String_Node_Str""+ (recordsProcessed * 1000) / timer.elapsed(TimeUnit.MILLISECONDS) + ""String_Node_Str"" + ""String_Node_Str"" + mismatchedSequence + ""String_Node_Str"" + recordsBeforeStart + ""String_Node_Str"" + recordsAfterEnd);
  return reads;
}",0.937192118226601
132269,"void processRecord(SAMRecord record){
  if (isWrongSequence(record)) {
    mismatchedSequence++;
    return;
  }
  if (record.getAlignmentStart() < shard.contig.start) {
    recordsBeforeStart++;
    return;
  }
  if (record.getAlignmentStart() > shard.contig.end) {
    recordsAfterEnd++;
    return;
  }
  c.output(ReadConverter.makeRead(record));
  recordsProcessed++;
}","void processRecord(SAMRecord record){
  if (isWrongSequence(record)) {
    mismatchedSequence++;
    return;
  }
  if (record.getAlignmentStart() < shard.contig.start) {
    recordsBeforeStart++;
    return;
  }
  if (record.getAlignmentStart() >= shard.contig.end) {
    recordsAfterEnd++;
    return;
  }
  c.output(ReadConverter.makeRead(record));
  recordsProcessed++;
}",0.998661311914324
132270,"public Sharder(Objects storageClient,String filePath,Iterable<Contig> requestedContigs,DoFn<String,BAMShard>.ProcessContext c){
  super();
  this.storageClient=storageClient;
  this.filePath=filePath;
  this.requestedContigs=requestedContigs;
  this.c=c;
}","public Sharder(Objects storageClient,String filePath,Iterable<Contig> requestedContigs,DoFn<String,BAMShard>.ProcessContext c){
  super();
  this.storageClient=storageClient;
  this.filePath=filePath;
  this.requestedContigs=Iterables.transform(requestedContigs,new Function<Contig,Contig>(){
    @Override @Nullable public Contig apply(    @Nullable Contig arg0){
      return new Contig(arg0.referenceName,arg0.start + 1,arg0.end + 1);
    }
  }
);
  this.c=c;
}",0.7111111111111111
132271,"void createShardsForReference(SAMSequenceRecord reference,Contig contig){
  final BitSet overlappingBins=GenomicIndexUtil.regionToBins((int)contig.start,(int)contig.end);
  if (overlappingBins == null) {
    LOG.warning(""String_Node_Str"" + contig.start + ""String_Node_Str""+ contig.end);
    return;
  }
  BAMShard currentShard=null;
  for (int binIndex=overlappingBins.nextSetBit(0); binIndex >= 0; binIndex=overlappingBins.nextSetBit(binIndex + 1)) {
    final Bin bin=index.getBinData(reference.getSequenceIndex(),binIndex);
    if (bin == null) {
      continue;
    }
    if (LOG.isLoggable(Level.FINE)) {
      LOG.fine(""String_Node_Str"" + index.getFirstLocusInBin(bin) + ""String_Node_Str""+ index.getLastLocusInBin(bin));
    }
    if (index.getLevelForBin(bin) != (GenomicIndexUtil.LEVEL_STARTS.length - 1)) {
      if (LOG.isLoggable(Level.FINEST)) {
        LOG.finest(""String_Node_Str"");
      }
      continue;
    }
    final List<Chunk> chunksInBin=bin.getChunkList();
    if (chunksInBin.isEmpty()) {
      if (LOG.isLoggable(Level.FINEST)) {
        LOG.finest(""String_Node_Str"");
      }
      continue;
    }
    if (currentShard == null) {
      if (LOG.isLoggable(Level.FINE)) {
        LOG.fine(""String_Node_Str"" + index.getFirstLocusInBin(bin));
      }
      currentShard=new BAMShard(filePath,reference.getSequenceName(),index.getFirstLocusInBin(bin));
    }
    currentShard.addBin(chunksInBin,index.getLastLocusInBin(bin));
    if (LOG.isLoggable(Level.FINE)) {
      LOG.fine(""String_Node_Str"" + index.getLastLocusInBin(bin));
    }
    if (shardingPolicy.shardBigEnough(currentShard)) {
      LOG.info(""String_Node_Str"" + currentShard.sizeInLoci() + ""String_Node_Str""+ currentShard.approximateSizeInBytes()+ ""String_Node_Str"");
      c.output(currentShard.finalize(index,-1));
      currentShard=null;
    }
  }
  if (currentShard != null) {
    LOG.info(""String_Node_Str"" + currentShard.sizeInLoci() + ""String_Node_Str""+ currentShard.approximateSizeInBytes()+ ""String_Node_Str"");
    c.output(currentShard.finalize(index,(int)contig.end));
  }
}","void createShardsForReference(SAMSequenceRecord reference,Contig contig){
  final BitSet overlappingBins=GenomicIndexUtil.regionToBins((int)contig.start,(int)contig.end);
  if (overlappingBins == null) {
    LOG.warning(""String_Node_Str"" + contig.start + ""String_Node_Str""+ contig.end);
    return;
  }
  BAMShard currentShard=null;
  for (int binIndex=overlappingBins.nextSetBit(0); binIndex >= 0; binIndex=overlappingBins.nextSetBit(binIndex + 1)) {
    final Bin bin=index.getBinData(reference.getSequenceIndex(),binIndex);
    if (bin == null) {
      continue;
    }
    if (LOG.isLoggable(Level.FINE)) {
      LOG.fine(""String_Node_Str"" + index.getFirstLocusInBin(bin) + ""String_Node_Str""+ index.getLastLocusInBin(bin));
    }
    if (index.getLevelForBin(bin) != (GenomicIndexUtil.LEVEL_STARTS.length - 1)) {
      if (LOG.isLoggable(Level.FINEST)) {
        LOG.finest(""String_Node_Str"");
      }
      continue;
    }
    final List<Chunk> chunksInBin=bin.getChunkList();
    if (chunksInBin.isEmpty()) {
      if (LOG.isLoggable(Level.FINEST)) {
        LOG.finest(""String_Node_Str"");
      }
      continue;
    }
    if (currentShard == null) {
      int startLocus=Math.max(index.getFirstLocusInBin(bin),(int)contig.start);
      if (LOG.isLoggable(Level.FINE)) {
        LOG.fine(""String_Node_Str"" + startLocus);
      }
      currentShard=new BAMShard(filePath,reference.getSequenceName(),startLocus);
    }
    currentShard.addBin(chunksInBin,index.getLastLocusInBin(bin));
    if (LOG.isLoggable(Level.FINE)) {
      LOG.fine(""String_Node_Str"" + index.getLastLocusInBin(bin));
    }
    if (shardingPolicy.shardBigEnough(currentShard)) {
      LOG.info(""String_Node_Str"" + currentShard.sizeInLoci() + ""String_Node_Str""+ currentShard.approximateSizeInBytes()+ ""String_Node_Str"");
      c.output(currentShard.finalize(index,Math.min(index.getLastLocusInBin(bin),(int)contig.end)));
      currentShard=null;
    }
  }
  if (currentShard != null) {
    LOG.info(""String_Node_Str"" + currentShard.sizeInLoci() + ""String_Node_Str""+ currentShard.approximateSizeInBytes()+ ""String_Node_Str"");
    c.output(currentShard.finalize(index,(int)contig.end));
  }
}",0.9495044832468146
132272,"public static void main(String[] args) throws IOException, GeneralSecurityException {
  Options options=PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);
  GenomicsDatasetOptions.Methods.validateOptions(options);
  Preconditions.checkState(options.getHasNonVariantSegments(),""String_Node_Str"" + ""String_Node_Str"");
  GenomicsFactory.OfflineAuth auth=GenomicsOptions.Methods.getGenomicsAuth(options);
  List<SearchVariantsRequest> requests=GenomicsDatasetOptions.Methods.getVariantRequests(options,auth,true);
  Pipeline p=Pipeline.create(options);
  DataflowWorkarounds.registerGenomicsCoders(p);
  PCollection<SearchVariantsRequest> input=DataflowWorkarounds.getPCollection(requests,p,options.getNumWorkers());
  PCollection<Variant> variants=JoinNonVariantSegmentsWithVariants.joinVariantsTransform(input,auth,JoinNonVariantSegmentsWithVariants.VARIANT_JOIN_FIELDS);
  variants.apply(ParDo.of(new FormatVariantsFn())).apply(BigQueryIO.Write.to(options.getOutput()).withSchema(getTableSchema()).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));
  p.run();
}","public static void main(String[] args) throws IOException, GeneralSecurityException {
  Options options=PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);
  GenomicsDatasetOptions.Methods.validateOptions(options);
  Preconditions.checkState(options.getHasNonVariantSegments(),""String_Node_Str"" + ""String_Node_Str"");
  GenomicsFactory.OfflineAuth auth=GenomicsOptions.Methods.getGenomicsAuth(options);
  List<SearchVariantsRequest> requests=GenomicsDatasetOptions.Methods.getVariantRequests(options,auth,false);
  Pipeline p=Pipeline.create(options);
  DataflowWorkarounds.registerGenomicsCoders(p);
  PCollection<SearchVariantsRequest> input=DataflowWorkarounds.getPCollection(requests,p,options.getNumWorkers());
  PCollection<Variant> variants=JoinNonVariantSegmentsWithVariants.joinVariantsTransform(input,auth,JoinNonVariantSegmentsWithVariants.VARIANT_JOIN_FIELDS);
  variants.apply(ParDo.of(new FormatVariantsFn())).apply(BigQueryIO.Write.to(options.getOutput()).withSchema(getTableSchema()).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));
  p.run();
}",0.9970351545955104
132273,"@Override public void processElement(ProcessContext c){
  Variant v=c.element();
  List<TableRow> calls=new ArrayList<>();
  for (  Call call : v.getCalls()) {
    calls.add(new TableRow().set(""String_Node_Str"",call.getCallSetName()).set(""String_Node_Str"",call.getPhaseset()).set(""String_Node_Str"",call.getGenotype()).set(""String_Node_Str"",call.getGenotypeLikelihood()));
  }
  TableRow row=new TableRow().set(""String_Node_Str"",v.getId()).set(""String_Node_Str"",v.getReferenceName()).set(""String_Node_Str"",v.getStart()).set(""String_Node_Str"",v.getEnd()).set(""String_Node_Str"",v.getReferenceBases()).set(""String_Node_Str"",v.getAlternateBases()).set(""String_Node_Str"",calls);
  c.output(row);
}","@Override public void processElement(ProcessContext c){
  Variant v=c.element();
  List<TableRow> calls=new ArrayList<>();
  for (  Call call : v.getCalls()) {
    calls.add(new TableRow().set(""String_Node_Str"",call.getCallSetName()).set(""String_Node_Str"",call.getPhaseset()).set(""String_Node_Str"",call.getGenotype()).set(""String_Node_Str"",(call.getGenotypeLikelihood() == null) ? new ArrayList<Double>() : call.getGenotypeLikelihood()));
  }
  TableRow row=new TableRow().set(""String_Node_Str"",v.getId()).set(""String_Node_Str"",v.getReferenceName()).set(""String_Node_Str"",v.getStart()).set(""String_Node_Str"",v.getEnd()).set(""String_Node_Str"",v.getReferenceBases()).set(""String_Node_Str"",v.getAlternateBases()).set(""String_Node_Str"",calls);
  c.output(row);
}",0.95376121463078
132274,"@Override public PDone apply(PCollection<KV<KV<String,String>,Long>> similarPairs){
  return similarPairs.apply(Sum.<KV<String,String>>longsPerKey()).apply(Combine.globally(TO_LIST)).apply(ParDo.named(""String_Node_Str"").of(PCoAnalysis.of())).apply(Convert.<PCoAnalysis.GraphResult>fromIterables()).apply(ParDo.named(""String_Node_Str"").of(fromSerializableFunction(TO_STRING))).apply(TextIO.Write.named(""String_Node_Str"").to(outputFile));
}","@Override public PDone apply(PCollection<KV<KV<String,String>,Long>> similarPairs){
  return similarPairs.apply(Sum.<KV<String,String>>longsPerKey()).apply(Combine.globally(TO_LIST)).apply(ParDo.named(""String_Node_Str"").of(PCoAnalysis.of())).apply(Convert.<PCoAnalysis.GraphResult>fromIterables()).apply(ParDo.named(""String_Node_Str"").of(fromSerializableFunction(TO_STRING))).setCoder(StringUtf8Coder.of()).apply(TextIO.Write.named(""String_Node_Str"").to(outputFile));
}",0.9658213891951488
132275,"@Override public ImmutableList.Builder<X> createAccumulator(){
  return ImmutableList.builder();
}","@Override public ArrayList<X> createAccumulator(){
  return new ArrayList<>();
}",0.7415730337078652
132276,"@Override public Iterable<X> extractOutput(ImmutableList.Builder<X> accumulator){
  return accumulator.build();
}","@Override public Iterable<X> extractOutput(ArrayList<X> accumulator){
  return accumulator;
}",0.8640776699029126
132277,"@Override public void addInput(ImmutableList.Builder<X> accumulator,X input){
  accumulator.add(input);
}","@Override public void addInput(ArrayList<X> accumulator,X input){
  accumulator.add(input);
}",0.898989898989899
132278,"private static <X>Combine.CombineFn<X,ImmutableList.Builder<X>,Iterable<X>> toImmutableList(){
  return new Combine.CombineFn<X,ImmutableList.Builder<X>,Iterable<X>>(){
    @Override public void addInput(    ImmutableList.Builder<X> accumulator,    X input){
      accumulator.add(input);
    }
    @Override public ImmutableList.Builder<X> createAccumulator(){
      return ImmutableList.builder();
    }
    @Override public Iterable<X> extractOutput(    ImmutableList.Builder<X> accumulator){
      return accumulator.build();
    }
    @Override public ImmutableList.Builder<X> mergeAccumulators(    Iterable<ImmutableList.Builder<X>> accumulators){
      ImmutableList.Builder<X> merged=ImmutableList.builder();
      for (      ImmutableList.Builder<X> accumulator : accumulators) {
        merged.addAll(accumulator.build());
      }
      return merged;
    }
  }
;
}","private static <X>Combine.CombineFn<X,ArrayList<X>,Iterable<X>> toImmutableList(){
  return new Combine.CombineFn<X,ArrayList<X>,Iterable<X>>(){
    @Override public void addInput(    ArrayList<X> accumulator,    X input){
      accumulator.add(input);
    }
    @Override public ArrayList<X> createAccumulator(){
      return new ArrayList<>();
    }
    @Override public Iterable<X> extractOutput(    ArrayList<X> accumulator){
      return accumulator;
    }
    @Override public ArrayList<X> mergeAccumulators(    Iterable<ArrayList<X>> accumulators){
      ArrayList<X> merged=new ArrayList<>();
      for (      ArrayList<X> accumulator : accumulators) {
        merged.addAll(accumulator);
      }
      return merged;
    }
  }
;
}",0.5452292441140025
132279,"@Override public ImmutableList.Builder<X> mergeAccumulators(Iterable<ImmutableList.Builder<X>> accumulators){
  ImmutableList.Builder<X> merged=ImmutableList.builder();
  for (  ImmutableList.Builder<X> accumulator : accumulators) {
    merged.addAll(accumulator.build());
  }
  return merged;
}","@Override public ArrayList<X> mergeAccumulators(Iterable<ArrayList<X>> accumulators){
  ArrayList<X> merged=new ArrayList<>();
  for (  ArrayList<X> accumulator : accumulators) {
    merged.addAll(accumulator);
  }
  return merged;
}",0.1174242424242424
132280,"@Override public T decode(InputStream in,Context context) throws IOException {
  JsonParser jsonParser=jacksonFactory.createJsonParser(in);
  T obj=jsonParser.parse(type);
  jsonParser.close();
  return obj;
}","@Override public T decode(InputStream in,Context context) throws IOException {
  if (context.isWholeStream) {
    JsonParser jsonParser=jacksonFactory.createJsonParser(in);
    T obj=jsonParser.parse(type);
    jsonParser.close();
    return obj;
  }
 else {
    in.mark(READ_LIMIT);
    JsonParser jsonParser=jacksonFactory.createJsonParser(in);
    T obj=jsonParser.parse(type);
    ByteArrayOutputStream buf=new ByteArrayOutputStream();
    encode(obj,buf,context);
    int skip=buf.size();
    in.reset();
    in.skip(skip);
    jsonParser.close();
    return obj;
  }
}",0.5031928480204342
132281,"@Override protected void processApiCall(GenomicsApi api,ProcessContext c,SearchReadsRequest request) throws IOException {
  long total=0;
  do {
    SearchReadsResponse response=api.executeRequest(api.getService().reads().search(request),readFields);
    if (response.getReads() == null) {
      break;
    }
    for (    Read read : response.getReads()) {
      c.output(read);
    }
    total+=response.getReads().size();
    LOG.info(""String_Node_Str"" + total + ""String_Node_Str"");
    request.setPageToken(response.getNextPageToken());
  }
 while (request.getPageToken() != null);
}","@Override protected void processApiCall(GenomicsApi api,ProcessContext c,SearchReadsRequest request) throws IOException {
  long total=0;
  LOG.info(""String_Node_Str"");
  do {
    SearchReadsResponse response=api.executeRequest(api.getService().reads().search(request),readFields);
    if (response.getReads() == null) {
      break;
    }
    for (    Read read : response.getReads()) {
      c.output(read);
    }
    total+=response.getReads().size();
    LOG.info(""String_Node_Str"" + total + ""String_Node_Str"");
    request.setPageToken(response.getNextPageToken());
  }
 while (request.getPageToken() != null);
}",0.4788029925187032
132282,"/** 
 * Use this function to get a valid access token from the user before running a dataflow pipeline.
 * @param clientSecretsFilename Where the client_secrets.json file is located.Usually this is an option specified by the user from the command line.
 * @param scopes The OAuth scopes needed by the pipeline.Common scopes are constants in this class.
 * @return An access token that can be used to make a GoogleCredential:new GoogleCredential().setAccessToken(accessToken)
 */
public static String getAccessToken(String clientSecretsFilename,List<String> scopes) throws GeneralSecurityException, IOException {
  GoogleClientSecrets clientSecrets=loadClientSecrets(clientSecretsFilename);
  FileDataStoreFactory dataStoreFactory=new FileDataStoreFactory(DATA_STORE_DIR);
  NetHttpTransport httpTransport=GoogleNetHttpTransport.newTrustedTransport();
  GoogleAuthorizationCodeFlow flow=new GoogleAuthorizationCodeFlow.Builder(httpTransport,JSON_FACTORY,clientSecrets,scopes).setAccessType(""String_Node_Str"").setDataStoreFactory(dataStoreFactory).build();
  Credential credential=new AuthorizationCodeInstalledApp(flow,new GooglePromptReceiver()).authorize(""String_Node_Str"");
  return credential.getAccessToken();
}","/** 
 * Use this function to get a valid access token from the user before running a dataflow pipeline.
 * @param clientSecretsFilename Where the client_secrets.json file is located.Usually this is an option specified by the user from the command line.
 * @param scopes The OAuth scopes needed by the pipeline.Common scopes are constants in this class.
 * @return An access token that can be used to make a GoogleCredential:new GoogleCredential().setAccessToken(accessToken)
 */
public static String getAccessToken(String clientSecretsFilename,List<String> scopes) throws GeneralSecurityException, IOException {
  GoogleClientSecrets clientSecrets=loadClientSecrets(clientSecretsFilename);
  FileDataStoreFactory dataStoreFactory=new FileDataStoreFactory(DATA_STORE_DIR);
  NetHttpTransport httpTransport=GoogleNetHttpTransport.newTrustedTransport();
  GoogleAuthorizationCodeFlow flow=new GoogleAuthorizationCodeFlow.Builder(httpTransport,JSON_FACTORY,clientSecrets,scopes).setAccessType(""String_Node_Str"").setDataStoreFactory(dataStoreFactory).build();
  Credential credential=new AuthorizationCodeInstalledApp(flow,new GooglePromptReceiver()).authorize(""String_Node_Str"");
  credential.refreshToken();
  return credential.getAccessToken();
}",0.9882065880439204
132283,"private void read() throws IOException {
  String tempDir=this.readRequiredString(""String_Node_Str"");
  if (tempDir.endsWith(""String_Node_Str"") || tempDir.endsWith(""String_Node_Str"")) {
    backupDir=tempDir;
  }
 else   if (tempDir.contains(""String_Node_Str"")) {
    backupDir=tempDir + ""String_Node_Str"";
  }
 else   if (tempDir.contains(""String_Node_Str"")) {
    backupDir=tempDir + ""String_Node_Str"";
  }
 else {
    backupDir=tempDir + File.separator;
  }
  this.localTimeRenew=this.readOptionalInt(""String_Node_Str"",0);
  this.redirect=readOptionalBoolean(""String_Node_Str"",false);
  if (this.readOptionalBoolean(""String_Node_Str"",false)) {
    this.sslOptions=new SSLOptions(new FileInputStream(this.readRequiredString(""String_Node_Str"")),this.readRequiredString(""String_Node_Str""),this.readRequiredString(""String_Node_Str""),new FileInputStream(this.readRequiredString(""String_Node_Str"")),this.readRequiredString(""String_Node_Str""),this.readRequiredString(""String_Node_Str""),this.readRequiredBoolean(""String_Node_Str""));
  }
  this.chunkSize=this.readOptionalInt(""String_Node_Str"",DEFAULT_MAX_CHUNK_SIZE);
  this.syncN=this.readOptionalInt(""String_Node_Str"",0);
  this.participants=new HashSet<InetSocketAddress>();
  this.address=this.readOptionalInetSocketAddr(""String_Node_Str"",""String_Node_Str"",null);
  int number=0;
  InetSocketAddress addrs;
  while ((addrs=readOptionalInetSocketAddr(""String_Node_Str"" + number,""String_Node_Str"" + number + ""String_Node_Str"",null)) != null) {
    participants.add(addrs);
    number++;
  }
  parseParticipants();
  policyName=readOptionalString(""String_Node_Str"",""String_Node_Str"");
  checkArgs();
}","private void read() throws IOException {
  String tempDir=this.readRequiredString(""String_Node_Str"");
  if (tempDir.endsWith(""String_Node_Str"") || tempDir.endsWith(""String_Node_Str"")) {
    backupDir=tempDir;
  }
 else   if (tempDir.contains(""String_Node_Str"")) {
    backupDir=tempDir + ""String_Node_Str"";
  }
 else   if (tempDir.contains(""String_Node_Str"")) {
    backupDir=tempDir + ""String_Node_Str"";
  }
 else {
    backupDir=tempDir + File.separator;
  }
  this.localTimeRenew=this.readOptionalInt(""String_Node_Str"",0);
  this.redirect=readOptionalBoolean(""String_Node_Str"",false);
  if (this.readOptionalBoolean(""String_Node_Str"",false)) {
    this.sslOptions=new SSLOptions(new FileInputStream(this.readRequiredString(""String_Node_Str"")),this.readRequiredString(""String_Node_Str""),this.readRequiredString(""String_Node_Str""),new FileInputStream(this.readRequiredString(""String_Node_Str"")),this.readRequiredString(""String_Node_Str""),this.readRequiredString(""String_Node_Str""),this.readRequiredBoolean(""String_Node_Str""),false,null);
  }
  this.chunkSize=this.readOptionalInt(""String_Node_Str"",DEFAULT_MAX_CHUNK_SIZE);
  this.syncN=this.readOptionalInt(""String_Node_Str"",0);
  this.participants=new HashSet<InetSocketAddress>();
  this.address=this.readOptionalInetSocketAddr(""String_Node_Str"",""String_Node_Str"",null);
  int number=0;
  InetSocketAddress addrs;
  while ((addrs=readOptionalInetSocketAddr(""String_Node_Str"" + number,""String_Node_Str"" + number + ""String_Node_Str"",null)) != null) {
    participants.add(addrs);
    number++;
  }
  parseParticipants();
  policyName=readOptionalString(""String_Node_Str"",""String_Node_Str"");
  checkArgs();
}",0.9966717095310136
132284,"@Override public void leaseFailed(ASCIIString cellId,FleaseException error){
  Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",cellId.toString(),error.getMessage());
synchronized (this) {
    lease=Flease.EMPTY_LEASE;
  }
}","@Override public void leaseFailed(ASCIIString cellId,FleaseException error){
  Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",cellId.toString(),error.getMessage());
synchronized (this) {
    lease=Flease.EMPTY_LEASE;
    notifyAll();
  }
}",0.9701230228471002
132285,"@Override public void init(StaticInitialization staticInit) throws BabuDBException {
synchronized (stopped) {
    responseManager.setLifeCycleListener(this);
    responseManager.start();
    snapshotManager.init();
    LSN dbLsn=null;
    for (    DatabaseInternal db : databaseManager.getDatabaseList()) {
      if (dbLsn == null)       dbLsn=db.getLSMDB().getOndiskLSN();
 else {
        LSN onDiskLSN=db.getLSMDB().getOndiskLSN();
        if (!(LSMDatabase.NO_DB_LSN.equals(dbLsn) || LSMDatabase.NO_DB_LSN.equals(onDiskLSN)))         dbLsn=dbLsn.compareTo(onDiskLSN) < 0 ? dbLsn : onDiskLSN;
      }
    }
    if (dbLsn == null) {
      dbLsn=new LSN(0,0);
    }
 else {
      dbLsn=new LSN(dbLsn.getViewId() == 0 ? 1 : dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
    }
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",dbLsn);
    LSN nextLSN=replayLogs(dbLsn);
    if (dbLsn.compareTo(nextLSN) > 0) {
      nextLSN=dbLsn;
    }
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
    try {
      logger=new DiskLogger(configuration.getDbLogDir(),nextLSN,configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * Math.max(1,configuration.getNumThreads()));
      logger.setLifeCycleListener(this);
      logger.start();
      logger.waitForStartup();
    }
 catch (    Exception ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
    this.txnMan.init(new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1L));
    this.txnMan.setLogger(logger);
    if (configuration.getNumThreads() > 0) {
      worker=new LSMDBWorker[configuration.getNumThreads()];
      for (int i=0; i < configuration.getNumThreads(); i++) {
        worker[i]=new LSMDBWorker(this,i,configuration.getMaxQueueLength());
        worker[i].start();
      }
    }
 else {
      assert(configuration.getNumThreads() == 0);
      worker=null;
    }
    if (dbConfigFile.isConversionRequired())     AutoConverter.completeConversion(this);
    this.dbCheckptr.init(logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    final LSN firstLSN=new LSN(1,1L);
    if (staticInit != null && nextLSN.equals(firstLSN)) {
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
      staticInit.initialize(databaseManager,snapshotManager);
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
    }
 else     if (staticInit != null) {
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"");
    }
    this.stopped.set(false);
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
  }
}","@Override public void init(StaticInitialization staticInit) throws BabuDBException {
synchronized (stopped) {
    responseManager.setLifeCycleListener(this);
    responseManager.start();
    snapshotManager.init();
    LSN dbLsn=null;
    for (    DatabaseInternal db : databaseManager.getDatabaseList()) {
      LSN onDiskLSN=db.getLSMDB().getOndiskLSN();
      if (LSMDatabase.NO_DB_LSN.equals(onDiskLSN))       continue;
      if (dbLsn == null || dbLsn.compareTo(onDiskLSN) > 0)       dbLsn=onDiskLSN;
    }
    if (dbLsn == null) {
      dbLsn=LSMDatabase.NO_DB_LSN;
    }
 else {
      dbLsn=new LSN(dbLsn.getViewId() == 0 ? 1 : dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
    }
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",dbLsn);
    LSN nextLSN=replayLogs(dbLsn);
    if (dbLsn.compareTo(nextLSN) > 0) {
      nextLSN=dbLsn;
    }
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
    try {
      logger=new DiskLogger(configuration.getDbLogDir(),nextLSN,configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * Math.max(1,configuration.getNumThreads()));
      logger.setLifeCycleListener(this);
      logger.start();
      logger.waitForStartup();
    }
 catch (    Exception ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
    this.txnMan.init(new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1L));
    this.txnMan.setLogger(logger);
    if (configuration.getNumThreads() > 0) {
      worker=new LSMDBWorker[configuration.getNumThreads()];
      for (int i=0; i < configuration.getNumThreads(); i++) {
        worker[i]=new LSMDBWorker(this,i,configuration.getMaxQueueLength());
        worker[i].start();
      }
    }
 else {
      assert(configuration.getNumThreads() == 0);
      worker=null;
    }
    if (dbConfigFile.isConversionRequired())     AutoConverter.completeConversion(this);
    this.dbCheckptr.init(logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    final LSN firstLSN=new LSN(1,1L);
    if (staticInit != null && nextLSN.equals(firstLSN)) {
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
      staticInit.initialize(databaseManager,snapshotManager);
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
    }
 else     if (staticInit != null) {
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"");
    }
    this.stopped.set(false);
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
  }
}",0.9114018691588786
132286,"/** 
 * Main loop.
 */
public void run(){
  List<LogEntry> tmpE=new ArrayList<LogEntry>(MAX_ENTRIES_PER_BLOCK);
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
  notifyStarted();
  while (!quit) {
    try {
synchronized (this) {
        if (entries.isEmpty()) {
          wait();
        }
        if (quit) {
          break;
        }
 else {
          LogEntry tmp=null;
          while (tmpE.size() < MAX_ENTRIES_PER_BLOCK - 1 && (tmp=entries.poll()) != null) {
            tmpE.add(tmp);
          }
          notifyAll();
          lock();
        }
      }
      processLogEntries(tmpE);
    }
 catch (    IOException ex) {
      Logging.logError(Logging.LEVEL_ERROR,this,ex);
      for (      LogEntry le : tmpE) {
        le.free();
        le.getListener().failed(ex);
      }
      tmpE.clear();
    }
catch (    InterruptedException ex) {
      if (!quit) {
        try {
          cleanUp();
        }
 catch (        IOException e) {
          Logging.logError(Logging.LEVEL_ERROR,this,e);
        }
        notifyCrashed(ex);
        return;
      }
    }
 finally {
      if (hasLock())       unlock();
    }
  }
  try {
    if (graceful) {
      try {
        lock();
        processLogEntries(entries);
      }
  finally {
        unlock();
      }
    }
    cleanUp();
    Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
    notifyStopped();
  }
 catch (  Exception e) {
    notifyCrashed(e);
  }
}","/** 
 * Main loop.
 */
public void run(){
  List<LogEntry> tmpE=new ArrayList<LogEntry>(MAX_ENTRIES_PER_BLOCK);
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
  notifyStarted();
  while (!quit) {
    try {
synchronized (this) {
        if (!quit && entries.isEmpty()) {
          wait();
        }
        if (quit) {
          break;
        }
 else {
          LogEntry tmp=null;
          while (tmpE.size() < MAX_ENTRIES_PER_BLOCK - 1 && (tmp=entries.poll()) != null) {
            tmpE.add(tmp);
          }
          notifyAll();
          lock();
        }
      }
      processLogEntries(tmpE);
    }
 catch (    IOException ex) {
      Logging.logError(Logging.LEVEL_ERROR,this,ex);
      for (      LogEntry le : tmpE) {
        le.free();
        le.getListener().failed(ex);
      }
      tmpE.clear();
    }
catch (    InterruptedException ex) {
      if (!quit) {
        try {
          cleanUp();
        }
 catch (        IOException e) {
          Logging.logError(Logging.LEVEL_ERROR,this,e);
        }
        notifyCrashed(ex);
        return;
      }
    }
 finally {
      if (hasLock())       unlock();
    }
  }
  try {
    if (graceful) {
      try {
        lock();
        processLogEntries(entries);
      }
  finally {
        unlock();
      }
    }
    cleanUp();
    Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
    notifyStopped();
  }
 catch (  Exception e) {
    notifyCrashed(e);
  }
}",0.9969082789419444
132287,"/** 
 * @return a map of all available databases identified by their names.
 * @throws BabuDBException 
 */
public Map<String,DatabaseInternal> getDatabases(){
  return getDBMan().getDatabasesInternal();
}","/** 
 * @return a map of all available databases identified by their names.
 * @throws BabuDBException 
 */
public Map<String,DatabaseProxy> getDatabases(){
  return dbMan.getDatabasesInternalNonblocking();
}",0.9200968523002422
132288,"/** 
 * @return a collection of all {@link DBFileMetaData} objects for the latestsnapshot available for every database, after cutting them into chunks of chunkSize at maximum.
 */
public Collection<DBFileMetaData> getAllSnapshotFiles(){
  List<DBFileMetaData> result=new Vector<DBFileMetaData>();
  for (  DatabaseInternal db : getDBMan().getDatabaseList()) {
    result.addAll(db.getLSMDB().getLastestSnapshotFiles());
  }
  return result;
}","/** 
 * @return a collection of all {@link DBFileMetaData} objects for the latestsnapshot available for every database, after cutting them into chunks of chunkSize at maximum.
 */
public Collection<DBFileMetaData> getAllSnapshotFiles(){
  List<DBFileMetaData> result=new Vector<DBFileMetaData>();
  for (  DatabaseProxy db : dbMan.getDatabaseListNonblocking()) {
    result.addAll(db.getLSMDB().getLastestSnapshotFiles());
  }
  return result;
}",0.9560315670800452
132289,"/** 
 * @param dbId
 * @return an instance of the local database available for the given identifier.
 * @throws BabuDBException 
 */
public DatabaseInternal getDatabase(int dbId) throws BabuDBException {
  return getDBMan().getDatabase(dbId);
}","/** 
 * @param dbId
 * @return an instance of the local database available for the given identifier.
 * @throws BabuDBException 
 */
public DatabaseProxy getDatabase(int dbId) throws BabuDBException {
  return dbMan.getDatabaseNonblocking(dbId);
}",0.9327902240325866
132290,"/** 
 * @return the {@link TransactionManagerInternal} of the local BabuDB instance.
 */
public TransactionManagerInternal getTransactionManager(){
  return dbs.getTransactionManager();
}","/** 
 * @return the {@link TransactionManagerProxy} of the local BabuDB instance.
 */
public TransactionManagerProxy getTransactionManager(){
  return (TransactionManagerProxy)dbs.getTransactionManager();
}",0.8702290076335878
132291,"/** 
 * @return the BabuDB modification lock object.
 */
public Object getDBModificationLock(){
  return getDBMan().getDBModificationLock();
}","/** 
 * @return the BabuDB modification lock object.
 */
public Object getDBModificationLock(){
  return dbs.getDatabaseManager().getDBModificationLock();
}",0.9463087248322148
132292,"@Override public BabuDBInternal start(BabuDBInternal babuDB,String configPath) throws BabuDBException {
  ReplicationConfig configuration;
  try {
    configuration=new ReplicationConfig(configPath,babuDB.getConfig());
  }
 catch (  IOException ioe) {
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,""String_Node_Str"",ioe.getCause());
  }
  try {
    new FileIO(configuration).replayBackupFiles();
  }
 catch (  IOException io) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + ""String_Node_Str"",io.getMessage());
  }
  ReplicationManager replMan;
  try {
    replMan=new ReplicationManager(babuDB,configuration);
  }
 catch (  Exception e) {
    if (e.getMessage() == null)     Logging.logError(Logging.LEVEL_ERROR,this,e);
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,e.getMessage(),e.getCause());
  }
  return new BabuDBProxy(babuDB,replMan,configuration.getReplicationPolicy());
}","@Override public BabuDBInternal start(BabuDBInternal babuDB,String configPath) throws BabuDBException {
  ReplicationConfig configuration;
  try {
    configuration=new ReplicationConfig(configPath,babuDB.getConfig());
  }
 catch (  IOException ioe) {
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,""String_Node_Str"",ioe.getCause());
  }
  try {
    new FileIO(configuration).replayBackupFiles();
  }
 catch (  IOException io) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + ""String_Node_Str"",io.getMessage());
  }
  BabuDBInterface dbInterface=new BabuDBInterface(babuDB);
  ReplicationManager replMan;
  try {
    replMan=new ReplicationManager(configuration,dbInterface);
  }
 catch (  Exception e) {
    if (e.getMessage() == null)     Logging.logError(Logging.LEVEL_ERROR,this,e);
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,e.getMessage(),e.getCause());
  }
  return new BabuDBProxy(babuDB,replMan,configuration.getReplicationPolicy(),dbInterface);
}",0.9401444788441692
132293,"/** 
 * <p>For setting up the   {@link BabuDB} with replication. Replication instance will be remaining stopped and in slave-mode.</p>
 * @param dbs
 * @param conf
 * @throws Exception 
 */
public ReplicationManager(BabuDBInternal dbs,ReplicationConfig conf) throws Exception {
  redirectIsVisible=conf.redirectIsVisible();
  TimeSync.initializeLocal(conf.getTimeSyncInterval(),conf.getLocalTimeRenew()).setLifeCycleListener(this);
  transmissionLayer=new TransmissionLayer(conf);
  serviceLayer=new ServiceLayer(conf,new BabuDBInterface(dbs),transmissionLayer);
  ControlLayer cl=new ControlLayer(serviceLayer,conf);
  controlLayer=cl;
  serviceLayer.init(controlLayer);
  transmissionLayer.setLifeCycleListener(this);
  serviceLayer.setLifeCycleListener(this);
  controlLayer.setLifeCycleListener(this);
}","/** 
 * <p>For setting up the   {@link BabuDB} with replication. Replication instance will be remaining stopped and in slave-mode.</p>
 * @param dbs
 * @param conf
 * @throws Exception 
 */
public ReplicationManager(ReplicationConfig conf,BabuDBInterface dbs) throws Exception {
  redirectIsVisible=conf.redirectIsVisible();
  TimeSync.initializeLocal(conf.getTimeSyncInterval(),conf.getLocalTimeRenew()).setLifeCycleListener(this);
  transmissionLayer=new TransmissionLayer(conf);
  serviceLayer=new ServiceLayer(conf,dbs,transmissionLayer);
  ControlLayer cl=new ControlLayer(serviceLayer,conf);
  controlLayer=cl;
  serviceLayer.init(controlLayer);
  transmissionLayer.setLifeCycleListener(this);
  serviceLayer.setLifeCycleListener(this);
  controlLayer.setLifeCycleListener(this);
}",0.6700125470514429
132294,"/** 
 * Blocking call.
 * @return the currently designated master.
 * @throws InterruptedException 
 */
public InetSocketAddress getMaster() throws InterruptedException {
  return controlLayer.getLeaseHolder(0);
}","/** 
 * Blocking call (for max. ReplicationConfig.REQUEST_TIMEOUT).
 * @param timeout - 0 for infinite blocking and < 0 for non-blocking.
 * @return the currently designated master.
 * @throws InterruptedException 
 */
public InetSocketAddress getMaster(int timeout) throws InterruptedException {
  return controlLayer.getLeaseHolder(timeout);
}",0.7598566308243727
132295,"@Override public void synced(LSN lsn){
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"",lsn.toString());
synchronized (finished) {
    if (finished.compareAndSet(false,true)) {
      finished.notify();
    }
 else {
      assert(false);
    }
  }
}","@Override public void synced(LSN lsn){
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"",lsn.toString());
  serviceInterface.reset();
synchronized (finished) {
    if (finished.compareAndSet(false,true)) {
      finished.notify();
    }
 else {
      assert(false);
    }
  }
}",0.9494584837545126
132296,"/** 
 * Enqueue a new failover request.
 * @param address - of the new replication master candidate.
 */
void queueFailoverRequest(InetSocketAddress address){
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",thisAddress.toString(),address.toString());
synchronized (failoverRequest) {
    if (failoverRequest.compareAndSet(null,address)) {
      failoverRequest.notify();
    }
  }
}","/** 
 * Enqueue a new failover request.
 * @param address - of the new replication master candidate.
 */
void queueFailoverRequest(InetSocketAddress address){
synchronized (failoverRequest) {
    if (failoverRequest.compareAndSet(null,address)) {
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",thisAddress.toString(),address.toString());
      failoverRequest.notify();
    }
  }
}",0.706766917293233
132297,"/** 
 * This server has to become the new master.
 * @throws Exception
 */
private void becomeMaster() throws Exception {
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
  final AtomicBoolean finished=new AtomicBoolean(false);
  final AtomicReference<Exception> exception=new AtomicReference<Exception>(null);
  serviceInterface.synchronize(new SyncListener(){
    @Override public void synced(    LSN lsn){
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"",lsn.toString());
synchronized (finished) {
        if (finished.compareAndSet(false,true)) {
          finished.notify();
        }
 else {
          assert(false);
        }
      }
    }
    @Override public void failed(    Exception ex){
      Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"" + ""String_Node_Str"",ex.getMessage());
      Logging.logError(Logging.LEVEL_DEBUG,this,ex);
synchronized (finished) {
        if (finished.compareAndSet(false,true)) {
          exception.set(ex);
          finished.notify();
        }
 else {
          assert(false);
        }
      }
    }
  }
,thisAddress.getPort());
synchronized (finished) {
    while (!finished.get()) {
      finished.wait();
    }
    if (exception.get() != null) {
      throw exception.get();
    }
  }
}","/** 
 * This server has to become the new master.
 * @throws Exception
 */
private void becomeMaster() throws Exception {
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
  final AtomicBoolean finished=new AtomicBoolean(false);
  final AtomicReference<Exception> exception=new AtomicReference<Exception>(null);
  serviceInterface.synchronize(new SyncListener(){
    @Override public void synced(    LSN lsn){
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"",lsn.toString());
      serviceInterface.reset();
synchronized (finished) {
        if (finished.compareAndSet(false,true)) {
          finished.notify();
        }
 else {
          assert(false);
        }
      }
    }
    @Override public void failed(    Exception ex){
      Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"" + ""String_Node_Str"",ex.getMessage());
      Logging.logError(Logging.LEVEL_DEBUG,this,ex);
synchronized (finished) {
        if (finished.compareAndSet(false,true)) {
          exception.set(ex);
          finished.notify();
        }
 else {
          assert(false);
        }
      }
    }
  }
,thisAddress.getPort());
synchronized (finished) {
    while (!finished.get()) {
      finished.wait();
    }
    if (exception.get() != null) {
      throw exception.get();
    }
  }
}",0.987682832948422
132298,"public BabuDBProxy(BabuDBInternal localDB,ReplicationManager replMan,Policy replicationPolicy){
  assert(localDB != null);
  this.localBabuDB=localDB;
  this.replMan=replMan;
  this.txnManProxy=new TransactionManagerProxy(replMan,localDB.getTransactionManager(),replicationPolicy,this);
  DatabaseManagerProxy dbMan=new DatabaseManagerProxy(localDB.getDatabaseManager(),replicationPolicy,replMan,this,txnManProxy,localDB.getResponseManager());
  this.client=replMan.getProxyClient(dbMan);
  this.dbManProxy=dbMan;
}","public BabuDBProxy(BabuDBInternal localDB,ReplicationManager replMan,Policy replicationPolicy,BabuDBInterface dbInt){
  assert(localDB != null);
  this.localBabuDB=localDB;
  this.replMan=replMan;
  this.txnManProxy=new TransactionManagerProxy(replMan,localDB.getTransactionManager(),replicationPolicy,this);
  DatabaseManagerProxy dbMan=new DatabaseManagerProxy(localDB.getDatabaseManager(),replicationPolicy,replMan,this,txnManProxy,localDB.getResponseManager());
  dbInt.init(dbMan);
  this.client=replMan.getProxyClient(dbMan);
  this.dbManProxy=dbMan;
}",0.9599254426840632
132299,"/** 
 * Replays the database operations log.
 * @param from - LSN to replay the logs from.
 * @return the LSN to assign to the next operation
 * @throws BabuDBException
 */
private LSN replayLogs(LSN from) throws BabuDBException {
  try {
    File f=new File(configuration.getDbLogDir());
    File[] logFiles=f.listFiles(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.endsWith(""String_Node_Str"");
      }
    }
);
    DiskLogIterator it=new DiskLogIterator(logFiles,from);
    LSN nextLSN=null;
    while (it.hasNext()) {
      LogEntry le=null;
      try {
        le=it.next();
        byte type=le.getPayloadType();
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"",le.getLSN().toString(),(int)type,le.getPayload().remaining());
        if (type == PAYLOAD_TYPE_TRANSACTION) {
          txnMan.replayTransaction(le);
        }
 else         if (type != PAYLOAD_TYPE_CREATE && type != PAYLOAD_TYPE_COPY && type != PAYLOAD_TYPE_DELETE) {
          InMemoryProcessing processingLogic=txnMan.getProcessingLogic().get(type);
          OperationInternal operation=processingLogic.convertToOperation(processingLogic.deserializeRequest(le.getPayload()));
          try {
            processingLogic.process(operation);
          }
 catch (          BabuDBException be) {
            if (!(type == PAYLOAD_TYPE_SNAP && be.getErrorCode() == ErrorCode.SNAP_EXISTS) && !(type == PAYLOAD_TYPE_SNAP_DELETE && be.getErrorCode() == ErrorCode.NO_SUCH_SNAPSHOT) && !(type == PAYLOAD_TYPE_INSERT && be.getErrorCode().equals(ErrorCode.NO_SUCH_DB))) {
              throw be;
            }
          }
        }
        nextLSN=new LSN(le.getViewId(),le.getLogSequenceNo() + 1L);
      }
  finally {
        if (le != null) {
          le.free();
        }
      }
    }
    it.destroy();
    if (nextLSN != null) {
      return nextLSN;
    }
 else {
      return new LSN(1,1);
    }
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
  }
catch (  Exception ex) {
    Logging.logError(Logging.LEVEL_ERROR,this,ex);
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
  }
}","/** 
 * Replays the database operations log.
 * @param from - LSN to replay the logs from.
 * @return the LSN to assign to the next operation
 * @throws BabuDBException
 */
private LSN replayLogs(LSN from) throws BabuDBException {
  try {
    File f=new File(configuration.getDbLogDir());
    File[] logFiles=f.listFiles(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.endsWith(""String_Node_Str"");
      }
    }
);
    DiskLogIterator it=new DiskLogIterator(logFiles,from);
    LSN nextLSN=null;
    while (it.hasNext()) {
      LogEntry le=null;
      try {
        le=it.next();
        byte type=le.getPayloadType();
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"",le.getLSN().toString(),(int)type,le.getPayload().remaining());
        if (type == PAYLOAD_TYPE_TRANSACTION) {
          txnMan.replayTransaction(le);
        }
 else         if (type != PAYLOAD_TYPE_CREATE && type != PAYLOAD_TYPE_COPY && type != PAYLOAD_TYPE_DELETE) {
          InMemoryProcessing processingLogic=txnMan.getProcessingLogic().get(type);
          OperationInternal operation=processingLogic.convertToOperation(processingLogic.deserializeRequest(le.getPayload()));
          try {
            processingLogic.process(operation);
          }
 catch (          BabuDBException be) {
            if (!(type == PAYLOAD_TYPE_SNAP && (be.getErrorCode() == ErrorCode.SNAP_EXISTS || be.getErrorCode() == ErrorCode.NO_SUCH_DB)) && !(type == PAYLOAD_TYPE_SNAP_DELETE && be.getErrorCode() == ErrorCode.NO_SUCH_SNAPSHOT) && !(type == PAYLOAD_TYPE_INSERT && be.getErrorCode().equals(ErrorCode.NO_SUCH_DB))) {
              throw be;
            }
          }
        }
        nextLSN=new LSN(le.getViewId(),le.getLogSequenceNo() + 1L);
      }
  finally {
        if (le != null) {
          le.free();
        }
      }
    }
    it.destroy();
    if (nextLSN != null) {
      return nextLSN;
    }
 else {
      return new LSN(1,1);
    }
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
  }
catch (  Exception ex) {
    Logging.logError(Logging.LEVEL_ERROR,this,ex);
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
  }
}",0.9896452963207756
132300,"@Override public void replayTransaction(TransactionInternal txn) throws BabuDBException {
  for (  OperationInternal operation : txn) {
    byte type=operation.getType();
    if (type != Operation.TYPE_COPY_DB && type != Operation.TYPE_CREATE_DB && type != Operation.TYPE_DELETE_DB) {
      InMemoryProcessing processing=inMemoryProcessing.get(type);
      try {
        processing.process(operation);
      }
 catch (      BabuDBException be) {
        if (!(type == Operation.TYPE_CREATE_SNAP && be.getErrorCode() == ErrorCode.SNAP_EXISTS) && !(type == Operation.TYPE_DELETE_SNAP && be.getErrorCode() == ErrorCode.NO_SUCH_SNAPSHOT) && !(type == Operation.TYPE_GROUP_INSERT && be.getErrorCode().equals(ErrorCode.NO_SUCH_DB))) {
          throw be;
        }
      }
    }
  }
}","@Override public void replayTransaction(TransactionInternal txn) throws BabuDBException {
  for (  OperationInternal operation : txn) {
    byte type=operation.getType();
    if (type != Operation.TYPE_COPY_DB && type != Operation.TYPE_CREATE_DB && type != Operation.TYPE_DELETE_DB) {
      InMemoryProcessing processing=inMemoryProcessing.get(type);
      try {
        processing.process(operation);
      }
 catch (      BabuDBException be) {
        if (!(type == Operation.TYPE_CREATE_SNAP && (be.getErrorCode() == ErrorCode.SNAP_EXISTS || be.getErrorCode() == ErrorCode.NO_SUCH_DB)) && !(type == Operation.TYPE_DELETE_SNAP && be.getErrorCode() == ErrorCode.NO_SUCH_SNAPSHOT) && !(type == Operation.TYPE_GROUP_INSERT && be.getErrorCode().equals(ErrorCode.NO_SUCH_DB))) {
          throw be;
        }
      }
    }
  }
}",0.9706799750467872
132301,"public void free(){
  if (currentBlock != null && currentBlock.readBuffer.getRefCount() > 0)   currentBlock.free();
}","public void free(){
  if (currentBlock != null && currentBlock.readBuffer != null && currentBlock.readBuffer.getRefCount() > 0)   currentBlock.free();
}",0.8698884758364313
132302,"/** 
 * Executes a range lookup. The result object contains an iterator to the database starting at the first key greater or equal to <code>from</code> and returning key/value pairs in ascending order. <p> Note that <code>from</code> needs to be smaller than or equal to <code>to</code> according to the comparator associated with the index. </p>
 * @param indexId index id (0..NumIndices-1)
 * @param from the key to start the iterator at
 * @param to the key to end the iterator at
 * @param context arbitrary context which is passed to the listener.
 * @return a future as proxy for the request result.
 */
public DatabaseRequestResult<ResultSet<byte[],byte[]>> rangeLookup(int indexId,byte[] from,byte[] to,Object context);","/** 
 * Executes a range lookup. The result object contains an iterator to the database starting at the first key greater or equal to <code>from</code> and returning key/value pairs in ascending order. <p> Note that <code>from</code> needs to be smaller than or equal to <code>to</code> according to the comparator associated with the index. </p>
 * @param indexId index id (0..NumIndices-1)
 * @param from the key to start the iterator at (inclusively)
 * @param to the key to end the iterator at (exclusively)
 * @param context arbitrary context which is passed to the listener.
 * @return a future as proxy for the request result.
 */
public DatabaseRequestResult<ResultSet<byte[],byte[]>> rangeLookup(int indexId,byte[] from,byte[] to,Object context);",0.98110661268556
132303,"/** 
 * Executes a range lookup. The result object contains an iterator to the database starting at the first key less or equal to <code>to</code> and returning key/value pairs in descending order. <p> Note that <code>from</code> needs to be larger than or equal to <code>to</code> according to the comparator associated with the index. </p>
 * @param indexId index id (0..NumIndices-1)
 * @param from the key to start the iterator at
 * @param to the key to end the iterator at
 * @param context arbitrary context which is passed to the listener.
 * @return a future as proxy for the request result.
 */
public DatabaseRequestResult<ResultSet<byte[],byte[]>> reverseRangeLookup(int indexId,byte[] from,byte[] to,Object context);","/** 
 * Executes a range lookup. The result object contains an iterator to the database starting at the first key less or equal to <code>to</code> and returning key/value pairs in descending order. <p> Note that <code>from</code> needs to be greater than or equal to <code>to</code> according to the comparator associated with the index. </p>
 * @param indexId index id (0..NumIndices-1)
 * @param from the key to start the iterator at (inclusively)
 * @param to the key to end the iterator at (exclusively)
 * @param context arbitrary context which is passed to the listener.
 * @return a future as proxy for the request result.
 */
public DatabaseRequestResult<ResultSet<byte[],byte[]>> reverseRangeLookup(int indexId,byte[] from,byte[] to,Object context);",0.976462676529926
132304,"private void read(){
  int i=0;
  String path;
  while ((path=readOptionalString(""String_Node_Str"" + i++,null)) != null) {
    if (!path.endsWith(""String_Node_Str"") && !path.endsWith(""String_Node_Str"")) {
      if (path.contains(""String_Node_Str"")) {
        path=path + ""String_Node_Str"";
      }
 else       if (path.contains(""String_Node_Str"")) {
        path=path + ""String_Node_Str"";
      }
 else {
        path=path + File.separator;
      }
    }
    paths.add(path);
  }
}","private void read(){
  int i=0;
  String path;
  while ((path=readOptionalString(""String_Node_Str"" + i++,null)) != null) {
    paths.add(path);
  }
}",0.473015873015873
132305,"/** 
 * @param logFiles
 * @param from - inclusive, if everything went fine, next() will return the log entry identified by LSN <code>from</code>.
 * @throws LogEntryException
 * @throws IOException
 */
public DiskLogIterator(File[] logFiles,LSN from) throws LogEntryException, IOException {
  this.from=from;
  if (logFiles != null && logFiles.length > 0) {
    dbLogDir=logFiles[0].getParent() + ""String_Node_Str"";
    int count=-1;
    SortedSet<LSN> orderedLogList=new TreeSet<LSN>();
    Pattern p=Pattern.compile(""String_Node_Str"");
    for (    File logFile : logFiles) {
      Matcher m=p.matcher(logFile.getName());
      m.matches();
      String tmp=m.group(1);
      int viewId=Integer.valueOf(tmp);
      tmp=m.group(2);
      int seqNo=Integer.valueOf(tmp);
      orderedLogList.add(new LSN(viewId,seqNo));
      count++;
    }
    LSN[] copy=orderedLogList.toArray(new LSN[orderedLogList.size()]);
    LSN last=null;
    LSN lastRemoved=null;
    for (    LSN lsn : copy) {
      if (last == null)       last=lsn;
 else {
        if (from != null && lsn.compareTo(from) < 0) {
          orderedLogList.remove(last);
          lastRemoved=last;
          last=lsn;
        }
 else         break;
      }
    }
    if (from != null && !LSMDatabase.NO_DB_LSN.equals(from) && last.compareTo(from) > 0)     throw new LogEntryException(""String_Node_Str"" + from.toString() + ""String_Node_Str""+ last.toString());
    if (lastRemoved != null && from.compareTo(last) < 0) {
      orderedLogList.add(lastRemoved);
    }
    logList=orderedLogList.iterator();
    findFirstEntry();
  }
}","/** 
 * @param logFiles
 * @param from - inclusive, if everything went fine, next() will return the log entry identified by LSN <code>from</code>.
 * @throws LogEntryException
 * @throws IOException
 */
public DiskLogIterator(File[] logFiles,LSN from) throws LogEntryException, IOException {
  this.from=from;
  if (logFiles != null && logFiles.length > 0) {
    dbLogDir=logFiles[0].getParent() + ""String_Node_Str"";
    int count=-1;
    SortedSet<LSN> orderedLogList=new TreeSet<LSN>();
    Pattern p=Pattern.compile(""String_Node_Str"");
    for (    File logFile : logFiles) {
      Matcher m=p.matcher(logFile.getName());
      m.matches();
      String tmp=m.group(1);
      int viewId=Integer.valueOf(tmp);
      tmp=m.group(2);
      int seqNo=Integer.valueOf(tmp);
      orderedLogList.add(new LSN(viewId,seqNo));
      count++;
    }
    LSN[] copy=orderedLogList.toArray(new LSN[orderedLogList.size()]);
    LSN last=null;
    LSN lastRemoved=null;
    for (    LSN lsn : copy) {
      if (last == null)       last=lsn;
 else {
        if (from != null && lsn.compareTo(from) <= 0) {
          orderedLogList.remove(last);
          lastRemoved=last;
          last=lsn;
        }
 else         break;
      }
    }
    if (from != null && !LSMDatabase.NO_DB_LSN.equals(from) && last.compareTo(from) > 0)     throw new LogEntryException(""String_Node_Str"" + from.toString() + ""String_Node_Str""+ last.toString());
    if (lastRemoved != null && from.compareTo(last) < 0) {
      orderedLogList.add(lastRemoved);
    }
    logList=orderedLogList.iterator();
    findFirstEntry();
  }
}",0.9996856334486012
132306,"@Test public void testMultipleIndicesAndCheckpoint() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",4);
  DatabaseInsertGroup ir=db.createInsertGroup();
  ir.addInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(1,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(2,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  db.insert(ir,null).get();
  database.getCheckpointer().checkpoint();
  byte[] result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  String value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(1,""String_Node_Str"".getBytes(),null).get();
  assertEquals(""String_Node_Str"",new String(result));
  result=db.lookup(2,""String_Node_Str"".getBytes(),null).get();
  assertEquals(""String_Node_Str"",new String(result));
  Iterator<Entry<byte[],byte[]>> iter=db.prefixLookup(3,""String_Node_Str"".getBytes(),null).get();
  assertFalse(iter.hasNext());
  ir=db.createInsertGroup();
  ir.addDelete(0,""String_Node_Str"".getBytes());
  ir.addInsert(1,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(2,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  db.insert(ir,null).get();
  database.getCheckpointer().checkpoint();
  iter=db.prefixLookup(2,""String_Node_Str"".getBytes(),null).get();
  assertTrue(iter.hasNext());
  Entry<byte[],byte[]> next=iter.next();
  assertEquals(""String_Node_Str"",new String(next.getKey()));
  assertEquals(""String_Node_Str"",new String(next.getValue()));
  assertTrue(!iter.hasNext());
  System.out.println(""String_Node_Str"");
  database.shutdown();
}","@Test public void testMultipleIndicesAndCheckpoint() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",4);
  DatabaseInsertGroup ir=db.createInsertGroup();
  ir.addInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(1,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(2,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  db.insert(ir,null).get();
  database.getCheckpointer().checkpoint();
  byte[] result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  String value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(1,""String_Node_Str"".getBytes(),null).get();
  assertEquals(""String_Node_Str"",new String(result));
  result=db.lookup(2,""String_Node_Str"".getBytes(),null).get();
  assertEquals(""String_Node_Str"",new String(result));
  Iterator<Entry<byte[],byte[]>> iter=db.prefixLookup(3,""String_Node_Str"".getBytes(),null).get();
  assertFalse(iter.hasNext());
  ir=db.createInsertGroup();
  ir.addDelete(0,""String_Node_Str"".getBytes());
  ir.addInsert(1,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(2,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  db.insert(ir,null).get();
  database.getCheckpointer().checkpoint();
  iter=db.prefixLookup(2,""String_Node_Str"".getBytes(),null).get();
  assertTrue(iter.hasNext());
  Entry<byte[],byte[]> next=iter.next();
  assertEquals(""String_Node_Str"",new String(next.getKey()));
  assertEquals(""String_Node_Str"",new String(next.getValue()));
  assertTrue(!iter.hasNext());
  System.out.println(""String_Node_Str"");
  database.shutdown();
}",0.9927243330638642
132307,"@Test public void testReplayAfterCrash() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",2);
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  database.getCheckpointer().checkpoint();
  byte[] result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  String value=new String(result);
  assertEquals(value,""String_Node_Str"");
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  ((BabuDBImpl)database).__test_killDB_dangerous();
  Thread.sleep(500);
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,2,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  db=database.getDatabaseManager().getDatabase(""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  System.out.println(""String_Node_Str"");
  database.shutdown();
}","@Test public void testReplayAfterCrash() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",2);
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  database.getCheckpointer().checkpoint();
  byte[] result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  String value=new String(result);
  assertEquals(value,""String_Node_Str"");
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  ((BabuDBImpl)database).__test_killDB_dangerous();
  Thread.sleep(500);
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,2,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  db=database.getDatabaseManager().getDatabase(""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  System.out.println(""String_Node_Str"");
  database.shutdown();
}",0.99268094334508
132308,"@Test public void testUserDefinedLookup() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  DatabaseInsertGroup ir=db.createInsertGroup();
  ir.addInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(1,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(2,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  db.insert(ir,null).get();
  UserDefinedLookup lookup=new UserDefinedLookup(){
    public Object execute(    LSMLookupInterface database) throws BabuDBException {
      if ((database.lookup(0,""String_Node_Str"".getBytes()) != null) && (database.lookup(1,""String_Node_Str"".getBytes()) != null)) {
        return new Boolean(true);
      }
 else {
        return new Boolean(false);
      }
    }
  }
;
  Boolean result=(Boolean)db.userDefinedLookup(lookup,null).get();
  assertTrue(result);
  System.out.println(""String_Node_Str"");
  database.shutdown();
}","@Test public void testUserDefinedLookup() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  DatabaseInsertGroup ir=db.createInsertGroup();
  ir.addInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(1,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(2,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  db.insert(ir,null).get();
  UserDefinedLookup lookup=new UserDefinedLookup(){
    public Object execute(    LSMLookupInterface database) throws BabuDBException {
      if ((database.lookup(0,""String_Node_Str"".getBytes()) != null) && (database.lookup(1,""String_Node_Str"".getBytes()) != null)) {
        return new Boolean(true);
      }
 else {
        return new Boolean(false);
      }
    }
  }
;
  Boolean result=(Boolean)db.userDefinedLookup(lookup,null).get();
  assertTrue(result);
  System.out.println(""String_Node_Str"");
  database.shutdown();
}",0.988100484795064
132309,"@Test public void testShutdownAfterCheckpoint() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",2);
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  byte[] result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  String value=new String(result);
  assertEquals(value,""String_Node_Str"");
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  database.getCheckpointer().checkpoint();
  database.shutdown();
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,2,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  db=database.getDatabaseManager().getDatabase(""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  System.out.println(""String_Node_Str"");
  database.shutdown();
}","@Test public void testShutdownAfterCheckpoint() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",2);
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  byte[] result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  String value=new String(result);
  assertEquals(value,""String_Node_Str"");
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  db.singleInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  database.getCheckpointer().checkpoint();
  database.shutdown();
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,2,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  db=database.getDatabaseManager().getDatabase(""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  System.out.println(""String_Node_Str"");
  database.shutdown();
}",0.9925062447960034
132310,"@Test public void testInsRangeLookup() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,0,0,0,SyncMode.ASYNC,0,0,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  for (int i=1000; i < 2000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    ir.addInsert(2,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    db.insert(ir,null);
  }
  Iterator<Entry<byte[],byte[]>> it=db.rangeLookup(0,new byte[0],new byte[0],null).get();
  for (int i=1000; i < 2000; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  it=db.rangeLookup(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  for (int i=1500; i < 1600; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  database.getCheckpointer().checkpoint();
  it=db.rangeLookup(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  for (int i=1500; i < 1600; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  database.shutdown();
}","@Test public void testInsRangeLookup() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,0,0,0,SyncMode.ASYNC,0,0,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  for (int i=1000; i < 2000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    ir.addInsert(2,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    db.insert(ir,null);
  }
  Iterator<Entry<byte[],byte[]>> it=db.rangeLookup(0,new byte[0],new byte[0],null).get();
  for (int i=1000; i < 2000; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  it=db.rangeLookup(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  for (int i=1500; i < 1600; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  database.getCheckpointer().checkpoint();
  it=db.rangeLookup(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null).get();
  for (int i=1500; i < 1600; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  database.shutdown();
}",0.9905429071803852
132311,"@Test public void testDirectAccess() throws Exception {
  Logging.logMessage(Logging.LEVEL_DEBUG,Category.test,this,BufferPool.getStatus());
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,0,0,0,SyncMode.ASYNC,0,50,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",2);
  for (int i=0; i < 100000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    db.insert(ir,null).get();
  }
  Logging.logMessage(Logging.LEVEL_DEBUG,Category.test,this,BufferPool.getStatus());
  database.getCheckpointer().checkpoint();
  for (int i=0; i < 100000; i++) {
    byte[] v0=db.lookup(0,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v1=db.lookup(1,(i + ""String_Node_Str"").getBytes(),null).get();
    assertEquals(""String_Node_Str"",new String(v0));
    assertEquals(""String_Node_Str"",new String(v1));
  }
  Logging.logMessage(Logging.LEVEL_DEBUG,Category.test,this,BufferPool.getStatus());
}","@Test public void testDirectAccess() throws Exception {
  Logging.logMessage(Logging.LEVEL_DEBUG,Category.test,this,BufferPool.getStatus());
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,0,0,0,SyncMode.ASYNC,0,50,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",2);
  for (int i=0; i < 100000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    db.insert(ir,null).get();
  }
  Logging.logMessage(Logging.LEVEL_DEBUG,Category.test,this,BufferPool.getStatus());
  database.getCheckpointer().checkpoint();
  for (int i=0; i < 100000; i++) {
    byte[] v0=db.lookup(0,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v1=db.lookup(1,(i + ""String_Node_Str"").getBytes(),null).get();
    assertEquals(""String_Node_Str"",new String(v0));
    assertEquals(""String_Node_Str"",new String(v1));
  }
  Logging.logMessage(Logging.LEVEL_DEBUG,Category.test,this,BufferPool.getStatus());
  database.shutdown();
}",0.978849407783418
132312,"@Test public void testInsDelGet() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,0,0,0,SyncMode.ASYNC,0,0,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  for (int i=0; i < 1000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    ir.addInsert(2,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    db.insert(ir,null).get();
  }
  byte[] data=new byte[2048];
  for (int i=0; i < 1000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),data);
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),data);
    ir.addInsert(2,(i + ""String_Node_Str"").getBytes(),data);
    db.insert(ir,null).get();
  }
  database.getCheckpointer().checkpoint();
  for (int i=0; i < 1000; i++) {
    byte[] v0=db.lookup(0,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v1=db.lookup(1,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v2=db.lookup(2,(i + ""String_Node_Str"").getBytes(),null).get();
    assertNotNull(v0);
    assertNotNull(v1);
    assertNotNull(v2);
  }
  for (int i=0; i < 1000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addDelete(0,(i + ""String_Node_Str"").getBytes());
    ir.addDelete(1,(i + ""String_Node_Str"").getBytes());
    ir.addDelete(2,(i + ""String_Node_Str"").getBytes());
    db.insert(ir,null).get();
  }
  for (int i=0; i < 1000; i++) {
    byte[] v0=db.lookup(0,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v1=db.lookup(1,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v2=db.lookup(2,(i + ""String_Node_Str"").getBytes(),null).get();
    assertNull(v0);
    assertNull(v1);
    assertNull(v2);
  }
}","@Test public void testInsDelGet() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,0,0,0,SyncMode.ASYNC,0,0,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  for (int i=0; i < 1000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    ir.addInsert(2,(i + ""String_Node_Str"").getBytes(),""String_Node_Str"".getBytes());
    db.insert(ir,null).get();
  }
  byte[] data=new byte[2048];
  for (int i=0; i < 1000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),data);
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),data);
    ir.addInsert(2,(i + ""String_Node_Str"").getBytes(),data);
    db.insert(ir,null).get();
  }
  database.getCheckpointer().checkpoint();
  for (int i=0; i < 1000; i++) {
    byte[] v0=db.lookup(0,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v1=db.lookup(1,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v2=db.lookup(2,(i + ""String_Node_Str"").getBytes(),null).get();
    assertNotNull(v0);
    assertNotNull(v1);
    assertNotNull(v2);
  }
  for (int i=0; i < 1000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addDelete(0,(i + ""String_Node_Str"").getBytes());
    ir.addDelete(1,(i + ""String_Node_Str"").getBytes());
    ir.addDelete(2,(i + ""String_Node_Str"").getBytes());
    db.insert(ir,null).get();
  }
  for (int i=0; i < 1000; i++) {
    byte[] v0=db.lookup(0,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v1=db.lookup(1,(i + ""String_Node_Str"").getBytes(),null).get();
    byte[] v2=db.lookup(2,(i + ""String_Node_Str"").getBytes(),null).get();
    assertNull(v0);
    assertNull(v1);
    assertNull(v2);
  }
  database.shutdown();
}",0.9873673572511368
132313,"@Test public void testInsPrefLookup() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,0,0,0,SyncMode.ASYNC,0,0,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  for (int i=1000; i < 2000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    ir.addInsert(2,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    db.insert(ir,null);
  }
  Iterator<Entry<byte[],byte[]>> it=db.prefixLookup(0,new byte[0],null).get();
  for (int i=1000; i < 2000; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  it=db.prefixLookup(0,""String_Node_Str"".getBytes(),null).get();
  for (int i=1500; i < 1600; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  database.getCheckpointer().checkpoint();
  it=db.prefixLookup(0,""String_Node_Str"".getBytes(),null).get();
  for (int i=1500; i < 1600; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  database.shutdown();
}","@Test public void testInsPrefLookup() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,0,0,0,SyncMode.ASYNC,0,0,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  for (int i=1000; i < 2000; i++) {
    DatabaseInsertGroup ir=db.createInsertGroup();
    ir.addInsert(0,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    ir.addInsert(1,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    ir.addInsert(2,(i + ""String_Node_Str"").getBytes(),(i + ""String_Node_Str"").getBytes());
    db.insert(ir,null);
  }
  Iterator<Entry<byte[],byte[]>> it=db.prefixLookup(0,new byte[0],null).get();
  for (int i=1000; i < 2000; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  it=db.prefixLookup(0,""String_Node_Str"".getBytes(),null).get();
  for (int i=1500; i < 1600; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  database.getCheckpointer().checkpoint();
  it=db.prefixLookup(0,""String_Node_Str"".getBytes(),null).get();
  for (int i=1500; i < 1600; i++)   assertEquals(i + ""String_Node_Str"",new String(it.next().getValue()));
  assertFalse(it.hasNext());
  database.shutdown();
}",0.99006987863185
132314,"@Test public void testMultipleIndices() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  DatabaseInsertGroup ir=db.createInsertGroup();
  ir.addInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(1,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(2,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  db.insert(ir,null).get();
  database.getCheckpointer().checkpoint();
  database.shutdown();
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,2,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  db=database.getDatabaseManager().getDatabase(""String_Node_Str"");
  byte[] result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  String value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(1,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(2,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  System.out.println(""String_Node_Str"");
  database.shutdown();
}","@Test public void testMultipleIndices() throws Exception {
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,1,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize,true,0,Logging.LEVEL_DEBUG));
  Database db=database.getDatabaseManager().createDatabase(""String_Node_Str"",3);
  DatabaseInsertGroup ir=db.createInsertGroup();
  ir.addInsert(0,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(1,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  ir.addInsert(2,""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  db.insert(ir,null).get();
  database.getCheckpointer().checkpoint();
  database.shutdown();
  database=BabuDBFactory.createBabuDB(new BabuDBConfig(baseDir,baseDir,2,0,0,SyncMode.SYNC_WRITE,0,0,compression,maxNumRecs,maxBlockFileSize));
  db=database.getDatabaseManager().getDatabase(""String_Node_Str"");
  byte[] result=db.lookup(0,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  String value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(1,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  result=db.lookup(2,""String_Node_Str"".getBytes(),null).get();
  assertNotNull(result);
  value=new String(result);
  assertEquals(value,""String_Node_Str"");
  System.out.println(""String_Node_Str"");
  database.shutdown();
}",0.9905759162303664
132315,"public void testLogIterator() throws Exception {
  final int numLogFiles=3;
  for (int k=0; k < numLogFiles; k++) {
    final AtomicInteger count=new AtomicInteger(0);
    SyncListener sl=new SyncListener(){
      public void synced(      LogEntry entry){
synchronized (entry) {
          count.incrementAndGet();
          entry.notifyAll();
        }
      }
      public void failed(      LogEntry entry,      Exception ex){
synchronized (entry) {
          count.incrementAndGet();
          entry.notifyAll();
        }
      }
    }
;
    LogEntry e=null;
    for (int i=0; i < 100; i++) {
      String pl=""String_Node_Str"" + (k * 100 + i + 1);
      ReusableBuffer plb=ReusableBuffer.wrap(pl.getBytes());
      e=new LogEntry(plb,sl,LogEntry.PAYLOAD_TYPE_INSERT);
      l.append(e);
    }
synchronized (e) {
      if (count.get() < 100)       e.wait(1000);
    }
    e.free();
    try {
      l.lockLogger();
      LSN lsn=l.switchLogFile(false);
    }
  finally {
      l.unlockLogger();
    }
  }
  File[] logFiles=new File(testdir).listFiles();
  DiskLogIterator it=new DiskLogIterator(logFiles,LSMDatabase.NO_DB_LSN);
  assertTrue(it.hasNext());
  for (int i=1; i <= numLogFiles * 100; i++) {
    LogEntry next=it.next();
    String entry=new String(next.getPayload().array());
    assertEquals(""String_Node_Str"" + i,entry);
    next.free();
  }
  assertFalse(it.hasNext());
  it.destroy();
  for (int k=0; k < 5; k++) {
    LSN lsn=new LSN(1,(int)(Math.random() * numLogFiles * 100));
    it=new DiskLogIterator(logFiles,lsn);
    assertTrue(it.hasNext());
    for (int i=(int)lsn.getSequenceNo(); i <= numLogFiles * 100; i++) {
      LogEntry next=it.next();
      String entry=new String(next.getPayload().array());
      assertEquals(""String_Node_Str"" + i,entry);
      next.free();
    }
    assertFalse(it.hasNext());
    it.destroy();
  }
}","public void testLogIterator() throws Exception {
  final int numLogFiles=3;
  for (int k=0; k < numLogFiles; k++) {
    final AtomicInteger count=new AtomicInteger(0);
    SyncListener sl=new SyncListener(){
      public void synced(      LogEntry entry){
synchronized (entry) {
          count.incrementAndGet();
          entry.notifyAll();
        }
      }
      public void failed(      LogEntry entry,      Exception ex){
synchronized (entry) {
          System.err.println(ex);
          count.incrementAndGet();
          entry.notifyAll();
        }
      }
    }
;
    LogEntry e=null;
    for (int i=0; i < 100; i++) {
      String pl=""String_Node_Str"" + (k * 100 + i + 1);
      ReusableBuffer plb=ReusableBuffer.wrap(pl.getBytes());
      e=new LogEntry(plb,sl,LogEntry.PAYLOAD_TYPE_INSERT);
      l.append(e);
    }
synchronized (e) {
      if (count.get() < 100)       e.wait();
    }
    e.free();
    try {
      l.lockLogger();
      LSN lsn=l.switchLogFile(false);
    }
  finally {
      l.unlockLogger();
    }
  }
  File[] logFiles=new File(testdir).listFiles();
  assertEquals(numLogFiles + 1,logFiles.length);
  DiskLogIterator it=new DiskLogIterator(logFiles,LSMDatabase.NO_DB_LSN);
  assertTrue(it.hasNext());
  for (int i=1; i <= numLogFiles * 100; i++) {
    LogEntry next=it.next();
    String entry=new String(next.getPayload().array());
    assertEquals(""String_Node_Str"" + i,entry);
    next.free();
  }
  assertFalse(it.hasNext());
  it.destroy();
  for (  int k : new int[]{1,100,101,200,201,300,77,112,189,222}) {
    LSN lsn=new LSN(1,k);
    it=new DiskLogIterator(logFiles,lsn);
    assertTrue(it.hasNext());
    for (int i=(int)lsn.getSequenceNo(); i <= numLogFiles * 100; i++) {
      LogEntry next=it.next();
      String entry=new String(next.getPayload().array());
      assertEquals(""String_Node_Str"" + i,entry);
      next.free();
    }
    assertFalse(it.hasNext());
    it.destroy();
  }
}",0.8624143384290985
132316,"public void failed(LogEntry entry,Exception ex){
synchronized (entry) {
    count.incrementAndGet();
    entry.notifyAll();
  }
}","public void failed(LogEntry entry,Exception ex){
synchronized (entry) {
    System.err.println(ex);
    count.incrementAndGet();
    entry.notifyAll();
  }
}",0.902097902097902
132317,"@Test public void testReadLogfile() throws Exception {
  final AtomicInteger count=new AtomicInteger(0);
  SyncListener sl=new SyncListener(){
    public void synced(    LogEntry entry){
synchronized (entry) {
        count.incrementAndGet();
        entry.notifyAll();
        System.out.println(""String_Node_Str"" + entry.getLogSequenceNo());
      }
    }
    public void failed(    LogEntry entry,    Exception ex){
synchronized (entry) {
        count.incrementAndGet();
        entry.notifyAll();
      }
    }
  }
;
  LogEntry e=null;
  for (int i=0; i < 100; i++) {
    String pl=""String_Node_Str"" + (i + 1);
    ReusableBuffer plb=ReusableBuffer.wrap(pl.getBytes());
    e=new LogEntry(plb,sl,LogEntry.PAYLOAD_TYPE_INSERT);
    l.append(e);
  }
synchronized (e) {
    if (count.get() < 100)     e.wait(1000);
  }
  e.free();
  System.out.println(""String_Node_Str"");
  try {
    l.lockLogger();
    l.switchLogFile(false);
  }
  finally {
    l.unlockLogger();
  }
  DiskLogFile f=new DiskLogFile(testdir + ""String_Node_Str"");
  while (f.hasNext()) {
    LogEntry tmp=f.next();
    byte[] data=tmp.getPayload().array();
    String s=new String(data);
    System.out.println(""String_Node_Str"" + s);
    tmp.free();
  }
}","@Test public void testReadLogfile() throws Exception {
  final AtomicInteger count=new AtomicInteger(0);
  SyncListener sl=new SyncListener(){
    public void synced(    LogEntry entry){
synchronized (entry) {
        count.incrementAndGet();
        entry.notifyAll();
      }
    }
    public void failed(    LogEntry entry,    Exception ex){
synchronized (entry) {
        count.incrementAndGet();
        entry.notifyAll();
      }
    }
  }
;
  LogEntry e=null;
  for (int i=0; i < 100; i++) {
    String pl=""String_Node_Str"" + (i + 1);
    ReusableBuffer plb=ReusableBuffer.wrap(pl.getBytes());
    e=new LogEntry(plb,sl,LogEntry.PAYLOAD_TYPE_INSERT);
    l.append(e);
  }
synchronized (e) {
    if (count.get() < 100)     e.wait(1000);
  }
  e.free();
  System.out.println(""String_Node_Str"");
  try {
    l.lockLogger();
    l.switchLogFile(false);
  }
  finally {
    l.unlockLogger();
  }
  DiskLogFile f=new DiskLogFile(testdir + ""String_Node_Str"");
  while (f.hasNext()) {
    LogEntry tmp=f.next();
    byte[] data=tmp.getPayload().array();
    String s=new String(data);
    tmp.free();
  }
}",0.9472329472329472
132318,"public void free(){
  if (currentBlock != null) {
    currentBlock.free();
  }
}","public void free(){
  if (currentBlock != null && currentBlock.readBuffer.getRefCount() > 0)   currentBlock.free();
}",0.751269035532995
132319,"/** 
 * Loads all classes from the jar found at path to be available to the ClassLoader.
 * @param path
 * @param classToSearchFor
 * @throws IOException 
 * @throws FileNotFoundException 
 */
private final String loadJar(String path,String classToSearchFor) throws FileNotFoundException, IOException {
  JarInputStream jis=new JarInputStream(new FileInputStream(path));
  JarEntry next=null;
  String main=null;
  while ((next=jis.getNextJarEntry()) != null) {
    if (!next.getName().endsWith(""String_Node_Str"")) {
      continue;
    }
    byte[] buf=new byte[4096];
    ByteArrayOutputStream out=new ByteArrayOutputStream();
    int len=-1;
    while ((len=jis.read(buf)) > 0) {
      out.write(buf,0,len);
    }
    String className=next.getName().substring(0,next.getName().length() - ""String_Node_Str"".length()).replace('/','.');
    if (classToSearchFor != null && className.endsWith(classToSearchFor)) {
      assert(main == null);
      main=className;
    }
    if (!classes.containsKey(className)) {
      classes.put(className,out.toByteArray());
    }
 else {
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"",className,path);
    }
    out.close();
  }
  jis.close();
  return main;
}","/** 
 * Loads all classes from the jar found at path to be available to the ClassLoader.
 * @param path
 * @param classToSearchFor
 * @throws IOException 
 * @throws FileNotFoundException 
 */
private final String loadJar(String path,String classToSearchFor) throws FileNotFoundException, IOException {
  JarInputStream jis=new JarInputStream(new FileInputStream(path));
  JarEntry next=null;
  String main=null;
  while ((next=jis.getNextJarEntry()) != null) {
    if (!next.getName().endsWith(""String_Node_Str"")) {
      continue;
    }
    byte[] buf=new byte[4096];
    ByteArrayOutputStream out=new ByteArrayOutputStream();
    int len=-1;
    while ((len=jis.read(buf)) > 0) {
      out.write(buf,0,len);
    }
    String className=next.getName().substring(0,next.getName().length() - ""String_Node_Str"".length()).replace('/','.');
    if (classToSearchFor != null && className.endsWith(classToSearchFor)) {
      assert(main == null);
      main=className;
    }
    if (!clazzes.containsKey(className)) {
      clazzes.put(className,out.toByteArray());
    }
 else {
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"",className,path);
    }
    out.close();
  }
  jis.close();
  return main;
}",0.9967637540453076
132320,"@Override public Class<?> loadClass(String name,boolean resolve) throws ClassNotFoundException {
  Class<?> clazz=null;
  try {
    clazz=findSystemClass(name);
  }
 catch (  Throwable t) {
    clazz=findLoadedClass(name);
    if (clazz == null) {
      byte[] classBytes=classes.get(name);
      if (classBytes != null) {
        clazz=defineClass(name,classBytes,0,classBytes.length);
      }
 else {
        throw new ClassNotFoundException(t.getMessage(),t);
      }
    }
  }
  if (resolve) {
    resolveClass(clazz);
  }
  return clazz;
}","@Override public Class<?> loadClass(String name,boolean resolve) throws ClassNotFoundException {
  Class<?> clazz=findLoadedClass(name);
  if (clazz == null) {
    byte[] classBytes=clazzes.get(name);
    if (classBytes != null) {
      clazz=defineClass(name,classBytes,0,classBytes.length);
    }
 else {
      clazz=getParent().loadClass(name);
    }
  }
  if (resolve) {
    resolveClass(clazz);
  }
  return clazz;
}",0.5678756476683938
132321,"/** 
 * Creates a BabuDB JAR class loader for the plugins defined within the configuration file. It will automatically attempt to load its JAR  for the given data version.
 * @param babuDBImpl
 * @throws IOException if an I/O error occurred
 */
private PluginLoader(BabuDBInternal dbs) throws IOException {
  super();
  this.babuDB=dbs;
  for (  Entry<String,String> plugin : dbs.getConfig().getPlugins().entrySet()) {
    String main=null;
    String pluginPath=plugin.getKey();
    String configPath=plugin.getValue();
    try {
      main=loadJar(pluginPath,""String_Node_Str"");
      if (main == null) {
        throw new Exception(""String_Node_Str"");
      }
      PluginMain m=(PluginMain)loadClass(main).newInstance();
      for (      String depPath : m.getDependencies(configPath)) {
        loadJar(depPath);
      }
      babuDB=m.execute(babuDB,configPath);
    }
 catch (    Exception e) {
      throw new IOException(""String_Node_Str"" + pluginPath + ""String_Node_Str""+ BABUDB_VERSION+ ((configPath != null) ? ""String_Node_Str"" + configPath : ""String_Node_Str"")+ ""String_Node_Str""+ e.getMessage()+ ""String_Node_Str"",e.getCause());
    }
  }
}","/** 
 * Creates a BabuDB JAR class loader for the plugins defined within the configuration file. It will automatically attempt to load its JAR  for the given data version.
 * @param babuDBImpl
 * @throws IOException if an I/O error occurred
 */
private PluginLoader(BabuDBInternal dbs) throws IOException {
  super();
  this.babuDB=dbs;
  for (  Entry<String,String> plugin : dbs.getConfig().getPlugins().entrySet()) {
    String main=null;
    String pluginPath=plugin.getKey();
    String configPath=plugin.getValue();
    try {
      main=loadJar(pluginPath,""String_Node_Str"");
      if (main == null) {
        throw new Exception(""String_Node_Str"");
      }
      PluginMain m=(PluginMain)loadClass(main).newInstance();
      for (      String depPath : m.getDependencies(configPath)) {
        Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",depPath);
        loadJar(depPath);
      }
      babuDB=m.execute(babuDB,configPath);
    }
 catch (    Exception e) {
      throw new IOException(""String_Node_Str"" + pluginPath + ""String_Node_Str""+ BABUDB_VERSION+ ((configPath != null) ? ""String_Node_Str"" + configPath : ""String_Node_Str"")+ ""String_Node_Str""+ e.getMessage()+ ""String_Node_Str"",e.getCause());
    }
  }
}",0.9669040636782572
132322,"@Override public LSN restart() throws BabuDBException {
synchronized (stopped) {
    if (!stopped.get()) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"");
    }
    databaseManager.reset();
    LSN dbLsn=null;
    for (    Database db : databaseManager.getDatabaseList()) {
      if (dbLsn == null)       dbLsn=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
 else {
        LSN onDiskLSN=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
        if (!(LSMDatabase.NO_DB_LSN.equals(dbLsn) || LSMDatabase.NO_DB_LSN.equals(onDiskLSN))) {
          dbLsn=dbLsn.compareTo(onDiskLSN) < 0 ? dbLsn : onDiskLSN;
        }
      }
    }
    if (dbLsn == null) {
      dbLsn=new LSN(0,0);
    }
 else {
      dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
    }
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    LSN nextLSN=replayLogs(dbLsn);
    if (dbLsn.compareTo(nextLSN) > 0)     nextLSN=dbLsn;
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + nextLSN);
    try {
      logger=new DiskLogger(configuration.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads());
      logger.start();
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
    }
    this.permMan.setLogger(logger);
    if (configuration.getNumThreads() > 0) {
      worker=new LSMDBWorker[configuration.getNumThreads()];
      for (int i=0; i < configuration.getNumThreads(); i++) {
        worker[i]=new LSMDBWorker(this,i,configuration.getMaxQueueLength());
        worker[i].start();
      }
    }
 else {
      assert(configuration.getNumThreads() == 0);
      worker=null;
    }
    this.dbCheckptr.init(logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
    this.stopped.set(false);
    return new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1L);
  }
}","@Override public LSN restart() throws BabuDBException {
synchronized (stopped) {
    if (!stopped.get()) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"");
    }
    databaseManager.reset();
    LSN dbLsn=null;
    for (    Database db : databaseManager.getDatabaseList()) {
      if (dbLsn == null)       dbLsn=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
 else {
        LSN onDiskLSN=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
        if (!(LSMDatabase.NO_DB_LSN.equals(dbLsn) || LSMDatabase.NO_DB_LSN.equals(onDiskLSN))) {
          dbLsn=dbLsn.compareTo(onDiskLSN) < 0 ? dbLsn : onDiskLSN;
        }
      }
    }
    if (dbLsn == null) {
      dbLsn=new LSN(0,0);
    }
 else {
      dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
    }
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    LSN nextLSN=replayLogs(dbLsn);
    if (dbLsn.compareTo(nextLSN) > 0)     nextLSN=dbLsn;
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + nextLSN);
    try {
      logger=new DiskLogger(configuration.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads());
      logger.start();
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
    }
    this.permMan.init(new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1L));
    this.permMan.setLogger(logger);
    if (configuration.getNumThreads() > 0) {
      worker=new LSMDBWorker[configuration.getNumThreads()];
      for (int i=0; i < configuration.getNumThreads(); i++) {
        worker[i]=new LSMDBWorker(this,i,configuration.getMaxQueueLength());
        worker[i].start();
      }
    }
 else {
      assert(configuration.getNumThreads() == 0);
      worker=null;
    }
    this.dbCheckptr.init(logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
    this.stopped.set(false);
    return new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1L);
  }
}",0.9816225907664724
132323,"/** 
 * Initializes the basic components of the BabuDB database system.
 * @param configuration
 * @throws BabuDBException
 */
BabuDBImpl(BabuDBConfig configuration) throws BabuDBException {
  this.configuration=configuration;
  this.databaseManager=new DatabaseManagerImpl(this);
  this.dbConfigFile=new DBConfig(this);
  this.snapshotManager=new SnapshotManagerImpl(this);
  this.dbCheckptr=new CheckpointerImpl(this);
  this.permMan=new PersistenceManagerImpl();
}","/** 
 * Initializes the basic components of the BabuDB database system.
 * @param configuration
 * @throws BabuDBException
 */
BabuDBImpl(BabuDBConfig configuration) throws BabuDBException {
  this.configuration=configuration;
  this.permMan=new PersistenceManagerImpl();
  this.databaseManager=new DatabaseManagerImpl(this);
  this.dbConfigFile=new DBConfig(this);
  this.snapshotManager=new SnapshotManagerImpl(this);
  this.dbCheckptr=new CheckpointerImpl(this);
}",0.9036402569593148
132324,"/** 
 * Replays the database operations log.
 * @param from - LSN to replay the logs from.
 * @return the LSN to assign to the next operation
 * @throws BabuDBException
 */
private LSN replayLogs(LSN from) throws BabuDBException {
  try {
    File f=new File(configuration.getDbLogDir());
    File[] logFiles=f.listFiles(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.endsWith(""String_Node_Str"");
      }
    }
);
    DiskLogIterator it=new DiskLogIterator(logFiles,from);
    LSN nextLSN=null;
    while (it.hasNext()) {
      LogEntry le=null;
      try {
        le=it.next();
switch (le.getPayloadType()) {
case LogEntry.PAYLOAD_TYPE_INSERT:
          InsertRecordGroup ai=InsertRecordGroup.deserialize(le.getPayload());
        databaseManager.insert(ai);
      break;
case LogEntry.PAYLOAD_TYPE_SNAP:
    ObjectInputStream oin=null;
  try {
    oin=new ObjectInputStream(new ByteArrayInputStream(le.getPayload().array()));
    int dbId=oin.readInt();
    SnapshotConfig snap=(SnapshotConfig)oin.readObject();
    Database db=databaseManager.getDatabase(dbId);
    if (db == null)     break;
    snapshotManager.createPersistentSnapshot(db.getName(),snap,false);
  }
 catch (  Exception e) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + e.getMessage(),e);
  }
 finally {
    if (oin != null)     oin.close();
  }
break;
case LogEntry.PAYLOAD_TYPE_SNAP_DELETE:
byte[] payload=le.getPayload().array();
int offs=payload[0];
String dbName=new String(payload,1,offs);
String snapName=new String(payload,offs + 1,payload.length - offs - 1);
snapshotManager.deletePersistentSnapshot(dbName,snapName,false);
break;
default :
break;
}
nextLSN=new LSN(le.getViewId(),le.getLogSequenceNo() + 1L);
}
  finally {
if (le != null) le.free();
}
}
it.destroy();
if (nextLSN != null) {
return nextLSN;
}
 else {
return new LSN(1,1);
}
}
 catch (IOException ex) {
throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
}
catch (Exception ex) {
if (ex.getCause() instanceof LogEntryException) {
throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex.getCause());
}
 else throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
}
}","/** 
 * Replays the database operations log.
 * @param from - LSN to replay the logs from.
 * @return the LSN to assign to the next operation
 * @throws BabuDBException
 */
private LSN replayLogs(LSN from) throws BabuDBException {
  try {
    File f=new File(configuration.getDbLogDir());
    File[] logFiles=f.listFiles(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.endsWith(""String_Node_Str"");
      }
    }
);
    DiskLogIterator it=new DiskLogIterator(logFiles,from);
    LSN nextLSN=null;
    while (it.hasNext()) {
      LogEntry le=null;
      try {
        le=it.next();
        InMemoryProcessing processingLogic=permMan.getProcessingLogic().get(le.getPayloadType());
        Object[] arguments=processingLogic.deserializeRequest(le.getPayload());
        processingLogic.before(arguments);
        processingLogic.after(arguments);
        nextLSN=new LSN(le.getViewId(),le.getLogSequenceNo() + 1L);
      }
  finally {
        if (le != null)         le.free();
      }
    }
    it.destroy();
    if (nextLSN != null) {
      return nextLSN;
    }
 else {
      return new LSN(1,1);
    }
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
  }
catch (  Exception ex) {
    if (ex.getCause() instanceof LogEntryException) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex.getCause());
    }
 else     throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + ""String_Node_Str"",ex);
  }
}",0.4961280330407847
132325,"/** 
 * Insert an group of inserts in the context of the invoking thread. Proper insertion is not guaranteed, since the result of the attempt to make the insert persistent is ignored, if   {@link SyncMode} is ASYNC.
 * @param irg - the group of inserts.
 * @param listener - to notify after insert.
 */
private void directInsert(BabuDBInsertGroup irg,BabuDBRequestResultImpl<Object> listener){
  try {
    dbs.getPersistenceManager().makePersistent(PAYLOAD_TYPE_INSERT,new Object[]{irg,lsmDB,listener});
    if (dbs.getConfig().getSyncMode() == SyncMode.ASYNC) {
      for (      InsertRecord ir : irg.getRecord().getInserts()) {
        LSMTree index=lsmDB.getIndex(ir.getIndexId());
        if (ir.getValue() != null) {
          index.insert(ir.getKey(),ir.getValue());
        }
 else {
          index.delete(ir.getKey());
        }
      }
      listener.finished();
    }
  }
 catch (  BabuDBException e) {
    listener.failed(e);
  }
}","/** 
 * Insert an group of inserts in the context of the invoking thread. Proper insertion is not guaranteed, since the result of the attempt to make the insert persistent is ignored, if   {@link SyncMode} is ASYNC.
 * @param irg - the group of inserts.
 * @param listener - to notify after insert.
 */
private void directInsert(BabuDBInsertGroup irg,BabuDBRequestResultImpl<Object> listener){
  try {
    dbs.getPersistenceManager().makePersistent(PAYLOAD_TYPE_INSERT,new Object[]{irg.getRecord(),lsmDB,listener});
    if (dbs.getConfig().getSyncMode() == SyncMode.ASYNC) {
      for (      InsertRecord ir : irg.getRecord().getInserts()) {
        LSMTree index=lsmDB.getIndex(ir.getIndexId());
        if (ir.getValue() != null) {
          index.insert(ir.getKey(),ir.getValue());
        }
 else {
          index.delete(ir.getKey());
        }
      }
      listener.finished();
    }
  }
 catch (  BabuDBException e) {
    listener.failed(e);
  }
}",0.9936775553213908
132326,"@Override public void before(Object[] args) throws BabuDBException {
  BabuDBInsertGroup irg=(BabuDBInsertGroup)args[0];
  LSMDatabase lsmDB=(LSMDatabase)args[1];
  if (lsmDB == null) {
    lsmDB=((DatabaseImpl)dbsById.get(irg.getRecord().getDatabaseId())).getLSMDB();
  }
  int numIndices=lsmDB.getIndexCount();
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    if ((ir.getIndexId() >= numIndices) || (ir.getIndexId() < 0)) {
      throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"" + ir.getIndexId() + ""String_Node_Str"");
    }
  }
}","@Override public void before(Object[] args) throws BabuDBException {
  InsertRecordGroup irg=(InsertRecordGroup)args[0];
  LSMDatabase lsmDB=(LSMDatabase)args[1];
  if (lsmDB == null) {
    lsmDB=((DatabaseImpl)dbsById.get(irg.getDatabaseId())).getLSMDB();
  }
  int numIndices=lsmDB.getIndexCount();
  for (  InsertRecord ir : irg.getInserts()) {
    if ((ir.getIndexId() >= numIndices) || (ir.getIndexId() < 0)) {
      throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"" + ir.getIndexId() + ""String_Node_Str"");
    }
  }
}",0.9349005424954792
132327,"/** 
 * Feed the persistenceManager with the knowledge to handle database-modifying related requests.
 */
private void initializePersistenceManager(){
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_CREATE,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      String databaseName=(String)args[0];
      int numIndices=(Integer)args[1];
      ReusableBuffer buf=ReusableBuffer.wrap(new byte[databaseName.getBytes().length + (Integer.SIZE * 3 / 8)]);
      buf.putInt(-1);
      buf.putString(databaseName);
      buf.putInt(numIndices);
      buf.flip();
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      serialized.getInt();
      String dbName=serialized.getString();
      int indices=serialized.getInt();
      serialized.flip();
      return new Object[]{dbName,indices,null};
    }
    @Override public void before(    Object[] args) throws BabuDBException {
      String databaseName=(String)args[0];
      int numIndices=(Integer)args[1];
      ByteRangeComparator[] com=(ByteRangeComparator[])args[2];
      if (com == null) {
        ByteRangeComparator[] comps=new ByteRangeComparator[numIndices];
        final ByteRangeComparator defaultComparator=compInstances.get(DefaultByteRangeComparator.class.getName());
        for (int i=0; i < numIndices; i++) {
          comps[i]=defaultComparator;
        }
        com=comps;
      }
      DatabaseImpl db=null;
synchronized (getDBModificationLock()) {
synchronized (dbs.getCheckpointer()) {
          if (dbsByName.containsKey(databaseName)) {
            throw new BabuDBException(ErrorCode.DB_EXISTS,""String_Node_Str"" + databaseName + ""String_Node_Str"");
          }
          final int dbId=nextDbId++;
          db=new DatabaseImpl(dbs,new LSMDatabase(databaseName,dbId,dbs.getConfig().getBaseDir() + databaseName + File.separatorChar,numIndices,false,com,dbs.getConfig().getCompression(),dbs.getConfig().getMaxNumRecordsPerBlock(),dbs.getConfig().getMaxBlockFileSize(),dbs.getConfig().getDisableMMap(),dbs.getConfig().getMMapLimit()));
          dbsById.put(dbId,db);
          dbsByName.put(databaseName,db);
          dbs.getDBConfigFile().save();
        }
      }
    }
  }
);
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_DELETE,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      String databaseName=(String)args[0];
      ReusableBuffer buf=ReusableBuffer.wrap(new byte[(Integer.SIZE / 2) + databaseName.getBytes().length]);
      buf.putInt(-1);
      buf.putString(databaseName);
      buf.flip();
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      serialized.getInt();
      String dbName=serialized.getString();
      serialized.flip();
      return new Object[]{dbName};
    }
    @Override public void before(    Object[] args) throws BabuDBException {
      String databaseName=(String)args[0];
      int dbId=-1;
synchronized (getDBModificationLock()) {
synchronized (dbs.getCheckpointer()) {
          if (!dbsByName.containsKey(databaseName)) {
            throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"" + databaseName + ""String_Node_Str"");
          }
          final LSMDatabase db=((DatabaseImpl)dbsByName.get(databaseName)).getLSMDB();
          dbId=db.getDatabaseId();
          dbsByName.remove(databaseName);
          dbsById.remove(dbId);
          ((SnapshotManagerImpl)dbs.getSnapshotManager()).deleteAllSnapshots(databaseName);
          dbs.getDBConfigFile().save();
          File dbDir=new File(dbs.getConfig().getBaseDir(),databaseName);
          if (dbDir.exists())           FSUtils.delTree(dbDir);
        }
      }
    }
  }
);
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_COPY,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      String sourceDB=(String)args[0];
      String destDB=(String)args[1];
      ReusableBuffer buf=ReusableBuffer.wrap(new byte[(Integer.SIZE / 2) + sourceDB.getBytes().length + destDB.getBytes().length]);
      buf.putInt(-1);
      buf.putInt(-1);
      buf.putString(sourceDB);
      buf.putString(destDB);
      buf.flip();
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      serialized.getInt();
      serialized.getInt();
      String sourceDB=serialized.getString();
      String destDB=serialized.getString();
      serialized.flip();
      return new Object[]{sourceDB,destDB};
    }
    @Override public void before(    Object[] args) throws BabuDBException {
      String sourceDB=(String)args[0];
      String destDB=(String)args[1];
      DatabaseImpl sDB=(DatabaseImpl)dbsByName.get(sourceDB);
      if (sDB == null) {
        throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"" + sourceDB + ""String_Node_Str"");
      }
      int dbId;
synchronized (getDBModificationLock()) {
synchronized (dbs.getCheckpointer()) {
          if (dbsByName.containsKey(destDB)) {
            throw new BabuDBException(ErrorCode.DB_EXISTS,""String_Node_Str"" + destDB + ""String_Node_Str"");
          }
          dbId=nextDbId++;
          dbsByName.put(destDB,null);
          dbs.getDBConfigFile().save();
        }
      }
      try {
        sDB.proceedSnapshot(destDB);
      }
 catch (      InterruptedException i) {
        throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",i);
      }
      Database newDB=new DatabaseImpl(dbs,new LSMDatabase(destDB,dbId,dbs.getConfig().getBaseDir() + destDB + File.separatorChar,sDB.getLSMDB().getIndexCount(),true,sDB.getComparators(),dbs.getConfig().getCompression(),dbs.getConfig().getMaxNumRecordsPerBlock(),dbs.getConfig().getMaxBlockFileSize(),dbs.getConfig().getDisableMMap(),dbs.getConfig().getMMapLimit()));
synchronized (dbModificationLock) {
        dbsById.put(dbId,newDB);
        dbsByName.put(destDB,newDB);
        dbs.getDBConfigFile().save();
      }
    }
  }
);
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_SNAP,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      int dbId=(Integer)args[0];
      SnapshotConfig snap=(SnapshotConfig)args[1];
      ReusableBuffer buf=null;
      try {
        ByteArrayOutputStream bout=new ByteArrayOutputStream();
        ObjectOutputStream oout=new ObjectOutputStream(bout);
        oout.writeInt(dbId);
        oout.writeObject(snap);
        buf=ReusableBuffer.wrap(bout.toByteArray());
        oout.close();
      }
 catch (      IOException exc) {
        throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + snap.getClass(),exc);
      }
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      ObjectInputStream oin=null;
      try {
        oin=new ObjectInputStream(new ByteArrayInputStream(serialized.array()));
        int dbId=oin.readInt();
        SnapshotConfig snap=(SnapshotConfig)oin.readObject();
        return new Object[]{dbId,snap};
      }
 catch (      Exception e) {
        throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + PAYLOAD_TYPE_SNAP + ""String_Node_Str""+ e.getMessage(),e);
      }
 finally {
        try {
          serialized.flip();
          if (oin != null)           oin.close();
        }
 catch (        IOException ioe) {
        }
      }
    }
  }
);
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_INSERT,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      BabuDBInsertGroup irg=(BabuDBInsertGroup)args[0];
      int size=irg.getRecord().getSize();
      ReusableBuffer buf=BufferPool.allocate(size);
      irg.getRecord().serialize(buf);
      buf.flip();
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      InsertRecordGroup irg=InsertRecordGroup.deserialize(serialized);
      serialized.flip();
      return new Object[]{irg,null,null};
    }
    @Override public void before(    Object[] args) throws BabuDBException {
      BabuDBInsertGroup irg=(BabuDBInsertGroup)args[0];
      LSMDatabase lsmDB=(LSMDatabase)args[1];
      if (lsmDB == null) {
        lsmDB=((DatabaseImpl)dbsById.get(irg.getRecord().getDatabaseId())).getLSMDB();
      }
      int numIndices=lsmDB.getIndexCount();
      for (      InsertRecord ir : irg.getRecord().getInserts()) {
        if ((ir.getIndexId() >= numIndices) || (ir.getIndexId() < 0)) {
          throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"" + ir.getIndexId() + ""String_Node_Str"");
        }
      }
    }
    @SuppressWarnings(""String_Node_Str"") @Override public void after(    Object[] args){
      BabuDBInsertGroup irg=(BabuDBInsertGroup)args[0];
      LSMDatabase lsmDB=(LSMDatabase)args[1];
      if (lsmDB == null) {
        lsmDB=((DatabaseImpl)dbsById.get(irg.getRecord().getDatabaseId())).getLSMDB();
      }
      BabuDBRequestResultImpl<Object> listener=(BabuDBRequestResultImpl<Object>)args[2];
      if (dbs.getConfig().getSyncMode() != SyncMode.ASYNC) {
        for (        InsertRecord ir : irg.getRecord().getInserts()) {
          LSMTree index=lsmDB.getIndex(ir.getIndexId());
          if (ir.getValue() != null) {
            index.insert(ir.getKey(),ir.getValue());
          }
 else {
            index.delete(ir.getKey());
          }
        }
        if (listener != null)         listener.finished();
      }
    }
  }
);
}","/** 
 * Feed the persistenceManager with the knowledge to handle database-modifying related requests.
 */
private void initializePersistenceManager(){
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_CREATE,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      String databaseName=(String)args[0];
      int numIndices=(Integer)args[1];
      ReusableBuffer buf=ReusableBuffer.wrap(new byte[databaseName.getBytes().length + (Integer.SIZE * 3 / 8)]);
      buf.putInt(-1);
      buf.putString(databaseName);
      buf.putInt(numIndices);
      buf.flip();
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      serialized.getInt();
      String dbName=serialized.getString();
      int indices=serialized.getInt();
      serialized.flip();
      return new Object[]{dbName,indices,null};
    }
    @Override public void before(    Object[] args) throws BabuDBException {
      String databaseName=(String)args[0];
      int numIndices=(Integer)args[1];
      ByteRangeComparator[] com=(ByteRangeComparator[])args[2];
      if (com == null) {
        ByteRangeComparator[] comps=new ByteRangeComparator[numIndices];
        final ByteRangeComparator defaultComparator=compInstances.get(DefaultByteRangeComparator.class.getName());
        for (int i=0; i < numIndices; i++) {
          comps[i]=defaultComparator;
        }
        com=comps;
      }
      DatabaseImpl db=null;
synchronized (getDBModificationLock()) {
synchronized (dbs.getCheckpointer()) {
          if (dbsByName.containsKey(databaseName)) {
            throw new BabuDBException(ErrorCode.DB_EXISTS,""String_Node_Str"" + databaseName + ""String_Node_Str"");
          }
          final int dbId=nextDbId++;
          db=new DatabaseImpl(dbs,new LSMDatabase(databaseName,dbId,dbs.getConfig().getBaseDir() + databaseName + File.separatorChar,numIndices,false,com,dbs.getConfig().getCompression(),dbs.getConfig().getMaxNumRecordsPerBlock(),dbs.getConfig().getMaxBlockFileSize(),dbs.getConfig().getDisableMMap(),dbs.getConfig().getMMapLimit()));
          dbsById.put(dbId,db);
          dbsByName.put(databaseName,db);
          dbs.getDBConfigFile().save();
        }
      }
    }
  }
);
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_DELETE,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      String databaseName=(String)args[0];
      ReusableBuffer buf=ReusableBuffer.wrap(new byte[(Integer.SIZE / 2) + databaseName.getBytes().length]);
      buf.putInt(-1);
      buf.putString(databaseName);
      buf.flip();
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      serialized.getInt();
      String dbName=serialized.getString();
      serialized.flip();
      return new Object[]{dbName};
    }
    @Override public void before(    Object[] args) throws BabuDBException {
      String databaseName=(String)args[0];
      int dbId=-1;
synchronized (getDBModificationLock()) {
synchronized (dbs.getCheckpointer()) {
          if (!dbsByName.containsKey(databaseName)) {
            throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"" + databaseName + ""String_Node_Str"");
          }
          final LSMDatabase db=((DatabaseImpl)dbsByName.get(databaseName)).getLSMDB();
          dbId=db.getDatabaseId();
          dbsByName.remove(databaseName);
          dbsById.remove(dbId);
          ((SnapshotManagerImpl)dbs.getSnapshotManager()).deleteAllSnapshots(databaseName);
          dbs.getDBConfigFile().save();
          File dbDir=new File(dbs.getConfig().getBaseDir(),databaseName);
          if (dbDir.exists())           FSUtils.delTree(dbDir);
        }
      }
    }
  }
);
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_COPY,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      String sourceDB=(String)args[0];
      String destDB=(String)args[1];
      ReusableBuffer buf=ReusableBuffer.wrap(new byte[(Integer.SIZE / 2) + sourceDB.getBytes().length + destDB.getBytes().length]);
      buf.putInt(-1);
      buf.putInt(-1);
      buf.putString(sourceDB);
      buf.putString(destDB);
      buf.flip();
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      serialized.getInt();
      serialized.getInt();
      String sourceDB=serialized.getString();
      String destDB=serialized.getString();
      serialized.flip();
      return new Object[]{sourceDB,destDB};
    }
    @Override public void before(    Object[] args) throws BabuDBException {
      String sourceDB=(String)args[0];
      String destDB=(String)args[1];
      DatabaseImpl sDB=(DatabaseImpl)dbsByName.get(sourceDB);
      if (sDB == null) {
        throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"" + sourceDB + ""String_Node_Str"");
      }
      int dbId;
synchronized (getDBModificationLock()) {
synchronized (dbs.getCheckpointer()) {
          if (dbsByName.containsKey(destDB)) {
            throw new BabuDBException(ErrorCode.DB_EXISTS,""String_Node_Str"" + destDB + ""String_Node_Str"");
          }
          dbId=nextDbId++;
          dbsByName.put(destDB,null);
          dbs.getDBConfigFile().save();
        }
      }
      try {
        sDB.proceedSnapshot(destDB);
      }
 catch (      InterruptedException i) {
        throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",i);
      }
      Database newDB=new DatabaseImpl(dbs,new LSMDatabase(destDB,dbId,dbs.getConfig().getBaseDir() + destDB + File.separatorChar,sDB.getLSMDB().getIndexCount(),true,sDB.getComparators(),dbs.getConfig().getCompression(),dbs.getConfig().getMaxNumRecordsPerBlock(),dbs.getConfig().getMaxBlockFileSize(),dbs.getConfig().getDisableMMap(),dbs.getConfig().getMMapLimit()));
synchronized (dbModificationLock) {
        dbsById.put(dbId,newDB);
        dbsByName.put(destDB,newDB);
        dbs.getDBConfigFile().save();
      }
    }
  }
);
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_SNAP,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      int dbId=(Integer)args[0];
      SnapshotConfig snap=(SnapshotConfig)args[1];
      ReusableBuffer buf=null;
      try {
        ByteArrayOutputStream bout=new ByteArrayOutputStream();
        ObjectOutputStream oout=new ObjectOutputStream(bout);
        oout.writeInt(dbId);
        oout.writeObject(snap);
        buf=ReusableBuffer.wrap(bout.toByteArray());
        oout.close();
      }
 catch (      IOException exc) {
        throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + snap.getClass(),exc);
      }
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      ObjectInputStream oin=null;
      try {
        oin=new ObjectInputStream(new ByteArrayInputStream(serialized.array()));
        int dbId=oin.readInt();
        SnapshotConfig snap=(SnapshotConfig)oin.readObject();
        return new Object[]{dbId,snap};
      }
 catch (      Exception e) {
        throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + PAYLOAD_TYPE_SNAP + ""String_Node_Str""+ e.getMessage(),e);
      }
 finally {
        try {
          serialized.flip();
          if (oin != null)           oin.close();
        }
 catch (        IOException ioe) {
        }
      }
    }
  }
);
  dbs.getPersistenceManager().registerInMemoryProcessing(PAYLOAD_TYPE_INSERT,new InMemoryProcessing(){
    @Override public ReusableBuffer serializeRequest(    Object[] args) throws BabuDBException {
      InsertRecordGroup irg=(InsertRecordGroup)args[0];
      int size=irg.getSize();
      ReusableBuffer buf=BufferPool.allocate(size);
      irg.serialize(buf);
      buf.flip();
      return buf;
    }
    @Override public Object[] deserializeRequest(    ReusableBuffer serialized) throws BabuDBException {
      InsertRecordGroup irg=InsertRecordGroup.deserialize(serialized);
      serialized.flip();
      return new Object[]{irg,null,null};
    }
    @Override public void before(    Object[] args) throws BabuDBException {
      InsertRecordGroup irg=(InsertRecordGroup)args[0];
      LSMDatabase lsmDB=(LSMDatabase)args[1];
      if (lsmDB == null) {
        lsmDB=((DatabaseImpl)dbsById.get(irg.getDatabaseId())).getLSMDB();
      }
      int numIndices=lsmDB.getIndexCount();
      for (      InsertRecord ir : irg.getInserts()) {
        if ((ir.getIndexId() >= numIndices) || (ir.getIndexId() < 0)) {
          throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"" + ir.getIndexId() + ""String_Node_Str"");
        }
      }
    }
    @SuppressWarnings(""String_Node_Str"") @Override public void after(    Object[] args){
      InsertRecordGroup irg=(InsertRecordGroup)args[0];
      LSMDatabase lsmDB=(LSMDatabase)args[1];
      if (lsmDB == null) {
        lsmDB=((DatabaseImpl)dbsById.get(irg.getDatabaseId())).getLSMDB();
      }
      BabuDBRequestResultImpl<Object> listener=(BabuDBRequestResultImpl<Object>)args[2];
      if (dbs.getConfig().getSyncMode() != SyncMode.ASYNC) {
        for (        InsertRecord ir : irg.getInserts()) {
          LSMTree index=lsmDB.getIndex(ir.getIndexId());
          if (ir.getValue() != null) {
            index.insert(ir.getKey(),ir.getValue());
          }
 else {
            index.delete(ir.getKey());
          }
        }
        if (listener != null)         listener.finished();
      }
    }
  }
);
}",0.982522101412458
132328,"@Override public ReusableBuffer serializeRequest(Object[] args) throws BabuDBException {
  BabuDBInsertGroup irg=(BabuDBInsertGroup)args[0];
  int size=irg.getRecord().getSize();
  ReusableBuffer buf=BufferPool.allocate(size);
  irg.getRecord().serialize(buf);
  buf.flip();
  return buf;
}","@Override public ReusableBuffer serializeRequest(Object[] args) throws BabuDBException {
  InsertRecordGroup irg=(InsertRecordGroup)args[0];
  int size=irg.getSize();
  ReusableBuffer buf=BufferPool.allocate(size);
  irg.serialize(buf);
  buf.flip();
  return buf;
}",0.9136690647482014
132329,"@SuppressWarnings(""String_Node_Str"") @Override public void after(Object[] args){
  BabuDBInsertGroup irg=(BabuDBInsertGroup)args[0];
  LSMDatabase lsmDB=(LSMDatabase)args[1];
  if (lsmDB == null) {
    lsmDB=((DatabaseImpl)dbsById.get(irg.getRecord().getDatabaseId())).getLSMDB();
  }
  BabuDBRequestResultImpl<Object> listener=(BabuDBRequestResultImpl<Object>)args[2];
  if (dbs.getConfig().getSyncMode() != SyncMode.ASYNC) {
    for (    InsertRecord ir : irg.getRecord().getInserts()) {
      LSMTree index=lsmDB.getIndex(ir.getIndexId());
      if (ir.getValue() != null) {
        index.insert(ir.getKey(),ir.getValue());
      }
 else {
        index.delete(ir.getKey());
      }
    }
    if (listener != null)     listener.finished();
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void after(Object[] args){
  InsertRecordGroup irg=(InsertRecordGroup)args[0];
  LSMDatabase lsmDB=(LSMDatabase)args[1];
  if (lsmDB == null) {
    lsmDB=((DatabaseImpl)dbsById.get(irg.getDatabaseId())).getLSMDB();
  }
  BabuDBRequestResultImpl<Object> listener=(BabuDBRequestResultImpl<Object>)args[2];
  if (dbs.getConfig().getSyncMode() != SyncMode.ASYNC) {
    for (    InsertRecord ir : irg.getInserts()) {
      LSMTree index=lsmDB.getIndex(ir.getIndexId());
      if (ir.getValue() != null) {
        index.insert(ir.getKey(),ir.getValue());
      }
 else {
        index.delete(ir.getKey());
      }
    }
    if (listener != null)     listener.finished();
  }
}",0.9510869565217392
132330,"/** 
 * Load the most recent snapshots of each tree.
 * @param numIndices the number of indices to read.
 * @throws java.io.IOException if the on-disk data cannot be read
 */
private void loadFromDisk(int numIndices) throws BabuDBException {
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName + ""String_Node_Str"");
  for (int index=0; index < numIndices; index++) {
    trees.add(null);
  }
  for (int index=0; index < numIndices; index++) {
    final int idx=index;
    File f=new File(databaseDir);
    String[] files=f.list(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.startsWith(""String_Node_Str"" + idx + ""String_Node_Str"");
      }
    }
);
    if (files == null)     throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + databaseDir + ""String_Node_Str"");
    int maxView=-1;
    int maxSeq=-1;
    Pattern p=Pattern.compile(SNAPSHOT_FILENAME_REGEXP);
    for (    String fname : files) {
      Matcher m=p.matcher(fname);
      m.matches();
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + fname);
      int view=Integer.valueOf(m.group(2));
      int seq=Integer.valueOf(m.group(3));
      if (view > maxView) {
        maxView=view;
        maxSeq=seq;
      }
 else       if (view == maxView) {
        if (seq > maxSeq)         maxSeq=seq;
      }
    }
    try {
      if (maxView > -1) {
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName + ""String_Node_Str""+ databaseDir+ File.separator+ ""String_Node_Str""+ index+ ""String_Node_Str""+ maxView+ ""String_Node_Str""+ maxSeq);
        assert(comparators[index] != null);
        trees.set(index,new LSMTree(databaseDir + File.separator + getSnapshotFilename(index,maxView,maxSeq),comparators[index],this.compression,this.maxEntriesPerBlock,this.maxBlockFileSize,!this.disableMMap,this.mmapLimit));
        ondiskLSN=new LSN(maxView,maxSeq);
      }
 else {
        ondiskLSN=NO_DB_LSN;
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName);
        assert(comparators[index] != null);
        trees.set(index,new LSMTree(null,comparators[index],this.compression,this.maxEntriesPerBlock,this.maxBlockFileSize,!this.disableMMap,this.mmapLimit));
      }
    }
 catch (    IOException ex) {
      Logging.logError(Logging.LEVEL_ERROR,this,ex);
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
  }
}","/** 
 * Load the most recent snapshots of each tree.
 * @param numIndices the number of indices to read.
 * @throws java.io.IOException if the on-disk data cannot be read
 */
private void loadFromDisk(int numIndices) throws BabuDBException {
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName + ""String_Node_Str"");
  for (int index=0; index < numIndices; index++) {
    trees.add(null);
  }
  for (int index=0; index < numIndices; index++) {
    final int idx=index;
    File f=new File(databaseDir);
    String[] files=f.list(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.startsWith(""String_Node_Str"" + idx + ""String_Node_Str"");
      }
    }
);
    if (files == null)     throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + databaseDir + ""String_Node_Str"");
    int maxView=-1;
    long maxSeq=-1;
    Pattern p=Pattern.compile(SNAPSHOT_FILENAME_REGEXP);
    for (    String fname : files) {
      Matcher m=p.matcher(fname);
      m.matches();
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + fname);
      int view=Integer.valueOf(m.group(2));
      long seq=Long.valueOf(m.group(3));
      if (view > maxView) {
        maxView=view;
        maxSeq=seq;
      }
 else       if (view == maxView) {
        if (seq > maxSeq)         maxSeq=seq;
      }
    }
    try {
      if (maxView > -1) {
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName + ""String_Node_Str""+ databaseDir+ File.separator+ ""String_Node_Str""+ index+ ""String_Node_Str""+ maxView+ ""String_Node_Str""+ maxSeq);
        assert(comparators[index] != null);
        trees.set(index,new LSMTree(databaseDir + File.separator + getSnapshotFilename(index,maxView,maxSeq),comparators[index],this.compression,this.maxEntriesPerBlock,this.maxBlockFileSize,!this.disableMMap,this.mmapLimit));
        ondiskLSN=new LSN(maxView,maxSeq);
      }
 else {
        ondiskLSN=NO_DB_LSN;
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName);
        assert(comparators[index] != null);
        trees.set(index,new LSMTree(null,comparators[index],this.compression,this.maxEntriesPerBlock,this.maxBlockFileSize,!this.disableMMap,this.mmapLimit));
      }
    }
 catch (    IOException ex) {
      Logging.logError(Logging.LEVEL_ERROR,this,ex);
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
  }
}",0.9949647532729105
132331,"private boolean useMmap(){
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + OutputUtils.formatBytes(totalOnDiskSize));
  return useMMap && totalOnDiskSize < mmapLimitBytes;
}","private boolean useMmap(){
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + OutputUtils.formatBytes(totalOnDiskSize));
  return useMMap && (mmapLimitBytes < 0 || totalOnDiskSize < mmapLimitBytes);
}",0.9408866995073892
132332,"/** 
 * Replay the database operations log.
 * @param from - LSN to replay the logs from.
 * @return the LSN to assign to the next operation
 * @throws BabuDBException
 */
private LSN replayLogs(LSN from) throws BabuDBException {
  try {
    File f=new File(configuration.getDbLogDir());
    File[] logFiles=f.listFiles(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.endsWith(""String_Node_Str"");
      }
    }
);
    DiskLogIterator it=new DiskLogIterator(logFiles,from);
    LSN nextLSN=null;
    while (it.hasNext()) {
      LogEntry le=null;
      try {
        le=it.next();
switch (le.getPayloadType()) {
case LogEntry.PAYLOAD_TYPE_INSERT:
          InsertRecordGroup ai=InsertRecordGroup.deserialize(le.getPayload());
        databaseManager.insert(ai);
      break;
case LogEntry.PAYLOAD_TYPE_SNAP:
    ObjectInputStream oin=null;
  try {
    oin=new ObjectInputStream(new ByteArrayInputStream(le.getPayload().array()));
    int dbId=oin.readInt();
    SnapshotConfig snap=(SnapshotConfig)oin.readObject();
    Database db=databaseManager.getDatabase(dbId);
    if (db == null)     break;
    snapshotManager.createPersistentSnapshot(db.getName(),snap,false);
  }
 catch (  Exception e) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + e.getMessage(),e);
  }
 finally {
    if (oin != null)     oin.close();
  }
break;
case LogEntry.PAYLOAD_TYPE_SNAP_DELETE:
byte[] payload=le.getPayload().array();
int offs=payload[0];
String dbName=new String(payload,1,offs);
String snapName=new String(payload,offs + 1,payload.length - offs - 1);
snapshotManager.deletePersistentSnapshot(dbName,snapName,false);
break;
default :
break;
}
nextLSN=new LSN(le.getViewId(),le.getLogSequenceNo() + 1L);
}
  finally {
if (le != null) le.free();
}
}
if (nextLSN != null) {
return nextLSN;
}
 else {
return new LSN(1,1);
}
}
 catch (IOException ex) {
throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
}
catch (Exception ex) {
if (ex.getCause() instanceof LogEntryException) {
throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex.getCause());
}
 else throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
}
}","/** 
 * Replay the database operations log.
 * @param from - LSN to replay the logs from.
 * @return the LSN to assign to the next operation
 * @throws BabuDBException
 */
private LSN replayLogs(LSN from) throws BabuDBException {
  try {
    File f=new File(configuration.getDbLogDir());
    File[] logFiles=f.listFiles(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.endsWith(""String_Node_Str"");
      }
    }
);
    DiskLogIterator it=new DiskLogIterator(logFiles,from);
    LSN nextLSN=null;
    while (it.hasNext()) {
      LogEntry le=null;
      try {
        le=it.next();
switch (le.getPayloadType()) {
case LogEntry.PAYLOAD_TYPE_INSERT:
          InsertRecordGroup ai=InsertRecordGroup.deserialize(le.getPayload());
        databaseManager.insert(ai);
      break;
case LogEntry.PAYLOAD_TYPE_SNAP:
    ObjectInputStream oin=null;
  try {
    oin=new ObjectInputStream(new ByteArrayInputStream(le.getPayload().array()));
    int dbId=oin.readInt();
    SnapshotConfig snap=(SnapshotConfig)oin.readObject();
    Database db=databaseManager.getDatabase(dbId);
    if (db == null)     break;
    snapshotManager.createPersistentSnapshot(db.getName(),snap,false);
  }
 catch (  Exception e) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + e.getMessage(),e);
  }
 finally {
    if (oin != null)     oin.close();
  }
break;
case LogEntry.PAYLOAD_TYPE_SNAP_DELETE:
byte[] payload=le.getPayload().array();
int offs=payload[0];
String dbName=new String(payload,1,offs);
String snapName=new String(payload,offs + 1,payload.length - offs - 1);
snapshotManager.deletePersistentSnapshot(dbName,snapName,false);
break;
default :
break;
}
nextLSN=new LSN(le.getViewId(),le.getLogSequenceNo() + 1L);
}
  finally {
if (le != null) le.free();
}
}
it.destroy();
if (nextLSN != null) {
return nextLSN;
}
 else {
return new LSN(1,1);
}
}
 catch (IOException ex) {
throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
}
catch (Exception ex) {
if (ex.getCause() instanceof LogEntryException) {
throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex.getCause());
}
 else throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
}
}",0.9968539325842696
132333,"public void destroy() throws IOException {
  LogEntry tmp=nextEntry;
  nextEntry=null;
  if (tmp != null)   tmp.free();
  currentFile.close();
}","public void destroy() throws IOException {
  LogEntry tmp=nextEntry;
  nextEntry=null;
  if (tmp != null)   tmp.free();
  if (currentFile != null)   currentFile.close();
}",0.9142857142857144
132334,"protected LogEntry findNextEntry() throws IOException, LogEntryException {
  if (logList == null)   return null;
  if (currentFile.hasNext())   return currentFile.next();
  if (!logList.hasNext())   return null;
  currentFile.close();
  if (!logList.hasNext())   return null;
  currentLog=logList.next();
  currentFile=new DiskLogFile(dbLogDir,currentLog);
  return findNextEntry();
}","protected LogEntry findNextEntry() throws IOException, LogEntryException {
  if (logList == null)   return null;
  if (currentFile.hasNext())   return currentFile.next();
  currentFile.close();
  if (!logList.hasNext())   return null;
  currentLog=logList.next();
  currentFile=new DiskLogFile(dbLogDir,currentLog);
  return findNextEntry();
}",0.8803301237964236
132335,"protected void findFirstEntry() throws IOException, LogEntryException {
  if (logList == null || !logList.hasNext())   return;
  do {
    currentLog=logList.next();
    currentFile=new DiskLogFile(dbLogDir,currentLog);
  }
 while (!currentFile.hasNext() && logList.hasNext());
  while (currentFile.hasNext()) {
    LogEntry le=currentFile.next();
    if (from == null || le.getLSN().compareTo(from) >= 0) {
      nextEntry=le;
      break;
    }
    le.free();
  }
}","protected void findFirstEntry() throws IOException, LogEntryException {
  if (logList == null || !logList.hasNext())   return;
  do {
    currentLog=logList.next();
    if (currentFile != null)     currentFile.close();
    currentFile=new DiskLogFile(dbLogDir,currentLog);
  }
 while (!currentFile.hasNext() && logList.hasNext());
  while (currentFile.hasNext()) {
    LogEntry le=currentFile.next();
    if (from == null || le.getLSN().compareTo(from) >= 0) {
      nextEntry=le;
      break;
    }
    le.free();
  }
}",0.9452332657200812
132336,"@Test public void testDefectiveEntries() throws Exception {
  File logFile=new File(testdir + ""String_Node_Str"");
  int[] offsets=new int[100];
  final AtomicInteger count=new AtomicInteger(0);
  SyncListener sl=new SyncListener(){
    public void synced(    LogEntry entry){
synchronized (entry) {
        count.incrementAndGet();
        entry.notifyAll();
      }
    }
    public void failed(    LogEntry entry,    Exception ex){
synchronized (entry) {
        count.incrementAndGet();
        entry.notifyAll();
      }
    }
  }
;
  LogEntry e=null;
  for (int i=0; i < 100; i++) {
    String pl=""String_Node_Str"" + (i + 1);
    ReusableBuffer plb=ReusableBuffer.wrap(pl.getBytes());
    e=new LogEntry(plb,sl,LogEntry.PAYLOAD_TYPE_INSERT);
    l.append(e);
    if (i < 99)     offsets[i + 1]=offsets[i] + LogEntry.headerLength + e.getPayload().remaining();
  }
synchronized (e) {
    if (count.get() < 100)     e.wait(1000);
  }
  e.free();
  System.out.println(""String_Node_Str"");
  try {
    l.lockLogger();
    l.switchLogFile(false);
  }
  finally {
    l.unlockLogger();
  }
  File tmpFile=new File(testdir + ""String_Node_Str"");
  FSUtils.copyTree(logFile,tmpFile);
  RandomAccessFile raf=new RandomAccessFile(tmpFile.getAbsolutePath(),""String_Node_Str"");
  raf.seek(Integer.SIZE / 8);
  raf.writeInt(999999);
  raf.close();
  DiskLogFile f=new DiskLogFile(tmpFile.getAbsolutePath());
  assertFalse(f.hasNext());
  tmpFile.delete();
  FSUtils.copyTree(logFile,tmpFile);
  raf=new RandomAccessFile(tmpFile.getAbsoluteFile(),""String_Node_Str"");
  raf.writeInt(2);
  raf.close();
  f=new DiskLogFile(tmpFile.getAbsolutePath());
  assertFalse(f.hasNext());
  tmpFile.delete();
  FSUtils.copyTree(logFile,tmpFile);
  raf=new RandomAccessFile(tmpFile.getAbsolutePath(),""String_Node_Str"");
  raf.seek(offsets[50]);
  raf.writeInt(79);
  raf.close();
  f=new DiskLogFile(tmpFile.getAbsolutePath());
  for (int i=0; i < 50; i++) {
    LogEntry next=f.next();
    assertNotNull(next);
    next.free();
  }
  assertFalse(f.hasNext());
  tmpFile.delete();
  FSUtils.copyTree(logFile,tmpFile);
  raf=new RandomAccessFile(tmpFile.getAbsolutePath(),""String_Node_Str"");
  raf.seek(offsets[50]);
  raf.writeInt(-122);
  raf.close();
  f=new DiskLogFile(tmpFile.getAbsolutePath());
  for (int i=0; i < 50; i++) {
    LogEntry next=f.next();
    assertNotNull(next);
    next.free();
  }
  assertFalse(f.hasNext());
  tmpFile.delete();
  FSUtils.copyTree(logFile,tmpFile);
  raf=new RandomAccessFile(tmpFile.getAbsolutePath(),""String_Node_Str"");
  raf.getChannel().truncate(offsets[99] + 5);
  raf.close();
  f=new DiskLogFile(tmpFile.getAbsolutePath());
  for (int i=0; i < 99; i++) {
    LogEntry next=f.next();
    assertNotNull(next);
    next.free();
  }
  assertFalse(f.hasNext());
  logFile.delete();
  FSUtils.copyTree(tmpFile,logFile);
  tmpFile.delete();
  l.shutdown();
  l=new DiskLogger(testdir,1,200,SyncMode.FSYNC,0,0,null);
  l.start();
  e=null;
  for (int i=99; i < 120; i++) {
    String pl=""String_Node_Str"" + (i + 1);
    ReusableBuffer plb=ReusableBuffer.wrap(pl.getBytes());
    e=new LogEntry(plb,sl,LogEntry.PAYLOAD_TYPE_INSERT);
    l.append(e);
  }
synchronized (e) {
    if (count.get() < 121)     e.wait(1000);
  }
  e.free();
  File[] logFiles=new File(testdir).listFiles();
  DiskLogIterator it=new DiskLogIterator(logFiles,null);
  for (int i=0; i < 120; i++) {
    LogEntry next=it.next();
    assertNotNull(next);
    next.free();
  }
  assertFalse(it.hasNext());
}","@Test public void testDefectiveEntries() throws Exception {
  File logFile=new File(testdir + ""String_Node_Str"");
  int[] offsets=new int[100];
  final AtomicInteger count=new AtomicInteger(0);
  SyncListener sl=new SyncListener(){
    public void synced(    LogEntry entry){
synchronized (entry) {
        count.incrementAndGet();
        entry.notifyAll();
      }
    }
    public void failed(    LogEntry entry,    Exception ex){
synchronized (entry) {
        count.incrementAndGet();
        entry.notifyAll();
      }
    }
  }
;
  LogEntry e=null;
  for (int i=0; i < 100; i++) {
    String pl=""String_Node_Str"" + (i + 1);
    ReusableBuffer plb=ReusableBuffer.wrap(pl.getBytes());
    e=new LogEntry(plb,sl,LogEntry.PAYLOAD_TYPE_INSERT);
    l.append(e);
    if (i < 99)     offsets[i + 1]=offsets[i] + LogEntry.headerLength + e.getPayload().remaining();
  }
synchronized (e) {
    if (count.get() < 100)     e.wait(1000);
  }
  e.free();
  System.out.println(""String_Node_Str"");
  try {
    l.lockLogger();
    l.switchLogFile(false);
  }
  finally {
    l.unlockLogger();
  }
  File tmpFile=new File(testdir + ""String_Node_Str"");
  copyFile(logFile,tmpFile);
  RandomAccessFile raf=new RandomAccessFile(tmpFile.getAbsolutePath(),""String_Node_Str"");
  raf.seek(Integer.SIZE / 8);
  raf.writeInt(999999);
  raf.close();
  DiskLogFile f=new DiskLogFile(tmpFile.getAbsolutePath());
  assertFalse(f.hasNext());
  f.close();
  assertTrue(tmpFile.delete());
  copyFile(logFile,tmpFile);
  raf=new RandomAccessFile(tmpFile.getAbsoluteFile(),""String_Node_Str"");
  raf.writeInt(2);
  raf.close();
  f=new DiskLogFile(tmpFile.getAbsolutePath());
  assertFalse(f.hasNext());
  f.close();
  assertTrue(tmpFile.delete());
  copyFile(logFile,tmpFile);
  raf=new RandomAccessFile(tmpFile.getAbsolutePath(),""String_Node_Str"");
  raf.seek(offsets[50]);
  raf.writeInt(79);
  raf.close();
  f=new DiskLogFile(tmpFile.getAbsolutePath());
  for (int i=0; i < 50; i++) {
    LogEntry next=f.next();
    assertNotNull(next);
    next.free();
  }
  assertFalse(f.hasNext());
  f.close();
  assertTrue(tmpFile.delete());
  copyFile(logFile,tmpFile);
  raf=new RandomAccessFile(tmpFile.getAbsolutePath(),""String_Node_Str"");
  raf.seek(offsets[50]);
  raf.writeInt(-122);
  raf.close();
  f=new DiskLogFile(tmpFile.getAbsolutePath());
  for (int i=0; i < 50; i++) {
    LogEntry next=f.next();
    assertNotNull(next);
    next.free();
  }
  assertFalse(f.hasNext());
  f.close();
  assertTrue(tmpFile.delete());
  copyFile(logFile,tmpFile);
  raf=new RandomAccessFile(tmpFile.getAbsolutePath(),""String_Node_Str"");
  raf.getChannel().truncate(offsets[99] + 5);
  raf.close();
  f=new DiskLogFile(tmpFile.getAbsolutePath());
  for (int i=0; i < 99; i++) {
    LogEntry next=f.next();
    assertNotNull(next);
    next.free();
  }
  assertFalse(f.hasNext());
  f.close();
  logFile.delete();
  copyFile(tmpFile,logFile);
  tmpFile.delete();
  l.shutdown();
  l=new DiskLogger(testdir,1,200,SyncMode.FSYNC,0,0,null);
  l.start();
  e=null;
  for (int i=99; i < 120; i++) {
    String pl=""String_Node_Str"" + (i + 1);
    ReusableBuffer plb=ReusableBuffer.wrap(pl.getBytes());
    e=new LogEntry(plb,sl,LogEntry.PAYLOAD_TYPE_INSERT);
    l.append(e);
  }
synchronized (e) {
    if (count.get() < 121)     e.wait(1000);
  }
  e.free();
  File[] logFiles=new File(testdir).listFiles();
  DiskLogIterator it=new DiskLogIterator(logFiles,null);
  for (int i=0; i < 120; i++) {
    LogEntry next=it.next();
    assertNotNull(next);
    next.free();
  }
  assertFalse(it.hasNext());
}",0.9550290821393106
132337,"@After public void tearDown(){
  l.shutdown();
}","@After public void tearDown() throws Exception {
  l.shutdown();
  l.waitForShutdown();
  try {
    l.finalize();
  }
 catch (  Throwable th) {
    throw new Exception(th);
  }
}",0.4247787610619469
132338,"/** 
 * Creates a new multi-overlay tree.
 * @param nullValue A value that will never be inserted in the tree. This value will be used to mark entries as deleted.
 * @param comparator The comparator for the keys. If a <code>null</code> comparator is provided, the natural ordering of the keys will be used if defined.
 */
public MultiOverlayTree(V nullValue,Comparator<K> comparator){
  if (comparator == null) {
    this.comparator=new Comparator<K>(){
      public int compare(      K o1,      K o2){
        return ((Comparable<K>)o1).compareTo(o2);
      }
    }
;
  }
 else   this.comparator=comparator;
  treeList=new OverlayTreeList<K,V>(new TreeMap<K,V>(comparator),null);
  overlayMap=new HashMap<Integer,OverlayTreeList<K,V>>();
  this.nullValue=nullValue;
}","/** 
 * Creates a new multi-overlay tree.
 * @param nullValue A value that will never be inserted in the tree. This value will be used to mark entries as deleted.
 * @param comparator The comparator for the keys. If a <code>null</code> comparator is provided, the natural ordering of the keys will be used if defined.
 */
public MultiOverlayTree(V nullValue,Comparator<K> comparator){
  if (comparator == null) {
    this.comparator=new Comparator<K>(){
      public int compare(      K o1,      K o2){
        return ((Comparable<K>)o1).compareTo(o2);
      }
    }
;
  }
 else   this.comparator=comparator;
  treeList=new OverlayTreeList<K,V>(new TreeMap<K,V>(comparator),null);
  overlayMap=Collections.synchronizedMap(new HashMap<Integer,OverlayTreeList<K,V>>());
  this.nullValue=nullValue;
}",0.9814696485623003
132339,"public void free(){
  if (!isBuffered)   BufferPool.free(readBuffer);
}","public void free(){
  if (readBuffer != null)   BufferPool.free(readBuffer);
}",0.8859060402684564
132340,"public byte[] lookup(byte[] key){
  int indexPosition=getBlockIndexPosition(key,blockIndex);
  if (indexPosition == -1)   return null;
  int startBlockOffset=getBlockOffset(indexPosition,blockIndex);
  int fileId=getBlockFileId(indexPosition,blockIndex);
  int endBlockOffset;
  if (indexPosition == blockIndex.getNumEntries() - 1)   endBlockOffset=-1;
 else {
    ByteRange indexPos=getBlockEntry(indexPosition + 1,blockIndex);
    ByteBuffer indexPosBuf=indexPos.getBuf();
    endBlockOffset=getBlockIndexOffset(indexPosBuf,indexPos.getStartOffset());
    if (getBlockIndexFileId(indexPosBuf,indexPos.getStartOffset()) > fileId)     endBlockOffset=-1;
  }
  BlockReader targetBlock=null;
  try {
    targetBlock=mmaped ? getBlock(startBlockOffset,endBlockOffset,dbFiles[fileId]) : getBlock(startBlockOffset,endBlockOffset,dbFileChannels[fileId]);
  }
 catch (  IOException e) {
    Logging.logError(Logging.LEVEL_ERROR,this,e);
  }
  ByteRange val=targetBlock.lookup(key);
  targetBlock.free();
  return val == null ? null : val.toBuffer();
}","public byte[] lookup(byte[] key){
  int indexPosition=getBlockIndexPosition(key,blockIndex);
  if (indexPosition == -1)   return null;
  int startBlockOffset=getBlockOffset(indexPosition,blockIndex);
  int fileId=getBlockFileId(indexPosition,blockIndex);
  int endBlockOffset;
  if (indexPosition == blockIndex.getNumEntries() - 1)   endBlockOffset=-1;
 else {
    ByteRange indexPos=getBlockEntry(indexPosition + 1,blockIndex);
    ByteBuffer indexPosBuf=indexPos.getBuf();
    endBlockOffset=getBlockIndexOffset(indexPosBuf,indexPos.getStartOffset());
    if (getBlockIndexFileId(indexPosBuf,indexPos.getStartOffset()) > fileId)     endBlockOffset=-1;
  }
  BlockReader targetBlock=null;
  try {
    targetBlock=mmaped ? getBlock(startBlockOffset,endBlockOffset,dbFiles[fileId]) : getBlock(startBlockOffset,endBlockOffset,dbFileChannels[fileId]);
  }
 catch (  IOException e) {
    Logging.logError(Logging.LEVEL_ERROR,this,e);
  }
  ByteRange val=targetBlock.lookup(key);
  byte[] result=val == null ? null : val.toBuffer();
  targetBlock.free();
  return result;
}",0.9299242424242424
132341,"@Override public Entry<byte[],byte[]> next(){
  if (!hasNext())   throw new NoSuchElementException();
  final Entry<ByteRange,ByteRange> entry=currentBlockIterator.next();
  return new Entry<byte[],byte[]>(){
    private byte[] key;
    private byte[] value;
{
      key=entry.getKey().toBuffer();
      value=entry.getValue().toBuffer();
    }
    @Override public byte[] getKey(){
      return key;
    }
    @Override public byte[] getValue(){
      return value;
    }
    @Override public byte[] setValue(    byte[] value){
      throw new UnsupportedOperationException();
    }
  }
;
}","@Override public Entry<byte[],byte[]> next(){
  if (!hasNext())   throw new NoSuchElementException();
  final Entry<ByteRange,ByteRange> entry=currentBlockIterator.next();
  return new Entry<byte[],byte[]>(){
    private byte[] key;
    private byte[] value;
{
      key=entry.getKey().toBuffer();
      value=entry.getValue().toBuffer();
      if (entry.getValue().getReusableBuf() != null)       BufferPool.free(entry.getValue().getReusableBuf());
    }
    @Override public byte[] getKey(){
      return key;
    }
    @Override public byte[] getValue(){
      return value;
    }
    @Override public byte[] setValue(    byte[] value){
      throw new UnsupportedOperationException();
    }
  }
;
}",0.91415313225058
132342,"private void getNextBlockData(){
  if (blockIndexStart == -1 && blockIndexEnd == -1)   return;
  if (ascending) {
    if (currentBlockIndex > blockIndexEnd) {
      if (currentBlock != null)       currentBlock.free();
      currentBlock=null;
      currentBlockIterator=null;
      return;
    }
  }
 else {
    if (currentBlockIndex < blockIndexStart) {
      if (currentBlock != null)       currentBlock.free();
      currentBlock=null;
      currentBlockIterator=null;
      return;
    }
  }
  int startOffset=DiskIndex.getBlockOffset(currentBlockIndex,blockIndexReader);
  int fileId=DiskIndex.getBlockFileId(currentBlockIndex,blockIndexReader);
  int endOffset;
  if (currentBlockIndex == blockIndexReader.getNumEntries() - 1)   endOffset=-1;
 else {
    ByteRange indexPos=DiskIndex.getBlockEntry(currentBlockIndex + 1,blockIndexReader);
    ByteBuffer indexPosBuf=indexPos.getBuf();
    endOffset=DiskIndex.getBlockIndexOffset(indexPosBuf,indexPos.getStartOffset());
    if (DiskIndex.getBlockIndexFileId(indexPosBuf,indexPos.getStartOffset()) > fileId)     endOffset=-1;
  }
  try {
    if (currentBlock != null)     currentBlock.free();
    currentBlock=maps != null ? index.getBlock(startOffset,endOffset,maps[fileId]) : index.getBlock(startOffset,endOffset,dbFileChannels[fileId]);
  }
 catch (  ClosedByInterruptException exc) {
    Logging.logError(Logging.LEVEL_DEBUG,this,exc);
  }
catch (  IOException exc) {
    Logging.logError(Logging.LEVEL_ERROR,this,exc);
  }
  currentBlockIterator=currentBlock == null ? null : currentBlock.rangeLookup(from == null ? null : from,to == null ? null : to,ascending);
}","private void getNextBlockData(){
  if (blockIndexStart == -1 && blockIndexEnd == -1)   return;
  if (ascending && currentBlockIndex > blockIndexEnd) {
    currentBlock=null;
    currentBlockIterator=null;
    return;
  }
 else   if (!ascending && currentBlockIndex < blockIndexStart) {
    currentBlock=null;
    currentBlockIterator=null;
    return;
  }
  int startOffset=DiskIndex.getBlockOffset(currentBlockIndex,blockIndexReader);
  int fileId=DiskIndex.getBlockFileId(currentBlockIndex,blockIndexReader);
  int endOffset;
  if (currentBlockIndex == blockIndexReader.getNumEntries() - 1)   endOffset=-1;
 else {
    ByteRange indexPos=DiskIndex.getBlockEntry(currentBlockIndex + 1,blockIndexReader);
    ByteBuffer indexPosBuf=indexPos.getBuf();
    endOffset=DiskIndex.getBlockIndexOffset(indexPosBuf,indexPos.getStartOffset());
    if (DiskIndex.getBlockIndexFileId(indexPosBuf,indexPos.getStartOffset()) > fileId)     endOffset=-1;
  }
  try {
    currentBlock=maps != null ? index.getBlock(startOffset,endOffset,maps[fileId]) : index.getBlock(startOffset,endOffset,dbFileChannels[fileId]);
  }
 catch (  ClosedByInterruptException exc) {
    Logging.logError(Logging.LEVEL_DEBUG,this,exc);
  }
catch (  IOException exc) {
    Logging.logError(Logging.LEVEL_ERROR,this,exc);
  }
  currentBlockIterator=currentBlock == null ? null : currentBlock.rangeLookup(from == null ? null : from,to == null ? null : to,ascending);
}",0.7958046542117339
132343,"/** 
 * <p> Needed for the initial load process of the babuDB, done by the replication. </p>
 * @throws BabuDBException
 * @return the next LSN.
 */
public LSN restart() throws BabuDBException {
synchronized (stopped) {
    if (!stopped.get())     throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"");
    databaseManager.reset();
    dbCheckptr=new CheckpointerImpl(this);
    LSN dbLsn=null;
    LSN zero=new LSN(0,0L);
    for (    Database dbRaw : databaseManager.getDatabaseList()) {
      DatabaseImpl db=(DatabaseImpl)dbRaw;
      LSN onDisk=db.getLSMDB().getOndiskLSN();
      if (dbLsn == null && !onDisk.equals(zero))       dbLsn=onDisk;
 else       if (dbLsn != null) {
        if (!onDisk.equals(zero) && !dbLsn.equals(onDisk))         throw new RuntimeException(""String_Node_Str"" + dbLsn.toString() + ""String_Node_Str""+ db.getLSMDB().getOndiskLSN().toString());
      }
    }
    if (dbLsn == null) {
      dbLsn=new LSN(0,0);
    }
 else {
      dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
    }
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    LSN nextLSN=replayLogs(dbLsn);
    if (dbLsn.compareTo(nextLSN) > 0)     nextLSN=dbLsn;
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
    try {
      logger=new DiskLogger(configuration.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads(),replicationManager);
      logger.start();
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
    if (configuration.getNumThreads() > 0) {
      worker=new LSMDBWorker[configuration.getNumThreads()];
      for (int i=0; i < configuration.getNumThreads(); i++) {
        worker[i]=new LSMDBWorker(logger,i,(configuration.getPseudoSyncWait() > 0),configuration.getMaxQueueLength());
        worker[i].start();
      }
    }
 else {
      assert(configuration.getNumThreads() == 0);
      worker=null;
    }
    dbCheckptr.init(logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    dbCheckptr.start();
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
    this.stopped.set(false);
    return new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1L);
  }
}","/** 
 * <p> Needed for the initial load process of the babuDB, done by the replication. </p>
 * @throws BabuDBException
 * @return the next LSN.
 */
public LSN restart() throws BabuDBException {
synchronized (stopped) {
    if (!stopped.get())     throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"");
    databaseManager.reset();
    dbCheckptr=new CheckpointerImpl(this);
    LSN dbLsn=null;
    for (    Database db : databaseManager.getDatabaseList()) {
      if (dbLsn == null)       dbLsn=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
 else {
        LSN onDiskLSN=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
        if (!(LSMDatabase.NO_DB_LSN.equals(dbLsn) || LSMDatabase.NO_DB_LSN.equals(onDiskLSN)))         dbLsn=dbLsn.compareTo(onDiskLSN) < 0 ? dbLsn : onDiskLSN;
      }
    }
    if (dbLsn == null) {
      dbLsn=new LSN(0,0);
    }
 else {
      dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
    }
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    LSN nextLSN=replayLogs(dbLsn);
    if (dbLsn.compareTo(nextLSN) > 0)     nextLSN=dbLsn;
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
    try {
      logger=new DiskLogger(configuration.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads(),replicationManager);
      logger.start();
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
    if (configuration.getNumThreads() > 0) {
      worker=new LSMDBWorker[configuration.getNumThreads()];
      for (int i=0; i < configuration.getNumThreads(); i++) {
        worker[i]=new LSMDBWorker(logger,i,(configuration.getPseudoSyncWait() > 0),configuration.getMaxQueueLength());
        worker[i].start();
      }
    }
 else {
      assert(configuration.getNumThreads() == 0);
      worker=null;
    }
    dbCheckptr.init(logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    dbCheckptr.start();
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
    this.stopped.set(false);
    return new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1L);
  }
}",0.8409997899600924
132344,"/** 
 * Starts the BabuDB database. If conf is instance of MasterConfig it comes with replication in master-mode. If conf is instance of SlaveConfig it comes with replication in slave-mode.
 * @param conf
 * @param staticInit
 * @throws BabuDBException
 */
BabuDB(BabuDBConfig conf,final StaticInitialization staticInit) throws BabuDBException {
  Logging.start(conf.getDebugLevel());
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + conf.getBaseDir());
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + conf.getDbLogDir());
  this.configuration=conf;
  this.databaseManager=new DatabaseManagerImpl(this);
  this.dbConfigFile=new DBConfig(this);
  this.snapshotManager=new SnapshotManagerImpl(this);
  this.dbCheckptr=new CheckpointerImpl(this);
  if (dbConfigFile.isConversionRequired()) {
    Logging.logMessage(Logging.LEVEL_WARN,Category.storage,this,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    AutoConverter.initiateConversion(dbConfigFile.getDBFormatVersion(),conf);
  }
  snapshotManager.init();
  try {
    if (conf instanceof ReplicationConfig)     new FileIO((ReplicationConfig)conf).replayBackupFiles();
  }
 catch (  IOException io) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + ""String_Node_Str"",io.getMessage());
  }
  LSN dbLsn=null;
  for (  Database db : databaseManager.getDatabaseList()) {
    if (dbLsn == null)     dbLsn=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
 else {
      LSN onDiskLSN=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
      if (!(LSMDatabase.NO_DB_LSN.equals(dbLsn) || LSMDatabase.NO_DB_LSN.equals(onDiskLSN)) && !dbLsn.equals(onDiskLSN))       throw new RuntimeException(""String_Node_Str"" + onDiskLSN + ""String_Node_Str""+ dbLsn);
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs(dbLsn);
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  try {
    if (conf instanceof ReplicationConfig) {
      this.replicationManager=new ReplicationManagerImpl(this);
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    }
 else {
      this.replicationManager=null;
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    }
  }
 catch (  Exception e) {
    Logging.logError(Logging.LEVEL_ERROR,this,e);
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,e.getMessage());
  }
  try {
    this.logger=new DiskLogger(conf.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),conf.getSyncMode(),conf.getPseudoSyncWait(),conf.getMaxQueueLength() * conf.getNumThreads(),replicationManager);
    this.logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
  if (conf.getNumThreads() > 0) {
    worker=new LSMDBWorker[conf.getNumThreads()];
    for (int i=0; i < conf.getNumThreads(); i++) {
      worker[i]=new LSMDBWorker(logger,i,(conf.getPseudoSyncWait() > 0),conf.getMaxQueueLength());
      worker[i].start();
    }
  }
 else {
    assert(conf.getNumThreads() == 0);
    worker=null;
  }
  if (dbConfigFile.isConversionRequired())   AutoConverter.completeConversion(this);
  dbCheckptr.init(logger,conf.getCheckInterval(),conf.getMaxLogfileSize());
  dbCheckptr.start();
  if (staticInit != null) {
    staticInit.initialize(databaseManager,snapshotManager,replicationManager);
  }
  this.stopped=new AtomicBoolean(false);
  if (replicationManager != null)   replicationManager.initialize();
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}","/** 
 * Starts the BabuDB database. If conf is instance of MasterConfig it comes with replication in master-mode. If conf is instance of SlaveConfig it comes with replication in slave-mode.
 * @param conf
 * @param staticInit
 * @throws BabuDBException
 */
BabuDB(BabuDBConfig conf,final StaticInitialization staticInit) throws BabuDBException {
  Logging.start(conf.getDebugLevel());
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + conf.getBaseDir());
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + conf.getDbLogDir());
  this.configuration=conf;
  this.databaseManager=new DatabaseManagerImpl(this);
  this.dbConfigFile=new DBConfig(this);
  this.snapshotManager=new SnapshotManagerImpl(this);
  this.dbCheckptr=new CheckpointerImpl(this);
  if (dbConfigFile.isConversionRequired()) {
    Logging.logMessage(Logging.LEVEL_WARN,Category.storage,this,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    AutoConverter.initiateConversion(dbConfigFile.getDBFormatVersion(),conf);
  }
  snapshotManager.init();
  try {
    if (conf instanceof ReplicationConfig)     new FileIO((ReplicationConfig)conf).replayBackupFiles();
  }
 catch (  IOException io) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + ""String_Node_Str"",io.getMessage());
  }
  LSN dbLsn=null;
  for (  Database db : databaseManager.getDatabaseList()) {
    if (dbLsn == null)     dbLsn=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
 else {
      LSN onDiskLSN=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
      if (!(LSMDatabase.NO_DB_LSN.equals(dbLsn) || LSMDatabase.NO_DB_LSN.equals(onDiskLSN)))       dbLsn=dbLsn.compareTo(onDiskLSN) < 0 ? dbLsn : onDiskLSN;
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",dbLsn);
  LSN nextLSN=replayLogs(dbLsn);
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  try {
    if (conf instanceof ReplicationConfig) {
      this.replicationManager=new ReplicationManagerImpl(this);
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    }
 else {
      this.replicationManager=null;
      Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    }
  }
 catch (  Exception e) {
    Logging.logError(Logging.LEVEL_ERROR,this,e);
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,e.getMessage());
  }
  try {
    this.logger=new DiskLogger(conf.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),conf.getSyncMode(),conf.getPseudoSyncWait(),conf.getMaxQueueLength() * conf.getNumThreads(),replicationManager);
    this.logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
  if (conf.getNumThreads() > 0) {
    worker=new LSMDBWorker[conf.getNumThreads()];
    for (int i=0; i < conf.getNumThreads(); i++) {
      worker[i]=new LSMDBWorker(logger,i,(conf.getPseudoSyncWait() > 0),conf.getMaxQueueLength());
      worker[i].start();
    }
  }
 else {
    assert(conf.getNumThreads() == 0);
    worker=null;
  }
  if (dbConfigFile.isConversionRequired())   AutoConverter.completeConversion(this);
  dbCheckptr.init(logger,conf.getCheckInterval(),conf.getMaxLogfileSize());
  dbCheckptr.start();
  if (staticInit != null) {
    staticInit.initialize(databaseManager,snapshotManager,replicationManager);
  }
  this.stopped=new AtomicBoolean(false);
  if (replicationManager != null)   replicationManager.initialize();
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}",0.980623199790521
132345,"/** 
 * Creates a new LSM tree.
 * @param indexFile the on-disk index file - may be <code>null</code>
 * @param comp a comparator for byte ranges
 * @param compressed Compression of disk-index
 * @throws IOException if an I/O error occurs when accessing the on-disk index file
 */
public LSMTree(String indexFile,ByteRangeComparator comp,boolean compressed,int maxEntriesPerBlock,int maxBlockFileSize) throws IOException {
  this.comp=comp;
  this.compressed=compressed;
  this.maxEntriesPerBlock=maxEntriesPerBlock;
  this.maxBlockFileSize=maxBlockFileSize;
  overlay=new MultiOverlayBufferTree(NULL_ELEMENT,comp);
  index=indexFile == null ? null : new DiskIndex(indexFile,comp,compressed,useMmap(new File(indexFile).length()));
  lock=new Object();
}","/** 
 * Creates a new LSM tree.
 * @param indexFile the on-disk index file - may be <code>null</code>
 * @param comp a comparator for byte ranges
 * @param compressed Compression of disk-index
 * @throws IOException if an I/O error occurs when accessing the on-disk index file
 */
public LSMTree(String indexFile,ByteRangeComparator comp,boolean compressed,int maxEntriesPerBlock,int maxBlockFileSize) throws IOException {
  this.comp=comp;
  this.compressed=compressed;
  this.maxEntriesPerBlock=maxEntriesPerBlock;
  this.maxBlockFileSize=maxBlockFileSize;
  overlay=new MultiOverlayBufferTree(NULL_ELEMENT,comp);
  totalOnDiskSize+=indexFile == null ? 0 : getTotalDirSize(new File(indexFile));
  index=indexFile == null ? null : new DiskIndex(indexFile,comp,compressed,useMmap());
  lock=new Object();
}",0.9300833867864016
132346,"/** 
 * Links the LSM tree to a new snapshot file. The on-disk index is replaced with the index stored in the given snapshot file, and all in-memory snapshots are discarded.
 * @param snapshotFile the snapshot file
 * @throws IOException if an I/O error occurred while reading the snapshot file
 */
public void linkToSnapshot(String snapshotFile) throws IOException {
  final DiskIndex oldIndex=index;
synchronized (lock) {
    totalOnDiskSize-=index == null ? 0 : index.getSize();
    index=new DiskIndex(snapshotFile,comp,this.compressed,useMmap(new File(snapshotFile).length()));
    totalOnDiskSize+=index.getSize();
    if (oldIndex != null)     oldIndex.destroy();
    overlay.cleanup();
  }
}","/** 
 * Links the LSM tree to a new snapshot file. The on-disk index is replaced with the index stored in the given snapshot file, and all in-memory snapshots are discarded.
 * @param snapshotFile the snapshot file
 * @throws IOException if an I/O error occurred while reading the snapshot file
 */
public void linkToSnapshot(String snapshotFile) throws IOException {
  final DiskIndex oldIndex=index;
synchronized (lock) {
    totalOnDiskSize-=index == null ? 0 : index.getSize();
    index=new DiskIndex(snapshotFile,comp,this.compressed,useMmap());
    totalOnDiskSize+=index.getSize();
    if (oldIndex != null)     oldIndex.destroy();
    overlay.cleanup();
  }
}",0.9773226042428677
132347,"private boolean useMmap(long currentFileSize){
  final long maxSize=500 * 1024 * 1024;
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + OutputUtils.formatBytes(totalOnDiskSize + currentFileSize));
  return ""String_Node_Str"".equals(System.getProperty(""String_Node_Str"")) || totalOnDiskSize + currentFileSize < maxSize;
}","private boolean useMmap(){
  final long maxSize=200 * 1024 * 1024;
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + OutputUtils.formatBytes(totalOnDiskSize));
  boolean mmap=""String_Node_Str"".equals(System.getProperty(""String_Node_Str"")) || totalOnDiskSize < maxSize;
  return mmap;
}",0.7786499215070644
132348,"private void createCheckpoint() throws BabuDBException, InterruptedException {
  Collection<Database> databases=((DatabaseManagerImpl)dbs.getDatabaseManager()).getDatabaseList();
synchronized (checkpointLock) {
    try {
      int[][] snapIds=new int[databases.size()][];
      int i=0;
      LSN lastWrittenLSN=null;
      try {
        logger.lockLogger();
        for (        Database db : databases) {
          snapIds[i++]=((DatabaseImpl)db).proceedCreateSnapshot();
        }
        lastWrittenLSN=logger.switchLogFile(incrementViewId);
      }
  finally {
        logger.unlockLogger();
      }
      i=0;
      for (      Database db : databases) {
        ((DatabaseImpl)db).proceedWriteSnapshot(lastWrittenLSN.getViewId(),lastWrittenLSN.getSequenceNo(),snapIds[i++]);
        ((DatabaseImpl)db).proceedCleanupSnapshot(lastWrittenLSN.getViewId(),lastWrittenLSN.getSequenceNo());
      }
      File f=new File(dbs.getConfig().getDbLogDir());
      String[] logs=f.list(new FilenameFilter(){
        public boolean accept(        File dir,        String name){
          return name.endsWith(""String_Node_Str"");
        }
      }
);
      if (logs != null) {
        Pattern p=Pattern.compile(""String_Node_Str"");
        for (        String log : logs) {
          Matcher m=p.matcher(log);
          m.matches();
          String tmp=m.group(1);
          int viewId=Integer.valueOf(tmp);
          tmp=m.group(2);
          int seqNo=Integer.valueOf(tmp);
          LSN logLSN=new LSN(viewId,seqNo);
          if (logLSN.compareTo(lastWrittenLSN) <= 0) {
            Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + log);
            f=new File(dbs.getConfig().getDbLogDir() + log);
            f.delete();
          }
        }
      }
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
  }
}","private void createCheckpoint() throws BabuDBException, InterruptedException {
  Collection<Database> databases=((DatabaseManagerImpl)dbs.getDatabaseManager()).getDatabaseList();
synchronized (checkpointLock) {
    try {
      int[][] snapIds=new int[databases.size()][];
      int i=0;
      LSN lastWrittenLSN=null;
      try {
        logger.lockLogger();
        for (        Database db : databases) {
          snapIds[i++]=((DatabaseImpl)db).proceedCreateSnapshot();
        }
        lastWrittenLSN=logger.switchLogFile(incrementViewId);
        incrementViewId=false;
      }
  finally {
        logger.unlockLogger();
      }
      i=0;
      for (      Database db : databases) {
        ((DatabaseImpl)db).proceedWriteSnapshot(lastWrittenLSN.getViewId(),lastWrittenLSN.getSequenceNo(),snapIds[i++]);
        ((DatabaseImpl)db).proceedCleanupSnapshot(lastWrittenLSN.getViewId(),lastWrittenLSN.getSequenceNo());
      }
      File f=new File(dbs.getConfig().getDbLogDir());
      String[] logs=f.list(new FilenameFilter(){
        public boolean accept(        File dir,        String name){
          return name.endsWith(""String_Node_Str"");
        }
      }
);
      if (logs != null) {
        Pattern p=Pattern.compile(""String_Node_Str"");
        for (        String log : logs) {
          Matcher m=p.matcher(log);
          m.matches();
          String tmp=m.group(1);
          int viewId=Integer.valueOf(tmp);
          tmp=m.group(2);
          int seqNo=Integer.valueOf(tmp);
          LSN logLSN=new LSN(viewId,seqNo);
          if (logLSN.compareTo(lastWrittenLSN) <= 0) {
            Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + log);
            f=new File(dbs.getConfig().getDbLogDir() + log);
            f.delete();
          }
        }
      }
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
  }
}",0.9918442515127598
132349,"private boolean useMmap(long currentFileSize){
  final long maxSize=500 * 1024 * 1024;
  System.out.println(""String_Node_Str"" + OutputUtils.formatBytes(totalOnDiskSize + currentFileSize));
  return ""String_Node_Str"".equals(System.getProperty(""String_Node_Str"")) || totalOnDiskSize + currentFileSize < maxSize;
}","private boolean useMmap(long currentFileSize){
  final long maxSize=500 * 1024 * 1024;
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + OutputUtils.formatBytes(totalOnDiskSize + currentFileSize));
  return ""String_Node_Str"".equals(System.getProperty(""String_Node_Str"")) || totalOnDiskSize + currentFileSize < maxSize;
}",0.9026275115919628
132350,"public DiskLogIterator(File[] logFiles,LSN from) throws LogEntryException, IOException {
  this.from=from;
  if (logFiles != null && logFiles.length > 0) {
    dbLogDir=logFiles[0].getParent() + ""String_Node_Str"";
    int count=-1;
    SortedSet<LSN> orderedLogList=new TreeSet<LSN>();
    Pattern p=Pattern.compile(""String_Node_Str"");
    for (    File logFile : logFiles) {
      Matcher m=p.matcher(logFile.getName());
      m.matches();
      String tmp=m.group(1);
      int viewId=Integer.valueOf(tmp);
      tmp=m.group(2);
      int seqNo=Integer.valueOf(tmp);
      orderedLogList.add(new LSN(viewId,seqNo));
      count++;
    }
    LSN[] copy=orderedLogList.toArray(new LSN[orderedLogList.size()]);
    LSN last=null;
    for (    LSN lsn : copy) {
      if (last == null)       last=lsn;
 else {
        if (from != null && lsn.compareTo(from) <= 0) {
          orderedLogList.remove(last);
          last=lsn;
        }
 else         break;
      }
    }
    logList=orderedLogList.iterator();
    findFirstEntry();
  }
}","/** 
 * @param logFiles
 * @param from - inclusive, if everything went fine, next() will return thelog entry identified by LSN <code>from</code>.
 * @throws LogEntryException
 * @throws IOException
 */
public DiskLogIterator(File[] logFiles,LSN from) throws LogEntryException, IOException {
  this.from=from;
  if (logFiles != null && logFiles.length > 0) {
    dbLogDir=logFiles[0].getParent() + ""String_Node_Str"";
    int count=-1;
    SortedSet<LSN> orderedLogList=new TreeSet<LSN>();
    Pattern p=Pattern.compile(""String_Node_Str"");
    for (    File logFile : logFiles) {
      Matcher m=p.matcher(logFile.getName());
      m.matches();
      String tmp=m.group(1);
      int viewId=Integer.valueOf(tmp);
      tmp=m.group(2);
      int seqNo=Integer.valueOf(tmp);
      orderedLogList.add(new LSN(viewId,seqNo));
      count++;
    }
    LSN[] copy=orderedLogList.toArray(new LSN[orderedLogList.size()]);
    LSN last=null;
    LSN lastRemoved=null;
    for (    LSN lsn : copy) {
      if (last == null)       last=lsn;
 else {
        if (from != null && lsn.compareTo(from) < 0) {
          orderedLogList.remove(last);
          lastRemoved=last;
          last=lsn;
        }
 else         break;
      }
    }
    if (lastRemoved != null && from.compareTo(last) < 0) {
      orderedLogList.add(lastRemoved);
    }
    logList=orderedLogList.iterator();
    findFirstEntry();
  }
}",0.8108776266996292
132351,"public LSN switchLogFile(boolean incrementViewId) throws IOException {
  if (!hasLock()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  final String newFileName=createLogFileName();
  channel.close();
  fos.close();
  currentLogFileName=newFileName;
  File lf=new File(this.currentLogFileName);
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + ""String_Node_Str"" + currentLogFileName);
LSN lastSyncedLSN=null;
if (incrementViewId) {
int view=this.currentViewId.getAndIncrement();
long seq=this.nextLogSequenceNo.getAndSet(1L) - 1L;
lastSyncedLSN=new LSN(view,seq);
}
 else {
lastSyncedLSN=new LSN(this.currentViewId.get(),this.nextLogSequenceNo.get() - 1L);
}
return lastSyncedLSN;
}","public LSN switchLogFile(boolean incrementViewId) throws IOException {
  if (!hasLock()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  LSN lastSyncedLSN=null;
  if (incrementViewId) {
    int view=this.currentViewId.getAndIncrement();
    long seq=this.nextLogSequenceNo.getAndSet(1L) - 1L;
    lastSyncedLSN=new LSN(view,seq);
  }
 else {
    lastSyncedLSN=new LSN(this.currentViewId.get(),this.nextLogSequenceNo.get() - 1L);
  }
  final String newFileName=createLogFileName();
  this.channel.close();
  this.fos.close();
  this.currentLogFileName=newFileName;
  File lf=new File(this.currentLogFileName);
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + ""String_Node_Str"" + currentLogFileName);
return lastSyncedLSN;
}",0.6684468179795282
132352,"@Override public boolean isMaster(){
  try {
    return this.controlLayer.hasLease();
  }
 catch (  InterruptedException e) {
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",e.getMessage());
    return false;
  }
}","@Override public boolean isMaster(){
  return this.controlLayer.hasLease();
}",0.4425287356321839
132353,"@Override public boolean hasLease() throws InterruptedException {
  return this.leaseHolder.amIOwner();
}","@Override public boolean hasLease(){
  return !this.replicationController.isSuspended() && this.leaseHolder.amIOwner();
}",0.672566371681416
132354,"/** 
 * Delays the request if there is a failover in progress.
 * @return true, if this server has the lease, false otherwise.
 */
public boolean hasLease() throws InterruptedException ;","/** 
 * @return true, if this server has the lease and is not suspended, false otherwise.
 */
public boolean hasLease();",0.6470588235294118
132355,"/** 
 * This server has to become the new master.
 * @throws InterruptedException 
 * @throws BabuDBException 
 * @throws IOException 
 */
private void becomeMaster() throws InterruptedException, BabuDBException, IOException {
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  this.serviceInterface.synchronize();
  this.ctrlLayer.getTimeDriftDetectorControl().start();
}","/** 
 * This server has to become the new master.
 * @throws InterruptedException 
 * @throws BabuDBException 
 * @throws IOException 
 */
private void becomeMaster() throws InterruptedException, BabuDBException, IOException {
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  this.serviceInterface.synchronize();
  this.ctrlLayer.getTimeDriftDetectorControl().start();
  this.serviceInterface.changeMaster(null);
}",0.946341463414634
132356,"/** 
 * Method to trigger a   {@link LOAD} manually.Listener will contain true on finish , if it was a request synchronization, or if just the log file was switched.  otherwise, if it was a load, the listener will notified with false.
 * @param listener
 * @return 
 */
public void manualLoad(BabuDBRequest<Boolean> listener,LSN from,LSN to){
  this.listener=listener;
  org.xtreemfs.babudb.interfaces.LSN start=new org.xtreemfs.babudb.interfaces.LSN(from.getViewId(),from.getSequenceNo());
  org.xtreemfs.babudb.interfaces.LSN end=new org.xtreemfs.babudb.interfaces.LSN(to.getViewId(),to.getSequenceNo());
  missing=new LSNRange(start,end);
  setLogic(REQUEST,""String_Node_Str"");
  if (q.isEmpty())   q.add(new StageRequest(null));
}","/** 
 * Method to trigger a   {@link LOAD} manually.Listener will contain true on finish , if it was a request synchronization, or if just the log file was switched.  otherwise, if it was a load, the listener will notified with false.
 * @param listener
 * @return 
 */
public void manualLoad(BabuDBRequest<Boolean> listener,LSN from,LSN to){
  assert(from.compareTo(to) < 0);
  this.listener=listener;
  org.xtreemfs.babudb.interfaces.LSN start=new org.xtreemfs.babudb.interfaces.LSN(from.getViewId(),from.getSequenceNo());
  org.xtreemfs.babudb.interfaces.LSN end=new org.xtreemfs.babudb.interfaces.LSN(to.getViewId(),to.getSequenceNo() + 1L);
  missing=new LSNRange(start,end);
  setLogic(REQUEST,""String_Node_Str"");
  if (q.isEmpty())   q.add(new StageRequest(null));
}",0.9741207697412076
132357,"@Override public void run(){
  notifyStarted();
  while (!quit) {
    boolean infarcted=false;
    try {
      if (tries != 0)       Thread.sleep(RETRY_DELAY_MS * tries);
      if (!infarcted && !logicID.equals(BASIC)) {
        this.pacemaker.infarction();
        infarcted=true;
      }
      logics.get(logicID).run();
      if (infarcted && logicID.equals(BASIC)) {
        this.pacemaker.updateLSN(lastInserted);
        infarcted=false;
      }
      tries=0;
    }
 catch (    ConnectionLostException cle) {
switch (cle.errNo) {
case LOG_REMOVED:
        setLogic(LOAD,""String_Node_Str"");
      break;
case BUSY:
    if (++tries < ReplicationConfig.MAX_RETRIES)     break;
case SERVICE_UNAVAILABLE:
default :
  Logging.logError(Logging.LEVEL_WARN,this,cle);
quit=true;
if (listener != null) {
listener.failed(new BabuDBException(ErrorCode.REPLICATION_FAILURE,cle.getMessage()));
}
this.roleChangeListener.suspend();
break;
}
}
catch (InterruptedException ie) {
if (!quit) {
quit=true;
notifyCrashed(ie);
return;
}
}
}
notifyStopped();
}","@Override public void run(){
  notifyStarted();
  while (!quit) {
    boolean infarcted=false;
    try {
      if (tries != 0)       Thread.sleep(RETRY_DELAY_MS * tries);
      if (!infarcted && !logicID.equals(BASIC)) {
        this.pacemaker.infarction();
        infarcted=true;
      }
      logics.get(logicID).run();
      if (infarcted && logicID.equals(BASIC)) {
        this.pacemaker.updateLSN(lastInserted);
        infarcted=false;
      }
      tries=0;
    }
 catch (    ConnectionLostException cle) {
switch (cle.errNo) {
case LOG_UNAVAILABLE:
        setLogic(LOAD,""String_Node_Str"" + cle.getMessage());
      break;
case BUSY:
    if (++tries < ReplicationConfig.MAX_RETRIES)     break;
case SERVICE_UNAVAILABLE:
default :
  Logging.logError(Logging.LEVEL_WARN,this,cle);
quit=true;
if (listener != null) {
listener.failed(new BabuDBException(ErrorCode.REPLICATION_FAILURE,cle.getMessage()));
}
this.roleChangeListener.suspend();
break;
}
}
catch (InterruptedException ie) {
if (!quit) {
quit=true;
notifyCrashed(ie);
return;
}
}
}
notifyStopped();
}",0.9834201800094742
132358,"/** 
 * Registers a new master given by its address, valid for all replication components.
 * @param address - may be null, if no master is available at the moment.
 */
public void changeMaster(InetAddress address);","/** 
 * Registers a new master given by its address, valid for all replication components.
 * @param address - may be null, if no master is available at the moment, orthe local instance is owner of the master privilege.
 */
public void changeMaster(InetAddress address);",0.8865979381443299
132359,"/** 
 * <p> Sets a new master, valid for all replication components. Resets the  states of all participants. </p>
 * @param address
 */
public void setMaster(InetAddress address){
  this.masterClient.set(this.stateTable.get(address).client);
  this.reset();
}","/** 
 * <p> Sets a new master, valid for all replication components. Resets the  states of all participants. </p>
 * @param address - may be null, if master is unknown or the local babuDBinstance is owner of the master privilege
 */
public void setMaster(InetAddress address){
  if (address != null)   this.masterClient.set(this.stateTable.get(address).client);
 else   this.masterClient.set(null);
  this.reset();
}",0.7318518518518519
132360,"/** 
 * Method to decide which logic shall be used next.
 */
private void finish(){
  if (stage.lastInserted.compareTo(new LSN(stage.missing.getEnd())) < 0) {
    stage.missing=new LSNRange(new org.xtreemfs.babudb.interfaces.LSN(stage.lastInserted.getViewId(),stage.lastInserted.getSequenceNo()),stage.missing.getEnd());
  }
 else {
    stage.missing=null;
    stage.setLogic(BASIC,""String_Node_Str"" + ""String_Node_Str"");
  }
}","/** 
 * Method to decide which logic shall be used next.
 */
private void finish(){
  LSN next=new LSN(stage.lastInserted.getViewId(),stage.lastInserted.getSequenceNo() + 1L);
  if (next.compareTo(new LSN(stage.missing.getEnd())) < 0) {
    stage.missing=new LSNRange(new org.xtreemfs.babudb.interfaces.LSN(stage.lastInserted.getViewId(),stage.lastInserted.getSequenceNo()),stage.missing.getEnd());
  }
 else {
    stage.missing=null;
    stage.setLogic(BASIC,""String_Node_Str"" + ""String_Node_Str"");
  }
}",0.8776824034334764
132361,"@Override public void startRequest(final Request rq){
  heartbeatRequest request=(heartbeatRequest)rq.getRequestMessage();
  LSN lsn=new LSN(request.getLsn());
  try {
    this.sManipulator.update(rq.getRPCRequest().getClientIdentity(),lsn,TimeSync.getGlobalTime());
    rq.sendSuccess(request.createDefaultResponse());
  }
 catch (  UnknownParticipantException e) {
    rq.sendReplicationException(ErrNo.SECURITY,""String_Node_Str"" + ""String_Node_Str"");
  }
}","@Override public void startRequest(final Request rq){
  heartbeatRequest request=(heartbeatRequest)rq.getRequestMessage();
  LSN lsn=new LSN(request.getLsn());
  try {
    this.sManipulator.update(rq.getRPCRequest().getClientIdentity(),lsn,TimeSync.getGlobalTime());
    rq.sendSuccess(request.createDefaultResponse());
  }
 catch (  UnknownParticipantException e) {
    rq.sendReplicationException(ErrNo.NO_ACCESS,""String_Node_Str"" + ""String_Node_Str"");
  }
}",0.9836779107725788
132362,"/** 
 * @param lastOnView
 * @param slaveView
 * @param roleChangeListener - listener to notify about the need of a role change.
 * @param pacemaker - for the hearbeat.
 * @param max_q - 0 means infinite.
 */
public ReplicationStage(int max_q,Pacemaker pacemaker,SlaveView slaveView,FileIOInterface fileIO,BabuDBInterface babuInterface){
  super(""String_Node_Str"");
  this.pacemaker=pacemaker;
  this.q=new PriorityBlockingQueue<StageRequest>();
  this.numRequests=new AtomicInteger(0);
  this.MAX_Q=max_q;
  this.quit=false;
  logics=new HashMap<LogicID,Logic>();
  Logic lg=new BasicLogic(this,pacemaker,slaveView,fileIO,babuInterface,q);
  logics.put(lg.getId(),lg);
  lg=new RequestLogic(this,pacemaker,slaveView,fileIO,babuInterface);
  logics.put(lg.getId(),lg);
  lg=new LoadLogic(this,pacemaker,slaveView,fileIO,babuInterface);
  logics.put(lg.getId(),lg);
}","/** 
 * @param lastOnView
 * @param slaveView
 * @param roleChangeListener - listener to notify about the need of a role change.
 * @param pacemaker - for the hearbeat.
 * @param max_q - 0 means infinite.
 * @param lastOnView - reference for the compare-variable lastOnView.
 */
public ReplicationStage(int max_q,Pacemaker pacemaker,SlaveView slaveView,FileIOInterface fileIO,BabuDBInterface babuInterface,AtomicReference<LSN> lastOnView){
  super(""String_Node_Str"");
  this.lastOnView=lastOnView;
  this.pacemaker=pacemaker;
  this.q=new PriorityBlockingQueue<StageRequest>();
  this.numRequests=new AtomicInteger(0);
  this.MAX_Q=max_q;
  this.quit=false;
  logics=new HashMap<LogicID,Logic>();
  Logic lg=new BasicLogic(this,pacemaker,slaveView,fileIO,babuInterface,q);
  logics.put(lg.getId(),lg);
  lg=new RequestLogic(this,pacemaker,slaveView,fileIO,babuInterface);
  logics.put(lg.getId(),lg);
  lg=new LoadLogic(this,pacemaker,slaveView,fileIO,babuInterface);
  logics.put(lg.getId(),lg);
}",0.9291845493562232
132363,"/** 
 * @param config
 * @param underlyingLayer
 * @param clientFactory
 * @param latest
 * @throws IOException if {@link Flease} could not be initialized.
 */
public ServiceLayer(ReplicationConfig config,BabuDBInterface babuDB,TransmissionToServiceInterface transLayer) throws IOException {
  this.transmissionInterface=transLayer;
  this.babuDBInterface=babuDB;
  int syncN=((config.getSyncN() > 0) ? config.getSyncN() - 1 : config.getSyncN());
  this.participantsStates=new ParticipantsStates(syncN,config.getParticipants(),transLayer);
  this.heartbeatThread=new HeartbeatThread(this.participantsStates);
  this.replicationStage=new ReplicationStage(config.getMaxQueueLength(),this.heartbeatThread,this,transLayer.getFileIOInterface(),this.babuDBInterface);
}","/** 
 * @param config
 * @param underlyingLayer
 * @param clientFactory
 * @param latest
 * @throws IOException if {@link Flease} could not be initialized.
 */
public ServiceLayer(ReplicationConfig config,BabuDBInterface babuDB,TransmissionToServiceInterface transLayer) throws IOException {
  this.transmissionInterface=transLayer;
  this.babuDBInterface=babuDB;
  int syncN=((config.getSyncN() > 0) ? config.getSyncN() - 1 : config.getSyncN());
  this.participantsStates=new ParticipantsStates(syncN,config.getParticipants(),transLayer);
  this.heartbeatThread=new HeartbeatThread(this.participantsStates);
  this.replicationStage=new ReplicationStage(config.getMaxQueueLength(),this.heartbeatThread,this,transLayer.getFileIOInterface(),this.babuDBInterface,this.lastOnView);
}",0.9896238651102464
132364,"@Override public void requestFinished(SlaveClient slave){
synchronized (stateTable) {
    final State s=stateTable.get(slave.getDefaultServerAddress().getAddress());
    s.openRequests--;
    if (s.openRequests == (MAX_OPEN_REQUESTS_PER_SLAVE - 1)) {
      availableSlaves++;
      stateTable.notify();
    }
  }
}","@Override public void requestFinished(SlaveClient slave){
synchronized (stateTable) {
    final State s=stateTable.get(slave.getDefaultServerAddress().getAddress());
    if (s.openRequests > 0)     s.openRequests--;
    if (s.openRequests == (MAX_OPEN_REQUESTS_PER_SLAVE - 1)) {
      availableSlaves++;
      stateTable.notify();
    }
  }
}",0.9573170731707316
132365,"@Override public void markAsDead(SlaveClient slave){
synchronized (stateTable) {
    final State s=stateTable.get(slave.getDefaultServerAddress().getAddress());
    s.openRequests--;
    if (!s.dead) {
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"",slave.getDefaultServerAddress().toString(),toString());
      s.dead=true;
      deadSlaves++;
      availableSlaves--;
      stateTable.notify();
    }
  }
}","@Override public void markAsDead(SlaveClient slave){
synchronized (stateTable) {
    final State s=stateTable.get(slave.getDefaultServerAddress().getAddress());
    s.openRequests=0;
    if (!s.dead) {
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"",slave.getDefaultServerAddress().toString(),toString());
      s.dead=true;
      deadSlaves++;
      availableSlaves--;
      stateTable.notify();
    }
  }
}",0.9953271028037384
132366,"public void close() throws IOException {
  channel.close();
  fis.close();
}","public void close() throws IOException {
  LogEntry tmp=next;
  next=null;
  if (tmp != null)   tmp.free();
  channel.close();
  fis.close();
}",0.6940639269406392
132367,"public void destroy() throws IOException {
  currentFile.close();
}","public void destroy() throws IOException {
  LogEntry tmp=nextEntry;
  nextEntry=null;
  if (tmp != null)   tmp.free();
  currentFile.close();
}",0.6350710900473934
132368,"/** 
 * Removes the detection task.
 */
void stop(){
synchronized (this) {
    this.task.cancel();
    this.task=null;
  }
}","/** 
 * Removes the detection task.
 */
void stop(){
synchronized (this) {
    if (this.task != null) {
      this.task.cancel();
      this.task=null;
    }
  }
}",0.8641114982578397
132369,"@Test public void testgetReplicaUnavailable() throws Exception {
  System.out.println(""String_Node_Str"");
  makeDB();
  insertData();
  long seqToRequest=1L;
  RPCResponse<LogEntries> result=client.getReplica(new LSNRange(1,seqToRequest,seqToRequest));
  try {
    result.get();
  }
 catch (  ONCRPCError e) {
    fail();
  }
 finally {
    result.freeBuffers();
  }
}","@Test public void testgetReplicaUnavailable() throws Exception {
  System.out.println(""String_Node_Str"");
  makeDB();
  insertData();
  long seqToRequest=1L;
  RPCResponse<LogEntries> result=client.getReplica(new LSNRange(viewID,seqToRequest,seqToRequest));
  try {
    result.get();
  }
 catch (  ONCRPCError e) {
    fail();
  }
 finally {
    result.freeBuffers();
  }
}",0.99055330634278
132370,"@Test public void testgetReplica() throws Exception {
  System.out.println(""String_Node_Str"");
  makeDB();
  insertData();
  long seqToRequest=2L;
  RPCResponse<LogEntries> result=client.getReplica(new LSNRange(1,seqToRequest,seqToRequest));
  LogEntries les=result.get();
  assertNotNull(les);
  assertEquals(1,les.size());
  LogEntry le=LogEntry.deserialize(les.get(0).getPayload(),new CRC32());
  assertEquals(viewID,le.getViewId());
  assertEquals(seqToRequest,le.getLogSequenceNo());
  assertEquals(viewID,le.getLSN().getViewId());
  assertEquals(seqToRequest,le.getLSN().getSequenceNo());
  assertNotNull(le.getPayload());
  result.freeBuffers();
  le.free();
}","@Test public void testgetReplica() throws Exception {
  System.out.println(""String_Node_Str"");
  makeDB();
  insertData();
  long seqToRequest=2L;
  RPCResponse<LogEntries> result=client.getReplica(new LSNRange(viewID,seqToRequest,seqToRequest));
  LogEntries les=result.get();
  assertNotNull(les);
  assertEquals(1,les.size());
  LogEntry le=LogEntry.deserialize(les.get(0).getPayload(),new CRC32());
  assertEquals(viewID,le.getViewId());
  assertEquals(seqToRequest,le.getLogSequenceNo());
  assertEquals(viewID,le.getLSN().getViewId());
  assertEquals(seqToRequest,le.getLSN().getSequenceNo());
  assertNotNull(le.getPayload());
  result.freeBuffers();
  le.free();
}",0.9947722180731888
132371,"@SuppressWarnings(""String_Node_Str"") @Override protected ReplicateResponse _replicate(LogEntry le,ReusableBuffer payload) throws NotEnoughAvailableSlavesException, InterruptedException {
  List<SlaveClient> slaves=getSlavesForBroadCast();
  final ReplicateResponse result=new ReplicateResponse(le,slaves.size() - getSyncN());
  if (slaves.size() == 0) {
    Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + ""String_Node_Str"");
  }
 else {
    for (    final SlaveClient slave : slaves) {
      ((RPCResponse<Object>)slave.replicate(le.getLSN(),payload.createViewBuffer())).registerListener(new RPCResponseAvailableListener<Object>(){
        @Override public void responseAvailable(        RPCResponse<Object> r){
          try {
            r.get();
            markSlaveAsFinished(slave);
          }
 catch (          Exception e) {
            markSlaveAsDead(slave);
            result.decrementPermittedFailures();
            Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",slave.getDefaultServerAddress().toString(),e.getMessage());
            if (e.getMessage() == null)             Logging.logError(Logging.LEVEL_DEBUG,this,e);
          }
 finally {
            if (r != null)             r.freeBuffers();
          }
        }
      }
);
    }
  }
  return result;
}","@SuppressWarnings(""String_Node_Str"") @Override protected ReplicateResponse _replicate(LogEntry le,ReusableBuffer payload) throws NotEnoughAvailableSlavesException, InterruptedException {
  List<SlaveClient> slaves=getSlavesForBroadCast();
  final ReplicateResponse result=new ReplicateResponse(le,slaves.size() - this.syncN);
  if (slaves.size() == 0) {
    Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + ""String_Node_Str"");
  }
 else {
    for (    final SlaveClient slave : slaves) {
      ((RPCResponse<Object>)slave.replicate(le.getLSN(),payload.createViewBuffer())).registerListener(new RPCResponseAvailableListener<Object>(){
        @Override public void responseAvailable(        RPCResponse<Object> r){
          try {
            r.get();
            markSlaveAsFinished(slave);
          }
 catch (          Exception e) {
            markSlaveAsDead(slave);
            result.decrementPermittedFailures();
            Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",slave.getDefaultServerAddress().toString(),e.getMessage());
            if (e.getMessage() == null)             Logging.logError(Logging.LEVEL_DEBUG,this,e);
          }
 finally {
            if (r != null)             r.freeBuffers();
          }
        }
      }
);
    }
  }
  return result;
}",0.9954093343534812
132372,"/** 
 * Constructor to change the   {@link RequestDispatcher} behavior tomaster using the old dispatcher.
 * @param oldDispatcher
 * @param own - address of the master.
 * @throws IOException 
 */
public MasterRequestDispatcher(RequestDispatcher oldDispatcher,InetSocketAddress own) throws IOException {
  super(""String_Node_Str"",oldDispatcher);
  assert(oldDispatcher.isPaused());
  DispatcherState oldState=oldDispatcher.getState();
  if (oldState.requestQueue != null)   for (  StageRequest rq : oldState.requestQueue)   rq.free();
  this.chunkSize=oldDispatcher.configuration.getChunkSize();
  this.syncN=oldDispatcher.configuration.getSyncN();
  List<InetSocketAddress> slaves=new LinkedList<InetSocketAddress>(oldDispatcher.configuration.getParticipants());
  this.states=new SlavesStates(((ReplicationConfig)dbs.getConfig()).getSyncN(),slaves,rpcClient);
  LSN initial=oldDispatcher.getState().latest;
  this.lastOnView=(initial.getSequenceNo() == 0L) ? new LSN(0,0L) : initial;
}","/** 
 * Constructor to change the   {@link RequestDispatcher} behavior tomaster using the old dispatcher.
 * @param oldDispatcher
 * @param own - address of the master.
 * @throws IOException 
 */
public MasterRequestDispatcher(RequestDispatcher oldDispatcher,InetSocketAddress own) throws IOException {
  super(""String_Node_Str"",oldDispatcher);
  assert(oldDispatcher.isPaused());
  DispatcherState oldState=oldDispatcher.getState();
  if (oldState.requestQueue != null)   for (  StageRequest rq : oldState.requestQueue)   rq.free();
  this.chunkSize=oldDispatcher.configuration.getChunkSize();
  this.syncN=(oldDispatcher.configuration.getSyncN() > 0) ? oldDispatcher.configuration.getSyncN() - 1 : oldDispatcher.configuration.getSyncN();
  List<InetSocketAddress> slaves=new LinkedList<InetSocketAddress>(oldDispatcher.configuration.getParticipants());
  this.states=new SlavesStates(this.syncN,slaves,rpcClient);
  LSN initial=oldDispatcher.getState().latest;
  this.lastOnView=(initial.getSequenceNo() == 0L) ? new LSN(0,0L) : initial;
}",0.9305076392311484
132373,"/** 
 * <p>Use this if you want to send requests to the slaves. The open-request-counter of the returned slaves will be incremented.</p> <p>Flow-control: If there are too many busy slaves, to get more or equal  syncN slaves, this operation blocks until enough slaves are available.</p>
 * @return a list of available slaves.
 * @throws NotEnoughAvailableSlavesException
 * @throws InterruptedException 
 */
public List<SlaveClient> getAvailableSlaves() throws NotEnoughAvailableSlavesException, InterruptedException {
  List<SlaveClient> result=new LinkedList<SlaveClient>();
synchronized (stateTable) {
    while (availableSlaves < syncN && !((slavesCount - deadSlaves) < syncN))     stateTable.wait();
    long time=TimeSync.getLocalSystemTime();
    if (!((slavesCount - deadSlaves) < syncN)) {
      for (      State slaveState : stateTable.values()) {
        if (!slaveState.dead) {
          if (slaveState.lastUpdate != 0L && time > (slaveState.lastUpdate + DELAY_TILL_DEAD)) {
            slaveState.dead=true;
            availableSlaves--;
          }
 else           if (slaveState.openRequests < MAX_OPEN_REQUESTS_PER_SLAVE) {
            slaveState.openRequests++;
            if (slaveState.openRequests == MAX_OPEN_REQUESTS_PER_SLAVE)             availableSlaves--;
            result.add(slaveState.client);
          }
        }
      }
    }
  }
  if (result.size() < syncN)   throw new NotEnoughAvailableSlavesException(""String_Node_Str"" + result.size() + ""String_Node_Str""+ ""String_Node_Str"");
  return result;
}","/** 
 * <p>Use this if you want to send requests to the slaves. The open-request-counter of the returned slaves will be incremented.</p> <p>Flow-control: If there are too many busy slaves, to get more or equal  syncN slaves, this operation blocks until enough slaves are available.</p>
 * @return a list of available slaves.
 * @throws NotEnoughAvailableSlavesException
 * @throws InterruptedException 
 */
public List<SlaveClient> getAvailableSlaves() throws NotEnoughAvailableSlavesException, InterruptedException {
  List<SlaveClient> result=new LinkedList<SlaveClient>();
synchronized (stateTable) {
    while (availableSlaves < syncN && !((slavesCount - deadSlaves) < syncN))     stateTable.wait(WAIT_TILL_REFUSE);
    long time=TimeSync.getLocalSystemTime();
    if (!((slavesCount - deadSlaves) < syncN)) {
      for (      State slaveState : stateTable.values()) {
        if (!slaveState.dead) {
          if (slaveState.lastUpdate != 0L && time > (slaveState.lastUpdate + DELAY_TILL_DEAD)) {
            slaveState.dead=true;
            availableSlaves--;
          }
 else           if (slaveState.openRequests < MAX_OPEN_REQUESTS_PER_SLAVE) {
            slaveState.openRequests++;
            if (slaveState.openRequests == MAX_OPEN_REQUESTS_PER_SLAVE)             availableSlaves--;
            result.add(slaveState.client);
          }
        }
      }
    }
    if (result.size() < syncN) {
      for (      SlaveClient c : result) {
        State s=stateTable.get(c.getDefaultServerAddress().getAddress());
        if (s.openRequests == MAX_OPEN_REQUESTS_PER_SLAVE)         availableSlaves++;
        s.openRequests--;
      }
      throw new NotEnoughAvailableSlavesException(""String_Node_Str"" + result.size() + ""String_Node_Str""+ ""String_Node_Str"");
    }
  }
  return result;
}",0.7428742874287428
132374,"public void read() throws IOException {
}","public void read() throws IOException {
  super.read();
  this.localTimeRenew=this.readOptionalInt(""String_Node_Str"",3000);
  if (this.readRequiredBoolean(""String_Node_Str"")) {
    this.sslOptions=new SSLOptions(new FileInputStream(this.readRequiredString(""String_Node_Str"")),this.readRequiredString(""String_Node_Str""),this.readRequiredString(""String_Node_Str""),new FileInputStream(this.readRequiredString(""String_Node_Str"")),this.readRequiredString(""String_Node_Str""),this.readRequiredString(""String_Node_Str""),this.readRequiredBoolean(""String_Node_Str""));
  }
  this.participants=new HashSet<InetSocketAddress>();
  int number=0;
  Socket s;
  InetSocketAddress addrs;
  while ((addrs=this.readOptionalInetSocketAddr(""String_Node_Str"" + number,""String_Node_Str"" + number + ""String_Node_Str"",null)) != null) {
    s=new Socket();
    try {
      s.bind(addrs);
      if (this.address != null && !this.address.equals(addrs))       throw new BindException();
      this.address=addrs;
    }
 catch (    BindException e) {
      this.participants.add(addrs);
    }
 finally {
      try {
        s.close();
      }
 catch (      IOException e) {
      }
    }
    number++;
  }
  if (this.address == null)   throw new IOException(""String_Node_Str"" + ""String_Node_Str"");
  this.chunkSize=this.readOptionalInt(""String_Node_Str"",DEFAULT_MAX_CHUNK_SIZE);
  this.syncN=this.readOptionalInt(""String_Node_Str"",0);
  if (this.syncN < 0 || this.syncN > this.participants.size())   throw new IOException(""String_Node_Str"" + ""String_Node_Str"");
  if (this.syncN != 0 && this.syncN <= participants.size() / 2)   throw new IOException(""String_Node_Str"" + this.syncN + ""String_Node_Str""+ ""String_Node_Str""+ this.participants.size()+ ""String_Node_Str""+ ""String_Node_Str""+ (this.participants.size() / 2)+ 1+ ""String_Node_Str"");
  String backupDir=this.readRequiredString(""String_Node_Str"");
  if (backupDir.equals(baseDir) || backupDir.equals(dbLogDir))   throw new IOException(""String_Node_Str"" + ""String_Node_Str"");
  this.backupDir=(backupDir.endsWith(File.separator)) ? backupDir : backupDir + File.separator;
}",0.0383536014967259
132375,"/** 
 * Starts the BabuDB database. If conf is instance of MasterConfig it comes with replication in master-mode. If conf is instance of SlaveConfig it comes with replication in slave-mode.
 * @param conf
 * @throws BabuDBException
 */
BabuDB(BabuDBConfig conf) throws BabuDBException {
  Logging.start(conf.getDebugLevel());
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + conf.getBaseDir());
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + conf.getDbLogDir());
  this.configuration=conf;
  this.databaseManager=new DatabaseManagerImpl(this);
  this.dbConfigFile=new DBConfig(this);
  this.snapshotManager=new SnapshotManagerImpl(this);
  this.dbCheckptr=new CheckpointerImpl(this);
  if (dbConfigFile.isConversionRequired()) {
    Logging.logMessage(Logging.LEVEL_WARN,Category.db,this,""String_Node_Str"");
    AutoConverter.initiateConversion(dbConfigFile.getDBFormatVersion(),conf);
  }
  try {
    if (conf instanceof ReplicationConfig)     DirectFileIO.replayBackupFiles((ReplicationConfig)conf);
  }
 catch (  IOException io) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + ""String_Node_Str"",io.getMessage());
  }
  LSN dbLsn=null;
  for (  Database db : databaseManager.getDatabaseList()) {
    if (dbLsn == null)     dbLsn=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
 else {
      LSN onDiskLSN=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
      if (!LSMDatabase.NO_DB_LSN.equals(onDiskLSN) && !dbLsn.equals(onDiskLSN))       throw new RuntimeException(""String_Node_Str"");
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs(dbLsn);
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  LSN lastLSN=new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1);
  try {
    if (conf instanceof ReplicationConfig)     this.replicationManager=new ReplicationManagerImpl(this,lastLSN);
 else     this.replicationManager=null;
  }
 catch (  Exception e) {
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,e.getMessage());
  }
  try {
    this.logger=new DiskLogger(conf.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),conf.getSyncMode(),conf.getPseudoSyncWait(),conf.getMaxQueueLength() * conf.getNumThreads(),replicationManager);
    this.logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
  if (conf.getNumThreads() > 0) {
    worker=new LSMDBWorker[conf.getNumThreads()];
    for (int i=0; i < conf.getNumThreads(); i++) {
      worker[i]=new LSMDBWorker(logger,i,(conf.getPseudoSyncWait() > 0),conf.getMaxQueueLength());
      worker[i].start();
    }
  }
 else {
    assert(conf.getNumThreads() == 0);
    worker=null;
  }
  if (dbConfigFile.isConversionRequired())   AutoConverter.completeConversion(this);
  snapshotManager.init();
  dbCheckptr.init(logger,conf.getCheckInterval(),conf.getMaxLogfileSize());
  dbCheckptr.start();
  this.stopped=new AtomicBoolean(false);
  if (this.replicationManager != null)   this.replicationManager.initialize();
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}","/** 
 * Starts the BabuDB database. If conf is instance of MasterConfig it comes with replication in master-mode. If conf is instance of SlaveConfig it comes with replication in slave-mode.
 * @param conf
 * @throws BabuDBException
 */
BabuDB(BabuDBConfig conf) throws BabuDBException {
  Logging.start(conf.getDebugLevel());
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + conf.getBaseDir());
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + conf.getDbLogDir());
  this.configuration=conf;
  this.databaseManager=new DatabaseManagerImpl(this);
  this.dbConfigFile=new DBConfig(this);
  this.snapshotManager=new SnapshotManagerImpl(this);
  this.dbCheckptr=new CheckpointerImpl(this);
  if (dbConfigFile.isConversionRequired()) {
    Logging.logMessage(Logging.LEVEL_WARN,Category.db,this,""String_Node_Str"");
    AutoConverter.initiateConversion(dbConfigFile.getDBFormatVersion(),conf);
  }
  snapshotManager.init();
  try {
    if (conf instanceof ReplicationConfig)     DirectFileIO.replayBackupFiles((ReplicationConfig)conf);
  }
 catch (  IOException io) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + ""String_Node_Str"",io.getMessage());
  }
  LSN dbLsn=null;
  for (  Database db : databaseManager.getDatabaseList()) {
    if (dbLsn == null)     dbLsn=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
 else {
      LSN onDiskLSN=((DatabaseImpl)db).getLSMDB().getOndiskLSN();
      if (!LSMDatabase.NO_DB_LSN.equals(onDiskLSN) && !dbLsn.equals(onDiskLSN))       throw new RuntimeException(""String_Node_Str"");
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs(dbLsn);
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  LSN lastLSN=new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1);
  try {
    if (conf instanceof ReplicationConfig)     this.replicationManager=new ReplicationManagerImpl(this,lastLSN);
 else     this.replicationManager=null;
  }
 catch (  Exception e) {
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,e.getMessage());
  }
  try {
    this.logger=new DiskLogger(conf.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),conf.getSyncMode(),conf.getPseudoSyncWait(),conf.getMaxQueueLength() * conf.getNumThreads(),replicationManager);
    this.logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
  if (conf.getNumThreads() > 0) {
    worker=new LSMDBWorker[conf.getNumThreads()];
    for (int i=0; i < conf.getNumThreads(); i++) {
      worker[i]=new LSMDBWorker(logger,i,(conf.getPseudoSyncWait() > 0),conf.getMaxQueueLength());
      worker[i].start();
    }
  }
 else {
    assert(conf.getNumThreads() == 0);
    worker=null;
  }
  if (dbConfigFile.isConversionRequired())   AutoConverter.completeConversion(this);
  dbCheckptr.init(logger,conf.getCheckInterval(),conf.getMaxLogfileSize());
  dbCheckptr.start();
  this.stopped=new AtomicBoolean(false);
  if (this.replicationManager != null)   this.replicationManager.initialize();
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}",0.992337164750958
132376,"public void deletePersistentSnapshot(String dbName,String snapshotName,boolean createLogEntry) throws BabuDBException {
  final Map<String,Snapshot> snapMap=snapshotDBs.get(dbName);
  final Snapshot snap=snapMap.get(snapshotName);
  if (snap == null)   throw new BabuDBException(ErrorCode.NO_SUCH_SNAPSHOT,""String_Node_Str"" + snapshotName + ""String_Node_Str"");
  snap.getView().shutdown();
  snapMap.remove(snapshotName);
  ((CheckpointerImpl)dbs.getCheckpointer()).removeSnapshotMaterializationRequest(dbName,snapshotName);
  FSUtils.delTree(new File(getSnapshotDir(dbName,snapshotName)));
  if (createLogEntry) {
    byte[] data=new byte[1 + dbName.length() + snapshotName.length()];
    byte[] dbNameBytes=dbName.getBytes();
    byte[] snapNameBytes=snapshotName.getBytes();
    assert(dbName.length() <= Byte.MAX_VALUE);
    data[0]=(byte)dbName.length();
    System.arraycopy(dbNameBytes,0,data,1,dbNameBytes.length);
    System.arraycopy(snapNameBytes,0,data,1 + dbNameBytes.length,snapNameBytes.length);
    ReusableBuffer buf=ReusableBuffer.wrap(data);
    DatabaseManagerImpl.metaInsert(LogEntry.PAYLOAD_TYPE_SNAP_DELETE,buf,dbs.getLogger());
  }
}","public void deletePersistentSnapshot(String dbName,String snapshotName,boolean createLogEntry) throws BabuDBException {
  final Map<String,Snapshot> snapMap=snapshotDBs.get(dbName);
  final Snapshot snap=snapMap.get(snapshotName);
  if (snap == null) {
    if (!createLogEntry)     return;
    throw new BabuDBException(ErrorCode.NO_SUCH_SNAPSHOT,""String_Node_Str"" + snapshotName + ""String_Node_Str"");
  }
  snap.getView().shutdown();
  snapMap.remove(snapshotName);
  ((CheckpointerImpl)dbs.getCheckpointer()).removeSnapshotMaterializationRequest(dbName,snapshotName);
  FSUtils.delTree(new File(getSnapshotDir(dbName,snapshotName)));
  if (createLogEntry) {
    byte[] data=new byte[1 + dbName.length() + snapshotName.length()];
    byte[] dbNameBytes=dbName.getBytes();
    byte[] snapNameBytes=snapshotName.getBytes();
    assert(dbName.length() <= Byte.MAX_VALUE);
    data[0]=(byte)dbName.length();
    System.arraycopy(dbNameBytes,0,data,1,dbNameBytes.length);
    System.arraycopy(snapNameBytes,0,data,1 + dbNameBytes.length,snapNameBytes.length);
    ReusableBuffer buf=ReusableBuffer.wrap(data);
    DatabaseManagerImpl.metaInsert(LogEntry.PAYLOAD_TYPE_SNAP_DELETE,buf,dbs.getLogger());
  }
}",0.9809241203899958
132377,"/** 
 * Triggers the creation of a persistent snapshot of a database. Snapshot properties can be determined in a fine-grained manner, i.e. single key ranges can be selected from single indices.
 * @param dbName the name of the database to create the snapshot from
 * @param snap the snapshot configuration
 * @throws BabuDBException if snapshot creation failed
 */
public void createPersistentSnapshot(String dbName,SnapshotConfig snap,boolean createLogEntry) throws BabuDBException {
  try {
    Map<String,Snapshot> snapMap=snapshotDBs.get(dbName);
    if (snapMap == null) {
      snapMap=new HashMap<String,Snapshot>();
      snapshotDBs.put(dbName,snapMap);
    }
    if (snapMap.containsKey(snap.getName()))     throw new BabuDBException(ErrorCode.SNAP_EXISTS,""String_Node_Str"" + snap.getName() + ""String_Node_Str"");
    snapMap.put(snap.getName(),new Snapshot(null));
    int[] snapIds=((DatabaseImpl)dbs.getDatabaseManager().getDatabase(dbName)).createSnapshot(snap,createLogEntry);
    ((CheckpointerImpl)dbs.getCheckpointer()).addSnapshotMaterializationRequest(dbName,snapIds,snap);
synchronized (snapshotDBs) {
      Snapshot s=snapMap.get(snap.getName());
      if (s.getView() == null)       s.setView(new InMemoryView(dbs,dbName,snap,snapIds));
    }
  }
 catch (  InterruptedException exc) {
    throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",exc);
  }
}","/** 
 * Triggers the creation of a persistent snapshot of a database. Snapshot properties can be determined in a fine-grained manner, i.e. single key ranges can be selected from single indices.
 * @param dbName the name of the database to create the snapshot from
 * @param snap the snapshot configuration
 * @throws BabuDBException if snapshot creation failed
 */
public void createPersistentSnapshot(String dbName,SnapshotConfig snap,boolean createLogEntry) throws BabuDBException {
  try {
    Map<String,Snapshot> snapMap=snapshotDBs.get(dbName);
    if (snapMap == null) {
      snapMap=new HashMap<String,Snapshot>();
      snapshotDBs.put(dbName,snapMap);
    }
    if (snapMap.containsKey(snap.getName())) {
      if (!createLogEntry)       return;
      throw new BabuDBException(ErrorCode.SNAP_EXISTS,""String_Node_Str"" + snap.getName() + ""String_Node_Str"");
    }
    snapMap.put(snap.getName(),new Snapshot(null));
    int[] snapIds=((DatabaseImpl)dbs.getDatabaseManager().getDatabase(dbName)).createSnapshot(snap,createLogEntry);
    ((CheckpointerImpl)dbs.getCheckpointer()).addSnapshotMaterializationRequest(dbName,snapIds,snap);
synchronized (snapshotDBs) {
      Snapshot s=snapMap.get(snap.getName());
      if (s.getView() == null)       s.setView(new InMemoryView(dbs,dbName,snap,snapIds));
    }
  }
 catch (  InterruptedException exc) {
    throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",exc);
  }
}",0.9819978821037768
132378,"public DiskLogIterator(File[] logFiles,LSN from){
  this.from=from;
  try {
    if (logFiles != null) {
      dbLogDir=logFiles[0].getParent() + ""String_Node_Str"";
      int count=-1;
      SortedSet<LSN> orderedLogList=new TreeSet<LSN>();
      Pattern p=Pattern.compile(""String_Node_Str"");
      for (      File logFile : logFiles) {
        Matcher m=p.matcher(logFile.getName());
        m.matches();
        String tmp=m.group(1);
        int viewId=Integer.valueOf(tmp);
        tmp=m.group(2);
        int seqNo=Integer.valueOf(tmp);
        orderedLogList.add(new LSN(viewId,seqNo));
        count++;
      }
      LSN[] copy=orderedLogList.toArray(new LSN[orderedLogList.size()]);
      LSN last=null;
      for (      LSN lsn : copy) {
        if (last == null)         last=lsn;
 else {
          if (from != null && lsn.compareTo(from) <= 0) {
            orderedLogList.remove(last);
            last=lsn;
          }
 else           break;
        }
      }
      logList=orderedLogList.iterator();
      findFirstEntry();
    }
  }
 catch (  LogEntryException exc) {
    throw new RuntimeException(exc);
  }
catch (  IOException exc) {
    throw new RuntimeException(exc);
  }
}","public DiskLogIterator(File[] logFiles,LSN from){
  this.from=from;
  try {
    if (logFiles != null && logFiles.length > 0) {
      dbLogDir=logFiles[0].getParent() + ""String_Node_Str"";
      int count=-1;
      SortedSet<LSN> orderedLogList=new TreeSet<LSN>();
      Pattern p=Pattern.compile(""String_Node_Str"");
      for (      File logFile : logFiles) {
        Matcher m=p.matcher(logFile.getName());
        m.matches();
        String tmp=m.group(1);
        int viewId=Integer.valueOf(tmp);
        tmp=m.group(2);
        int seqNo=Integer.valueOf(tmp);
        orderedLogList.add(new LSN(viewId,seqNo));
        count++;
      }
      LSN[] copy=orderedLogList.toArray(new LSN[orderedLogList.size()]);
      LSN last=null;
      for (      LSN lsn : copy) {
        if (last == null)         last=lsn;
 else {
          if (from != null && lsn.compareTo(from) <= 0) {
            orderedLogList.remove(last);
            last=lsn;
          }
 else           break;
        }
      }
      logList=orderedLogList.iterator();
      findFirstEntry();
    }
  }
 catch (  LogEntryException exc) {
    throw new RuntimeException(exc);
  }
catch (  IOException exc) {
    throw new RuntimeException(exc);
  }
}",0.9904524699045248
132379,"public static void initiateConversion(int dbVer,final BabuDBConfig cfg) throws BabuDBException {
  if (!DBWriter.checkVersionSupport(dbVer))   throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + dbVer + ""String_Node_Str""+ BabuDB.BABUDB_DB_FORMAT_VERSION+ ""String_Node_Str"");
 else   Logging.logMessage(Logging.LEVEL_INFO,Category.db,(Object)null,""String_Node_Str"");
  final File dbDir=new File(cfg.getBaseDir());
  final File dbLogDir=new File(cfg.getDbLogDir());
  final File targetDir=new File(dbDir,""String_Node_Str"");
  final File cfgFile=new File(dbDir,cfg.getDbCfgFile());
  final File backupDir=new File(cfg.getBaseDir(),""String_Node_Str"" + dbVer);
  final File backupDBDir=new File(backupDir,""String_Node_Str"");
  final File backupLogDir=new File(backupDir,""String_Node_Str"");
  if (targetDir.exists())   FSUtils.delTree(targetDir);
  try {
    DBWriter.writeDB(cfg,dbVer,targetDir.getAbsolutePath());
    File[] dbFilesToMove=dbDir.listFiles(new FileFilter(){
      public boolean accept(      File pathname){
        return !(pathname.equals(targetDir) || pathname.equals(cfgFile) || pathname.equals(backupDir));
      }
    }
);
    File[] logFilesToMove=dbLogDir.equals(dbDir) ? new File[0] : dbLogDir.listFiles();
    backupDBDir.mkdirs();
    backupLogDir.mkdirs();
    if (backupDBDir.exists() && backupLogDir.exists()) {
      for (      File f : dbFilesToMove)       if (!f.renameTo(new File(backupDBDir,f.getName())))       throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + f.getAbsolutePath() + ""String_Node_Str"");
      for (      File f : logFilesToMove)       if (!f.renameTo(new File(backupLogDir,f.getName())))       throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + f.getAbsolutePath() + ""String_Node_Str"");
    }
 else     throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"");
    FSUtils.copyTree(cfgFile,new File(backupDir,cfgFile.getName()));
  }
 catch (  IOException exc) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",exc);
  }
}","public static void initiateConversion(int dbVer,final BabuDBConfig cfg) throws BabuDBException {
  if (!DBWriter.checkVersionSupport(dbVer))   throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + dbVer + ""String_Node_Str""+ BabuDB.BABUDB_DB_FORMAT_VERSION+ ""String_Node_Str"");
 else   Logging.logMessage(Logging.LEVEL_INFO,Category.db,(Object)null,""String_Node_Str"");
  final File dbDir=new File(cfg.getBaseDir());
  final File dbLogDir=new File(cfg.getDbLogDir());
  final File targetDir=new File(dbDir,""String_Node_Str"");
  final File cfgFile=new File(dbDir,cfg.getDbCfgFile());
  final File backupDir=new File(cfg.getBaseDir(),""String_Node_Str"" + dbVer);
  final File backupDBDir=new File(backupDir,""String_Node_Str"");
  final File backupLogDir=new File(backupDir,""String_Node_Str"");
  if (targetDir.exists())   FSUtils.delTree(targetDir);
  try {
    DBWriter.writeDB(cfg,dbVer,targetDir.getAbsolutePath());
    File[] dbFilesToMove=dbDir.listFiles(new FileFilter(){
      public boolean accept(      File pathname){
        return !(pathname.equals(targetDir) || pathname.equals(cfgFile) || pathname.equals(backupDir)|| pathname.equals(dbLogDir));
      }
    }
);
    File[] logFilesToMove=dbLogDir.equals(dbDir) ? new File[0] : dbLogDir.listFiles();
    backupDBDir.mkdirs();
    backupLogDir.mkdirs();
    if (backupDBDir.exists() && backupLogDir.exists()) {
      for (      File f : dbFilesToMove)       if (!f.renameTo(new File(backupDBDir,f.getName())))       throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + f.getAbsolutePath() + ""String_Node_Str"");
      for (      File f : logFilesToMove)       if (!f.renameTo(new File(backupLogDir,f.getName())))       throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"" + f.getAbsolutePath() + ""String_Node_Str"");
    }
 else     throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"");
    FSUtils.copyTree(cfgFile,new File(backupDir,cfgFile.getName()));
  }
 catch (  IOException exc) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",exc);
  }
}",0.9931740614334472
132380,"public void testDescendingPrefixLookup() throws Exception {
  final String[] keys={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final String[] vals={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  TreeMap<byte[],byte[]> testMap=new TreeMap<byte[],byte[]>(COMP);
  for (int i=0; i < keys.length; i++)   testMap.put(keys[i].getBytes(),vals[i].getBytes());
  new File(PATH2).delete();
  DiskIndexWriter index=new DiskIndexWriter(PATH2,4,COMPRESSED);
  index.writeIndex(testMap.entrySet().iterator());
  DiskIndex diskIndex=new DiskIndex(PATH2,new DefaultByteRangeComparator(),COMPRESSED);
  Iterator<Entry<byte[],byte[]>> it=diskIndex.rangeLookup(keys[1].getBytes(),keys[5].getBytes(),false);
  for (int i=4; i > 0; i--) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  for (int i=6; i > 0; i--) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  assertFalse(it.hasNext());
  index=new DiskIndexWriter(PATH1,4,COMPRESSED);
  index.writeIndex(new HashMap().entrySet().iterator());
  diskIndex=new DiskIndex(PATH1,new DefaultByteRangeComparator(),COMPRESSED);
  it=diskIndex.rangeLookup(new byte[0],new byte[0],false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,null,false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),null,false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,""String_Node_Str"".getBytes(),false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  assertFalse(it.hasNext());
}","public void testDescendingPrefixLookup() throws Exception {
  final String[] keys={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final String[] vals={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  TreeMap<byte[],byte[]> testMap=new TreeMap<byte[],byte[]>(COMP);
  for (int i=0; i < keys.length; i++)   testMap.put(keys[i].getBytes(),vals[i].getBytes());
  new File(PATH2).delete();
  DiskIndexWriter index=new DiskIndexWriter(PATH2,4,COMPRESSED);
  index.writeIndex(testMap.entrySet().iterator());
  DiskIndex diskIndex=new DiskIndex(PATH2,new DefaultByteRangeComparator(),COMPRESSED);
  Iterator<Entry<byte[],byte[]>> it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  for (int i=5; i >= 1; i--) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  for (int i=6; i > 0; i--) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  assertFalse(it.hasNext());
  index=new DiskIndexWriter(PATH1,4,COMPRESSED);
  index.writeIndex(new HashMap().entrySet().iterator());
  diskIndex=new DiskIndex(PATH1,new DefaultByteRangeComparator(),COMPRESSED);
  it=diskIndex.rangeLookup(new byte[0],new byte[0],false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,null,false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),null,false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,""String_Node_Str"".getBytes(),false);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),false);
  assertFalse(it.hasNext());
}",0.9672447013487476
132381,"public void testPrefixLookup() throws Exception {
  final String[] keys={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final String[] vals={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  SortedMap<byte[],byte[]> testMap=new TreeMap<byte[],byte[]>(COMP);
  for (int i=0; i < keys.length; i++)   testMap.put(keys[i].getBytes(),vals[i].getBytes());
  new File(PATH2).delete();
  DiskIndexWriter index=new DiskIndexWriter(PATH2,4,COMPRESSED);
  index.writeIndex(testMap.entrySet().iterator());
  DiskIndex diskIndex=new DiskIndex(PATH2,new DefaultByteRangeComparator(),COMPRESSED);
  Iterator<Entry<byte[],byte[]>> it=diskIndex.rangeLookup(keys[1].getBytes(),keys[5].getBytes(),true);
  for (int i=1; i < 5; i++) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  for (int i=1; i < 7; i++) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  assertFalse(it.hasNext());
  index=new DiskIndexWriter(PATH1,4,COMPRESSED);
  index.writeIndex(new HashMap().entrySet().iterator());
  diskIndex=new DiskIndex(PATH1,new DefaultByteRangeComparator(),COMPRESSED);
  it=diskIndex.rangeLookup(new byte[0],new byte[0],true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,null,true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),null,true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,""String_Node_Str"".getBytes(),true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  assertFalse(it.hasNext());
}","public void testPrefixLookup() throws Exception {
  final String[] keys={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final String[] vals={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  SortedMap<byte[],byte[]> testMap=new TreeMap<byte[],byte[]>(COMP);
  for (int i=0; i < keys.length; i++)   testMap.put(keys[i].getBytes(),vals[i].getBytes());
  new File(PATH2).delete();
  DiskIndexWriter index=new DiskIndexWriter(PATH2,4,COMPRESSED);
  index.writeIndex(testMap.entrySet().iterator());
  DiskIndex diskIndex=new DiskIndex(PATH2,new DefaultByteRangeComparator(),COMPRESSED);
  Iterator<Entry<byte[],byte[]>> it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  for (int i=1; i < 5; i++) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  for (int i=1; i < 7; i++) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  assertFalse(it.hasNext());
  index=new DiskIndexWriter(PATH1,4,COMPRESSED);
  index.writeIndex(new HashMap().entrySet().iterator());
  diskIndex=new DiskIndex(PATH1,new DefaultByteRangeComparator(),COMPRESSED);
  it=diskIndex.rangeLookup(new byte[0],new byte[0],true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,null,true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),null,true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,""String_Node_Str"".getBytes(),true);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),true);
  assertFalse(it.hasNext());
}",0.989646246764452
132382,"private static String createRandomString(int minLength,int maxLength){
  char[] chars=new char[(int)(Math.random() * (maxLength + 1)) + minLength];
  for (int i=0; i < chars.length; i++)   chars[i]=(char)(Math.random() * 0xFFFF);
  return new String(chars);
}","private static String createRandomString(int minLength,int maxLength){
  char[] chars=new char[(int)(rnd.nextDouble() * (maxLength + 1)) + minLength];
  for (int i=0; i < chars.length; i++)   chars[i]=(char)(rnd.nextDouble() * 0xFFFF);
  return new String(chars);
}",0.9122137404580152
132383,"public void declareToMaster() throws NotEnoughAvailableSlavesException, IOException, ONCRPCException, InterruptedException, BabuDBException {
  if (dispatcher.isMaster)   return;
  DispatcherState state=stop();
  if (state.requestQueue != null)   for (  StageRequest rq : state.requestQueue)   rq.free();
  state.requestQueue=null;
  this.dispatcher=new SlaveRequestDispatcher(this.dispatcher);
  SlaveRequestDispatcher lDisp=(SlaveRequestDispatcher)dispatcher;
  List<InetSocketAddress> slaves=new LinkedList<InetSocketAddress>(dispatcher.configuration.getParticipants());
  slaves.remove(dispatcher.configuration.getInetSocketAddress());
  if (slaves.size() > 0) {
    Map<InetSocketAddress,LSN> states=stopAll(slaves);
    if (states.size() < dispatcher.configuration.getSyncN())     throw new NotEnoughAvailableSlavesException(""String_Node_Str"");
    if (states.size() > 0) {
      List<LSN> values=new LinkedList<LSN>(states.values());
      Collections.sort(values);
      LSN latest=values.get(values.size() - 1);
      if (latest.compareTo(state.latest) > 0) {
        for (        Entry<InetSocketAddress,LSN> entry : states.entrySet()) {
          if (entry.getValue().equals(latest)) {
            lDisp.coin(entry.getKey());
            StateClient c=new StateClient(dispatcher.rpcClient,entry.getKey());
            RPCResponse<Object> rp=null;
            try {
              rp=c.toMaster();
              rp.get();
            }
  finally {
              if (rp != null)               rp.freeBuffers();
            }
            lDisp.synchronize(state.latest,latest);
            RPCResponse<LSN> rps=null;
            try {
              rps=c.remoteStop();
              rps.get();
            }
  finally {
              if (rps != null)               rps.freeBuffers();
            }
            break;
          }
        }
      }
    }
    allToSlaves(slaves,dispatcher.configuration.getSyncN(),dispatcher.configuration.getInetSocketAddress());
  }
  this.dispatcher=new MasterRequestDispatcher(dispatcher,dispatcher.configuration.getInetSocketAddress());
  restart(state);
}","public void declareToMaster() throws NotEnoughAvailableSlavesException, IOException, ONCRPCException, InterruptedException, BabuDBException {
  if (dispatcher.isMaster)   return;
  DispatcherState state=null;
  if (isRunning()) {
    state=stop();
  }
 else {
    state=dispatcher.getState();
  }
  if (state.requestQueue != null)   for (  StageRequest rq : state.requestQueue)   rq.free();
  state.requestQueue=null;
  this.dispatcher=new SlaveRequestDispatcher(this.dispatcher);
  SlaveRequestDispatcher lDisp=(SlaveRequestDispatcher)dispatcher;
  List<InetSocketAddress> slaves=new LinkedList<InetSocketAddress>(dispatcher.configuration.getParticipants());
  slaves.remove(dispatcher.configuration.getInetSocketAddress());
  if (slaves.size() > 0) {
    Map<InetSocketAddress,LSN> states=stopAll(slaves);
    if (states.size() < dispatcher.configuration.getSyncN())     throw new NotEnoughAvailableSlavesException(""String_Node_Str"");
    if (states.size() > 0) {
      List<LSN> values=new LinkedList<LSN>(states.values());
      Collections.sort(values);
      LSN latest=values.get(values.size() - 1);
      if (latest.compareTo(state.latest) > 0) {
        for (        Entry<InetSocketAddress,LSN> entry : states.entrySet()) {
          if (entry.getValue().equals(latest)) {
            lDisp.coin(entry.getKey());
            StateClient c=new StateClient(dispatcher.rpcClient,entry.getKey());
            RPCResponse<Object> rp=null;
            try {
              rp=c.toMaster();
              rp.get();
            }
  finally {
              if (rp != null)               rp.freeBuffers();
            }
            lDisp.synchronize(state.latest,latest);
            RPCResponse<LSN> rps=null;
            try {
              rps=c.remoteStop();
              rps.get();
            }
  finally {
              if (rps != null)               rps.freeBuffers();
            }
            break;
          }
        }
      }
    }
    allToSlaves(slaves,dispatcher.configuration.getSyncN(),dispatcher.configuration.getInetSocketAddress());
  }
  this.dispatcher=new MasterRequestDispatcher(dispatcher,dispatcher.configuration.getInetSocketAddress());
  restart(state);
}",0.976657329598506
132384,"public void setLogic(LogicID lgc,SimplifiedBabuDBRequestListener listener){
  setLogic(lgc,""String_Node_Str"");
}","/** 
 * <p>Changes the currently used logic to the given <code>lgc</code>.</p> <b>WARNING: This operation is not thread safe!</b> <br>
 * @param lgc
 * @param reason - for the logic change, needed for logging purpose.
 */
public void setLogic(LogicID lgc,String reason){
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",lgc.toString(),reason);
  this.logicID=lgc;
  if (listener != null && lgc.equals(BASIC)) {
    listener.finished(null);
    listener=null;
  }
}",0.2067796610169491
132385,"/** 
 * @param args - first argument has to be the {@link LSN} for ordering the requests.
 */
public StageRequest(Object[] args){
  this.args=args;
  if (args[0] instanceof LSN)   this.lsn=(LSN)args[0];
 else   this.lsn=null;
}","/** 
 * @param args - first argument has to be the {@link LSN} for ordering the requests.
 */
public StageRequest(Object[] args){
  this.args=args;
  if (args != null && args[0] instanceof LSN)   this.lsn=(LSN)args[0];
 else   this.lsn=null;
}",0.9659574468085106
132386,"@Override public void responseAvailable(RPCResponse<?> r){
  if (count.incrementAndGet() == responses.length) {
    listener.responsesAvailable();
  }
}","@Override public void responseAvailable(RPCResponse r){
  if (count.incrementAndGet() == responses.length) {
    listener.responsesAvailable();
  }
}",0.9900332225913622
132387,"/** 
 * Wait for responses of a broadcast-request.
 * @param responses
 * @param listener
 */
@SuppressWarnings(""String_Node_Str"") public void waitForResponses(final RPCResponse<?>[] responses,final ResponsesListener listener){
  assert(responses.length > 0);
  final AtomicInteger count=new AtomicInteger(0);
  final RPCResponseAvailableListener<?> l=new RPCResponseAvailableListener<?>(){
    @Override public void responseAvailable(    RPCResponse<?> r){
      if (count.incrementAndGet() == responses.length) {
        listener.responsesAvailable();
      }
    }
  }
;
  for (  RPCResponse r : responses) {
    r.registerListener(l);
  }
}","/** 
 * Wait for responses of a broadcast-request.
 * @param responses
 * @param listener
 */
@SuppressWarnings(""String_Node_Str"") public void waitForResponses(final RPCResponse[] responses,final ResponsesListener listener){
  assert(responses.length > 0);
  final AtomicInteger count=new AtomicInteger(0);
  final RPCResponseAvailableListener l=new RPCResponseAvailableListener(){
    @Override public void responseAvailable(    RPCResponse r){
      if (count.incrementAndGet() == responses.length) {
        listener.responsesAvailable();
      }
    }
  }
;
  for (  RPCResponse r : responses) {
    r.registerListener(l);
  }
}",0.9905956112852664
132388,"/** 
 * shut down files.
 */
protected void finalize() throws Throwable {
  try {
    fdes.sync();
    fos.close();
  }
 catch (  SyncFailedException ex) {
  }
catch (  IOException ex) {
  }
 finally {
    super.finalize();
  }
}","/** 
 * shut down files.
 */
protected void finalize() throws Throwable {
  try {
    fdes.sync();
    fos.close();
  }
 catch (  IOException ex) {
  }
 finally {
    super.finalize();
  }
}",0.9069212410501192
132389,"@Override public void responseAvailable(RPCResponse<Object> r){
  try {
    r.get();
    markSlaveAsFinished(slave);
  }
 catch (  Exception e) {
    result.decrementPermittedFailures();
    markSlaveAsDead(slave);
  }
 finally {
    if (r != null)     r.freeBuffers();
  }
}","@Override public void responseAvailable(RPCResponse<Object> r){
  try {
    r.get();
    markSlaveAsFinished(slave);
  }
 catch (  Exception e) {
    markSlaveAsDead(slave);
    result.decrementPermittedFailures();
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",slave.getDefaultServerAddress().toString(),e.getMessage());
    if (e.getMessage() == null)     Logging.logError(Logging.LEVEL_DEBUG,this,e);
  }
 finally {
    if (r != null)     r.freeBuffers();
  }
}",0.6472919418758256
132390,"@SuppressWarnings(""String_Node_Str"") @Override protected void _replicate(final LogEntry le) throws NotEnoughAvailableSlavesException, InterruptedException, IOException {
  try {
    ReusableBuffer buffer=le.serialize(checksum);
    List<SlaveClient> slaves=getSlavesForBroadCast();
    final ReplicateResponse result=new ReplicateResponse(le.getLSN(),slaves.size() - getSyncN()){
      @Override public void upToDate(){
        super.upToDate();
        le.getListener().synced(le);
      }
    }
;
    subscribeListener(result);
    if (slaves.size() == 0) {
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + ""String_Node_Str"");
      if (buffer != null)       BufferPool.free(buffer);
    }
 else {
      for (      final SlaveClient slave : slaves) {
        ((RPCResponse<Object>)slave.replicate(le.getLSN(),buffer)).registerListener(new RPCResponseAvailableListener<Object>(){
          @Override public void responseAvailable(          RPCResponse<Object> r){
            try {
              r.get();
              markSlaveAsFinished(slave);
            }
 catch (            Exception e) {
              result.decrementPermittedFailures();
              markSlaveAsDead(slave);
            }
 finally {
              if (r != null)               r.freeBuffers();
            }
          }
        }
);
      }
    }
  }
  finally {
    checksum.reset();
  }
}","@SuppressWarnings(""String_Node_Str"") @Override protected ReplicateResponse _replicate(LogEntry le,ReusableBuffer payload) throws NotEnoughAvailableSlavesException, InterruptedException {
  List<SlaveClient> slaves=getSlavesForBroadCast();
  final ReplicateResponse result=new ReplicateResponse(le,slaves.size() - getSyncN());
  if (slaves.size() == 0) {
    Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + ""String_Node_Str"");
  }
 else {
    for (    final SlaveClient slave : slaves) {
      ((RPCResponse<Object>)slave.replicate(le.getLSN(),payload.createViewBuffer())).registerListener(new RPCResponseAvailableListener<Object>(){
        @Override public void responseAvailable(        RPCResponse<Object> r){
          try {
            r.get();
            markSlaveAsFinished(slave);
          }
 catch (          Exception e) {
            markSlaveAsDead(slave);
            result.decrementPermittedFailures();
            Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"",slave.getDefaultServerAddress().toString(),e.getMessage());
            if (e.getMessage() == null)             Logging.logError(Logging.LEVEL_DEBUG,this,e);
          }
 finally {
            if (r != null)             r.freeBuffers();
          }
        }
      }
);
    }
  }
  return result;
}",0.610760667903525
132391,"/** 
 * If an instance has to be notified about latest   {@link LSN} changes,register it here.
 * @param listener
 */
public void subscribeListener(LatestLSNUpdateListener listener){
  states.subscribeListener(listener);
}","@Override public void subscribeListener(LatestLSNUpdateListener listener){
  states.subscribeListener(listener);
}",0.6369047619047619
132392,"/** 
 * Use this function to update the permitted failures on this response.
 */
synchronized void decrementPermittedFailures(){
  if (--permittedFailures == 0 && !finished) {
    failed=true;
    finished=true;
    notify();
  }
}","/** 
 * Use this function to update the permitted failures on this response.
 */
synchronized void decrementPermittedFailures(){
  if (permittedFailures == 0 && !finished) {
    finished=true;
    logEntry.getListener().failed(logEntry,new BabuDBException(ErrorCode.REPLICATION_FAILURE,""String_Node_Str"" + ""String_Node_Str""));
  }
  permittedFailures--;
}",0.4778156996587031
132393,"@Override public synchronized void upToDate(){
  finished=true;
  notify();
}","@Override public synchronized void upToDate(){
  if (!finished) {
    finished=true;
    logEntry.getListener().synced(logEntry);
  }
}",0.6981132075471698
132394,"/** 
 * Initializes the response object waiting for the given   {@link LSN} to becomethe next stable state.
 * @param lsn
 * @param slavesThatCanFail - buffer for negative RPCResponses.
 */
ReplicateResponse(LSN lsn,int slavesThatCanFail){
  super(lsn);
  permittedFailures=slavesThatCanFail;
}","/** 
 * Initializes the response object waiting for the given   {@link LSN} to becomethe next stable state.
 * @param le - {@link LogEntry} associated with the {@link LSN}.
 * @param slavesThatCanFail - buffer for negative RPCResponses.
 */
ReplicateResponse(LogEntry le,int slavesThatCanFail){
  super(le.getLSN());
  logEntry=le;
  permittedFailures=slavesThatCanFail;
}",0.7597597597597597
132395,"/** 
 * <p> Needed for the initial load process of the babuDB, done by the replication. </p>
 * @throws BabuDBException
 * @return the next LSN.
 */
public LSN restart() throws BabuDBException {
  if (!this.stopped)   throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"");
  databaseManager.reset();
  dbCheckptr=new CheckpointerImpl(this);
  LSN dbLsn=null;
  LSN zero=new LSN(0,0L);
  for (  Database dbRaw : databaseManager.getDatabaseList()) {
    DatabaseImpl db=(DatabaseImpl)dbRaw;
    LSN onDisk=db.getLSMDB().getOndiskLSN();
    if (dbLsn == null && !onDisk.equals(zero))     dbLsn=onDisk;
 else     if (dbLsn != null) {
      if (!onDisk.equals(zero) && !dbLsn.equals(onDisk))       throw new RuntimeException(""String_Node_Str"" + dbLsn.toString() + ""String_Node_Str""+ db.getLSMDB().getOndiskLSN().toString());
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs(dbLsn);
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  try {
    logger=new DiskLogger(configuration.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo() + 1L,configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads());
    logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
  worker=new LSMDBWorker[configuration.getNumThreads()];
  for (int i=0; i < configuration.getNumThreads(); i++) {
    worker[i]=new LSMDBWorker(logger,i,(configuration.getPseudoSyncWait() > 0),configuration.getMaxQueueLength(),replicationManager);
    worker[i].start();
  }
  dbCheckptr.init(logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
  dbCheckptr.start();
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
  this.stopped=false;
  return new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo());
}","/** 
 * <p> Needed for the initial load process of the babuDB, done by the replication. </p>
 * @throws BabuDBException
 * @return the next LSN.
 */
public LSN restart() throws BabuDBException {
  if (!this.stopped)   throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"");
  databaseManager.reset();
  dbCheckptr=new CheckpointerImpl(this);
  LSN dbLsn=null;
  LSN zero=new LSN(0,0L);
  for (  Database dbRaw : databaseManager.getDatabaseList()) {
    DatabaseImpl db=(DatabaseImpl)dbRaw;
    LSN onDisk=db.getLSMDB().getOndiskLSN();
    if (dbLsn == null && !onDisk.equals(zero))     dbLsn=onDisk;
 else     if (dbLsn != null) {
      if (!onDisk.equals(zero) && !dbLsn.equals(onDisk))       throw new RuntimeException(""String_Node_Str"" + dbLsn.toString() + ""String_Node_Str""+ db.getLSMDB().getOndiskLSN().toString());
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs(dbLsn);
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  try {
    logger=new DiskLogger(configuration.getDbLogDir(),nextLSN.getViewId(),nextLSN.getSequenceNo(),configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads());
    logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
  worker=new LSMDBWorker[configuration.getNumThreads()];
  for (int i=0; i < configuration.getNumThreads(); i++) {
    worker[i]=new LSMDBWorker(logger,i,(configuration.getPseudoSyncWait() > 0),configuration.getMaxQueueLength(),replicationManager);
    worker[i].start();
  }
  dbCheckptr.init(logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
  dbCheckptr.start();
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + ""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
  this.stopped=false;
  return new LSN(nextLSN.getViewId(),nextLSN.getSequenceNo() - 1L);
}",0.9976958525345622
132396,"/** 
 * <p>Clean Up operation to avoid file ambiguity.</p> 
 * @param parent - the path to cleanUp.
 */
private static void cleanUpFiles(File parent){
  if (parent.exists()) {
    assert(parent.isDirectory());
    for (    File f : parent.listFiles()) {
      if (f.isFile() && !f.getName().contains(""String_Node_Str""))       f.delete();
 else       if (f.isDirectory()) {
        cleanUpFiles(f);
        if (f.listFiles().length == 0)         f.delete();
      }
    }
  }
}","/** 
 * <p>Clean Up operation to avoid file ambiguity.</p> 
 * @param parent - the path to cleanUp.
 */
private static void cleanUpFiles(File parent){
  if (parent.exists()) {
    assert(parent.isDirectory());
    for (    File f : parent.listFiles()) {
      if (f.isFile())       f.delete();
 else       if (f.isDirectory()) {
        cleanUpFiles(f);
        if (f.listFiles().length == 0)         f.delete();
      }
 else       assert(false);
    }
  }
}",0.9240641711229948
132397,"/** 
 * <p>Copies the directories including files recursively from sourceDir to destDir.</p>
 * @param sourceDir
 * @param destDir
 * @param exclude - name of files to exclude from copying.
 * @throws IOException
 */
private static void copyDir(File sourceDir,File destDir) throws IOException {
  File[] files=sourceDir.listFiles();
  File newFile=null;
  for (  File f : files) {
    if (f.isFile() && !f.getName().contains(""String_Node_Str"")) {
      newFile=new File(destDir.getPath() + File.separator + f.getName());
      newFile.createNewFile();
      copyFile(f,newFile);
    }
 else     if (f.isDirectory()) {
      newFile=new File(destDir.getPath() + File.separator + f.getName());
      newFile.mkdir();
      copyDir(f,newFile);
    }
  }
}","/** 
 * <p>Copies the directories including files recursively from sourceDir to destDir.</p>
 * @param sourceDir
 * @param destDir
 * @param exclude - name of files to exclude from copying.
 * @throws IOException
 */
private static void copyDir(File sourceDir,File destDir) throws IOException {
  File[] files=sourceDir.listFiles();
  File newFile=null;
  for (  File f : files) {
    if (f.isFile()) {
      newFile=new File(destDir.getPath() + File.separator + f.getName());
      newFile.createNewFile();
      copyFile(f,newFile);
    }
 else     if (f.isDirectory()) {
      newFile=new File(destDir.getPath() + File.separator + f.getName());
      newFile.mkdir();
      copyDir(f,newFile);
    }
 else     assert(false);
  }
}",0.6181818181818182
132398,"/** 
 * <p>Changes the replication configuration.</p> <p>Makes a new checkPoint and increments the viewID,  if a <code>config</code> is a   {@link MasterConfig},  or   {@link SlaveConfig}.</p> <p><b>The replication has to be stopped before!</b></p>
 * @param config
 * @param lastState
 * @throws IOException
 * @throws BabuDBException 
 * @throws NotEnoughAvailableSlavesException 
 * @see ReplcationManager.stop()
 */
public void changeConfiguration(ReplicationConfig config,DispatcherState lastState) throws IOException, BabuDBException {
  if (config instanceof MasterConfig) {
    ((CheckpointerImpl)dbs.getCheckpointer()).checkpoint(true);
    set((MasterConfig)config,lastState);
  }
 else   if (config instanceof SlaveConfig) {
    ((CheckpointerImpl)dbs.getCheckpointer()).checkpoint(false);
    set((SlaveConfig)config,lastState);
  }
 else   assert(false);
}","/** 
 * <p>Changes the replication configuration synchronously.</p> <p>Makes a new checkPoint and increments the viewID,  if a <code>config</code> is a   {@link MasterConfig}.</p>
 * @param config
 * @throws IOException
 * @throws BabuDBException 
 */
private void changeConfiguration(ReplicationConfig config) throws IOException, BabuDBException {
  shutdown();
  DispatcherState lastState=dispatcher.getState();
  if (config instanceof MasterConfig) {
    ((CheckpointerImpl)dbs.getCheckpointer()).checkpoint(true);
    dispatcher=new MasterRequestDispatcher((MasterConfig)config,dbs,lastState);
  }
 else   if (config instanceof SlaveConfig) {
    ((CheckpointerImpl)dbs.getCheckpointer()).checkpoint(false);
    DirectFileIO.replayBackupFiles((SlaveConfig)config);
    dispatcher=new SlaveRequestDispatcher((SlaveConfig)config,dbs,lastState);
  }
 else   assert(false);
  initialize();
}",0.6693181818181818
132399,"/** 
 * <p> Makes the locally BabuDB running in master-mode. </p> <p> Stops all declared slaves and synchronizes the local BabuDB  with the latest slave. Then restarts all participants in slave mode  </p>
 * @param conf
 * @throws NotEnoughAvailableSlavesException 
 * @throws IOException 
 * @throws InterruptedException 
 * @throws ONCRPCException 
 */
public void declareToMaster(MasterConfig conf) throws NotEnoughAvailableSlavesException, IOException, ONCRPCException, InterruptedException {
  if (dispatcher.isMaster)   return;
  Map<InetSocketAddress,LSN> states=stopAll(conf.getSlaves());
  if (states.size() < conf.getSyncN())   throw new NotEnoughAvailableSlavesException(""String_Node_Str"");
  List<LSN> values=new LinkedList<LSN>(states.values());
  Collections.sort(values);
  LSN latest=values.get(values.size() - 1);
  String backupDir=new File(conf.getBaseDir()).getParent() + File.separator + ""String_Node_Str"";
  DispatcherState state=stop();
  if (state.requestQueue != null)   for (  StageRequest rq : state.requestQueue)   rq.free();
  if (latest.compareTo(state.latest) > 0) {
    for (    Entry<InetSocketAddress,LSN> entry : states.entrySet()) {
      if (entry.getValue().equals(latest)) {
        set(new SlaveConfig(conf,entry.getKey(),backupDir),dispatcher.new DispatcherState(state.latest));
        StateClient c=new StateClient(dispatcher.rpcClient,entry.getKey());
        c.toMaster(conf.getMaster()).get();
        ((SlaveRequestDispatcher)dispatcher).synchronize(latest);
        c.remoteStop().get();
        break;
      }
    }
    state=stop();
  }
  allToSlaves(conf.getSlaves(),conf.getSyncN(),conf.getMaster());
  set(conf,state);
}","/** 
 * <p> Makes the locally BabuDB running in master-mode. </p> <p> Stops all declared slaves and synchronizes the local BabuDB  with the latest slave. Then restarts all participants in slave mode  </p>
 * @param conf
 * @throws NotEnoughAvailableSlavesException 
 * @throws IOException 
 * @throws InterruptedException 
 * @throws ONCRPCException 
 * @throws BabuDBException 
 */
public void declareToMaster(MasterConfig conf) throws NotEnoughAvailableSlavesException, IOException, ONCRPCException, InterruptedException, BabuDBException {
  if (dispatcher.isMaster)   return;
  Map<InetSocketAddress,LSN> states=stopAll(conf.getSlaves());
  if (states.size() < conf.getSyncN())   throw new NotEnoughAvailableSlavesException(""String_Node_Str"");
  List<LSN> values=new LinkedList<LSN>(states.values());
  Collections.sort(values);
  LSN latest=values.get(values.size() - 1);
  String backupDir=new File(conf.getBaseDir()).getParent() + File.separator + ""String_Node_Str"";
  DispatcherState state=stop();
  if (state.requestQueue != null)   for (  StageRequest rq : state.requestQueue)   rq.free();
  if (latest.compareTo(state.latest) > 0) {
    for (    Entry<InetSocketAddress,LSN> entry : states.entrySet()) {
      if (entry.getValue().equals(latest)) {
        changeConfiguration(new SlaveConfig(conf,entry.getKey(),backupDir));
        StateClient c=new StateClient(dispatcher.rpcClient,entry.getKey());
        c.toMaster(conf.getMaster()).get();
        ((SlaveRequestDispatcher)dispatcher).synchronize(latest);
        c.remoteStop().get();
        break;
      }
    }
  }
  allToSlaves(conf.getSlaves(),conf.getSyncN(),conf.getMaster());
  changeConfiguration(conf);
}",0.9093619558735838
132400,"/** 
 * <p> Stops all currently running replication operations. Incoming requests will be rejected, even if the replication was running in master-mode and the request was a user operation, like inserts or create, copy, delete operation. </p>
 * @return dispatcher backup state.
 * @throws InterruptedException 
 */
public DispatcherState stop() throws InterruptedException {
  final AtomicBoolean ready=new AtomicBoolean(false);
  dispatcher.pauses(new SimplifiedBabuDBRequestListener(){
    @Override public void finished(    BabuDBException error){
synchronized (ready) {
        ready.set(true);
        ready.notify();
      }
    }
  }
);
synchronized (ready) {
    if (!ready.get())     ready.wait();
  }
  return dispatcher.getState();
}","/** 
 * <p> Stops all currently running replication operations. Incoming requests will be rejected, even if the replication was running in master-mode and the request was a user operation, like inserts or create, copy, delete operation. </p>
 * @see ReplicationManager.restart()
 * @return dispatcher backup state.
 * @throws InterruptedException 
 */
public DispatcherState stop() throws InterruptedException {
  final AtomicBoolean ready=new AtomicBoolean(false);
  dispatcher.pauses(new SimplifiedBabuDBRequestListener(){
    @Override public void finished(    BabuDBException error){
synchronized (ready) {
        ready.set(true);
        ready.notify();
      }
    }
  }
);
synchronized (ready) {
    if (!ready.get())     ready.wait();
  }
  return dispatcher.getState();
}",0.9757377049180328
132401,"@Override public void continues(DispatcherState state) throws BabuDBException {
  this.replication=new ReplicationStage(this,configuration.getMaxQ(),state.requestQueue,state.latest);
  this.heartbeat=new HeartbeatThread(this,state.latest);
  this.start();
  super.continues(state);
}","@Override public void continues(DispatcherState state) throws BabuDBException {
  this.replication=new ReplicationStage(this,configuration.getMaxQ(),state.requestQueue,state.latest);
  this.heartbeat=new HeartbeatThread(this,state.latest);
  try {
    replication.start();
    heartbeat.start();
    replication.waitForStartup();
    heartbeat.waitForStartup();
  }
 catch (  Exception ex) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"");
    Logging.logMessage(Logging.LEVEL_ERROR,this,ex.getMessage());
    System.exit(1);
  }
  super.continues(state);
}",0.5674418604651162
132402,"@Override public void shutdown(){
  try {
    replication.shutdown();
    heartbeat.shutdown();
    replication.waitForShutdown();
    heartbeat.waitForShutdown();
    replication.clearQueue();
  }
 catch (  Exception e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"");
  }
  super.shutdown();
}","@Override public void shutdown(){
  if (!stopped) {
    try {
      replication.shutdown();
      heartbeat.shutdown();
      replication.waitForShutdown();
      heartbeat.waitForShutdown();
      replication.clearQueue();
    }
 catch (    Exception e) {
      Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"");
    }
  }
  super.shutdown();
}",0.8130563798219584
132403,"/** 
 * Checks the last insert group of consistency.
 * @return true, if the check was successful. false, otherwise.
 * @throws Exception
 */
private static boolean performConsistencyCheck() throws Exception {
  boolean result=false;
  DispatcherState lastState=DBS.getReplicationManager().stop();
  if (lastState.latest != null) {
    LookupGroup lookupGroup=generator.getLookupGroup(lastState.latest);
    for (int i=0; i < lookupGroup.size(); i++) {
      byte[] value=DBS.hiddenLookup(lookupGroup.dbName,lookupGroup.getIndex(i),lookupGroup.getKey(i));
      if (!new String(value).equals(new String(lookupGroup.getValue(i)))) {
        System.err.println(""String_Node_Str"" + lastState.latest.toString() + ""String_Node_Str""+ ""String_Node_Str""+ new String(value)+ ""String_Node_Str""+ new String(lookupGroup.getValue(i)));
        System.exit(1);
      }
    }
    System.out.println(""String_Node_Str"" + lastState.latest.toString() + ""String_Node_Str"");
    result=true;
  }
 else   System.out.println(""String_Node_Str"");
  DBS.getReplicationManager().changeConfiguration(CONFIGURATION,lastState);
  return result;
}","/** 
 * Checks the last insert group of consistency.
 * @return true, if the check was successful. false, otherwise.
 * @throws Exception
 */
private static boolean performConsistencyCheck() throws Exception {
  boolean result=false;
  DispatcherState lastState=DBS.getReplicationManager().stop();
  if (lastState.latest != null) {
    LookupGroup lookupGroup=generator.getLookupGroup(lastState.latest);
    for (int i=0; i < lookupGroup.size(); i++) {
      byte[] value=DBS.hiddenLookup(lookupGroup.dbName,lookupGroup.getIndex(i),lookupGroup.getKey(i));
      if (!new String(value).equals(new String(lookupGroup.getValue(i)))) {
        System.err.println(""String_Node_Str"" + lastState.latest.toString() + ""String_Node_Str""+ ""String_Node_Str""+ new String(value)+ ""String_Node_Str""+ new String(lookupGroup.getValue(i)));
        System.exit(1);
      }
    }
    System.out.println(""String_Node_Str"" + lastState.latest.toString() + ""String_Node_Str"");
    result=true;
  }
 else   System.out.println(""String_Node_Str"");
  DBS.getReplicationManager().restart(lastState);
  return result;
}",0.9809610154125114
132404,"/** 
 * @param args
 * @throws Exception 
 */
public static void main(String[] args) throws Exception {
  System.out.println(""String_Node_Str"");
  Logging.start(Logging.LEVEL_WARN);
  if (args.length != 2)   usage();
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + PATH);
  p.waitFor();
  long seed=0L;
  try {
    seed=Long.valueOf(args[0]);
  }
 catch (  NumberFormatException e) {
    error(""String_Node_Str"" + args[0]);
  }
  List<InetSocketAddress> slaves=new LinkedList<InetSocketAddress>();
  if (args[1].indexOf(""String_Node_Str"") == -1)   slaves.add(parseAddress(args[1]));
 else   for (  String adr : args[1].split(""String_Node_Str""))   slaves.add(parseAddress(adr));
  generator=new ContinuesRandomGenerator(seed,ReplicationLongrunTestConfig.MAX_SEQUENCENO);
  Map<Integer,List<Object>> scenario=generator.getOperationsScenario();
  MasterConfig config=new MasterConfig(PATH,PATH,NUM_WKS,MAX_LOG_FILE_SIZE,CHECK_INTERVAL,SyncMode.ASYNC,0,0,ReplicationInterface.DEFAULT_MASTER_PORT,InetAddress.getLocalHost(),new InetSocketAddress(InetAddress.getLocalHost(),ReplicationInterface.DEFAULT_MASTER_PORT),slaves,50,null,MAX_REPLICATION_Q_LENGTH,0);
  DBS=(BabuDB)BabuDBFactory.createMasterBabuDB(config);
  int nOmetaOp=0;
  long metaOpTime=0L;
  int nOinsertOp=0;
  long insertOpTime=0L;
  long time;
  for (int i=1; i < ReplicationLongrunTestConfig.MAX_SEQUENCENO; i++) {
    List<Object> operation=scenario.get(i);
    if (operation != null) {
      nOmetaOp++;
      time=System.currentTimeMillis();
      performOperation(operation);
      metaOpTime+=System.currentTimeMillis() - time;
    }
 else {
      nOinsertOp++;
      time=System.currentTimeMillis();
      performInsert(i);
      insertOpTime+=System.currentTimeMillis() - time;
    }
  }
  double metaTroughput=((double)nOmetaOp) / (((double)metaOpTime) / 1000.0);
  double insertThroughput=((double)nOinsertOp) / (((double)insertOpTime) / 1000.0);
  System.out.println(""String_Node_Str"");
  System.out.format(""String_Node_Str"",metaTroughput);
  System.out.format(""String_Node_Str"",insertThroughput);
  DBS.shutdown();
}","/** 
 * @param args
 * @throws Exception 
 */
public static void main(String[] args) throws Exception {
  System.out.println(""String_Node_Str"");
  Logging.start(Logging.LEVEL_WARN);
  if (args.length != 2)   usage();
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + PATH);
  p.waitFor();
  long seed=0L;
  try {
    seed=Long.valueOf(args[0]);
  }
 catch (  NumberFormatException e) {
    error(""String_Node_Str"" + args[0]);
  }
  List<InetSocketAddress> slaves=new LinkedList<InetSocketAddress>();
  if (args[1].indexOf(""String_Node_Str"") == -1)   slaves.add(parseAddress(args[1]));
 else   for (  String adr : args[1].split(""String_Node_Str""))   slaves.add(parseAddress(adr));
  generator=new ContinuesRandomGenerator(seed,ReplicationLongrunTestConfig.MAX_SEQUENCENO);
  Map<Integer,List<Object>> scenario=generator.getOperationsScenario();
  MasterConfig config=new MasterConfig(PATH,PATH,NUM_WKS,MAX_LOG_FILE_SIZE,CHECK_INTERVAL,SyncMode.ASYNC,0,0,ReplicationInterface.DEFAULT_MASTER_PORT,InetAddress.getLocalHost(),new InetSocketAddress(InetAddress.getLocalHost(),ReplicationInterface.DEFAULT_MASTER_PORT),slaves,50,null,MAX_REPLICATION_Q_LENGTH,0);
  DBS=(BabuDB)BabuDBFactory.createMasterBabuDB(config);
  int nOmetaOp=0;
  long metaOpTime=0L;
  int nOinsertOp=0;
  long insertOpTime=0L;
  long time;
  for (int i=1; i < ReplicationLongrunTestConfig.MAX_SEQUENCENO; i++) {
    List<Object> operation=scenario.get(i);
    if (operation != null) {
      nOmetaOp++;
      time=System.currentTimeMillis();
      performOperation(operation);
      metaOpTime+=System.currentTimeMillis() - time;
    }
 else {
      nOinsertOp++;
      time=System.currentTimeMillis();
      performInsert(i);
      insertOpTime+=System.currentTimeMillis() - time;
    }
  }
  double metaTroughput=((double)nOmetaOp) / (((double)metaOpTime) / 1000.0);
  double insertThroughput=((double)nOinsertOp) / (((double)insertOpTime) / 1000.0);
  System.out.println(""String_Node_Str"");
  System.out.format(""String_Node_Str"",metaTroughput);
  System.out.format(""String_Node_Str"",insertThroughput);
  Thread.sleep(60 * 1000);
  DBS.shutdown();
}",0.9936155119413572
132405,"/** 
 * Checks the last insert group of consistency.
 * @throws Exception
 */
private static void performConsistencyCheck() throws Exception {
  DispatcherState state=DBS.getReplicationManager().stop();
  System.out.println(""String_Node_Str"" + state.latest);
  if (state.latest != null && state.latest.getSequenceNo() > 0L) {
    LookupGroup lookupGroup=generator.getLookupGroup(state.latest.getSequenceNo());
    if (lookupGroup != null) {
      for (int i=0; i < lookupGroup.size(); i++) {
        byte[] value=DBS.hiddenLookup(lookupGroup.dbName,lookupGroup.getIndex(i),lookupGroup.getKey(i));
        if (lookupGroup.getValue(i) != null) {
          if (value == null) {
            System.err.println(""String_Node_Str"" + i);
            System.err.println(lookupGroup.toString());
            System.err.println((value == null) ? ""String_Node_Str"" : ""String_Node_Str"");
          }
 else {
            if (!new String(value).equals(new String(lookupGroup.getValue(i)))) {
              System.err.println(""String_Node_Str"" + state.latest.toString() + ""String_Node_Str""+ ""String_Node_Str""+ new String(value)+ ""String_Node_Str""+ new String(lookupGroup.getValue(i)));
              System.exit(1);
            }
          }
        }
      }
      System.out.println(""String_Node_Str"" + state.latest.toString() + ""String_Node_Str"");
    }
 else {
      System.out.println(""String_Node_Str"" + state.latest.toString() + ""String_Node_Str"");
    }
  }
 else {
    System.out.println(""String_Node_Str"" + state.latest.toString() + ""String_Node_Str"");
  }
  DBS.getReplicationManager().changeConfiguration(CONFIGURATION,state);
}","/** 
 * Checks the last insert group of consistency.
 * @throws Exception
 */
private static void performConsistencyCheck() throws Exception {
  DispatcherState state=DBS.getReplicationManager().stop();
  System.out.println(""String_Node_Str"" + state.latest);
  if (state.latest != null && state.latest.getSequenceNo() > 0L) {
    LookupGroup lookupGroup=generator.getLookupGroup(state.latest.getSequenceNo());
    if (lookupGroup != null) {
      for (int i=0; i < lookupGroup.size(); i++) {
        byte[] value=DBS.hiddenLookup(lookupGroup.dbName,lookupGroup.getIndex(i),lookupGroup.getKey(i));
        if (lookupGroup.getValue(i) != null) {
          if (value == null) {
            System.err.println(""String_Node_Str"" + i);
            System.err.println(lookupGroup.toString());
            System.err.println((value == null) ? ""String_Node_Str"" : ""String_Node_Str"");
          }
 else {
            if (!new String(value).equals(new String(lookupGroup.getValue(i)))) {
              System.err.println(""String_Node_Str"" + state.latest.toString() + ""String_Node_Str""+ ""String_Node_Str""+ new String(value)+ ""String_Node_Str""+ new String(lookupGroup.getValue(i)));
              System.exit(1);
            }
          }
        }
      }
      System.out.println(""String_Node_Str"" + state.latest.toString() + ""String_Node_Str"");
    }
 else {
      System.out.println(""String_Node_Str"" + state.latest.toString() + ""String_Node_Str"");
    }
  }
 else {
    System.out.println(""String_Node_Str"" + state.latest.toString() + ""String_Node_Str"");
  }
  DBS.getReplicationManager().restart(state);
}",0.9869646182495344
132406,"/** 
 * Restarts the BabuDB with complete data-loss. Remains stopped a random down-time.
 * @param master
 * @param slaves
 * @throws IOException
 * @throws InterruptedException
 * @throws BabuDBException
 */
private static void performCleanAndRestart(Random random,InetSocketAddress master,List<InetSocketAddress> slaves) throws IOException, InterruptedException, BabuDBException {
  DispatcherState state=DBS.getReplicationManager().stop();
  int downTime=random.nextInt(MAX_DOWN_TIME - MIN_DOWN_TIME) + MIN_DOWN_TIME;
  System.out.println(""String_Node_Str"" + downTime / 60000.0 + ""String_Node_Str"");
  Thread.sleep(downTime);
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + PATH);
  p.waitFor();
  DBS.getReplicationManager().changeConfiguration(CONFIGURATION,state);
}","/** 
 * Restarts the BabuDB with complete data-loss. Remains stopped a random down-time.
 * @param master
 * @param slaves
 * @throws IOException
 * @throws InterruptedException
 * @throws BabuDBException
 */
private static void performCleanAndRestart(Random random,InetSocketAddress master,List<InetSocketAddress> slaves) throws IOException, InterruptedException, BabuDBException {
  DispatcherState state=DBS.getReplicationManager().stop();
  int downTime=random.nextInt(MAX_DOWN_TIME - MIN_DOWN_TIME) + MIN_DOWN_TIME;
  System.out.println(""String_Node_Str"" + downTime / 60000.0 + ""String_Node_Str"");
  Thread.sleep(downTime);
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + PATH);
  p.waitFor();
  DBS.getReplicationManager().restart(state);
}",0.9726918075422628
132407,"/** 
 * <p>Sets a new readLock for contextSwitches on babuDB in case of a babuDB reset.</p>
 * @param l
 */
public void changeContextSwitchLock(Lock l){
synchronized (lock) {
    this.babuDBLockcontextSwitchLock=l;
  }
}","/** 
 * <p>Sets a new readLock for contextSwitches on babuDB in case of a babuDB reset.</p>
 * @param l
 */
public void changeContextSwitchLock(Lock l){
synchronized (lock) {
    this.babuDBcontextSwitchLock=l;
  }
}",0.9908256880733946
132408,"/** 
 * <p>Starts the   {@link ReplicationThread} with {@link org.xtreemfs.include.foundation.pinky.PipelinedPinky} listening on <code>port</code>.<p>Is a Slave with a <code>master</code>.</p> <p>If the <code>sslOption</code> is not <code>null</code> SSL will be used for all connections.</p>
 * @param port
 * @param master
 * @param sslOptions
 * @param babu
 * @param mode
 * @param qLimit
 * @param maxRetries
 * @throws BabuDBException - if replication could not be initialized.
 */
public Replication(InetSocketAddress master,SSLOptions sslOptions,int port,BabuDBImpl babu,int mode,int qLimit,Lock lock,int maxRetries) throws BabuDBException {
}","/** 
 * <p>Starts the   {@link ReplicationThread} with {@link org.xtreemfs.include.foundation.pinky.PipelinedPinky} listening on <code>port</code>.<p>Is a Slave with a <code>master</code>.</p> <p>If the <code>sslOption</code> is not <code>null</code> SSL will be used for all connections.</p>
 * @param port
 * @param master
 * @param sslOptions
 * @param babu
 * @param mode
 * @param qLimit
 * @param maxRetries
 * @throws BabuDBException - 
 */
public Replication(InetSocketAddress master,SSLOptions sslOptions,int port,BabuDBImpl babu,int mode,int qLimit,Lock lock,int maxRetries) throws BabuDBException {
}",0.9683042789223456
132409,"@SuppressWarnings(""String_Node_Str"") @Override public void insertFinished(Object context){
  try {
    if (context == null) {
      String msg=""String_Node_Str"";
      Logging.logMessage(Logging.LEVEL_ERROR,this,msg);
      dbInterface.replication_runtime_failure(msg);
    }
    Status<Request> rq=(Status<Request>)context;
    if (rq != null) {
      updateLastWrittenLSN(rq.getValue().getLSN());
      Request parsed=RequestPreProcessor.getReplicationRequest(rq);
      if (parsed != null)       replication.sendACK((Request)parsed);
      Logging.logMessage(Logging.LEVEL_TRACE,replication,""String_Node_Str"" + rq.getValue().getLSN().toString());
      rq.finished();
    }
  }
 catch (  ReplicationException e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
catch (  Exception e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void insertFinished(Object context){
  try {
    if (context == null) {
      String msg=""String_Node_Str"";
      Logging.logMessage(Logging.LEVEL_ERROR,this,msg);
      dbInterface.replication_runtime_failure(msg);
    }
    Status<Request> rq=(Status<Request>)context;
    if (rq != null) {
      updateLastWrittenLSN(rq.getValue().getLSN());
      Request parsed=RequestPreProcessor.getReplicationRequest(rq);
      if (parsed != null)       replication.sendACK((Request)parsed);
      Logging.logMessage(Logging.LEVEL_ERROR,replication,""String_Node_Str"" + rq.getValue().getLSN().toString());
      rq.finished();
    }
  }
 catch (  ReplicationException e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
catch (  Exception e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
}",0.9951076320939336
132410,"/** 
 * <p>Removes requests from the queues of the   {@link ReplicationThread}.</p> <p>Resets the nextExpected, if necessary.</p>
 * @param < T >
 * @param rq
 * @throws ReplicationException 
 */
@SuppressWarnings(""String_Node_Str"") <T>void remove(Status<T> rq) throws ReplicationException {
  Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + rq.toString());
  if (rq.getValue() instanceof Request) {
    pending.remove((Status<Request>)rq);
    Chunk chunk=((Request)rq.getValue()).getChunkDetails();
    if (hasHighPriority((Status<Request>)rq))     resetNextExpected();
    if (missing != null && missing.remove(new Status<LSN>(((Request)rq.getValue()).getLSN())))     resetNextExpected();
    if (missingChunks != null && chunk != null) {
      if (missingChunks.remove(new Status<Chunk>(chunk)))       resetNextExpected();
      if (missingChunks.isEmpty()) {
        try {
          LSN lsn=frontEnd.getLastWrittenLSN();
          frontEnd.dbInterface.reset(lsn);
          frontEnd.switchCondition(CONDITION.RUNNING);
          Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + lsn.toString());
        }
 catch (        BabuDBException e) {
          throw new ReplicationException(""String_Node_Str"" + e.getMessage());
        }
      }
 else {
        setNextExpected(new Status<Request>(RequestPreProcessor.getExpectedCHUNK_RP(chunk),true));
      }
    }
    ((Request)rq.getValue()).free();
  }
 else   if (rq.getValue() instanceof LSN) {
    missing.remove((Status<LSN>)rq);
  }
 else   if (rq.getValue() instanceof Chunk) {
    missingChunks.remove((Status<Chunk>)rq);
  }
 else   throw new ReplicationException(""String_Node_Str"" + rq.toString());
}","/** 
 * <p>Removes requests from the queues of the   {@link ReplicationThread}.</p> <p>Resets the nextExpected, if necessary.</p>
 * @param < T >
 * @param rq
 * @throws ReplicationException 
 */
@SuppressWarnings(""String_Node_Str"") <T>void remove(Status<T> rq) throws ReplicationException {
  Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + rq.toString());
  if (rq.getValue() instanceof Request) {
    pending.remove((Status<Request>)rq);
    Chunk chunk=((Request)rq.getValue()).getChunkDetails();
    LSN lsn=((Request)rq.getValue()).getLSN();
    if (hasHighPriority((Status<Request>)rq))     resetNextExpected();
    if (lsn != null && missing != null && missing.remove(new Status<LSN>(lsn)))     resetNextExpected();
    if (chunk != null && missingChunks != null) {
      if (missingChunks.remove(new Status<Chunk>(chunk)))       resetNextExpected();
      if (missingChunks.isEmpty()) {
        try {
          lsn=frontEnd.getLastWrittenLSN();
          frontEnd.dbInterface.reset(lsn);
          frontEnd.switchCondition(CONDITION.RUNNING);
          Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + lsn.toString());
        }
 catch (        BabuDBException e) {
          throw new ReplicationException(""String_Node_Str"" + e.getMessage());
        }
      }
 else {
        setNextExpected(new Status<Request>(RequestPreProcessor.getExpectedCHUNK_RP(chunk),true));
      }
    }
    ((Request)rq.getValue()).free();
  }
 else   if (rq.getValue() instanceof LSN) {
    missing.remove((Status<LSN>)rq);
  }
 else   if (rq.getValue() instanceof Chunk) {
    missingChunks.remove((Status<Chunk>)rq);
  }
 else   throw new ReplicationException(""String_Node_Str"" + rq.toString());
}",0.960117302052786
132411,"/** 
 * <p>Default constructor. Synchronous start-up for pinky and speedy component.</p>
 * @param frontEnd 
 * @param port 
 * @param pendingLimit
 * @param sslOptions
 */
ReplicationThread(Replication frontEnd,int port,int pendingLimit,SSLOptions sslOptions) throws ReplicationException {
  super((frontEnd.isMaster()) ? ""String_Node_Str"" : ""String_Node_Str"");
  this.pendingQueueLimit=pendingLimit;
  this.frontEnd=frontEnd;
  pending=new PriorityBlockingQueue<Status<Request>>(100,PENDING_QUEUE_COMPARATOR);
  missing=(!frontEnd.isMaster()) ? new PriorityBlockingQueue<Status<LSN>>(100,new PriorityQueueComparator<LSN>()) : null;
  missingChunks=(!frontEnd.isMaster()) ? new PriorityBlockingQueue<Status<Chunk>>(10,new PriorityQueueComparator<Chunk>()) : null;
  this.slavesStatus=new SlavesStatus(frontEnd.slaves);
  try {
    pinky=new PipelinedPinky(port,null,frontEnd,sslOptions);
    pinky.setLifeCycleListener(this);
    pinky.setUncaughtExceptionHandler(this);
    speedy=new MultiSpeedy(sslOptions);
    speedy.registerSingleListener(frontEnd);
    speedy.setLifeCycleListener(this);
    speedy.setUncaughtExceptionHandler(this);
  }
 catch (  IOException io) {
    String msg=""String_Node_Str"" + io.getMessage();
    Logging.logMessage(Logging.LEVEL_ERROR,this,msg);
    throw new ReplicationException(msg);
  }
}","/** 
 * <p>Default constructor. Synchronous start-up for pinky and speedy component.</p>
 * @param frontEnd 
 * @param port 
 * @param pendingLimit
 * @param sslOptions
 */
ReplicationThread(Replication frontEnd,int port,SSLOptions sslOptions) throws ReplicationException {
  super((frontEnd.isMaster()) ? ""String_Node_Str"" : ""String_Node_Str"");
  this.frontEnd=frontEnd;
  pending=new PriorityBlockingQueue<Status<Request>>(100,PENDING_QUEUE_COMPARATOR);
  missing=(!frontEnd.isMaster()) ? new PriorityBlockingQueue<Status<LSN>>(100,new PriorityQueueComparator<LSN>()) : null;
  missingChunks=(!frontEnd.isMaster()) ? new PriorityBlockingQueue<Status<Chunk>>(10,new PriorityQueueComparator<Chunk>()) : null;
  this.slavesStatus=new SlavesStatus(frontEnd.slaves);
  try {
    pinky=new PipelinedPinky(port,null,frontEnd,sslOptions);
    pinky.setLifeCycleListener(this);
    pinky.setUncaughtExceptionHandler(this);
    speedy=new MultiSpeedy(sslOptions);
    speedy.registerSingleListener(frontEnd);
    speedy.setLifeCycleListener(this);
    speedy.setUncaughtExceptionHandler(this);
  }
 catch (  IOException io) {
    String msg=""String_Node_Str"" + io.getMessage();
    Logging.logMessage(Logging.LEVEL_ERROR,this,msg);
    throw new ReplicationException(msg);
  }
}",0.9784283513097072
132412,"/** 
 * <p>Sends an   {@link Token}.LOAD request to the master.</p> <p>Sets the nextExpected flag to a higher priority.</p>
 * @param rq - the original {@link Request}.
 * @throws IOException - if an error occurs.
 * @throws IllegalStateException - if an error occurs.
 */
private void sendLOAD(Status<Request> rq) throws IllegalStateException, IOException {
  frontEnd.switchCondition(CONDITION.LOADING);
  SpeedyRequest sReq=new SpeedyRequest(HTTPUtils.POST_TOKEN,Token.LOAD.toString(),null,null,null,DATA_TYPE.BINARY);
  sReq.genericAttatchment=rq;
  speedy.sendRequest(sReq,frontEnd.master);
  setNextExpected(RequestPreProcessor.getExpectedLOAD_RP());
}","/** 
 * <p>Sends an   {@link Token}.LOAD request to the master.</p> <p>Sets the nextExpected flag to a higher priority.</p> <p>Appends the original   {@link Request} to the {@link SpeedyRequest}.</p>
 * @param rq - the original {@link Request}.
 * @throws IOException if an error occurs.
 * @throws IllegalStateException if an error occurs.
 */
private void sendLOAD(Status<Request> rq) throws IllegalStateException, IOException {
  frontEnd.switchCondition(CONDITION.LOADING);
  SpeedyRequest sReq=new SpeedyRequest(HTTPUtils.POST_TOKEN,Token.LOAD.toString(),null,null,null,DATA_TYPE.BINARY);
  sReq.genericAttatchment=rq;
  speedy.sendRequest(sReq,frontEnd.master);
  setNextExpected(RequestPreProcessor.getExpectedLOAD_RP());
}",0.9005763688760807
132413,"/** 
 * <p>Needed for the initial load process of the babuDB, done by the replication.</p>
 * @param latest - {@link LSN} until which the loading is done.
 * @throws BabuDBException 
 */
public void reset(LSN latest) throws BabuDBException {
  for (  LSMDBWorker w : worker) {
    w.shutdown();
  }
  logger.shutdown();
  if (dbCheckptr != null) {
    dbCheckptr.shutdown();
  }
  try {
    logger.waitForShutdown();
    for (    LSMDBWorker w : worker) {
      w.waitForShutdown();
    }
    if (dbCheckptr != null) {
      dbCheckptr.waitForShutdown();
    }
  }
 catch (  InterruptedException ex) {
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  dbNames.clear();
  databases.clear();
  nextDbId=1;
  compInstances.clear();
  compInstances.put(DefaultByteRangeComparator.class.getName(),new DefaultByteRangeComparator());
  loadDBs();
  LSN dbLsn=null;
  for (  LSMDatabase db : databases.values()) {
    if (dbLsn == null)     dbLsn=db.getOndiskLSN();
 else {
      if (!dbLsn.equals(db.getOndiskLSN()))       throw new RuntimeException(""String_Node_Str"");
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs();
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  try {
    logger=new DiskLogger(configuration.getDbLogDir(),latest.getViewId(),latest.getSequenceNo() + 1L,configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads());
    logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
synchronized (dbModificationLock) {
    dbModificationLock.notifyAll();
  }
synchronized (checkpointLock) {
    checkpointLock.notifyAll();
  }
  overlaySwitchLock=new ReentrantReadWriteLock();
  replicationFacade.changeContextSwitchLock(overlaySwitchLock.readLock());
  worker=new LSMDBWorker[configuration.getNumThreads()];
  for (int i=0; i < configuration.getNumThreads(); i++) {
    worker[i]=new LSMDBWorker(logger,i,overlaySwitchLock,(configuration.getPseudoSyncWait() > 0),configuration.getMaxQueueLength(),replicationFacade);
    worker[i].start();
  }
  if (configuration.getCheckInterval() > 0) {
    dbCheckptr=new Checkpointer(this,logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    dbCheckptr.start();
  }
 else {
    dbCheckptr=null;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}","/** 
 * <p>Needed for the initial load process of the babuDB, done by the replication.</p>
 * @param latest - {@link LSN} until which the loading is done.
 * @throws BabuDBException 
 */
public void reset(LSN latest) throws BabuDBException {
  for (  LSMDBWorker w : worker) {
    w.shutdown();
  }
  logger.shutdown();
  if (dbCheckptr != null) {
    dbCheckptr.shutdown();
  }
  try {
    logger.waitForShutdown();
    for (    LSMDBWorker w : worker) {
      w.waitForShutdown();
    }
    if (dbCheckptr != null) {
      dbCheckptr.waitForShutdown();
    }
  }
 catch (  InterruptedException ex) {
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  dbNames.clear();
  databases.clear();
  nextDbId=1;
  compInstances.clear();
  compInstances.put(DefaultByteRangeComparator.class.getName(),new DefaultByteRangeComparator());
  loadDBs();
  try {
    logger=new DiskLogger(configuration.getDbLogDir(),latest.getViewId(),latest.getSequenceNo() + 1L,configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads());
    logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
synchronized (dbModificationLock) {
    dbModificationLock.notifyAll();
  }
synchronized (checkpointLock) {
    checkpointLock.notifyAll();
  }
  overlaySwitchLock=new ReentrantReadWriteLock();
  replicationFacade.changeContextSwitchLock(overlaySwitchLock.readLock());
  worker=new LSMDBWorker[configuration.getNumThreads()];
  for (int i=0; i < configuration.getNumThreads(); i++) {
    worker[i]=new LSMDBWorker(logger,i,overlaySwitchLock,(configuration.getPseudoSyncWait() > 0),configuration.getMaxQueueLength(),replicationFacade);
    worker[i].start();
  }
  if (configuration.getCheckInterval() > 0) {
    dbCheckptr=new Checkpointer(this,logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    dbCheckptr.start();
  }
 else {
    dbCheckptr=null;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}",0.8776739356178609
132414,"/** 
 * <p>Builds up a   {@link SpeedyRequest} and sends it to the given destination.</p>
 * @param token
 * @param buffer
 * @param attachment
 * @param destination
 * @throws BabuDBConnectionException if request could not be send.
 */
void sendRequest(Token token,ReusableBuffer buffer,Object attachment,InetSocketAddress destination) throws BabuDBConnectionException {
  SpeedyRequest sReq=new SpeedyRequest(HTTPUtils.POST_TOKEN,token.toString(),null,null,buffer,DATA_TYPE.BINARY);
  sReq.genericAttatchment=attachment;
  try {
    speedy.sendRequest(sReq,destination);
  }
 catch (  Exception e) {
    sReq.freeBuffer();
    throw new BabuDBConnectionException(""String_Node_Str"" + token.toString() + ""String_Node_Str""+ destination.toString()+ ""String_Node_Str""+ e.getMessage());
  }
  Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + token.toString() + ""String_Node_Str""+ destination.toString()+ ""String_Node_Str"");
}","/** 
 * <p>Builds up a   {@link SpeedyRequest} and sends it to the given destination.</p>
 * @param token
 * @param buffer
 * @param attachment
 * @param destination
 * @throws BabuDBConnectionException if request could not be send.
 */
void sendRequest(Token token,ReusableBuffer buffer,Object attachment,InetSocketAddress destination) throws BabuDBConnectionException {
  SpeedyRequest sReq=null;
  try {
    sReq=new SpeedyRequest(HTTPUtils.POST_TOKEN,token.toString(),null,null,buffer,DATA_TYPE.BINARY);
    sReq.genericAttatchment=attachment;
    speedy.sendRequest(sReq,destination);
  }
 catch (  Exception e) {
    if (sReq != null)     sReq.freeBuffer();
    throw new BabuDBConnectionException(""String_Node_Str"" + token.toString() + ""String_Node_Str""+ destination.toString()+ ""String_Node_Str""+ e.getMessage());
  }
  Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + token.toString() + ""String_Node_Str""+ destination.toString()+ ""String_Node_Str"");
}",0.9712192569335426
132415,"@Override public void receiveRequest(SpeedyRequest theRequest){
  try {
    RequestPreProcessor.getReplicationRequest(theRequest,this);
  }
 catch (  PreProcessException ppe) {
    Logging.logMessage(Logging.LEVEL_WARN,this,ppe.getMessage());
  }
catch (  Exception e) {
    dbInterface.replication_runtime_failure(e.getMessage());
    Logging.logMessage(Logging.LEVEL_ERROR,this,e.getMessage());
  }
  theRequest.freeBuffer();
}","@Override public void receiveRequest(SpeedyRequest theRequest){
  try {
    RequestPreProcessor.getReplicationRequest(theRequest,this);
  }
 catch (  PreProcessException ppe) {
    Logging.logMessage(Logging.LEVEL_WARN,this,ppe.getMessage());
  }
catch (  Exception e) {
    dbInterface.replication_runtime_failure(e.getMessage());
    Logging.logMessage(Logging.LEVEL_ERROR,this,e.getMessage());
  }
 finally {
    theRequest.freeBuffer();
  }
}",0.9805714285714284
132416,"/** 
 * <p>Verify that no other setting of the replication changes while it is paused!</p>
 * @return the LSN of the last written entry, or null, if the condition is LOADING.Because no consistent state is ensured in this case.
 * @throws InterruptedException
 */
public synchronized LSN pause() throws InterruptedException {
synchronized (replication.halt) {
    replication.halt.set(true);
    replication.halt.wait();
  }
  if (getCondition() != LOADING)   return getLastWrittenLSN();
 else   return null;
}","/** 
 * <p>Verify that no other setting of the replication changes while it is paused!</p>
 * @return the LSN of the last written entry, or null, if the condition is LOADING.Because no consistent state is ensured in this case.
 * @throws InterruptedException
 */
public synchronized LSN pause() throws InterruptedException {
synchronized (replication.halt) {
    replication.halt.set(true);
    replication.halt.wait();
  }
  if (getCondition() != LOADING) {
    LSN last=getLastWrittenLSN();
    if (!last.equals(new LSN(1,0L)))     return last;
  }
  return null;
}",0.895910780669145
132417,"/** 
 * @param rq - set as nextExpected.
 */
private void setNextExpected(Status<Request> rq){
synchronized (nextExpected) {
    nextExpected.set(rq);
  }
  Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + rq.toString() + ""String_Node_Str"");
}","/** 
 * @param rq - set as nextExpected.
 */
private void setNextExpected(Status<Request> rq){
  nextExpected.set(rq);
  Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + rq.toString() + ""String_Node_Str"");
}",0.8833333333333333
132418,"@Override public String toString(){
  String string=new String();
  if (token != null)   string+=""String_Node_Str"" + token.toString() + ""String_Node_Str"";
  if (source != null)   string+=""String_Node_Str"" + source.toString() + ""String_Node_Str"";
  if (lsn != null)   string+=""String_Node_Str"" + lsn.toString() + ""String_Node_Str"";
  if (logEntry != null)   string+=""String_Node_Str"" + logEntry.toString() + ""String_Node_Str"";
  if (chunk != null)   string+=""String_Node_Str"" + chunk.toString() + ""String_Node_Str"";
  if (Logging.tracingEnabled()) {
    if (data != null)     string+=""String_Node_Str"" + data.toString() + ""String_Node_Str"";
    if (lsmDbMetaData != null)     string+=""String_Node_Str"" + lsmDbMetaData.toString() + ""String_Node_Str"";
    if (context != null)     string+=""String_Node_Str"" + context.toString() + ""String_Node_Str"";
  }
 else   string+=""String_Node_Str"";
  string+=""String_Node_Str"" + super.toString();
  return string;
}","@Override public String toString(){
  String string=new String();
  if (token != null)   string+=""String_Node_Str"" + token.toString() + ""String_Node_Str"";
  if (source != null)   string+=""String_Node_Str"" + source.toString() + ""String_Node_Str"";
  if (lsn != null)   string+=""String_Node_Str"" + lsn.toString() + ""String_Node_Str"";
  if (logEntry != null)   string+=""String_Node_Str"" + logEntry.toString() + ""String_Node_Str"";
  if (chunk != null)   string+=""String_Node_Str"" + chunk.toString() + ""String_Node_Str"";
  if (Logging.tracingEnabled()) {
    if (data != null)     string+=""String_Node_Str"" + data.toString() + ""String_Node_Str"";
    if (lsmDbMetaData != null)     string+=""String_Node_Str"" + lsmDbMetaData.toString() + ""String_Node_Str"";
    if (context != null)     string+=""String_Node_Str"" + context.toString() + ""String_Node_Str"";
  }
 else   string+=""String_Node_Str"";
  return string;
}",0.9741100323624596
132419,"public void checkpoint() throws BabuDBException, InterruptedException {
  if (replication_isSlave()) {
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,slaveProtection);
  }
  List<LSMDatabase> dbListCopy;
synchronized (dbModificationLock) {
    dbListCopy=new ArrayList<LSMDatabase>(databases.size());
    for (    LSMDatabase db : databases.values()) {
      dbListCopy.add(db);
    }
  }
synchronized (checkpointLock) {
    try {
      int[][] snapIds=new int[dbListCopy.size()][];
      int i=0;
      LSN lastWrittenLSN=null;
      try {
        logger.lockLogger();
        for (        LSMDatabase db : dbListCopy) {
          snapIds[i++]=db.createSnapshot();
        }
        lastWrittenLSN=logger.switchLogFile();
      }
  finally {
        logger.unlockLogger();
      }
      i=0;
      for (      LSMDatabase db : dbListCopy) {
        db.writeSnapshot(lastWrittenLSN.getViewId(),lastWrittenLSN.getSequenceNo(),snapIds[i++]);
        db.cleanupSnapshot(lastWrittenLSN.getViewId(),lastWrittenLSN.getSequenceNo());
      }
      File f=new File(configuration.getDbLogDir());
      String[] logs=f.list(new FilenameFilter(){
        public boolean accept(        File dir,        String name){
          return name.endsWith(""String_Node_Str"");
        }
      }
);
      if (logs != null) {
        Pattern p=Pattern.compile(""String_Node_Str"");
        for (        String log : logs) {
          Matcher m=p.matcher(log);
          m.matches();
          String tmp=m.group(1);
          int viewId=Integer.valueOf(tmp);
          tmp=m.group(2);
          int seqNo=Integer.valueOf(tmp);
          LSN logLSN=new LSN(viewId,seqNo);
          if (logLSN.compareTo(lastWrittenLSN) <= 0) {
            Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + log);
            f=new File(configuration.getDbLogDir() + log);
            f.delete();
          }
        }
      }
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
  }
}","private void checkpoint(boolean incrementViewId) throws BabuDBException, InterruptedException {
  List<LSMDatabase> dbListCopy;
synchronized (dbModificationLock) {
    dbListCopy=new ArrayList<LSMDatabase>(databases.size());
    for (    LSMDatabase db : databases.values()) {
      dbListCopy.add(db);
    }
  }
synchronized (checkpointLock) {
    try {
      int[][] snapIds=new int[dbListCopy.size()][];
      int i=0;
      LSN lastWrittenLSN=null;
      try {
        logger.lockLogger();
        for (        LSMDatabase db : dbListCopy) {
          snapIds[i++]=db.createSnapshot();
        }
        lastWrittenLSN=logger.switchLogFile(incrementViewId);
      }
  finally {
        logger.unlockLogger();
      }
      i=0;
      for (      LSMDatabase db : dbListCopy) {
        db.writeSnapshot(lastWrittenLSN.getViewId(),lastWrittenLSN.getSequenceNo(),snapIds[i++]);
        db.cleanupSnapshot(lastWrittenLSN.getViewId(),lastWrittenLSN.getSequenceNo());
      }
      File f=new File(configuration.getDbLogDir());
      String[] logs=f.list(new FilenameFilter(){
        public boolean accept(        File dir,        String name){
          return name.endsWith(""String_Node_Str"");
        }
      }
);
      if (logs != null) {
        Pattern p=Pattern.compile(""String_Node_Str"");
        for (        String log : logs) {
          Matcher m=p.matcher(log);
          m.matches();
          String tmp=m.group(1);
          int viewId=Integer.valueOf(tmp);
          tmp=m.group(2);
          int seqNo=Integer.valueOf(tmp);
          LSN logLSN=new LSN(viewId,seqNo);
          if (logLSN.compareTo(lastWrittenLSN) <= 0) {
            Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + log);
            f=new File(configuration.getDbLogDir() + log);
            f.delete();
          }
        }
      }
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
  }
}",0.958835341365462
132420,"public LSN switchLogFile() throws IOException {
  if (!sync.isHeldByCurrentThread()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  LSN lastSyncedLSN=new LSN(this.currentViewId.get(),this.nextLogSequenceNo.get() - 1);
  final String newFileName=createLogFileName();
  channel.close();
  fos.close();
  currentLogFileName=newFileName;
  File lf=new File(this.currentLogFileName);
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + currentLogFileName);
return lastSyncedLSN;
}","public LSN switchLogFile(boolean incrementViewId) throws IOException {
  if (!sync.isHeldByCurrentThread()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  LSN lastSyncedLSN=new LSN(this.currentViewId.get(),this.nextLogSequenceNo.get() - 1);
  final String newFileName=createLogFileName();
  channel.close();
  fos.close();
  currentLogFileName=newFileName;
  File lf=new File(this.currentLogFileName);
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + currentLogFileName);
if (incrementViewId) this.currentViewId.incrementAndGet();
return lastSyncedLSN;
}",0.9529411764705882
132421,"/** 
 * Creates a new instance of DiskLogger
 * @param logfile Name and path of file to use for append log.
 * @throws java.io.FileNotFoundException If that file cannot be created.
 * @throws java.io.IOException If that file cannot be created.
 */
public DiskLogger(String logfileDir,int viewId,long nextLSN,SyncMode syncMode,int pseudoSyncWait,int maxQ) throws FileNotFoundException, IOException {
  super(""String_Node_Str"");
  if (logfileDir == null) {
    throw new RuntimeException(""String_Node_Str"");
  }
  if (logfileDir.endsWith(""String_Node_Str"")) {
    this.logfileDir=logfileDir;
  }
 else {
    this.logfileDir=logfileDir + ""String_Node_Str"";
  }
  this.pseudoSyncWait=pseudoSyncWait;
  this.pseudoSyncModeEnabled=(pseudoSyncWait > 0);
  this.currentViewId=new AtomicInteger(viewId);
  assert(nextLSN > 0);
  this.nextLogSequenceNo=new AtomicLong(nextLSN);
  this.currentLogFileName=createLogFileName();
  File lf=new File(this.currentLogFileName);
  if (!lf.getParentFile().exists() && !lf.getParentFile().mkdirs()) {
    throw new IOException(""String_Node_Str"");
  }
  this.syncMode=syncMode;
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
if (maxQ > 0) entries=new LinkedBlockingQueue(maxQ);
 else entries=new LinkedBlockingQueue();
quit=false;
this.down=new AtomicBoolean(false);
sync=new ReentrantLock();
final String sMode=(pseudoSyncWait == 0) ? ""String_Node_Str"" : ""String_Node_Str"";
Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + sMode + ""String_Node_Str"");
Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + syncMode);
}","/** 
 * Creates a new instance of DiskLogger
 * @param logfile Name and path of file to use for append log.
 * @throws java.io.FileNotFoundException If that file cannot be created.
 * @throws java.io.IOException If that file cannot be created.
 */
public DiskLogger(String logfileDir,int viewId,long nextLSN,SyncMode syncMode,int pseudoSyncWait,int maxQ) throws FileNotFoundException, IOException {
  super(""String_Node_Str"");
  if (logfileDir == null) {
    throw new RuntimeException(""String_Node_Str"");
  }
  if (logfileDir.endsWith(""String_Node_Str"")) {
    this.logfileDir=logfileDir;
  }
 else {
    this.logfileDir=logfileDir + ""String_Node_Str"";
  }
  this.pseudoSyncWait=pseudoSyncWait;
  this.pseudoSyncModeEnabled=(pseudoSyncWait > 0);
  this.currentViewId=new AtomicInteger(viewId);
  assert(nextLSN > 0);
  this.nextLogSequenceNo=new AtomicLong(nextLSN);
  this.currentLogFileName=createLogFileName();
  File lf=new File(this.currentLogFileName);
  if (!lf.getParentFile().exists() && !lf.getParentFile().mkdirs()) {
    throw new IOException(""String_Node_Str"");
  }
  this.syncMode=syncMode;
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
if (maxQ > 0) entries=new LinkedBlockingQueue<LogEntry>(maxQ);
 else entries=new LinkedBlockingQueue<LogEntry>();
quit=false;
this.down=new AtomicBoolean(false);
sync=new ReentrantLock();
final String sMode=(pseudoSyncWait == 0) ? ""String_Node_Str"" : ""String_Node_Str"";
Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + sMode + ""String_Node_Str"");
Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + syncMode);
}",0.9947970863683664
132422,"/** 
 * <p>Sets the lastWrittenLSN to <code>lsn</code>, if it's bigger.</p>
 * @param lsn
 */
void updateLastWrittenLSN(LSN lsn){
  boolean running=true;
  while (running) {
    LSN actual=lastWrittenLSN.get();
    if (actual.compareTo(lsn) < 0)     if (!lastWrittenLSN.compareAndSet(actual,lsn))     continue;
    running=false;
  }
}","/** 
 * <p>Sets the lastWrittenLSN to <code>lsn</code>, if it's bigger.</p>
 * @param lsn
 */
void updateLastWrittenLSN(LSN lsn){
synchronized (lastWrittenLSN) {
    LSN actual=lastWrittenLSN.get();
    if (actual.compareTo(lsn) < 0) {
      lastWrittenLSN.set(lsn);
      Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + lsn.toString() + ""String_Node_Str"");
    }
  }
}",0.4138888888888888
132423,"private V lookup(K key,OverlayTreeList list){
  for (; list != null; list=list.next) {
    V value=list.tree.get(key);
    if (value == nullValue)     return null;
    if (value != null)     return value;
  }
  return null;
}","private V lookup(K key,OverlayTreeList list){
  for (; list != null; list=list.next) {
    V value=list.tree.get(key);
    if (value != null)     return value;
  }
  return null;
}",0.8888888888888888
132424,"public void testOverlayBufferTree(){
  MultiOverlayBufferTree tree=new MultiOverlayBufferTree(new byte[0],new DefaultByteRangeComparator());
  final int numElements=200;
  final SortedMap<String,byte[]> map1=new TreeMap<String,byte[]>();
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    String valString=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    byte[] key=keyString.getBytes();
    byte[] val=valString.getBytes();
    map1.put(keyString,val);
    tree.insert(key,val);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map1.get(keyString));
  }
  int snap1=tree.newOverlay();
  final SortedMap<String,byte[]> map2=new TreeMap<String,byte[]>(map1);
  for (int i=0; i < numElements; i+=2) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    tree.insert(key,null);
    map2.remove(keyString);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map2.get(keyString));
  }
  int snap2=tree.newOverlay();
  final SortedMap<String,byte[]> map3=new TreeMap<String,byte[]>(map2);
  for (int i=0; i < numElements; i+=5) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    byte[] val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE)).getBytes();
    tree.insert(key,val);
    map3.put(keyString,val);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map3.get(keyString));
  }
  Iterator<Entry<byte[],byte[]>> it=tree.prefixLookup(null,false);
  Iterator<byte[]> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap1,false);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap2,false);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(""String_Node_Str"".getBytes(),false);
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}","public void testOverlayBufferTree(){
  MultiOverlayBufferTree tree=new MultiOverlayBufferTree(new byte[0],new DefaultByteRangeComparator());
  final int numElements=200;
  final SortedMap<String,byte[]> map1=new TreeMap<String,byte[]>();
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    String valString=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    byte[] key=keyString.getBytes();
    byte[] val=valString.getBytes();
    map1.put(keyString,val);
    tree.insert(key,val);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map1.get(keyString));
  }
  int snap1=tree.newOverlay();
  final SortedMap<String,byte[]> map2=new TreeMap<String,byte[]>(map1);
  for (int i=0; i < numElements; i+=2) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    tree.insert(key,null);
    map2.remove(keyString);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    byte[] mapVal=map2.get(keyString);
    byte[] val=tree.lookup(key);
    if (mapVal != null)     assertEquals(mapVal,val);
 else     assertTrue(val == null || val.length == 0);
  }
  int snap2=tree.newOverlay();
  final SortedMap<String,byte[]> map3=new TreeMap<String,byte[]>(map2);
  for (int i=0; i < numElements; i+=5) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    byte[] val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE)).getBytes();
    tree.insert(key,val);
    map3.put(keyString,val);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    byte[] mapVal=map3.get(keyString);
    byte[] val=tree.lookup(key);
    if (mapVal != null)     assertEquals(mapVal,val);
 else     assertTrue(val == null || val.length == 0);
  }
  Iterator<Entry<byte[],byte[]>> it=tree.prefixLookup(null,false);
  Iterator<byte[]> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap1,false);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap2,false);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(""String_Node_Str"".getBytes(),false);
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}",0.9297610427226648
132425,"protected void assertEquals(ReusableBuffer expected,ReusableBuffer val){
  if (expected == null)   return;
  String e=new String(expected.array());
  String v=new String(val.array());
  assertEquals(e,v);
}","protected void assertEquals(byte[] expected,byte[] val){
  if (expected == null && val == null)   return;
  String e=new String(expected);
  String v=new String(val);
  assertEquals(e,v);
}",0.830379746835443
132426,"public void testOverlayTree(){
  MultiOverlayTree<String,String> tree=new MultiOverlayTree<String,String>(""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  int snap1=tree.newOverlay();
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",null);
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  int snap2=tree.newOverlay();
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  assertNull(tree.lookup(""String_Node_Str""));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertNull(tree.lookup(""String_Node_Str"",snap2));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap2));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  final int numElements=200;
  tree=new MultiOverlayTree<String,String>(""String_Node_Str"");
  final SortedMap<String,String> map1=new TreeMap<String,String>();
  for (int i=0x10; i < numElements; i++) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    map1.put(key,val);
    tree.insert(key,val);
  }
  snap1=tree.newOverlay();
  final SortedMap<String,String> map2=new TreeMap<String,String>(map1);
  for (int i=0x10; i < numElements; i+=2) {
    String key=Integer.toHexString(i);
    tree.insert(key,null);
    map2.remove(key);
  }
  snap2=tree.newOverlay();
  final SortedMap<String,String> map3=new TreeMap<String,String>(map2);
  for (int i=0x10; i < numElements; i+=5) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    tree.insert(key,val);
    map3.put(key,val);
  }
  Iterator<Entry<String,String>> it=tree.rangeLookup(null,null,false);
  Iterator<String> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(null,null,snap1,false);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(null,null,snap2,false);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(""String_Node_Str"",""String_Node_Str"",false);
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}","public void testOverlayTree(){
  MultiOverlayTree<String,String> tree=new MultiOverlayTree<String,String>(""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  int snap1=tree.newOverlay();
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",null);
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  int snap2=tree.newOverlay();
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertNull(tree.lookup(""String_Node_Str"",snap2));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap2));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  final int numElements=200;
  tree=new MultiOverlayTree<String,String>(""String_Node_Str"");
  final SortedMap<String,String> map1=new TreeMap<String,String>();
  for (int i=0x10; i < numElements; i++) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    map1.put(key,val);
    tree.insert(key,val);
  }
  snap1=tree.newOverlay();
  final SortedMap<String,String> map2=new TreeMap<String,String>(map1);
  for (int i=0x10; i < numElements; i+=2) {
    String key=Integer.toHexString(i);
    tree.insert(key,null);
    map2.remove(key);
  }
  snap2=tree.newOverlay();
  final SortedMap<String,String> map3=new TreeMap<String,String>(map2);
  for (int i=0x10; i < numElements; i+=5) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    tree.insert(key,val);
    map3.put(key,val);
  }
  Iterator<Entry<String,String>> it=tree.rangeLookup(null,null,false);
  Iterator<String> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(null,null,snap1,false);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(null,null,snap2,false);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(""String_Node_Str"",""String_Node_Str"",false);
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}",0.9951124144672532
132427,"/** 
 * Load the most recent snapshots of each tree.
 * @param numIndices the number of indices to read.
 * @throws java.io.IOException if the on-disk data cannot be read
 */
private void loadFromDisk(int numIndices) throws BabuDBException {
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName + ""String_Node_Str"");
  for (int index=0; index < numIndices; index++) {
    trees.add(null);
  }
  for (int index=0; index < numIndices; index++) {
    final int idx=index;
    File f=new File(databaseDir);
    String[] files=f.list(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.startsWith(""String_Node_Str"" + idx + ""String_Node_Str"");
      }
    }
);
    int maxView=-1;
    int maxSeq=-1;
    Pattern p=Pattern.compile(SNAPSHOT_FILENAME_REGEXP);
    for (    String fname : files) {
      Matcher m=p.matcher(fname);
      m.matches();
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + fname);
      int view=Integer.valueOf(m.group(2));
      int seq=Integer.valueOf(m.group(3));
      if (view > maxView) {
        maxView=view;
        maxSeq=seq;
      }
 else       if (view == maxView) {
        if (seq > maxSeq)         maxSeq=seq;
      }
    }
    try {
      if (maxView > -1) {
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName + ""String_Node_Str""+ databaseDir+ ""String_Node_Str""+ index+ ""String_Node_Str""+ maxView+ ""String_Node_Str""+ maxSeq);
        assert(comparators[index] != null);
        trees.set(index,new LSMTree(databaseDir + getSnapshotFilename(index,maxView,maxSeq),comparators[index]));
        ondiskLSN=new LSN(maxView,maxSeq);
      }
 else {
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName);
        assert(comparators[index] != null);
        trees.set(index,new LSMTree(null,comparators[index]));
      }
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
  }
}","/** 
 * Load the most recent snapshots of each tree.
 * @param numIndices the number of indices to read.
 * @throws java.io.IOException if the on-disk data cannot be read
 */
private void loadFromDisk(int numIndices) throws BabuDBException {
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName + ""String_Node_Str"");
  for (int index=0; index < numIndices; index++) {
    trees.add(null);
  }
  for (int index=0; index < numIndices; index++) {
    final int idx=index;
    File f=new File(databaseDir);
    String[] files=f.list(new FilenameFilter(){
      public boolean accept(      File dir,      String name){
        return name.startsWith(""String_Node_Str"" + idx + ""String_Node_Str"");
      }
    }
);
    int maxView=-1;
    int maxSeq=-1;
    Pattern p=Pattern.compile(SNAPSHOT_FILENAME_REGEXP);
    for (    String fname : files) {
      Matcher m=p.matcher(fname);
      m.matches();
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + fname);
      int view=Integer.valueOf(m.group(2));
      int seq=Integer.valueOf(m.group(3));
      if (view > maxView) {
        maxView=view;
        maxSeq=seq;
      }
 else       if (view == maxView) {
        if (seq > maxSeq)         maxSeq=seq;
      }
    }
    try {
      if (maxView > -1) {
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName + ""String_Node_Str""+ databaseDir+ ""String_Node_Str""+ index+ ""String_Node_Str""+ maxView+ ""String_Node_Str""+ maxSeq);
        assert(comparators[index] != null);
        trees.set(index,new LSMTree(databaseDir + getSnapshotFilename(index,maxView,maxSeq),comparators[index]));
        ondiskLSN=new LSN(maxView,maxSeq);
      }
 else {
        ondiskLSN=new LSN(0,0);
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + this.databaseName);
        assert(comparators[index] != null);
        trees.set(index,new LSMTree(null,comparators[index]));
      }
    }
 catch (    IOException ex) {
      throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
    }
  }
}",0.9922555663117134
132428,"/** 
 * <p>Stops the Replication.</p> (synchronous)
 * @throws BabuDBException if replication could not be stopped. 
 */
public void shutdown() throws BabuDBException {
  try {
    if (condition.equals(RUNNING)) {
      replication.shutdown();
      replication.waitForShutdown();
    }
    connectionControl.shutdown();
    switchCondition(DEAD);
  }
 catch (  Exception e) {
    dbInterface.replication_runtime_failure(e.getMessage());
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",e.getCause());
  }
}","/** 
 * <p>Stops the Replication.</p> (synchronous)
 * @throws BabuDBException if replication could not be stopped. 
 */
public void shutdown() throws BabuDBException {
  try {
    if (!condition.equals(DEAD) && !condition.equals(STOPPED)) {
synchronized (lock) {
synchronized (replication.halt) {
          replication.halt.set(true);
          replication.halt.wait();
          connectionControl.shutdown();
          replication.shutdown();
          replication.halt.set(false);
          replication.halt.notify();
        }
        replication.waitForShutdown();
        switchCondition(DEAD);
      }
    }
  }
 catch (  Exception e) {
    dbInterface.replication_runtime_failure(e.getMessage());
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",e.getCause());
  }
}",0.5846621108580107
132429,"/** 
 * Checks the last insert group of consistency.
 * @throws Exception
 */
private static void performConsistencyCheck() throws Exception {
  LSN last=DBS.replication_pause();
  if (last != null) {
    LookupGroup lookupGroup=generator.getLookupGroup(last);
    for (int i=0; i < lookupGroup.size(); i++) {
      byte[] value=DBS.hiddenLookup(lookupGroup.dbName,lookupGroup.getIndex(i),lookupGroup.getKey(i));
      if (!new String(value).equals(new String(lookupGroup.getValue(i)))) {
        System.err.println(""String_Node_Str"" + last.toString() + ""String_Node_Str""+ ""String_Node_Str""+ new String(value)+ ""String_Node_Str""+ new String(lookupGroup.getValue(i)));
        System.exit(1);
      }
    }
    System.out.println(""String_Node_Str"" + last.toString() + ""String_Node_Str"");
  }
  DBS.replication_resume();
}","/** 
 * Checks the last insert group of consistency.
 * @throws Exception
 */
private static void performConsistencyCheck() throws Exception {
  LSN last=DBS.replication_pause();
  if (last != null) {
    LookupGroup lookupGroup=generator.getLookupGroup(last);
    for (int i=0; i < lookupGroup.size(); i++) {
      byte[] value=DBS.hiddenLookup(lookupGroup.dbName,lookupGroup.getIndex(i),lookupGroup.getKey(i));
      if (!new String(value).equals(new String(lookupGroup.getValue(i)))) {
        System.err.println(""String_Node_Str"" + last.toString() + ""String_Node_Str""+ ""String_Node_Str""+ new String(value)+ ""String_Node_Str""+ new String(lookupGroup.getValue(i)));
        System.exit(1);
      }
    }
    System.out.println(""String_Node_Str"" + last.toString() + ""String_Node_Str"");
  }
 else   System.err.println(""String_Node_Str"");
  DBS.replication_resume();
}",0.972139893301719
132430,"/** 
 * Restarts the BabuDB. Remains stopped a random down-time.
 * @param master
 * @param slaves
 * @throws IOException
 * @throws InterruptedException
 * @throws BabuDBException
 */
private static void performRestart(Random random,InetSocketAddress master,List<InetSocketAddress> slaves) throws IOException, InterruptedException, BabuDBException {
  DBS.shutdown();
  int downTime=random.nextInt(MAX_DOWN_TIME - MIN_DOWN_TIME) + MIN_DOWN_TIME;
  System.out.println(""String_Node_Str"" + downTime / 60000 + ""String_Node_Str"");
  Thread.sleep(downTime);
  DBS=(BabuDBImpl)BabuDBFactory.getSlaveBabuDB(PATH,PATH,NUM_WKS,1,0,SyncMode.ASYNC,0,0,master,slaves,Replication.SLAVE_PORT,null,Replication.DEFAULT_MAX_Q);
  System.out.println(""String_Node_Str"");
}","/** 
 * Restarts the BabuDB. Remains stopped a random down-time.
 * @param master
 * @param slaves
 * @throws IOException
 * @throws InterruptedException
 * @throws BabuDBException
 */
private static void performRestart(Random random,InetSocketAddress master,List<InetSocketAddress> slaves) throws IOException, InterruptedException, BabuDBException {
  DBS.shutdown();
  int downTime=random.nextInt(MAX_DOWN_TIME - MIN_DOWN_TIME) + MIN_DOWN_TIME;
  System.out.println(""String_Node_Str"" + downTime / 60000 + ""String_Node_Str"");
  Thread.sleep(downTime);
  DBS=(BabuDBImpl)BabuDBFactory.getSlaveBabuDB(PATH,PATH,NUM_WKS,1,0,SyncMode.ASYNC,0,0,master,slaves,Replication.SLAVE_PORT,null,Replication.DEFAULT_MAX_Q);
}",0.9720136518771332
132431,"public static void main(String[] args) throws Exception {
  Logging.start(Logging.LEVEL_ERROR);
  if (args.length != 3)   usage();
  long seed=0L;
  try {
    seed=Long.valueOf(args[0]);
  }
 catch (  NumberFormatException e) {
    error(""String_Node_Str"" + args[0]);
  }
  InetSocketAddress master=parseAddress(args[1]);
  List<InetSocketAddress> slaves=new LinkedList<InetSocketAddress>();
  if (args[2].indexOf(""String_Node_Str"") == -1)   slaves.add(parseAddress(args[2]));
 else   for (  String adr : args[2].split(""String_Node_Str""))   slaves.add(parseAddress(adr));
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + PATH);
  p.waitFor();
  DBS=(BabuDBImpl)BabuDBFactory.getSlaveBabuDB(PATH,PATH,NUM_WKS,1,0,SyncMode.ASYNC,0,0,master,slaves,Replication.SLAVE_PORT,null,Replication.DEFAULT_MAX_Q);
  generator.initialize(seed);
  Random random=new Random();
  System.out.println(""String_Node_Str"");
  boolean ccheck=true;
  while (true) {
    int sleepInterval=0;
    if (ccheck)     sleepInterval=CCHECK_SLEEP_INTERVAL;
 else     sleepInterval=random.nextInt(MAX_SLEEP_INTERVAL - MIN_SLEEP_INTERVAL) + MIN_SLEEP_INTERVAL;
    System.out.println(""String_Node_Str"" + sleepInterval / 60000 + ""String_Node_Str"");
    Thread.sleep(sleepInterval);
    int event=random.nextInt(100);
    if (event < P_CCHECK) {
      performConsistencyCheck();
      ccheck=true;
    }
 else     if (event < (P_CCHECK + P_CLEAN_RESTART)) {
      performCleanAndRestart(random,master,slaves);
      ccheck=false;
    }
 else {
      performRestart(random,master,slaves);
      ccheck=false;
    }
  }
}","public static void main(String[] args) throws Exception {
  Logging.start(Logging.LEVEL_ERROR);
  if (args.length != 3)   usage();
  long seed=0L;
  try {
    seed=Long.valueOf(args[0]);
  }
 catch (  NumberFormatException e) {
    error(""String_Node_Str"" + args[0]);
  }
  InetSocketAddress master=parseAddress(args[1]);
  List<InetSocketAddress> slaves=new LinkedList<InetSocketAddress>();
  if (args[2].indexOf(""String_Node_Str"") == -1)   slaves.add(parseAddress(args[2]));
 else   for (  String adr : args[2].split(""String_Node_Str""))   slaves.add(parseAddress(adr));
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + PATH);
  p.waitFor();
  DBS=(BabuDBImpl)BabuDBFactory.getSlaveBabuDB(PATH,PATH,NUM_WKS,1,0,SyncMode.ASYNC,0,0,master,slaves,Replication.SLAVE_PORT,null,Replication.DEFAULT_MAX_Q);
  generator.initialize(seed);
  Random random=new Random();
  System.out.println(""String_Node_Str"");
  boolean ccheck=true;
  while (true) {
    int sleepInterval=0;
    if (ccheck)     sleepInterval=CCHECK_SLEEP_INTERVAL;
 else     sleepInterval=random.nextInt(MAX_SLEEP_INTERVAL - MIN_SLEEP_INTERVAL) + MIN_SLEEP_INTERVAL;
    System.out.println(""String_Node_Str"" + sleepInterval / 60000 + ""String_Node_Str"");
    Thread.sleep(sleepInterval);
    int event=random.nextInt(100);
    if (event < P_CCHECK) {
      System.out.println(""String_Node_Str"");
      performConsistencyCheck();
      ccheck=true;
    }
 else     if (event < (P_CCHECK + P_CLEAN_RESTART)) {
      System.out.println(""String_Node_Str"");
      performCleanAndRestart(random,master,slaves);
      ccheck=false;
    }
 else {
      System.out.println(""String_Node_Str"");
      performRestart(random,master,slaves);
      ccheck=false;
    }
  }
}",0.9593005728067532
132432,"/** 
 * Restarts the BabuDB with complete data-loss. Remains stopped a random down-time.
 * @param master
 * @param slaves
 * @throws IOException
 * @throws InterruptedException
 * @throws BabuDBException
 */
private static void performCleanAndRestart(Random random,InetSocketAddress master,List<InetSocketAddress> slaves) throws IOException, InterruptedException, BabuDBException {
  DBS.shutdown();
  int downTime=random.nextInt(MAX_DOWN_TIME - MIN_DOWN_TIME) + MIN_DOWN_TIME;
  System.out.println(""String_Node_Str"" + downTime / 60000 + ""String_Node_Str"");
  Thread.sleep(downTime);
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + PATH);
  p.waitFor();
  DBS=(BabuDBImpl)BabuDBFactory.getSlaveBabuDB(PATH,PATH,NUM_WKS,1,0,SyncMode.ASYNC,0,0,master,slaves,Replication.SLAVE_PORT,null,Replication.DEFAULT_MAX_Q);
  System.out.println(""String_Node_Str"");
}","/** 
 * Restarts the BabuDB with complete data-loss. Remains stopped a random down-time.
 * @param master
 * @param slaves
 * @throws IOException
 * @throws InterruptedException
 * @throws BabuDBException
 */
private static void performCleanAndRestart(Random random,InetSocketAddress master,List<InetSocketAddress> slaves) throws IOException, InterruptedException, BabuDBException {
  DBS.shutdown();
  int downTime=random.nextInt(MAX_DOWN_TIME - MIN_DOWN_TIME) + MIN_DOWN_TIME;
  System.out.println(""String_Node_Str"" + downTime / 60000 + ""String_Node_Str"");
  Thread.sleep(downTime);
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + PATH);
  p.waitFor();
  DBS=(BabuDBImpl)BabuDBFactory.getSlaveBabuDB(PATH,PATH,NUM_WKS,1,0,SyncMode.ASYNC,0,0,master,slaves,Replication.SLAVE_PORT,null,Replication.DEFAULT_MAX_Q);
}",0.975725281231498
132433,"public LSN switchLogFile(boolean incrementViewId) throws IOException {
  if (!sync.isHeldByCurrentThread()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  LSN lastSyncedLSN=new LSN(this.currentViewId.get(),this.nextLogSequenceNo.get() - 1);
  final String newFileName=createLogFileName();
  channel.close();
  fos.close();
  currentLogFileName=newFileName;
  File lf=new File(this.currentLogFileName);
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
    break;
case FSYNC:
  break;
case FDATASYNC:
{
  openMode=""String_Node_Str"";
  break;
}
case SYNC_WRITE:
{
openMode=""String_Node_Str"";
break;
}
case SYNC_WRITE_METADATA:
{
openMode=""String_Node_Str"";
break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + currentLogFileName);
if (incrementViewId) {
this.currentViewId.incrementAndGet();
this.nextLogSequenceNo.set(1L);
}
return lastSyncedLSN;
}","public LSN switchLogFile(boolean incrementViewId) throws IOException {
  if (!sync.isHeldByCurrentThread()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  LSN lastSyncedLSN=new LSN(this.currentViewId.get(),this.nextLogSequenceNo.get() - 1);
  final String newFileName=createLogFileName();
  channel.close();
  fos.close();
  currentLogFileName=newFileName;
  File lf=new File(this.currentLogFileName);
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + currentLogFileName);
if (incrementViewId) {
this.currentViewId.incrementAndGet();
this.nextLogSequenceNo.set(1L);
}
return lastSyncedLSN;
}",0.977205153617443
132434,"/** 
 * <p>For replication purpose.</p> <p>A master can mark the LSN as recommended for the next checkpoint, if all slaves have acknowledged replicas until LSN.</p>
 * @param lsn
 */
public synchronized void designateRecommendedCheckpoint(LSN lsn){
  this.recommended=lsn;
}","/** 
 * <p>For replication purpose.</p> <p>A master can mark the LSN as recommended for the next checkpoint, if all slaves have acknowledged replicas until LSN.</p>
 * @param lsn
 */
public void designateRecommendedCheckpoint(LSN lsn){
  this.recommended=lsn;
}",0.9757009345794392
132435,"@SuppressWarnings(""String_Node_Str"") @Override public void insertFinished(Object context){
  try {
    if (context == null) {
      String msg=""String_Node_Str"";
      Logging.logMessage(Logging.LEVEL_ERROR,this,msg);
      dbInterface.replication_runtime_failure(msg);
    }
    Status<Request> rq=(Status<Request>)context;
    updateLastWrittenLSN(rq.getValue().getLSN());
    replication.enqueueRequest(RequestPreProcessor.getACK_RQ(rq.getValue().getLSN(),rq.getValue().getSource()));
    Logging.logMessage(Logging.LEVEL_TRACE,replication,""String_Node_Str"" + rq.getValue().getLSN().toString());
    rq.finished();
  }
 catch (  ReplicationException e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
catch (  Exception e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void insertFinished(Object context){
  try {
    if (context == null) {
      String msg=""String_Node_Str"";
      Logging.logMessage(Logging.LEVEL_ERROR,this,msg);
      dbInterface.replication_runtime_failure(msg);
    }
    Status<Request> rq=(Status<Request>)context;
    updateLastWrittenLSN(rq.getValue().getLSN());
    replication.enqueueRequest(RequestPreProcessor.getACK_RQ(rq.getValue().getLSN(),rq.getValue().getSource()));
    Logging.logMessage(Logging.LEVEL_INFO,replication,""String_Node_Str"" + rq.getValue().getLSN().toString());
    rq.finished();
  }
 catch (  ReplicationException e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
catch (  Exception e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
}",0.9953246753246752
132436,"/** 
 * <p>Initializes the   {@link Hashtable} with <code>slaves</code> with LSN ""0:0"" as initial acknowledged LSN.</p>
 * @param slaves
 */
SlavesStatus(List<InetSocketAddress> slaves){
  super();
  if (slaves != null) {
    for (    InetSocketAddress slave : slaves)     put(slave.getAddress(),new LSN(0,0));
    statusListener=new Hashtable<InetSocketAddress,List<Status<Request>>>(slaves.size());
  }
 else   statusListener=new Hashtable<InetSocketAddress,List<Status<Request>>>();
  latestCommon=new LSN(0,0);
}","/** 
 * <p>Initializes the   {@link Hashtable} with <code>slaves</code> with LSN ""0:0"" as initial acknowledged LSN.</p>
 * @param slaves
 */
SlavesStatus(List<InetSocketAddress> slaves){
  super();
  if (slaves != null) {
    for (    InetSocketAddress slave : slaves)     put(slave,new LSN(0,0));
    statusListener=new Hashtable<InetSocketAddress,List<Status<Request>>>(slaves.size());
  }
 else   statusListener=new Hashtable<InetSocketAddress,List<Status<Request>>>();
  latestCommon=new LSN(0,0);
}",0.987242394504416
132437,"/** 
 * <p>Updates the latest acknowledged   {@link LSN} of a <code>slave</code>.</p><p>Compares all  {@link LSN}s if necessary, and updates the latest common acknowledged   {@link LSN}.</p> <p>Notify's the listener if set.</p>
 * @param slave
 * @param acknowledgedLSN
 * @return <code>true</code>, if the latestCommon {@link LSN} has changed, <code>false</code> otherwise.
 * @throws ReplicationException 
 */
synchronized boolean update(InetAddress slave,LSN acknowledgedLSN) throws ReplicationException {
  if (get(slave) != null) {
    if (get(slave).compareTo(acknowledgedLSN) < 0) {
      put(slave,acknowledgedLSN);
      LSN latestCommon=acknowledgedLSN;
      for (      LSN lsn : values()) {
        latestCommon=(lsn.compareTo(latestCommon) < 0) ? lsn : latestCommon;
      }
      if (!this.latestCommon.equals(latestCommon)) {
        this.latestCommon=latestCommon;
        return true;
      }
    }
  }
 else {
    put(slave,acknowledgedLSN);
    if (this.latestCommon.compareTo(acknowledgedLSN) > 0) {
      this.latestCommon=acknowledgedLSN;
      return true;
    }
  }
  List<Status<Request>> listeners=statusListener.get(slave);
  if (listeners != null && !listeners.isEmpty()) {
    Collection<Status<Request>> ready=new HashSet<Status<Request>>();
    for (    Status<Request> listener : listeners) {
      if (listener.getValue().getLSN().compareTo(acknowledgedLSN) <= 0) {
        listener.finished();
        ready.add(listener);
      }
    }
    listeners.removeAll(ready);
  }
  return false;
}","/** 
 * <p>Updates the latest acknowledged   {@link LSN} of a <code>slave</code>.</p><p>Compares all  {@link LSN}s if necessary, and updates the latest common acknowledged   {@link LSN}.</p> <p>Notify's the listener if set.</p>
 * @param slave
 * @param acknowledgedLSN
 * @return <code>true</code>, if the latestCommon {@link LSN} has changed, <code>false</code> otherwise.
 * @throws ReplicationException 
 */
synchronized boolean update(InetSocketAddress slave,LSN acknowledgedLSN) throws ReplicationException {
  if (get(slave) != null) {
    if (get(slave).compareTo(acknowledgedLSN) < 0) {
      put(slave,acknowledgedLSN);
      LSN latestCommon=acknowledgedLSN;
      for (      LSN lsn : values()) {
        latestCommon=(lsn.compareTo(latestCommon) < 0) ? lsn : latestCommon;
      }
      if (!this.latestCommon.equals(latestCommon)) {
        this.latestCommon=latestCommon;
        return true;
      }
    }
  }
 else {
    put(slave,acknowledgedLSN);
    if (this.latestCommon.compareTo(acknowledgedLSN) > 0) {
      this.latestCommon=acknowledgedLSN;
      return true;
    }
  }
  List<Status<Request>> listeners=statusListener.get(slave);
  if (listeners != null && !listeners.isEmpty()) {
    Collection<Status<Request>> ready=new HashSet<Status<Request>>();
    for (    Status<Request> listener : listeners) {
      if (listener.getValue().getLSN().compareTo(acknowledgedLSN) <= 0) {
        listener.finished();
        ready.add(listener);
      }
    }
    listeners.removeAll(ready);
  }
  return false;
}",0.9980353634577604
132438,"/** 
 * Decreases the counter for expectable sub-requests.
 * @return true, if counter was decreased to 0, false otherwise.
 */
private boolean decreaseMinExpectableResp(){
  int remainingExpected=minExpectableResp.decrementAndGet();
  int remainingReceivable=maxReceivableResp.decrementAndGet();
  return remainingExpected == 0 || (remainingReceivable == 0 && remainingExpected < 0);
}","/** 
 * <p>Decreases the counter for expectable sub-requests.<br> This function is used in the case, that the request succeeds.</p>
 * @return true, if counter was decreased to 0, false otherwise.
 */
private boolean decreaseMinExpectableResp(){
  int remainingExpected=minExpectableResp.decrementAndGet();
  int remainingReceivable=maxReceivableResp.decrementAndGet();
  return remainingExpected == -1 || (remainingExpected == 0 && remainingReceivable >= 0);
}",0.8618654073199528
132439,"/** 
 * <p>Sets the request's status to pending and reorders the queue it is in.</p>
 * @throws ReplicationException if status could not be changed.
 */
void pending() throws ReplicationException {
  state.set(PENDING);
  statusListener.statusChanged(this);
}","/** 
 * <p>Sets the request's status to pending and reorders the queue it is in.</p>
 * @throws ReplicationException if status could not be changed.
 */
void pending() throws ReplicationException {
synchronized (state) {
    state.set(PENDING);
    statusListener.statusChanged(this);
  }
}",0.9435336976320584
132440,"/** 
 * Decreases the counter for receivable sub-requests by <code>count</code>.
 * @param count
 * @return true, if the maximal number of receivable responses is GE than the minimal number of ACKs expected by the application. false otherwise 
 */
private boolean decreaseMaxReceivableResp(){
}","/** 
 * <p>Decreases the counter for receivable sub-requests by <code>count</code>.<br> This function is used in the case, that the request fails.</p>
 * @param count
 * @return true, if the maximal number of receivable responses is GE than the minimal number of ACKs expected by the application. false otherwise. 
 */
private boolean decreaseMaxReceivableResp(){
}",0.8922610015174507
132441,"/** 
 * <p>Resets the state of the actual request.</p>
 * @throws ReplicationException if status could not be changed.
 */
void retry() throws ReplicationException {
  state.set(OPEN);
  maxReceivableResp.set(1);
  minExpectableResp.set(0);
  statusListener.statusChanged(this);
}","/** 
 * <p>Resets the state of the actual request.</p>
 * @throws ReplicationException if status could not be changed.
 */
void retry() throws ReplicationException {
synchronized (state) {
    state.set(OPEN);
    maxReceivableResp.set(1);
    minExpectableResp.set(0);
    statusListener.statusChanged(this);
  }
}",0.9411764705882352
132442,"/** 
 * <p>Needed for the initial load process of the babuDB, done by the replication.</p>
 * @param latest - {@link LSN} until which the loading is done.
 * @throws BabuDBException 
 */
public void reset(LSN latest) throws BabuDBException {
  for (  LSMDBWorker w : worker) {
    w.shutdown();
  }
  logger.shutdown();
  if (dbCheckptr != null) {
    dbCheckptr.shutdown();
  }
  try {
    logger.waitForShutdown();
    for (    LSMDBWorker w : worker) {
      w.waitForShutdown();
    }
    if (dbCheckptr != null) {
      dbCheckptr.waitForShutdown();
    }
  }
 catch (  InterruptedException ex) {
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  dbNames.clear();
  databases.clear();
  nextDbId=1;
  compInstances.clear();
  compInstances.put(DefaultByteRangeComparator.class.getName(),new DefaultByteRangeComparator());
  loadDBs();
  LSN dbLsn=null;
  for (  LSMDatabase db : databases.values()) {
    if (dbLsn == null)     dbLsn=db.getOndiskLSN();
 else {
      if (!dbLsn.equals(db.getOndiskLSN()))       throw new RuntimeException(""String_Node_Str"");
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs();
  Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + latest.toString() + ""String_Node_Str""+ dbLsn.toString()+ ""String_Node_Str""+ nextLSN.toString());
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  try {
    logger=new DiskLogger(configuration.getDbLogDir(),latest.getViewId(),latest.getSequenceNo() + 1L,configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads());
    logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
synchronized (dbModificationLock) {
    dbModificationLock.notifyAll();
  }
synchronized (checkpointLock) {
    checkpointLock.notifyAll();
  }
  overlaySwitchLock=new ReentrantReadWriteLock();
  replicationFacade.changeContextSwitchLock(overlaySwitchLock.readLock());
  worker=new LSMDBWorker[configuration.getNumThreads()];
  for (int i=0; i < configuration.getNumThreads(); i++) {
    worker[i]=new LSMDBWorker(logger,i,overlaySwitchLock,(configuration.getPseudoSyncWait() > 0),configuration.getMaxQueueLength(),replicationFacade);
    worker[i].start();
  }
  if (configuration.getCheckInterval() > 0) {
    dbCheckptr=new Checkpointer(this,logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    dbCheckptr.start();
  }
 else {
    dbCheckptr=null;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}","/** 
 * <p>Needed for the initial load process of the babuDB, done by the replication.</p>
 * @param latest - {@link LSN} until which the loading is done.
 * @throws BabuDBException 
 */
public void reset(LSN latest) throws BabuDBException {
  for (  LSMDBWorker w : worker) {
    w.shutdown();
  }
  logger.shutdown();
  if (dbCheckptr != null) {
    dbCheckptr.shutdown();
  }
  try {
    logger.waitForShutdown();
    for (    LSMDBWorker w : worker) {
      w.waitForShutdown();
    }
    if (dbCheckptr != null) {
      dbCheckptr.waitForShutdown();
    }
  }
 catch (  InterruptedException ex) {
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  dbNames.clear();
  databases.clear();
  nextDbId=1;
  compInstances.clear();
  compInstances.put(DefaultByteRangeComparator.class.getName(),new DefaultByteRangeComparator());
  loadDBs();
  LSN dbLsn=null;
  for (  LSMDatabase db : databases.values()) {
    if (dbLsn == null)     dbLsn=db.getOndiskLSN();
 else {
      if (!dbLsn.equals(db.getOndiskLSN()))       throw new RuntimeException(""String_Node_Str"");
    }
  }
  if (dbLsn == null) {
    dbLsn=new LSN(0,0);
  }
 else {
    dbLsn=new LSN(dbLsn.getViewId(),dbLsn.getSequenceNo() + 1);
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs();
  if (dbLsn.compareTo(nextLSN) > 0) {
    nextLSN=dbLsn;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + nextLSN);
  try {
    logger=new DiskLogger(configuration.getDbLogDir(),latest.getViewId(),latest.getSequenceNo() + 1L,configuration.getSyncMode(),configuration.getPseudoSyncWait(),configuration.getMaxQueueLength() * configuration.getNumThreads());
    logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
synchronized (dbModificationLock) {
    dbModificationLock.notifyAll();
  }
synchronized (checkpointLock) {
    checkpointLock.notifyAll();
  }
  overlaySwitchLock=new ReentrantReadWriteLock();
  replicationFacade.changeContextSwitchLock(overlaySwitchLock.readLock());
  worker=new LSMDBWorker[configuration.getNumThreads()];
  for (int i=0; i < configuration.getNumThreads(); i++) {
    worker[i]=new LSMDBWorker(logger,i,overlaySwitchLock,(configuration.getPseudoSyncWait() > 0),configuration.getMaxQueueLength(),replicationFacade);
    worker[i].start();
  }
  if (configuration.getCheckInterval() > 0) {
    dbCheckptr=new Checkpointer(this,logger,configuration.getCheckInterval(),configuration.getMaxLogfileSize());
    dbCheckptr.start();
  }
 else {
    dbCheckptr=null;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}",0.9707203161487336
132443,"/** 
 * @return the byte-range beginning position.
 */
public long getBegin(){
  return begin;
}","/** 
 * @return the byte-range beginning position.
 */
long getBegin(){
  return begin;
}",0.9621621621621622
132444,"@Override public String toString(){
  return ""String_Node_Str"" + fileName + ""String_Node_Str""+ begin+ ""String_Node_Str""+ end+ ""String_Node_Str"";
}","@Override public String toString(){
  return ""String_Node_Str"" + fileName + ""String_Node_Str""+ begin+ ""String_Node_Str""+ end+ ""String_Node_Str""+ ""String_Node_Str""+ ((data == null) ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str"";
}",0.7525773195876289
132445,"/** 
 * <p>Converts the given <code>json</code> back to a   {@link Chunk}.</p>
 * @param json
 */
Chunk(List<Object> json){
}","/** 
 * <p>Allocates the given range (<code>r1</code>,<code>r2</code>) in the correct order to begin and end of the byte range of file with <code>fName</code>.</p>
 * @param fName
 * @param r1
 * @param r2
 */
Chunk(String fName,long r1,long r2){
}",0.225201072386059
132446,"/** 
 * @return the byte-range ending position.
 */
public long getEnd(){
  return end;
}","/** 
 * @return the byte-range ending position.
 */
long getEnd(){
  return end;
}",0.95906432748538
132447,"/** 
 * @return the fileName, where the chunk is located at.
 */
public String getFileName(){
  return fileName;
}","/** 
 * @return the fileName, where the chunk is located at.
 */
String getFileName(){
  return fileName;
}",0.9683257918552036
132448,"@Override public int compare(Status<T> o1,Status<T> o2){
  if ((o1.isPending() && o2.isPending()) || (!o1.isPending() && !o2.isPending())) {
    if (o1.getValue() instanceof Request) {
      Request rq1=(Request)o1.getValue();
      Request rq2=(Request)o2.getValue();
      if (rq2 == null)       return +1;
      Token rq1t=rq1.getToken();
      Token rq2t=rq2.getToken();
      if (rq1t.compareTo(rq2t) == 0) {
switch (rq1t) {
case ACK:
          return rq2.getLSN().compareTo(rq1.getLSN());
case RQ:
        return rq1.getLSN().compareTo(rq2.getLSN());
case REPLICA_BROADCAST:
      return rq1.getLSN().compareTo(rq2.getLSN());
case CHUNK:
    return rq1.getChunkDetails().compareTo(rq2.getChunkDetails());
case ACK_RQ:
  if (rq1.getLSN().equals(rq2.getLSN()))   return -1;
 else   return rq1.getLSN().compareTo(rq2.getLSN());
case REPLICA:
return rq1.getLSN().compareTo(rq2.getLSN());
case CHUNK_RP:
return rq1.getChunkDetails().compareTo(rq2.getChunkDetails());
default :
return 0;
}
}
return rq1t.compareTo(rq2t);
}
 else if (o1.getValue() instanceof LSN) {
LSN lsn1=(LSN)o1.getValue();
LSN lsn2=(LSN)o2.getValue();
return lsn1.compareTo(lsn2);
}
 else if (o1.getValue() instanceof Chunk) {
Chunk chunk1=(Chunk)o1.getValue();
Chunk chunk2=(Chunk)o2.getValue();
return chunk1.compareTo(chunk2);
}
 else return +1;
}
 else if (o1.isPending()) return +1;
 else return -1;
}","@Override public int compare(Status<T> o1,Status<T> o2){
  if ((o1.isPending() && o2.isPending()) || (!o1.isPending() && !o2.isPending())) {
    if (o1.getValue() instanceof Request) {
      Request rq1=(Request)o1.getValue();
      Request rq2=(Request)o2.getValue();
      if (rq2 == null)       return +1;
      Token rq1t=rq1.getToken();
      Token rq2t=rq2.getToken();
      if (rq1t.compareTo(rq2t) == 0) {
switch (rq1t) {
case ACK:
          return rq2.getLSN().compareTo(rq1.getLSN());
case RQ:
        return rq1.getLSN().compareTo(rq2.getLSN());
case REPLICA_BROADCAST:
      return rq1.getLSN().compareTo(rq2.getLSN());
case CHUNK:
    return rq1.getChunk().compareTo(rq2.getChunk());
case ACK_RQ:
  if (rq1.getLSN().equals(rq2.getLSN()))   return -1;
 else   return rq1.getLSN().compareTo(rq2.getLSN());
case REPLICA:
return rq1.getLSN().compareTo(rq2.getLSN());
case CHUNK_RP:
return rq1.getChunk().compareTo(rq2.getChunk());
default :
return 0;
}
}
return rq1t.compareTo(rq2t);
}
 else if (o1.getValue() instanceof LSN) {
LSN lsn1=(LSN)o1.getValue();
LSN lsn2=(LSN)o2.getValue();
return lsn1.compareTo(lsn2);
}
 else if (o1.getValue() instanceof Chunk) {
Chunk chunk1=(Chunk)o1.getValue();
Chunk chunk2=(Chunk)o2.getValue();
return chunk1.compareTo(chunk2);
}
 else return +1;
}
 else if (o1.isPending()) return +1;
 else return -1;
}",0.9897285399853264
132449,"/** 
 * sets the inner state
 * @param c
 */
public void switchCondition(CONDITION c){
  this.condition=c;
  Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + c.toString() + ""String_Node_Str"");
}","/** 
 * sets the inner state
 * @param c
 */
void switchCondition(CONDITION c){
  this.condition=c;
  Logging.logMessage(Logging.LEVEL_TRACE,this,""String_Node_Str"" + c.toString() + ""String_Node_Str"");
}",0.9829683698296836
132450,"/** 
 * @return the inner state
 */
public CONDITION getCondition(){
  return this.condition;
}","/** 
 * @return the inner state
 */
CONDITION getCondition(){
  return this.condition;
}",0.9617486338797814
132451,"@SuppressWarnings(""String_Node_Str"") @Override public void insertFinished(Object context){
  try {
    if (context == null) {
      String msg=""String_Node_Str"";
      Logging.logMessage(Logging.LEVEL_ERROR,this,msg);
      dbInterface.replication_runtime_failure(msg);
    }
    Status<Request> rq=(Status<Request>)context;
    updateLastWrittenLSN(rq.getValue().getLSN());
    replication.enqueueRequest(RequestPreProcessor.getACK_RQ(rq.getValue().getLSN(),rq.getValue().getSource()));
    if ((rq.getValue().getLSN().getSequenceNo() % 10) == 0)     Logging.logMessage(Logging.LEVEL_ERROR,replication,""String_Node_Str"" + rq.getValue().getLSN().toString());
    rq.finished();
  }
 catch (  ReplicationException e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
catch (  Exception e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void insertFinished(Object context){
  try {
    if (context == null) {
      String msg=""String_Node_Str"";
      Logging.logMessage(Logging.LEVEL_ERROR,this,msg);
      dbInterface.replication_runtime_failure(msg);
    }
    Status<Request> rq=(Status<Request>)context;
    updateLastWrittenLSN(rq.getValue().getLSN());
    replication.enqueueRequest(RequestPreProcessor.getACK_RQ(rq.getValue().getLSN(),rq.getValue().getSource()));
    Logging.logMessage(Logging.LEVEL_TRACE,replication,""String_Node_Str"" + rq.getValue().getLSN().toString());
    rq.finished();
  }
 catch (  ReplicationException e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
catch (  Exception e) {
    Logging.logMessage(Logging.LEVEL_ERROR,this,e.getMessage());
    dbInterface.replication_runtime_failure(e.getMessage());
  }
}",0.932527693856999
132452,"public void directInsert(BabuDBInsertGroup irg) throws BabuDBException {
  final LSMDatabase db=databases.get(irg.getRecord().getDatabaseId());
  if (db == null) {
    throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"");
  }
  final int numIndices=db.getIndexCount();
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    if ((ir.getIndexId() >= numIndices) || (ir.getIndexId() < 0)) {
      throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"" + ir.getIndexId() + ""String_Node_Str"");
    }
  }
  int size=irg.getRecord().getSize();
  ReusableBuffer buf=BufferPool.allocate(size);
  irg.getRecord().serialize(buf);
  buf.flip();
  final AsyncResult result=new AsyncResult();
  LogEntry e=new LogEntry(buf,new SyncListener(){
    public void synced(    LogEntry entry){
synchronized (result) {
        result.done=true;
        result.notifyAll();
      }
    }
    public void failed(    LogEntry entry,    Exception ex){
synchronized (result) {
        result.done=true;
        result.error=new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
        result.notifyAll();
      }
    }
  }
);
  try {
    logger.append(e);
  }
 catch (  InterruptedException ex) {
    throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",ex);
  }
synchronized (result) {
    if (!result.done) {
      try {
        result.wait();
      }
 catch (      InterruptedException ex) {
        throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",ex);
      }
    }
  }
  if (result.error != null) {
    throw result.error;
  }
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    final LSMTree index=db.getIndex(ir.getIndexId());
    if (ir.getValue() != null) {
      index.insert(ir.getKey(),ir.getValue());
    }
 else {
      index.delete(ir.getKey());
    }
  }
}","public void directInsert(BabuDBInsertGroup irg) throws BabuDBException {
  final LSMDatabase db=databases.get(irg.getRecord().getDatabaseId());
  if (db == null) {
    throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"");
  }
  final int numIndices=db.getIndexCount();
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    if ((ir.getIndexId() >= numIndices) || (ir.getIndexId() < 0)) {
      throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"" + ir.getIndexId() + ""String_Node_Str"");
    }
  }
  int size=irg.getRecord().getSize();
  ReusableBuffer buf=BufferPool.allocate(size);
  irg.getRecord().serialize(buf);
  buf.flip();
  final AsyncResult result=new AsyncResult();
  LogEntry e=new LogEntry(buf,new SyncListener(){
    public void synced(    LogEntry entry){
synchronized (result) {
        result.done=true;
        result.notifyAll();
      }
    }
    public void failed(    LogEntry entry,    Exception ex){
synchronized (result) {
        result.done=true;
        result.error=new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
        result.notifyAll();
      }
    }
  }
);
  try {
    logger.append(e);
  }
 catch (  InterruptedException ex) {
    throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",ex);
  }
synchronized (result) {
    if (!result.done) {
      try {
        result.wait();
      }
 catch (      InterruptedException ex) {
        throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",ex);
      }
    }
  }
  if (result.error != null) {
    throw result.error;
  }
  e.free();
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    final LSMTree index=db.getIndex(ir.getIndexId());
    if (ir.getValue() != null) {
      index.insert(ir.getKey(),ir.getValue());
    }
 else {
      index.delete(ir.getKey());
    }
  }
}",0.996742671009772
132453,"/** 
 * Deletes a key-value pair. This method is equivalent to <code>insert(key, null)</code>.
 * @param key the key
 */
public void delete(byte[] key){
  overlay.insert(key,null);
}","/** 
 * Deletes a key-value pair. This method is equivalent to <code>insert(key, null)</code>.
 * @param key the key
 */
public void delete(byte[] key){
synchronized (lock) {
    overlay.insert(key,null);
  }
}",0.9285714285714286
132454,"public LSN switchLogFile() throws IOException {
  if (!sync.isHeldByCurrentThread()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  LSN lastSyncedLSN=new LSN(this.currentViewId.get(),this.nextLogSequenceNo.get() - 1);
  final String newFileName=createLogFileName();
  channel.close();
  fos.close();
  currentLogFileName=newFileName;
  File lf=new File(this.currentLogFileName);
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
System.out.println(""String_Node_Str"" + openMode);
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + currentLogFileName);
return lastSyncedLSN;
}","public LSN switchLogFile() throws IOException {
  if (!sync.isHeldByCurrentThread()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  LSN lastSyncedLSN=new LSN(this.currentViewId.get(),this.nextLogSequenceNo.get() - 1);
  final String newFileName=createLogFileName();
  channel.close();
  fos.close();
  currentLogFileName=newFileName;
  File lf=new File(this.currentLogFileName);
  String openMode=""String_Node_Str"";
switch (syncMode) {
case ASYNC:
case FSYNC:
case FDATASYNC:
{
      openMode=""String_Node_Str"";
      break;
    }
case SYNC_WRITE:
{
    openMode=""String_Node_Str"";
    break;
  }
case SYNC_WRITE_METADATA:
{
  openMode=""String_Node_Str"";
  break;
}
}
fos=new RandomAccessFile(lf,openMode);
fos.setLength(0);
channel=fos.getChannel();
fdes=fos.getFD();
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + currentLogFileName);
return lastSyncedLSN;
}",0.972796517954298
132455,"/** 
 * Links the indices to the latest on-disk snapshot, cleans up any  unnecessary in-memory and on-disk data
 * @param viewId the viewId of the snapshot
 * @param sequenceNo the sequenceNo of the snaphot
 * @throws java.io.IOException if snapshots cannot be cleaned up
 */
public void cleanupSnapshot(final int viewId,final long sequenceNo) throws IOException {
  for (int index=0; index < trees.size(); index++) {
    final LSMTree tree=trees.get(index);
    System.out.println(""String_Node_Str"" + databaseDir + getSnaphotFilename(index,viewId,sequenceNo));
    tree.linkToSnapshot(databaseDir + getSnaphotFilename(index,viewId,sequenceNo));
    ondiskLSN=new LSN(viewId,sequenceNo);
    File f=new File(databaseDir);
    String[] files=f.list();
    Pattern p=Pattern.compile(SNAPSHOT_FILENAME_REGEXP);
    for (    String fname : files) {
      Matcher m=p.matcher(fname);
      if (m.matches()) {
        int fView=Integer.valueOf(m.group(2));
        int fSeq=Integer.valueOf(m.group(3));
        if ((fView < viewId) || ((fView == viewId) && (fSeq < sequenceNo))) {
          File snap=new File(databaseDir + fname);
          snap.delete();
        }
      }
    }
  }
}","/** 
 * Links the indices to the latest on-disk snapshot, cleans up any  unnecessary in-memory and on-disk data
 * @param viewId the viewId of the snapshot
 * @param sequenceNo the sequenceNo of the snaphot
 * @throws java.io.IOException if snapshots cannot be cleaned up
 */
public void cleanupSnapshot(final int viewId,final long sequenceNo) throws IOException {
  for (int index=0; index < trees.size(); index++) {
    final LSMTree tree=trees.get(index);
    if (Logging.isDebug())     Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + databaseDir + getSnaphotFilename(index,viewId,sequenceNo));
    tree.linkToSnapshot(databaseDir + getSnaphotFilename(index,viewId,sequenceNo));
    ondiskLSN=new LSN(viewId,sequenceNo);
    File f=new File(databaseDir);
    String[] files=f.list();
    Pattern p=Pattern.compile(SNAPSHOT_FILENAME_REGEXP);
    for (    String fname : files) {
      Matcher m=p.matcher(fname);
      if (m.matches()) {
        int fView=Integer.valueOf(m.group(2));
        int fSeq=Integer.valueOf(m.group(3));
        if ((fView < viewId) || ((fView == viewId) && (fSeq < sequenceNo))) {
          File snap=new File(databaseDir + fname);
          snap.delete();
        }
      }
    }
  }
}",0.9626865671641792
132456,"public static void usage(){
  System.out.println(""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
}","public static void usage(){
  System.out.println(""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.print(""String_Node_Str"");
  for (  SyncMode tmp : SyncMode.values()) {
    System.out.print(tmp.name() + ""String_Node_Str"");
  }
  System.out.println(""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
  System.out.println(""String_Node_Str"" + ""String_Node_Str"");
}",0.3987987987987988
132457,"public static void main(String[] args){
  try {
    Logging.start(Logging.LEVEL_WARN);
    Map<String,CLIParser.CliOption> options=new HashMap();
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.FILE,new File(""String_Node_Str"")));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.STRING,""String_Node_Str""));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,0));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,0));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,1));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,1));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,50));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,2));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,20));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.SWITCH,false));
    List<String> arguments=new ArrayList(1);
    CLIParser.parseCLI(args,options,arguments);
    if ((arguments.size() != 1) || (options.get(""String_Node_Str"").switchValue)) {
      usage();
      System.exit(1);
    }
    int numKeys=Integer.valueOf(arguments.get(0));
    BabuDBBenchmark benchmark=new BabuDBBenchmark(options.get(""String_Node_Str"").fileValue.getAbsolutePath(),SyncMode.valueOf(options.get(""String_Node_Str"").stringValue),options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue(),numKeys,options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue());
    double tpIns=benchmark.benchmarkInserts();
    double tpIter=benchmark.benchmarkIterate();
    double durCP=benchmark.checkpoint();
    double tpLookup=benchmark.benchmarkLookup();
    System.out.println(""String_Node_Str"");
    System.out.format(""String_Node_Str"",tpIns);
    System.out.format(""String_Node_Str"",tpIter);
    System.out.format(""String_Node_Str"",tpLookup);
    System.out.format(""String_Node_Str"",durCP);
    benchmark.shutdown();
  }
 catch (  Exception ex) {
    System.out.println(""String_Node_Str"");
    ex.printStackTrace();
    System.exit(1);
  }
}","public static void main(String[] args){
  try {
    Logging.start(Logging.LEVEL_WARN);
    Map<String,CLIParser.CliOption> options=new HashMap();
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.FILE,new File(""String_Node_Str"")));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.STRING,SyncMode.FSYNC.name()));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,0));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,0));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,1));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,1));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,50));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,2));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.NUMBER,20));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.SWITCH,false));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.SWITCH,false));
    options.put(""String_Node_Str"",new CLIParser.CliOption(CLIParser.CliOption.OPTIONTYPE.SWITCH,false));
    List<String> arguments=new ArrayList(1);
    CLIParser.parseCLI(args,options,arguments);
    if ((arguments.size() != 1) || (options.get(""String_Node_Str"").switchValue)) {
      usage();
      System.exit(1);
    }
    int numKeys=Integer.valueOf(arguments.get(0));
    BabuDBBenchmark benchmark=new BabuDBBenchmark(options.get(""String_Node_Str"").fileValue.getAbsolutePath(),SyncMode.valueOf(options.get(""String_Node_Str"").stringValue),options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue(),numKeys,options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue(),options.get(""String_Node_Str"").numValue.intValue());
    double tpIns=benchmark.benchmarkInserts();
    double tpIter=benchmark.benchmarkIterate();
    double durCP=0;
    if (options.get(""String_Node_Str"").switchValue == false)     durCP=benchmark.checkpoint();
    double tpLookup=benchmark.benchmarkLookup();
    double tpDLookup=benchmark.benchmarkDirectLookup();
    double tpUDL=benchmark.benchmarkUDL();
    double tpSecondLookup=0;
    if (options.get(""String_Node_Str"").switchValue)     tpSecondLookup=benchmark.benchmarkLookup();
    System.out.println(""String_Node_Str"");
    System.out.format(""String_Node_Str"",tpIns);
    System.out.format(""String_Node_Str"",tpIter);
    System.out.format(""String_Node_Str"",tpLookup);
    System.out.format(""String_Node_Str"",tpDLookup);
    System.out.format(""String_Node_Str"",tpUDL * 10.0);
    if (options.get(""String_Node_Str"").switchValue)     System.out.format(""String_Node_Str"",tpSecondLookup);
    if (options.get(""String_Node_Str"").switchValue == false)     System.out.format(""String_Node_Str"",durCP);
 else     System.out.println(""String_Node_Str"");
    benchmark.shutdown();
  }
 catch (  Exception ex) {
    System.out.println(""String_Node_Str"");
    ex.printStackTrace();
    System.exit(1);
  }
}",0.8378828229027963
132458,"public void run(){
  try {
switch (operation) {
case INSERT:
      performInserts();
    break;
case LOOKUP:
  performLookups();
break;
case ITERATE:
countKeys();
break;
}
}
 catch (Exception ex) {
error=ex;
}
synchronized (this) {
done=true;
this.notifyAll();
}
}","public void run(){
  try {
switch (operation) {
case INSERT:
      performInserts();
    break;
case LOOKUP:
  performLookups();
break;
case DIRECT_LOOKUP:
performDirectLookups();
break;
case ITERATE:
countKeys();
break;
case UDL:
performUserDefinedLookups();
break;
}
}
 catch (Exception ex) {
error=ex;
}
synchronized (this) {
done=true;
this.notifyAll();
}
}",0.8448
132459,"/** 
 * Creates a new instance of ConnectionStatus
 * @param channel the channel to which this state object belongs.
 */
public ConnectionState(ChannelIO channel){
  active=new AtomicBoolean(true);
  this.channel=channel;
  data=BufferPool.allocate(BUFFSIZE);
  this.status=STATUS_IDLE;
  waitFor=null;
  sendQ=new LinkedBlockingQueue<SpeedyRequest>(MultiSpeedy.MAX_CLIENT_QUEUE);
  receiveQ=new LinkedList<SpeedyRequest>();
  this.conRetries=0;
  this.numReconnectCycles=0;
  this.lastUsed=TimeSync.getLocalSystemTime();
}","/** 
 * Creates a new instance of ConnectionStatus
 * @param channel the channel to which this state object belongs.
 */
public ConnectionState(ChannelIO channel){
  active=new AtomicBoolean(true);
  this.channel=channel;
  data=BufferPool.allocate(BUFFSIZE);
  this.status=STATUS_IDLE;
  waitFor=null;
  sendQ=new LinkedBlockingQueue<SpeedyRequest>(MultiSpeedy.MAX_CLIENT_QUEUE);
  receiveQ=new LinkedList<SpeedyRequest>();
  this.conRetries=0;
  this.numReconnectCycles=0;
  this.lastUsed=System.currentTimeMillis();
}",0.9587727708533078
132460,"public void connectFailed(){
  this.numReconnectCycles++;
  long waitt=Math.round(RETRY_RESET_IN_MS * Math.pow(2,this.numReconnectCycles));
  if (waitt > MAX_RETRY_WAIT)   waitt=MAX_RETRY_WAIT;
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + (waitt / 1000) + ""String_Node_Str""+ this.numReconnectCycles);
  this.nextReconnectTime=System.currentTimeMillis() + waitt;
  this.lastUsed=TimeSync.getLocalSystemTime();
}","public void connectFailed(){
  this.numReconnectCycles++;
  long waitt=Math.round(RETRY_RESET_IN_MS * Math.pow(2,this.numReconnectCycles));
  if (waitt > MAX_RETRY_WAIT)   waitt=MAX_RETRY_WAIT;
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + (waitt / 1000) + ""String_Node_Str""+ this.numReconnectCycles);
  this.nextReconnectTime=System.currentTimeMillis() + waitt;
  this.lastUsed=System.currentTimeMillis();
}",0.9452852153667056
132461,"private void checkForTimers(){
  long now=System.currentTimeMillis();
  if (now >= lastCheck + TIMEOUT_GRANULARITY) {
synchronized (connections) {
      Iterator<ConnectionState> conIter=connections.values().iterator();
      while (conIter.hasNext()) {
        ConnectionState con=conIter.next();
        if (con.lastUsed < (TimeSync.getLocalSystemTime() - CONNECTION_REMOVE_TIMEOUT)) {
          Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + con.endpoint);
          try {
            conIter.remove();
            cancelRequests(con);
            con.channel.close();
          }
 catch (          Exception ex) {
          }
 finally {
            con.freeBuffers();
          }
        }
        for (int i=1; i < 3; i++) {
          Iterator<SpeedyRequest> iter=(i == 1) ? con.receiveQ.iterator() : con.sendQ.iterator();
          while (iter.hasNext()) {
            SpeedyRequest rq=iter.next();
            if ((rq.status != SpeedyRequest.RequestStatus.FAILED) && (rq.status != SpeedyRequest.RequestStatus.FINISHED) && (rq.timeout > 0)) {
              rq.waited+=TIMEOUT_GRANULARITY;
              if (rq.waited > rq.timeout) {
                try {
                  Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + rq.waited + ""String_Node_Str""+ rq.timeout+ ""String_Node_Str""+ con.channel.keyFor(selector).interestOps()+ ""String_Node_Str""+ con.receiveQ.size());
                }
 catch (                Exception e) {
                }
                rq.status=SpeedyRequest.RequestStatus.FAILED;
                assert(!rq.listenerNotified);
                rq.listener.receiveRequest(rq);
                rq.listenerNotified=true;
                rq.freeBuffer();
                iter.remove();
                try {
                  rq.con.channel.close();
                  con.freeBuffers();
                  conIter.remove();
                  cancelRequests(rq.con);
                }
 catch (                Exception ex2) {
                  Logging.logMessage(Logging.LEVEL_DEBUG,this,ex2);
                }
              }
            }
          }
        }
      }
    }
    lastCheck=now;
  }
}","private void checkForTimers(){
  long now=System.currentTimeMillis();
  if (now >= lastCheck + TIMEOUT_GRANULARITY) {
synchronized (connections) {
      Iterator<ConnectionState> conIter=connections.values().iterator();
      while (conIter.hasNext()) {
        ConnectionState con=conIter.next();
        if (con.lastUsed < (System.currentTimeMillis() - CONNECTION_REMOVE_TIMEOUT)) {
          Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + con.endpoint);
          try {
            conIter.remove();
            cancelRequests(con);
            con.channel.close();
          }
 catch (          Exception ex) {
          }
 finally {
            con.freeBuffers();
          }
        }
        for (int i=1; i < 3; i++) {
          Iterator<SpeedyRequest> iter=(i == 1) ? con.receiveQ.iterator() : con.sendQ.iterator();
          while (iter.hasNext()) {
            SpeedyRequest rq=iter.next();
            if ((rq.status != SpeedyRequest.RequestStatus.FAILED) && (rq.status != SpeedyRequest.RequestStatus.FINISHED) && (rq.timeout > 0)) {
              rq.waited+=TIMEOUT_GRANULARITY;
              if (rq.waited > rq.timeout) {
                try {
                  Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"" + rq.waited + ""String_Node_Str""+ rq.timeout+ ""String_Node_Str""+ con.channel.keyFor(selector).interestOps()+ ""String_Node_Str""+ con.receiveQ.size());
                }
 catch (                Exception e) {
                }
                rq.status=SpeedyRequest.RequestStatus.FAILED;
                assert(!rq.listenerNotified);
                rq.listener.receiveRequest(rq);
                rq.listenerNotified=true;
                rq.freeBuffer();
                iter.remove();
                try {
                  rq.con.channel.close();
                  con.freeBuffers();
                  conIter.remove();
                  cancelRequests(rq.con);
                }
 catch (                Exception ex2) {
                  Logging.logMessage(Logging.LEVEL_DEBUG,this,ex2);
                }
              }
            }
          }
        }
      }
    }
    lastCheck=now;
  }
}",0.9900301414328774
132462,"/** 
 * Called to send a response.
 * @attention rq must have a connection attached!
 * @param rq the request to be sent
 * @throws java.lang.IllegalStateException if the send queue is full
 * @throws java.io.IOException passes all exceptions from the used IO primitives
 */
public void sendRequest(SpeedyRequest rq,InetSocketAddress server) throws IOException, IllegalStateException {
  if (rq.listener == null) {
    if (singleListener != null) {
      rq.listener=singleListener;
    }
 else {
      rq.listener=this.listeners.get(server);
      if (rq.listener == null)       throw new RuntimeException(""String_Node_Str"" + server);
    }
  }
  ConnectionState con=null;
synchronized (connections) {
    con=connections.get(server);
    if (con == null) {
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + server);
      ChannelIO channel;
      try {
        if (sslOptions == null) {
          channel=new ChannelIO(SocketChannel.open());
        }
 else {
          channel=new SSLChannelIO(SocketChannel.open(),sslOptions,true);
        }
      }
 catch (      IOException ex) {
        System.out.println(""String_Node_Str"");
        System.out.println(this.getStatus());
        System.out.println();
        throw ex;
      }
      channel.configureBlocking(false);
      channel.socket().setTcpNoDelay(true);
      channel.socket().setReceiveBufferSize(256 * 1024);
      channel.connect(server);
      con=new ConnectionState(channel);
      con.endpoint=server;
      newCons.add(con);
      connections.put(server,con);
      rq.registerConnection(con);
      rq.status=SpeedyRequest.RequestStatus.PENDING;
      con.sendQ.add(rq);
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
      selector.wakeup();
    }
 else {
      if (con.conRetries >= MultiSpeedy.MAX_RECONNECT) {
        if (con.canReconnect()) {
          con.conRetries=0;
          con.channel=null;
          Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + con.endpoint);
        }
 else {
          throw new IOException(""String_Node_Str"");
        }
      }
      if (con.channel == null) {
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + server);
        reconnect(con);
      }
      con.lastUsed=TimeSync.getLocalSystemTime();
      rq.registerConnection(con);
      rq.status=SpeedyRequest.RequestStatus.PENDING;
      con.sendQ.add(rq);
      SelectionKey key=con.channel.keyFor(selector);
      if (key != null) {
        if (key.isValid()) {
synchronized (key) {
            key.interestOps(key.interestOps() | SelectionKey.OP_WRITE);
          }
        }
 else         Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"" + server);
      }
 else {
        Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"" + server);
      }
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + server);
      selector.wakeup();
    }
  }
}","/** 
 * Called to send a response.
 * @attention rq must have a connection attached!
 * @param rq the request to be sent
 * @throws java.lang.IllegalStateException if the send queue is full
 * @throws java.io.IOException passes all exceptions from the used IO primitives
 */
public void sendRequest(SpeedyRequest rq,InetSocketAddress server) throws IOException, IllegalStateException {
  if (rq.listener == null) {
    if (singleListener != null) {
      rq.listener=singleListener;
    }
 else {
      rq.listener=this.listeners.get(server);
      if (rq.listener == null)       throw new RuntimeException(""String_Node_Str"" + server);
    }
  }
  ConnectionState con=null;
synchronized (connections) {
    con=connections.get(server);
    if (con == null) {
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + server);
      ChannelIO channel;
      try {
        if (sslOptions == null) {
          channel=new ChannelIO(SocketChannel.open());
        }
 else {
          channel=new SSLChannelIO(SocketChannel.open(),sslOptions,true);
        }
      }
 catch (      IOException ex) {
        System.out.println(""String_Node_Str"");
        System.out.println(this.getStatus());
        System.out.println();
        throw ex;
      }
      channel.configureBlocking(false);
      channel.socket().setTcpNoDelay(true);
      channel.socket().setReceiveBufferSize(256 * 1024);
      channel.connect(server);
      con=new ConnectionState(channel);
      con.endpoint=server;
      newCons.add(con);
      connections.put(server,con);
      rq.registerConnection(con);
      rq.status=SpeedyRequest.RequestStatus.PENDING;
      con.sendQ.add(rq);
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
      selector.wakeup();
    }
 else {
      if (con.conRetries >= MultiSpeedy.MAX_RECONNECT) {
        if (con.canReconnect()) {
          con.conRetries=0;
          con.channel=null;
          Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + con.endpoint);
        }
 else {
          throw new IOException(""String_Node_Str"");
        }
      }
      if (con.channel == null) {
        Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + server);
        reconnect(con);
      }
      con.lastUsed=System.currentTimeMillis();
      rq.registerConnection(con);
      rq.status=SpeedyRequest.RequestStatus.PENDING;
      con.sendQ.add(rq);
      SelectionKey key=con.channel.keyFor(selector);
      if (key != null) {
        if (key.isValid()) {
synchronized (key) {
            key.interestOps(key.interestOps() | SelectionKey.OP_WRITE);
          }
        }
 else         Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"" + server);
      }
 else {
        Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"" + server);
      }
      Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + server);
      selector.wakeup();
    }
  }
}",0.9926658707146512
132463,"/** 
 * Performs a lookup in a given snapshot.
 * @param key the key to look up
 * @param snapId the snapshot ID
 * @return the value associated with the key in the snapshot
 */
public byte[] lookup(byte[] key,int snapId){
  byte[] result=overlay.lookup(key,snapId);
  if (result != null)   return result;
  return index == null ? null : index.lookup(key);
}","/** 
 * Performs a lookup in a given snapshot.
 * @param key the key to look up
 * @param snapId the snapshot ID
 * @return the value associated with the key in the snapshot
 */
public byte[] lookup(byte[] key,int snapId){
  byte[] result=overlay.lookup(key,snapId);
  if (result == NULL_ELEMENT)   return null;
  if (result != null)   return result;
  return index == null ? null : index.lookup(key);
}",0.9408672798948752
132464,"/** 
 * Performs a prefix lookup in a given snapshot. Key-value paris are returned in an iterator in ascending key order, where only such keys are returned with a matching prefix according to the comparator.
 * @param prefix the prefix
 * @return an iterator with key-value pairs
 */
public Iterator<Entry<byte[],byte[]>> prefixLookup(byte[] prefix,int snapId){
  if (prefix != null && prefix.length == 0)   prefix=null;
  List<Iterator<Entry<byte[],byte[]>>> list=new ArrayList<Iterator<Entry<byte[],byte[]>>>(2);
  list.add(overlay.prefixLookup(prefix,snapId));
  if (index != null) {
    byte[][] rng=comp.prefixToRange(prefix);
    list.add(index.rangeLookup(rng[0],rng[1]));
  }
  return new OverlayMergeIterator<byte[],byte[]>(list,comp,null);
}","/** 
 * Performs a prefix lookup in a given snapshot. Key-value paris are returned in an iterator in ascending key order, where only such keys are returned with a matching prefix according to the comparator.
 * @param prefix the prefix
 * @return an iterator with key-value pairs
 */
public Iterator<Entry<byte[],byte[]>> prefixLookup(byte[] prefix,int snapId){
  if (prefix != null && prefix.length == 0)   prefix=null;
  List<Iterator<Entry<byte[],byte[]>>> list=new ArrayList<Iterator<Entry<byte[],byte[]>>>(2);
  list.add(overlay.prefixLookup(prefix,snapId,true));
  if (index != null) {
    byte[][] rng=comp.prefixToRange(prefix);
    list.add(index.rangeLookup(rng[0],rng[1]));
  }
  return new OverlayMergeIterator<byte[],byte[]>(list,comp,null);
}",0.9966821499668216
132465,"private Entry<K,V> getNextElement(){
  for (; ; ) {
    int smallest=0;
    for (int i=1; i < nextElements.length; i++) {
      if (nextElements[i] == null)       continue;
      if (nextElements[smallest] == null || comp.compare(nextElements[i].getKey(),nextElements[smallest].getKey()) < 0)       smallest=i;
 else       if (comp.compare(nextElements[i].getKey(),nextElements[smallest].getKey()) == 0) {
        Iterator<Entry<K,V>> it=itList.get(i);
        nextElements[i]=it.hasNext() ? it.next() : null;
      }
    }
    Entry<K,V> entry=nextElements[smallest];
    Iterator<Entry<K,V>> it=itList.get(smallest);
    nextElements[smallest]=it.hasNext() ? it.next() : null;
    if (entry == null)     return null;
    if (entry.getValue() != nullValue)     return entry;
  }
}","private Entry<K,V> getNextElement(){
  for (; ; ) {
    int smallest=0;
    for (int i=1; i < nextElements.length; i++) {
      if (nextElements[i] == null)       continue;
      if (nextElements[smallest] == null || comp.compare(nextElements[i].getKey(),nextElements[smallest].getKey()) < 0)       smallest=i;
 else       if (comp.compare(nextElements[i].getKey(),nextElements[smallest].getKey()) == 0) {
        Iterator<Entry<K,V>> it=itList.get(i);
        nextElements[i]=it.hasNext() ? it.next() : null;
      }
    }
    Entry<K,V> entry=nextElements[smallest];
    Iterator<Entry<K,V>> it=itList.get(smallest);
    nextElements[smallest]=it.hasNext() ? it.next() : null;
    if (entry == null)     return null;
    if (nullValue == null || entry.getValue() != nullValue)     return entry;
  }
}",0.9867340492735313
132466,"public Iterator<Entry<byte[],byte[]>> prefixLookup(byte[] prefix,int overlayId){
  byte[][] keyRange=comp.prefixToRange(prefix);
  assert(keyRange.length == 2);
  return rangeLookup(keyRange[0],keyRange[1],overlayId);
}","public Iterator<Entry<byte[],byte[]>> prefixLookup(byte[] prefix,int overlayId,boolean includeDeletedEntries){
  byte[][] keyRange=comp.prefixToRange(prefix);
  assert(keyRange.length == 2);
  return rangeLookup(keyRange[0],keyRange[1],overlayId,includeDeletedEntries);
}",0.8938775510204081
132467,"public Iterator<Entry<String,V>> prefixLookup(String prefix,int overlayId){
  return rangeLookup(prefix,getNextPrefix(prefix),overlayId);
}","public Iterator<Entry<String,V>> prefixLookup(String prefix,int overlayId,boolean includeDeletedEntries){
  return rangeLookup(prefix,getNextPrefix(prefix),overlayId,includeDeletedEntries);
}",0.8424242424242424
132468,"/** 
 * Destroys any read-only overlay trees, such that only the current read-write tree remains.
 */
public void cleanup(){
  overlayMap.clear();
  treeList.next=null;
}","/** 
 * Destroys any read-only overlay trees, such that only the current read-write tree remains.
 */
public void cleanup(){
  overlayMap.clear();
  treeList.next=null;
  overlayId=0;
}",0.9577464788732394
132469,"private Iterator<Entry<K,V>> rangeLookup(K from,K to,OverlayTreeList treeList){
  final List<Iterator<Entry<K,V>>> itList=new ArrayList<Iterator<Entry<K,V>>>();
  for (OverlayTreeList list=treeList; list != null; list=list.next) {
    if (from != null && to != null)     itList.add(list.tree.subMap(from,to).entrySet().iterator());
 else     if (from == null && to == null)     itList.add(list.tree.entrySet().iterator());
 else     if (from != null && to == null)     itList.add(list.tree.tailMap(from).entrySet().iterator());
 else     itList.add(list.tree.headMap(to).entrySet().iterator());
  }
  return new OverlayMergeIterator<K,V>(itList,comparator,nullValue);
}","private Iterator<Entry<K,V>> rangeLookup(K from,K to,OverlayTreeList treeList,boolean includeDeletedEntries){
  final List<Iterator<Entry<K,V>>> itList=new ArrayList<Iterator<Entry<K,V>>>();
  for (OverlayTreeList list=treeList; list != null; list=list.next) {
    if (from != null && to != null)     itList.add(list.tree.subMap(from,to).entrySet().iterator());
 else     if (from == null && to == null)     itList.add(list.tree.entrySet().iterator());
 else     if (from != null && to == null)     itList.add(list.tree.tailMap(from).entrySet().iterator());
 else     itList.add(list.tree.headMap(to).entrySet().iterator());
  }
  return new OverlayMergeIterator<K,V>(itList,comparator,includeDeletedEntries ? null : nullValue);
}",0.956397426733381
132470,"/** 
 * Inserts a key-value pair in the LSM tree. If the value is <code>null</code>, the key will be removed.
 * @param key the key
 * @param value the value
 */
public void insert(K key,V value){
  if (value == null) {
    if (overlayId == 0 || lookup(key,overlayId - 1) == null) {
      treeList.tree.remove(key);
    }
 else     treeList.tree.put(key,nullValue);
  }
 else   treeList.tree.put(key,value);
}","/** 
 * Inserts a key-value pair in the LSM tree. If the value is <code>null</code>, the key will be removed.
 * @param key the key
 * @param value the value
 */
public void insert(K key,V value){
  if (value == null)   treeList.tree.put(key,nullValue);
 else   treeList.tree.put(key,value);
}",0.6809116809116809
132471,"public CopyDatabaseTest(){
  Logging.start(Logging.LEVEL_DEBUG);
}","public CopyDatabaseTest(){
}",0.5957446808510638
132472,"@Before public void setUp() throws Exception {
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + baseDir);
  p.waitFor();
}","@Before public void setUp() throws Exception {
  Logging.start(Logging.LEVEL_DEBUG);
  Process p=Runtime.getRuntime().exec(""String_Node_Str"" + baseDir);
  p.waitFor();
}",0.8733333333333333
132473,"public void setUp(){
  new File(SNAP_FILE).delete();
  new File(SNAP_FILE2).delete();
}","public void setUp(){
  Logging.start(Logging.LEVEL_ERROR);
  new File(SNAP_FILE).delete();
  new File(SNAP_FILE2).delete();
}",0.8207547169811321
132474,"@Before public void setUp(){
}","@Before public void setUp(){
  Logging.start(Logging.LEVEL_ERROR);
}",0.6122448979591837
132475,"public void testOverlayBufferTree(){
  MultiOverlayBufferTree tree=new MultiOverlayBufferTree(new byte[0],new DefaultByteRangeComparator());
  final int numElements=200;
  final SortedMap<String,byte[]> map1=new TreeMap<String,byte[]>();
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    String valString=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    byte[] key=keyString.getBytes();
    byte[] val=valString.getBytes();
    map1.put(keyString,val);
    tree.insert(key,val);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map1.get(keyString));
  }
  int snap1=tree.newOverlay();
  final SortedMap<String,byte[]> map2=new TreeMap<String,byte[]>(map1);
  for (int i=0; i < numElements; i+=2) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    tree.insert(key,null);
    map2.remove(keyString);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map2.get(keyString));
  }
  int snap2=tree.newOverlay();
  final SortedMap<String,byte[]> map3=new TreeMap<String,byte[]>(map2);
  for (int i=0; i < numElements; i+=5) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    byte[] val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE)).getBytes();
    tree.insert(key,val);
    map3.put(keyString,val);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map3.get(keyString));
  }
  Iterator<Entry<byte[],byte[]>> it=tree.prefixLookup(null);
  Iterator<byte[]> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap1);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap2);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(""String_Node_Str"".getBytes());
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}","public void testOverlayBufferTree(){
  MultiOverlayBufferTree tree=new MultiOverlayBufferTree(new byte[0],new DefaultByteRangeComparator());
  final int numElements=200;
  final SortedMap<String,byte[]> map1=new TreeMap<String,byte[]>();
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    String valString=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    byte[] key=keyString.getBytes();
    byte[] val=valString.getBytes();
    map1.put(keyString,val);
    tree.insert(key,val);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map1.get(keyString));
  }
  int snap1=tree.newOverlay();
  final SortedMap<String,byte[]> map2=new TreeMap<String,byte[]>(map1);
  for (int i=0; i < numElements; i+=2) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    tree.insert(key,null);
    map2.remove(keyString);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map2.get(keyString));
  }
  int snap2=tree.newOverlay();
  final SortedMap<String,byte[]> map3=new TreeMap<String,byte[]>(map2);
  for (int i=0; i < numElements; i+=5) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    byte[] val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE)).getBytes();
    tree.insert(key,val);
    map3.put(keyString,val);
  }
  for (int i=0; i < numElements; i++) {
    String keyString=Integer.toHexString(i);
    byte[] key=keyString.getBytes();
    assertEquals(tree.lookup(key),map3.get(keyString));
  }
  Iterator<Entry<byte[],byte[]>> it=tree.prefixLookup(null,false);
  Iterator<byte[]> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap1,false);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap2,false);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(""String_Node_Str"".getBytes(),false);
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}",0.9954303122619954
132476,"public void testOverlayStringTree(){
  MultiOverlayStringTree<String> tree=new MultiOverlayStringTree<String>(""String_Node_Str"");
  final int numElements=200;
  tree=new MultiOverlayStringTree<String>(""String_Node_Str"");
  final SortedMap<String,String> map1=new TreeMap<String,String>();
  for (int i=0x10; i < numElements; i++) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    map1.put(key,val);
    tree.insert(key,val);
  }
  int snap1=tree.newOverlay();
  final SortedMap<String,String> map2=new TreeMap<String,String>(map1);
  for (int i=0x10; i < numElements; i+=2) {
    String key=Integer.toHexString(i);
    tree.insert(key,null);
    map2.remove(key);
  }
  int snap2=tree.newOverlay();
  final SortedMap<String,String> map3=new TreeMap<String,String>(map2);
  for (int i=0x10; i < numElements; i+=5) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    tree.insert(key,val);
    map3.put(key,val);
  }
  Iterator<Entry<String,String>> it=tree.prefixLookup(null);
  Iterator<String> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap1);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap2);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(""String_Node_Str"");
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}","public void testOverlayStringTree(){
  MultiOverlayStringTree<String> tree=new MultiOverlayStringTree<String>(""String_Node_Str"");
  final int numElements=200;
  tree=new MultiOverlayStringTree<String>(""String_Node_Str"");
  final SortedMap<String,String> map1=new TreeMap<String,String>();
  for (int i=0x10; i < numElements; i++) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    map1.put(key,val);
    tree.insert(key,val);
  }
  int snap1=tree.newOverlay();
  final SortedMap<String,String> map2=new TreeMap<String,String>(map1);
  for (int i=0x10; i < numElements; i+=2) {
    String key=Integer.toHexString(i);
    tree.insert(key,null);
    map2.remove(key);
  }
  int snap2=tree.newOverlay();
  final SortedMap<String,String> map3=new TreeMap<String,String>(map2);
  for (int i=0x10; i < numElements; i+=5) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    tree.insert(key,val);
    map3.put(key,val);
  }
  Iterator<Entry<String,String>> it=tree.prefixLookup(null,false);
  Iterator<String> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap1,false);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(null,snap2,false);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.prefixLookup(""String_Node_Str"",false);
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}",0.9937759336099584
132477,"public void testOverlayTree(){
  MultiOverlayTree<String,String> tree=new MultiOverlayTree<String,String>(""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  int snap1=tree.newOverlay();
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",null);
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  int snap2=tree.newOverlay();
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  assertNull(tree.lookup(""String_Node_Str""));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertNull(tree.lookup(""String_Node_Str"",snap2));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap2));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  final int numElements=200;
  tree=new MultiOverlayTree<String,String>(""String_Node_Str"");
  final SortedMap<String,String> map1=new TreeMap<String,String>();
  for (int i=0x10; i < numElements; i++) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    map1.put(key,val);
    tree.insert(key,val);
  }
  snap1=tree.newOverlay();
  final SortedMap<String,String> map2=new TreeMap<String,String>(map1);
  for (int i=0x10; i < numElements; i+=2) {
    String key=Integer.toHexString(i);
    tree.insert(key,null);
    map2.remove(key);
  }
  snap2=tree.newOverlay();
  final SortedMap<String,String> map3=new TreeMap<String,String>(map2);
  for (int i=0x10; i < numElements; i+=5) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    tree.insert(key,val);
    map3.put(key,val);
  }
  Iterator<Entry<String,String>> it=tree.rangeLookup(null,null);
  Iterator<String> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(null,null,snap1);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(null,null,snap2);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(""String_Node_Str"",""String_Node_Str"");
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}","public void testOverlayTree(){
  MultiOverlayTree<String,String> tree=new MultiOverlayTree<String,String>(""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  int snap1=tree.newOverlay();
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",null);
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  int snap2=tree.newOverlay();
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  tree.insert(""String_Node_Str"",""String_Node_Str"");
  assertNull(tree.lookup(""String_Node_Str""));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  assertNull(tree.lookup(""String_Node_Str"",snap2));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap1));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str"",snap2));
  assertEquals(""String_Node_Str"",tree.lookup(""String_Node_Str""));
  final int numElements=200;
  tree=new MultiOverlayTree<String,String>(""String_Node_Str"");
  final SortedMap<String,String> map1=new TreeMap<String,String>();
  for (int i=0x10; i < numElements; i++) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    map1.put(key,val);
    tree.insert(key,val);
  }
  snap1=tree.newOverlay();
  final SortedMap<String,String> map2=new TreeMap<String,String>(map1);
  for (int i=0x10; i < numElements; i+=2) {
    String key=Integer.toHexString(i);
    tree.insert(key,null);
    map2.remove(key);
  }
  snap2=tree.newOverlay();
  final SortedMap<String,String> map3=new TreeMap<String,String>(map2);
  for (int i=0x10; i < numElements; i+=5) {
    String key=Integer.toHexString(i);
    String val=Integer.toHexString((int)(Math.random() * Integer.MAX_VALUE));
    tree.insert(key,val);
    map3.put(key,val);
  }
  Iterator<Entry<String,String>> it=tree.rangeLookup(null,null,false);
  Iterator<String> itExpected=map3.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(null,null,snap1,false);
  itExpected=map1.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(null,null,snap2,false);
  itExpected=map2.values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
  it=tree.rangeLookup(""String_Node_Str"",""String_Node_Str"",false);
  itExpected=map3.subMap(""String_Node_Str"",""String_Node_Str"").values().iterator();
  while (it.hasNext())   assertEquals(itExpected.next(),it.next().getValue());
  assertFalse(itExpected.hasNext());
}",0.9960617000328192
132478,"public Iterator<Entry<byte[],byte[]>> directPrefixLookup(String databaseName,int indexId,byte[] key) throws BabuDBException {
  final LSMDatabase db=dbNames.get(databaseName);
  if (db == null) {
    throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"");
  }
  if ((indexId >= db.getIndexCount()) || (indexId < 0)) {
    throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"");
  }
  return db.getIndex(indexId).prefixLookup(key);
}","public Iterator<Entry<byte[],byte[]>> directPrefixLookup(String databaseName,int indexId,byte[] key) throws BabuDBException {
  if (isSlave()) {
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,slaveProtection);
  }
  final LSMDatabase db=dbNames.get(databaseName);
  if (db == null) {
    throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"");
  }
  if ((indexId >= db.getIndexCount()) || (indexId < 0)) {
    throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"");
  }
  return db.getIndex(indexId).prefixLookup(key);
}",0.9004926108374385
132479,"public byte[] directLookup(String databaseName,int indexId,byte[] key) throws BabuDBException {
  final LSMDatabase db=dbNames.get(databaseName);
  if (db == null) {
    throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"");
  }
  if ((indexId >= db.getIndexCount()) || (indexId < 0)) {
    throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"");
  }
  return db.getIndex(indexId).lookup(key);
}","public byte[] directLookup(String databaseName,int indexId,byte[] key) throws BabuDBException {
  if (isSlave()) {
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,slaveProtection);
  }
  final LSMDatabase db=dbNames.get(databaseName);
  if (db == null) {
    throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"");
  }
  if ((indexId >= db.getIndexCount()) || (indexId < 0)) {
    throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"");
  }
  return db.getIndex(indexId).lookup(key);
}",0.8928950159066809
132480,"public void directInsert(BabuDBInsertGroup irg) throws BabuDBException {
  final LSMDatabase db=databases.get(irg.getRecord().getDatabaseId());
  if (db == null) {
    throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"");
  }
  final int numIndices=db.getIndexCount();
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    if ((ir.getIndexId() >= numIndices) || (ir.getIndexId() < 0)) {
      throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"" + ir.getIndexId() + ""String_Node_Str"");
    }
  }
  int size=irg.getRecord().getSize();
  ReusableBuffer buf=BufferPool.allocate(size);
  irg.getRecord().serialize(buf);
  buf.flip();
  final AsyncResult result=new AsyncResult();
  LogEntry e=new LogEntry(buf,new SyncListener(){
    public void synced(    LogEntry entry){
synchronized (result) {
        result.done=true;
        result.notifyAll();
      }
    }
    public void failed(    LogEntry entry,    Exception ex){
synchronized (result) {
        result.done=true;
        result.error=new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
        result.notifyAll();
      }
    }
  }
);
  try {
    logger.append(e);
  }
 catch (  InterruptedException ex) {
    throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",ex);
  }
synchronized (result) {
    if (!result.done) {
      try {
        result.wait();
      }
 catch (      InterruptedException ex) {
        throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",ex);
      }
    }
  }
  if (result.error != null) {
    throw result.error;
  }
  e.free();
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    final LSMTree index=db.getIndex(ir.getIndexId());
    if (ir.getValue() != null) {
      index.insert(ir.getKey(),ir.getValue());
    }
 else {
      index.delete(ir.getKey());
    }
  }
}","public void directInsert(BabuDBInsertGroup irg) throws BabuDBException {
  if (isSlave()) {
    throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,slaveProtection);
  }
  final LSMDatabase db=databases.get(irg.getRecord().getDatabaseId());
  if (db == null) {
    throw new BabuDBException(ErrorCode.NO_SUCH_DB,""String_Node_Str"");
  }
  final int numIndices=db.getIndexCount();
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    if ((ir.getIndexId() >= numIndices) || (ir.getIndexId() < 0)) {
      throw new BabuDBException(ErrorCode.NO_SUCH_INDEX,""String_Node_Str"" + ir.getIndexId() + ""String_Node_Str"");
    }
  }
  int size=irg.getRecord().getSize();
  ReusableBuffer buf=BufferPool.allocate(size);
  irg.getRecord().serialize(buf);
  buf.flip();
  final AsyncResult result=new AsyncResult();
  LogEntry e=new LogEntry(buf,new SyncListener(){
    public void synced(    LogEntry entry){
synchronized (result) {
        result.done=true;
        result.notifyAll();
      }
    }
    public void failed(    LogEntry entry,    Exception ex){
synchronized (result) {
        result.done=true;
        result.error=new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
        result.notifyAll();
      }
    }
  }
);
  try {
    logger.append(e);
  }
 catch (  InterruptedException ex) {
    throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",ex);
  }
synchronized (result) {
    if (!result.done) {
      try {
        result.wait();
      }
 catch (      InterruptedException ex) {
        throw new BabuDBException(ErrorCode.INTERNAL_ERROR,""String_Node_Str"",ex);
      }
    }
  }
  if (result.error != null) {
    throw result.error;
  }
  e.free();
  for (  InsertRecord ir : irg.getRecord().getInserts()) {
    final LSMTree index=db.getIndex(ir.getIndexId());
    if (ir.getValue() != null) {
      index.insert(ir.getKey(),ir.getValue());
    }
 else {
      index.delete(ir.getKey());
    }
  }
}",0.973400052673163
132481,"@Override public void insertFinished(Object context){
  Status<Request> rq=(Status<Request>)context;
  PinkyRequest orgReq=(PinkyRequest)rq.getValue().getOriginal();
  if (orgReq != null) {
    orgReq.setResponse(HTTPUtils.SC_OKAY);
    replication.sendResponse(orgReq);
  }
 else {
    rq.getValue().free();
    try {
      replication.sendACK(new Status(RequestPreProcessor.getACKRequest(rq.getValue().getLSN(),rq.getValue().getSource())));
    }
 catch (    ReplicationException e) {
      Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"");
    }
  }
  rq.getValue().free();
  replication.removeMissing(rq);
}","@Override public void insertFinished(Object context){
  Status<Request> rq=(Status<Request>)context;
  PinkyRequest orgReq=(PinkyRequest)rq.getValue().getOriginal();
  if (orgReq != null) {
    orgReq.setResponse(HTTPUtils.SC_OKAY);
    replication.sendResponse(orgReq);
  }
 else {
    rq.getValue().free();
    try {
      replication.sendACK(new Status(RequestPreProcessor.getACKRequest(rq.getValue().getLSN(),rq.getValue().getSource())));
    }
 catch (    ReplicationException e) {
      Logging.logMessage(Logging.LEVEL_WARN,this,""String_Node_Str"");
    }
  }
synchronized (lock) {
    lastWrittenLSN=(lastWrittenLSN.compareTo(rq.getValue().getLSN()) < 0) ? rq.getValue().getLSN() : lastWrittenLSN;
  }
  rq.getValue().free();
  replication.removeMissing(rq);
}",0.8943206326383897
132482,"public ByteRange(ByteBuffer buf,int startOffset,int endOffset){
  this.buf=buf;
  this.startOffset=startOffset;
  this.endOffset=endOffset;
  this.size=endOffset - startOffset;
}","public ByteRange(ByteBuffer buf,int startOffset,int endOffset){
}",0.5349794238683128
132483,"public void testPrefixLookup() throws Exception {
  final String[] keys={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final String[] vals={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  SortedMap<byte[],byte[]> testMap=new TreeMap<byte[],byte[]>(COMP);
  for (int i=0; i < keys.length; i++)   testMap.put(keys[i].getBytes(),vals[i].getBytes());
  new File(PATH2).delete();
  DiskIndexWriter index=new DiskIndexWriter(PATH2,4);
  index.writeIndex(testMap.entrySet().iterator());
  DiskIndex diskIndex=new DiskIndex(PATH2,new DefaultByteRangeComparator());
  Iterator<Entry<byte[],byte[]>> it=diskIndex.rangeLookup(keys[1].getBytes(),keys[5].getBytes());
  for (int i=1; i < 5; i++) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  for (int i=1; i < 7; i++) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
}","public void testPrefixLookup() throws Exception {
  final String[] keys={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  final String[] vals={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  SortedMap<byte[],byte[]> testMap=new TreeMap<byte[],byte[]>(COMP);
  for (int i=0; i < keys.length; i++)   testMap.put(keys[i].getBytes(),vals[i].getBytes());
  new File(PATH2).delete();
  DiskIndexWriter index=new DiskIndexWriter(PATH2,4);
  index.writeIndex(testMap.entrySet().iterator());
  DiskIndex diskIndex=new DiskIndex(PATH2,new DefaultByteRangeComparator());
  Iterator<Entry<byte[],byte[]>> it=diskIndex.rangeLookup(keys[1].getBytes(),keys[5].getBytes());
  for (int i=1; i < 5; i++) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  for (int i=1; i < 7; i++) {
    Entry<byte[],byte[]> entry=it.next();
    assertEquals(keys[i],new String(entry.getKey()));
    assertEquals(vals[i],new String(entry.getValue()));
  }
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  assertFalse(it.hasNext());
  index=new DiskIndexWriter(PATH1,4);
  index.writeIndex(new HashMap().entrySet().iterator());
  diskIndex=new DiskIndex(PATH1,new DefaultByteRangeComparator());
  it=diskIndex.rangeLookup(new byte[0],new byte[0]);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,null);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),null);
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(null,""String_Node_Str"".getBytes());
  assertFalse(it.hasNext());
  it=diskIndex.rangeLookup(""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes());
  assertFalse(it.hasNext());
}",0.765164162493044
132484,"public Iterator<Entry<ByteRange,ByteRange>> rangeLookup(byte[] from,byte[] to){
  final int startIndex;
  final int endIndex;
{
    startIndex=keys.getTopPosition(from);
    endIndex=keys.getBottomPosition(to);
  }
  return new Iterator<Entry<ByteRange,ByteRange>>(){
    int currentIndex=startIndex;
    @Override public boolean hasNext(){
      return currentIndex <= endIndex;
    }
    @Override public Entry<ByteRange,ByteRange> next(){
      if (!hasNext())       throw new NoSuchElementException();
      Entry<ByteRange,ByteRange> entry=new Entry<ByteRange,ByteRange>(){
        final ByteRange key=keys.getEntry(currentIndex);
        final ByteRange value=values.getEntry(currentIndex);
        @Override public ByteRange getValue(){
          return value;
        }
        @Override public ByteRange getKey(){
          return key;
        }
        @Override public ByteRange setValue(        ByteRange value){
          throw new UnsupportedOperationException();
        }
      }
;
      currentIndex++;
      return entry;
    }
    @Override public void remove(){
      throw new UnsupportedOperationException();
    }
  }
;
}","public Iterator<Entry<ByteRange,ByteRange>> rangeLookup(byte[] from,byte[] to){
}",0.1322448979591836
132485,"public DiskIndex(String path,ByteRangeComparator comp) throws IOException {
  this.comp=comp;
  dbFile=new RandomAccessFile(path,""String_Node_Str"");
  dbFile.seek(dbFile.length() - Integer.SIZE / 8);
  blockIndexOffset=dbFile.readInt();
  blockIndexBuf=ByteBuffer.allocate((int)(dbFile.length() - Integer.SIZE / 8 - blockIndexOffset));
  FileChannel channel=dbFile.getChannel();
  channel.position(blockIndexOffset);
  channel.read(blockIndexBuf);
  blockIndex=new BlockReader(blockIndexBuf,0,blockIndexBuf.limit(),comp);
  mappedFile=channel.map(MapMode.READ_ONLY,0,blockIndexOffset);
  assert(channel.size() <= Integer.MAX_VALUE);
}","public DiskIndex(String path,ByteRangeComparator comp) throws IOException {
  this.comp=comp;
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  dbFile=new RandomAccessFile(path,""String_Node_Str"");
  dbFile.seek(dbFile.length() - Integer.SIZE / 8);
  blockIndexOffset=dbFile.readInt();
  blockIndexBuf=ByteBuffer.allocate((int)(dbFile.length() - Integer.SIZE / 8 - blockIndexOffset));
  FileChannel channel=dbFile.getChannel();
  channel.position(blockIndexOffset);
  channel.read(blockIndexBuf);
  blockIndex=new BlockReader(blockIndexBuf,0,blockIndexBuf.limit(),comp);
  mappedFile=channel.map(MapMode.READ_ONLY,0,blockIndexOffset);
  assert(channel.size() <= Integer.MAX_VALUE);
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + channel.size());
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + blockIndexOffset);
}",0.8458972648432288
132486,"public Iterator<Entry<byte[],byte[]>> rangeLookup(final byte[] from,final byte[] to){
  final BlockReader itBlockIndex=blockIndex.clone();
  mappedFile.position(0);
  final ByteBuffer map=mappedFile.slice();
  final int blockIndexStart=from == null ? 0 : getBlockIndexPosition(from,itBlockIndex);
  final int blockIndexEnd=to == null ? itBlockIndex.getNumEntries() - 1 : getBlockIndexPosition(to,itBlockIndex);
  return new Iterator<Entry<byte[],byte[]>>(){
    private int currentBlockIndex;
    private Iterator<Entry<ByteRange,ByteRange>> currentBlockIterator;
    private BlockReader currentBlock;
{
      currentBlockIndex=blockIndexStart;
      getNextBlockData();
    }
    @Override public boolean hasNext(){
      while (currentBlockIterator != null) {
        if (currentBlockIterator.hasNext())         return true;
        currentBlockIndex++;
        getNextBlockData();
      }
      return false;
    }
    @Override public Entry<byte[],byte[]> next(){
      if (!hasNext())       throw new NoSuchElementException();
      final Entry<ByteRange,ByteRange> entry=currentBlockIterator.next();
      return new Entry<byte[],byte[]>(){
        private byte[] key;
        private byte[] value;
{
          key=entry.getKey().toBuffer();
          value=entry.getValue().toBuffer();
        }
        @Override public byte[] getKey(){
          return key;
        }
        @Override public byte[] getValue(){
          return value;
        }
        @Override public byte[] setValue(        byte[] value){
          throw new UnsupportedOperationException();
        }
      }
;
    }
    @Override public void remove(){
      throw new UnsupportedOperationException();
    }
    private void getNextBlockData(){
      if (blockIndexStart == -1 && blockIndexEnd == -1)       return;
      if (currentBlockIndex > blockIndexEnd) {
        currentBlock=null;
        currentBlockIterator=null;
        return;
      }
      int startOffset=getBlockOffset(currentBlockIndex,itBlockIndex);
      int endOffset=currentBlockIndex == itBlockIndex.getNumEntries() - 1 ? blockIndexOffset : getBlockOffset(currentBlockIndex + 1,itBlockIndex);
      currentBlock=getBlock(startOffset,endOffset,map);
      currentBlockIterator=currentBlock == null ? null : currentBlock.rangeLookup(from == null ? null : from,to == null ? null : to);
    }
  }
;
}","public Iterator<Entry<byte[],byte[]>> rangeLookup(final byte[] from,final byte[] to){
  final BlockReader itBlockIndex=blockIndex.clone();
  mappedFile.position(0);
  final ByteBuffer map=mappedFile.slice();
  int tmp=from == null ? 0 : getBlockIndexPosition(from,itBlockIndex);
  if (tmp < 0)   tmp=0;
  final int blockIndexStart=tmp;
  tmp=to == null ? itBlockIndex.getNumEntries() - 1 : getBlockIndexPosition(to,itBlockIndex);
  if (tmp > itBlockIndex.getNumEntries() - 1)   tmp=itBlockIndex.getNumEntries() - 1;
  final int blockIndexEnd=tmp;
  return new Iterator<Entry<byte[],byte[]>>(){
    private int currentBlockIndex;
    private Iterator<Entry<ByteRange,ByteRange>> currentBlockIterator;
    private BlockReader currentBlock;
{
      currentBlockIndex=blockIndexStart;
      getNextBlockData();
    }
    @Override public boolean hasNext(){
      while (currentBlockIterator != null) {
        if (currentBlockIterator.hasNext())         return true;
        currentBlockIndex++;
        getNextBlockData();
      }
      return false;
    }
    @Override public Entry<byte[],byte[]> next(){
      if (!hasNext())       throw new NoSuchElementException();
      final Entry<ByteRange,ByteRange> entry=currentBlockIterator.next();
      return new Entry<byte[],byte[]>(){
        private byte[] key;
        private byte[] value;
{
          key=entry.getKey().toBuffer();
          value=entry.getValue().toBuffer();
        }
        @Override public byte[] getKey(){
          return key;
        }
        @Override public byte[] getValue(){
          return value;
        }
        @Override public byte[] setValue(        byte[] value){
          throw new UnsupportedOperationException();
        }
      }
;
    }
    @Override public void remove(){
      throw new UnsupportedOperationException();
    }
    private void getNextBlockData(){
      if (blockIndexStart == -1 && blockIndexEnd == -1)       return;
      if (currentBlockIndex > blockIndexEnd) {
        currentBlock=null;
        currentBlockIterator=null;
        return;
      }
      int startOffset=getBlockOffset(currentBlockIndex,itBlockIndex);
      int endOffset=currentBlockIndex == itBlockIndex.getNumEntries() - 1 ? blockIndexOffset : getBlockOffset(currentBlockIndex + 1,itBlockIndex);
      currentBlock=getBlock(startOffset,endOffset,map);
      currentBlockIterator=currentBlock == null ? null : currentBlock.rangeLookup(from == null ? null : from,to == null ? null : to);
    }
  }
;
}",0.9520066197765824
132487,"public MiniPage(int numEntries,ByteBuffer buf,int offset,ByteRangeComparator comp){
  this.numEntries=numEntries;
  this.buf=buf;
  this.offset=offset;
  this.comp=comp;
}","public MiniPage(int numEntries,ByteBuffer buf,int offset,ByteRangeComparator comp){
}",0.6640625
132488,"public static int getExclBottomOffset(MiniPage page,byte[] entry,ByteRangeComparator comp){
  int low=0;
  int high=page.getNumEntries() - 1;
  int mid=high;
  int cmp=0;
  while (low <= high) {
    mid=(low + high) >>> 1;
    ByteRange currKey=page.getEntry(mid);
    cmp=comp.compare(currKey,entry);
    if (cmp < 0)     low=mid + 1;
 else     if (cmp > 0)     high=mid - 1;
 else     return mid - 1;
  }
  return cmp > 0 ? mid - 1 : mid;
}","public static int getExclBottomOffset(MiniPage page,byte[] entry,ByteRangeComparator comp){
}",0.3476635514018691
132489,"public static int getInclBottomOffset(MiniPage page,byte[] entry,ByteRangeComparator comp){
  int low=0;
  int high=page.getNumEntries() - 1;
  int mid=high;
  int cmp=0;
  while (low <= high) {
    mid=(low + high) >>> 1;
    ByteRange currKey=page.getEntry(mid);
    cmp=comp.compare(currKey,entry);
    if (cmp < 0)     low=mid + 1;
 else     if (cmp > 0)     high=mid - 1;
 else     return mid;
  }
  return cmp > 0 ? mid - 1 : mid;
}","public static int getInclBottomOffset(MiniPage page,byte[] entry,ByteRangeComparator comp){
}",0.3502824858757062
132490,"public static int getOffset(MiniPage page,byte[] entry,ByteRangeComparator comp){
  int low=0;
  int high=page.getNumEntries() - 1;
  int mid=high;
  int cmp=0;
  while (low <= high) {
    mid=(low + high) >>> 1;
    ByteRange currKey=page.getEntry(mid);
    cmp=comp.compare(currKey,entry);
    if (cmp < 0)     low=mid + 1;
 else     if (cmp > 0)     high=mid - 1;
 else     return mid;
  }
  return -1;
}","public static int getOffset(MiniPage page,byte[] entry,ByteRangeComparator comp){
}",0.3387755102040816
132491,"public static int getInclTopOffset(MiniPage page,byte[] entry,ByteRangeComparator comp){
  int low=0;
  int high=page.getNumEntries() - 1;
  int mid=high;
  int cmp=0;
  while (low <= high) {
    mid=(low + high) >>> 1;
    ByteRange currKey=page.getEntry(mid);
    cmp=comp.compare(currKey,entry);
    if (cmp < 0)     low=mid + 1;
 else     if (cmp > 0)     high=mid - 1;
 else     return mid;
  }
  return cmp > 0 ? mid : mid + 1;
}","public static int getInclTopOffset(MiniPage page,byte[] entry,ByteRangeComparator comp){
}",0.3428571428571428
132492,"/** 
 * Links the indices to the latest on-disk snapshot, cleans up any  unnecessary in-memory and on-disk data
 * @param viewId the viewId of the snapshot
 * @param sequenceNo the sequenceNo of the snaphot
 * @throws java.io.IOException if snapshots cannot be cleaned up
 */
public void cleanupSnapshot(final int viewId,final long sequenceNo) throws IOException {
  for (int index=0; index < trees.size(); index++) {
    final LSMTree tree=trees.get(index);
    if (Logging.isDebug())     Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + databaseDir + getSnaphotFilename(index,viewId,sequenceNo));
    tree.linkToSnapshot(databaseDir + getSnaphotFilename(index,viewId,sequenceNo));
    ondiskLSN=new LSN(viewId,sequenceNo);
    File f=new File(databaseDir);
    String[] files=f.list();
    Pattern p=Pattern.compile(SNAPSHOT_FILENAME_REGEXP);
    for (    String fname : files) {
      Matcher m=p.matcher(fname);
      if (m.matches()) {
        int fView=Integer.valueOf(m.group(2));
        int fSeq=Integer.valueOf(m.group(3));
        if ((fView < viewId) || ((fView == viewId) && (fSeq < sequenceNo))) {
          File snap=new File(databaseDir + fname);
          snap.delete();
        }
      }
    }
  }
}","/** 
 * Links the indices to the latest on-disk snapshot, cleans up any  unnecessary in-memory and on-disk data
 * @param viewId the viewId of the snapshot
 * @param sequenceNo the sequenceNo of the snaphot
 * @throws java.io.IOException if snapshots cannot be cleaned up
 */
public void cleanupSnapshot(final int viewId,final long sequenceNo) throws IOException {
  for (int index=0; index < trees.size(); index++) {
    final LSMTree tree=trees.get(index);
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + databaseDir + getSnaphotFilename(index,viewId,sequenceNo)+ ""String_Node_Str""+ databaseName+ ""String_Node_Str""+ index);
    tree.linkToSnapshot(databaseDir + getSnaphotFilename(index,viewId,sequenceNo));
    Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
    ondiskLSN=new LSN(viewId,sequenceNo);
    File f=new File(databaseDir);
    String[] files=f.list();
    Pattern p=Pattern.compile(SNAPSHOT_FILENAME_REGEXP);
    for (    String fname : files) {
      Matcher m=p.matcher(fname);
      if (m.matches()) {
        int fView=Integer.valueOf(m.group(2));
        int fSeq=Integer.valueOf(m.group(3));
        if ((fView < viewId) || ((fView == viewId) && (fSeq < sequenceNo))) {
          File snap=new File(databaseDir + fname);
          snap.delete();
        }
      }
    }
  }
}",0.936768149882904
132493,"public void setUp() throws Exception {
}","public void setUp() throws Exception {
  Logging.start(Logging.LEVEL_ERROR);
}",0.6779661016949152
132494,"private void getNextBlockData(){
  if (currentBlockIndex > blockIndexEnd) {
    currentBlock=null;
    currentBlockIterator=null;
    return;
  }
  int startOffset=getBlockOffset(currentBlockIndex,itBlockIndex);
  int endOffset=currentBlockIndex == itBlockIndex.getNumEntries() - 1 ? blockIndexOffset : getBlockOffset(currentBlockIndex + 1,itBlockIndex);
  currentBlock=getBlock(startOffset,endOffset,map);
  currentBlockIterator=currentBlock == null ? null : currentBlock.rangeLookup(from == null ? null : from,to == null ? null : to);
}","private void getNextBlockData(){
  if (blockIndexStart == -1 && blockIndexEnd == -1)   return;
  if (currentBlockIndex > blockIndexEnd) {
    currentBlock=null;
    currentBlockIterator=null;
    return;
  }
  int startOffset=getBlockOffset(currentBlockIndex,itBlockIndex);
  int endOffset=currentBlockIndex == itBlockIndex.getNumEntries() - 1 ? blockIndexOffset : getBlockOffset(currentBlockIndex + 1,itBlockIndex);
  currentBlock=getBlock(startOffset,endOffset,map);
  currentBlockIterator=currentBlock == null ? null : currentBlock.rangeLookup(from == null ? null : from,to == null ? null : to);
}",0.945518453427065
132495,"public Iterator<Entry<byte[],byte[]>> rangeLookup(final byte[] from,final byte[] to){
  final BlockReader itBlockIndex=blockIndex.clone();
  mappedFile.position(0);
  final ByteBuffer map=mappedFile.slice();
  final int blockIndexStart=from == null ? 0 : getBlockIndexPosition(from,itBlockIndex);
  final int blockIndexEnd=to == null ? itBlockIndex.getNumEntries() - 1 : getBlockIndexPosition(to,itBlockIndex);
  return new Iterator<Entry<byte[],byte[]>>(){
    private int currentBlockIndex;
    private Iterator<Entry<ByteRange,ByteRange>> currentBlockIterator;
    private BlockReader currentBlock;
{
      currentBlockIndex=blockIndexStart;
      getNextBlockData();
    }
    @Override public boolean hasNext(){
      while (currentBlockIterator != null) {
        if (currentBlockIterator.hasNext())         return true;
        currentBlockIndex++;
        getNextBlockData();
      }
      return false;
    }
    @Override public Entry<byte[],byte[]> next(){
      if (!hasNext())       throw new NoSuchElementException();
      final Entry<ByteRange,ByteRange> entry=currentBlockIterator.next();
      return new Entry<byte[],byte[]>(){
        private byte[] key;
        private byte[] value;
{
          key=entry.getKey().toBuffer();
          value=entry.getValue().toBuffer();
        }
        @Override public byte[] getKey(){
          return key;
        }
        @Override public byte[] getValue(){
          return value;
        }
        @Override public byte[] setValue(        byte[] value){
          throw new UnsupportedOperationException();
        }
      }
;
    }
    @Override public void remove(){
      throw new UnsupportedOperationException();
    }
    private void getNextBlockData(){
      if (currentBlockIndex > blockIndexEnd) {
        currentBlock=null;
        currentBlockIterator=null;
        return;
      }
      int startOffset=getBlockOffset(currentBlockIndex,itBlockIndex);
      int endOffset=currentBlockIndex == itBlockIndex.getNumEntries() - 1 ? blockIndexOffset : getBlockOffset(currentBlockIndex + 1,itBlockIndex);
      currentBlock=getBlock(startOffset,endOffset,map);
      currentBlockIterator=currentBlock == null ? null : currentBlock.rangeLookup(from == null ? null : from,to == null ? null : to);
    }
  }
;
}","public Iterator<Entry<byte[],byte[]>> rangeLookup(final byte[] from,final byte[] to){
  final BlockReader itBlockIndex=blockIndex.clone();
  mappedFile.position(0);
  final ByteBuffer map=mappedFile.slice();
  final int blockIndexStart=from == null ? 0 : getBlockIndexPosition(from,itBlockIndex);
  final int blockIndexEnd=to == null ? itBlockIndex.getNumEntries() - 1 : getBlockIndexPosition(to,itBlockIndex);
  return new Iterator<Entry<byte[],byte[]>>(){
    private int currentBlockIndex;
    private Iterator<Entry<ByteRange,ByteRange>> currentBlockIterator;
    private BlockReader currentBlock;
{
      currentBlockIndex=blockIndexStart;
      getNextBlockData();
    }
    @Override public boolean hasNext(){
      while (currentBlockIterator != null) {
        if (currentBlockIterator.hasNext())         return true;
        currentBlockIndex++;
        getNextBlockData();
      }
      return false;
    }
    @Override public Entry<byte[],byte[]> next(){
      if (!hasNext())       throw new NoSuchElementException();
      final Entry<ByteRange,ByteRange> entry=currentBlockIterator.next();
      return new Entry<byte[],byte[]>(){
        private byte[] key;
        private byte[] value;
{
          key=entry.getKey().toBuffer();
          value=entry.getValue().toBuffer();
        }
        @Override public byte[] getKey(){
          return key;
        }
        @Override public byte[] getValue(){
          return value;
        }
        @Override public byte[] setValue(        byte[] value){
          throw new UnsupportedOperationException();
        }
      }
;
    }
    @Override public void remove(){
      throw new UnsupportedOperationException();
    }
    private void getNextBlockData(){
      if (blockIndexStart == -1 && blockIndexEnd == -1)       return;
      if (currentBlockIndex > blockIndexEnd) {
        currentBlock=null;
        currentBlockIterator=null;
        return;
      }
      int startOffset=getBlockOffset(currentBlockIndex,itBlockIndex);
      int endOffset=currentBlockIndex == itBlockIndex.getNumEntries() - 1 ? blockIndexOffset : getBlockOffset(currentBlockIndex + 1,itBlockIndex);
      currentBlock=getBlock(startOffset,endOffset,map);
      currentBlockIterator=currentBlock == null ? null : currentBlock.rangeLookup(from == null ? null : from,to == null ? null : to);
    }
  }
;
}",0.9848746758859118
132496,"/** 
 * Performs a prefix lookup in a given snapshot. Key-value paris are returned in an iterator in ascending key order, where only such keys are returned with a matching prefix according to the comparator.
 * @param prefix the prefix
 * @return an iterator with key-value pairs
 */
public Iterator<Entry<byte[],byte[]>> prefixLookup(byte[] prefix,int snapId){
  if (prefix != null && prefix.length == 0)   prefix=null;
  List<Iterator<Entry<byte[],byte[]>>> list=new ArrayList<Iterator<Entry<byte[],byte[]>>>(2);
  list.add(overlay.prefixLookup(prefix,snapId,true));
  if (index != null) {
    byte[][] rng=comp.prefixToRange(prefix);
    list.add(index.rangeLookup(rng[0],rng[1]));
  }
  return new OverlayMergeIterator<byte[],byte[]>(list,comp,null);
}","/** 
 * Performs a prefix lookup in a given snapshot. Key-value paris are returned in an iterator in ascending key order, where only such keys are returned with a matching prefix according to the comparator.
 * @param prefix the prefix
 * @return an iterator with key-value pairs
 */
public Iterator<Entry<byte[],byte[]>> prefixLookup(byte[] prefix,int snapId){
  if (prefix != null && prefix.length == 0)   prefix=null;
  List<Iterator<Entry<byte[],byte[]>>> list=new ArrayList<Iterator<Entry<byte[],byte[]>>>(2);
  list.add(overlay.prefixLookup(prefix,snapId,true));
  if (index != null) {
    byte[][] rng=comp.prefixToRange(prefix);
    list.add(index.rangeLookup(rng[0],rng[1]));
  }
  return new OverlayMergeIterator<byte[],byte[]>(list,comp,NULL_ELEMENT);
}",0.9894736842105264
132497,"public void testSnapshots() throws Exception {
  byte[][] keys=new byte[][]{""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes()};
  byte[][] vals=new byte[][]{""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes()};
  LSMTree tree=new LSMTree(null,new DefaultByteRangeComparator());
  SortedMap<byte[],byte[]> map=new TreeMap<byte[],byte[]>(new DefaultByteRangeComparator());
  for (int i=0; i < keys.length; i++) {
    tree.insert(keys[i],vals[i]);
    map.put(keys[i],vals[i]);
  }
  for (  byte[] key : map.keySet())   assertEquals(new String(map.get(key)),new String(tree.lookup(key)));
  int snapId=tree.createSnapshot();
  tree.materializeSnapshot(SNAP_FILE,snapId);
  tree.linkToSnapshot(SNAP_FILE);
  for (  byte[] key : map.keySet())   assertEquals(new String(map.get(key)),new String(tree.lookup(key)));
  byte[][] newKeys=new byte[][]{""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes()};
  byte[][] newVals=new byte[][]{""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes()};
  for (int i=0; i < newKeys.length; i++) {
    tree.insert(newKeys[i],newVals[i]);
    map.put(newKeys[i],newVals[i]);
  }
  for (  byte[] key : map.keySet())   assertEquals(new String(map.get(key)),new String(tree.lookup(key)));
  snapId=tree.createSnapshot();
  tree.materializeSnapshot(SNAP_FILE2,snapId);
  tree.linkToSnapshot(SNAP_FILE2);
  for (  byte[] key : map.keySet())   assertEquals(new String(map.get(key)),new String(tree.lookup(key)));
}","public void testSnapshots() throws Exception {
  byte[][] keys=new byte[][]{""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes()};
  byte[][] vals=new byte[][]{""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes()};
  LSMTree tree=new LSMTree(null,new DefaultByteRangeComparator());
  SortedMap<byte[],byte[]> map=new TreeMap<byte[],byte[]>(new DefaultByteRangeComparator());
  for (int i=0; i < keys.length; i++) {
    tree.insert(keys[i],vals[i]);
    map.put(keys[i],vals[i]);
  }
  for (  byte[] key : map.keySet())   assertEquals(new String(map.get(key)),new String(tree.lookup(key)));
  int snapId=tree.createSnapshot();
  tree.materializeSnapshot(SNAP_FILE,snapId);
  tree.linkToSnapshot(SNAP_FILE);
  for (  byte[] key : map.keySet())   assertEquals(new String(map.get(key)),new String(tree.lookup(key)));
  byte[][] newKeys=new byte[][]{""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes()};
  byte[][] newVals=new byte[][]{""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),""String_Node_Str"".getBytes(),null};
  for (int i=0; i < newKeys.length; i++) {
    tree.insert(newKeys[i],newVals[i]);
    if (newVals[i] != null)     map.put(newKeys[i],newVals[i]);
 else     map.remove(newKeys[i]);
  }
  for (  byte[] key : map.keySet())   assertEquals(new String(map.get(key)),new String(tree.lookup(key)));
  Iterator<Entry<byte[],byte[]>> it=tree.prefixLookup(new byte[0]);
  Iterator<Entry<byte[],byte[]>> entries=map.entrySet().iterator();
  int count=0;
  for (; it.hasNext(); count++) {
    Entry<byte[],byte[]> entry=it.next();
    Entry<byte[],byte[]> mapEntry=entries.next();
    assertEquals(new String(entry.getKey()),new String(mapEntry.getKey()));
    assertEquals(new String(entry.getValue()),new String(mapEntry.getValue()));
  }
  assertEquals(map.size(),count);
  snapId=tree.createSnapshot();
  tree.materializeSnapshot(SNAP_FILE2,snapId);
  tree.linkToSnapshot(SNAP_FILE2);
  for (  byte[] key : map.keySet())   assertEquals(new String(map.get(key)),new String(tree.lookup(key)));
  for (  byte[] key : keys) {
    map.remove(key);
    tree.delete(key);
  }
  for (  byte[] key : newKeys) {
    map.remove(key);
    tree.delete(key);
  }
  assertEquals(0,map.size());
  assertFalse(tree.prefixLookup(new byte[0]).hasNext());
  snapId=tree.createSnapshot();
  tree.materializeSnapshot(SNAP_FILE3,snapId);
  tree.linkToSnapshot(SNAP_FILE3);
  it=tree.prefixLookup(new byte[0]);
  assertFalse(it.hasNext());
}",0.784799131378936
132498,"public void setUp(){
  Logging.start(Logging.LEVEL_ERROR);
  new File(SNAP_FILE).delete();
  new File(SNAP_FILE2).delete();
}","public void setUp(){
  Logging.start(Logging.LEVEL_ERROR);
  new File(SNAP_FILE).delete();
  new File(SNAP_FILE2).delete();
  new File(SNAP_FILE3).delete();
}",0.8833922261484098
132499,"public void tearDown() throws Exception {
  new File(SNAP_FILE).delete();
  new File(SNAP_FILE2).delete();
}","public void tearDown() throws Exception {
  new File(SNAP_FILE).delete();
  new File(SNAP_FILE2).delete();
  new File(SNAP_FILE3).delete();
}",0.8674698795180723
132500,"public ByteRange getEntry(int n){
  return new ByteRange(buf,offset + n * entrySize,offset + (n + 1) * entrySize);
}","public ByteRange getEntry(int n){
}",0.4635761589403973
132501,"/** 
 * Starts the BabuDB database (with replication). Hidden main constructor.
 * @param baseDir directory in which the database snapshots are stored
 * @param dbLogDir directory in which the database append logs are stored (can be same as baseDir)
 * @param numThreads number of worker threads to use
 * @param maxLogfileSize a checkpoint is generated if  maxLogfileSize is exceeded
 * @param checkInterval interval between two checks in seconds, 0 disables auto checkpointing
 * @param syncMode the synchronization mode to use for the logfile
 * @param pseudoSyncWait if value > 0 then requests are immediateley aknowledged and synced to disk everypseudoSyncWait ms.
 * @param maxQ if > 0, the queue for each worker is limited to maxQ
 * @param master host, from which replicas are received
 * @param slaves hosts, where the replicas should be send to
 * @param isMaster <p>if true, the BabuDB will be started as Master, if false it will be started as slaves. But if the <code>slaves</code> or <code>master</code> are set to null it will be started without replication.</p>
 * @param port where the application listens at. (use 0 for default configuration)
 * @param ssl if set SSL will be used while replication.
 * @param repMode repMode == 0: asynchronous replication mode</br>repMode == slaves.size(): synchronous replication mode</br> repMode > 0 && repMode < slaves.size(): N -sync replication mode with N = repMode
 * @param qLimit if > 0, the queue for the replication-requests is limited to qLimit
 * @throws BabuDBException
 */
BabuDBImpl(String baseDir,String dbLogDir,int numThreads,long maxLogfileSize,int checkInterval,SyncMode syncMode,int pseudoSyncWait,int maxQ,InetSocketAddress master,List<InetSocketAddress> slaves,int port,SSLOptions ssl,boolean isMaster,int repMode,int qLimit) throws BabuDBException {
  overlaySwitchLock=new ReentrantReadWriteLock();
  if (!baseDir.endsWith(File.separator)) {
    baseDir=baseDir + File.separatorChar;
  }
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + baseDir);
  if (!dbLogDir.endsWith(File.separator)) {
    dbLogDir=dbLogDir + File.separatorChar;
  }
  Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + dbLogDir);
  this.configuration=new BabuDBConfiguration(baseDir,dbLogDir,maxQ,numThreads,syncMode,maxLogfileSize,checkInterval,pseudoSyncWait);
  if (master != null && slaves != null) {
    this.configuration.replication_master=master;
    this.configuration.replication_slaves=slaves;
    try {
      if (isMaster) {
        replicationFacade=new Replication(slaves,ssl,port,this,repMode,qLimit,overlaySwitchLock.readLock(),Replication.DEFAULT_NO_TRIES);
      }
 else {
        replicationFacade=new Replication(master,ssl,port,this,repMode,qLimit,overlaySwitchLock.readLock(),Replication.DEFAULT_NO_TRIES);
      }
    }
 catch (    Exception e) {
      throw new BabuDBException(ErrorCode.REPLICATION_FAILURE,""String_Node_Str"" + e.getMessage(),e.getCause());
    }
  }
 else {
    replicationFacade=null;
  }
  dbNames=new HashMap<String,LSMDatabase>();
  databases=new HashMap<Integer,LSMDatabase>();
  nextDbId=1;
  compInstances=new HashMap<String,ByteRangeComparator>();
  compInstances.put(DefaultByteRangeComparator.class.getName(),new DefaultByteRangeComparator());
  loadDBs();
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  LSN nextLSN=replayLogs();
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"");
  try {
    logger=new DiskLogger(dbLogDir,nextLSN.getViewId(),nextLSN.getSequenceNo(),syncMode,pseudoSyncWait,maxQ * numThreads);
    logger.start();
  }
 catch (  IOException ex) {
    throw new BabuDBException(ErrorCode.IO_ERROR,""String_Node_Str"",ex);
  }
  dbModificationLock=new Object();
  checkpointLock=new Object();
  worker=new LSMDBWorker[numThreads];
  for (int i=0; i < numThreads; i++) {
    worker[i]=new LSMDBWorker(logger,i,overlaySwitchLock,(pseudoSyncWait > 0),maxQ,replicationFacade);
    worker[i].start();
  }
  if (checkInterval > 0) {
    dbCheckptr=new Checkpointer(this,logger,checkInterval,maxLogfileSize);
    dbCheckptr.start();
  }
 else {
    dbCheckptr=null;
  }
  Logging.logMessage(Logging.LEVEL_INFO,this,""String_Node_Str"" + BABUDB_VERSION + ""String_Node_Str"");
}","/** 
 * DUMMY - constructor for testing only!
 */
public BabuDBImpl(){
  replicationFacade=null;
  dbNames=null;
  dbModificationLock=null;
  databases=null;
  configuration=null;
  compInstances=null;
  checkpointLock=null;
}",0.0125195618153364
132502,"public void run(){
  quit=false;
  down.set(false);
  while (!quit) {
    try {
      final LSMDBRequest r=requests.take();
switch (r.getOperation()) {
case INSERT:
        doInsert(r);
      break;
case LOOKUP:
    doLookup(r);
  break;
case PREFIX_LOOKUP:
doPrefixLookup(r);
break;
case USER_DEFINED_LOOKUP:
doUserLookup(r);
break;
default :
{
Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"");
System.exit(1);
}
}
}
 catch (InterruptedException ex) {
}
}
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
synchronized (down) {
down.set(true);
down.notifyAll();
}
}","@Override public void run(){
  while (!quit) {
    try {
      final LSMDBRequest r=requests.take();
switch (r.getOperation()) {
case INSERT:
        doInsert(r);
      break;
case LOOKUP:
    doLookup(r);
  break;
case PREFIX_LOOKUP:
doPrefixLookup(r);
break;
case USER_DEFINED_LOOKUP:
doUserLookup(r);
break;
default :
{
Logging.logMessage(Logging.LEVEL_ERROR,this,""String_Node_Str"");
System.exit(1);
}
}
}
 catch (InterruptedException ex) {
}
}
Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"");
synchronized (down) {
down.set(true);
down.notifyAll();
}
}",0.9630901287553648
132503,"public void waitForShutdown() throws InterruptedException {
synchronized (down) {
    if (!down.get())     down.wait();
  }
}","public void waitForShutdown() throws InterruptedException {
synchronized (down) {
    if (!down.get()) {
      down.wait();
    }
  }
}",0.9615384615384616
132504,"public LSMDBWorker(DiskLogger logger,int id,ReadWriteLock insertLock,boolean pseudoSync,int maxQ,Replication replication){
  super(""String_Node_Str"" + id);
  this.replicationFacade=replication;
  if (maxQ > 0)   requests=new LinkedBlockingQueue<LSMDBRequest>(maxQ);
 else   requests=new LinkedBlockingQueue<LSMDBRequest>();
  down=new AtomicBoolean(false);
  this.lookupIfs=new HashMap<LSMDatabase,LSMLookupInterface>();
  this.logger=logger;
  this.insertLock=insertLock;
  this.pseudoSync=pseudoSync;
}","public LSMDBWorker(DiskLogger logger,int id,ReadWriteLock insertLock,boolean pseudoSync,int maxQ,Replication replication){
  super(""String_Node_Str"" + id);
  down=new AtomicBoolean(false);
  this.replicationFacade=replication;
  if (maxQ > 0)   requests=new LinkedBlockingQueue<LSMDBRequest>(maxQ);
 else   requests=new LinkedBlockingQueue<LSMDBRequest>();
  this.lookupIfs=new HashMap<LSMDatabase,LSMLookupInterface>();
  this.logger=logger;
  this.insertLock=insertLock;
  this.pseudoSync=pseudoSync;
}",0.9345238095238096
132505,"@Override public int compare(Status<T> o1,Status<T> o2){
  if (o1.getStatus().equals(o2.getStatus())) {
    if (o1.getValue() instanceof Request) {
      Request rq1=(Request)o1.getValue();
      Request rq2=(Request)o2.getValue();
      if (rq2 == null)       return +1;
      Token rq1t=rq1.getToken();
      Token rq2t=rq2.getToken();
      if (rq1t.compareTo(rq2t) == 0) {
switch (rq1t) {
case ACK:
          return rq2.getLSN().compareTo(rq1.getLSN());
case RQ:
        return rq1.getLSN().compareTo(rq2.getLSN());
case REPLICA_BROADCAST:
      return rq1.getLSN().compareTo(rq2.getLSN());
case CHUNK:
    return rq1.getChunkDetails().compareTo(rq2.getChunkDetails());
case REPLICA:
  return rq1.getLSN().compareTo(rq2.getLSN());
case CHUNK_RP:
return rq1.getChunkDetails().compareTo(rq2.getChunkDetails());
default :
return 0;
}
}
return rq1t.compareTo(rq2t);
}
 else if (o1.getValue() instanceof LSN) {
LSN lsn1=(LSN)o1.getValue();
LSN lsn2=(LSN)o2.getValue();
return lsn1.compareTo(lsn2);
}
 else if (o1.getValue() instanceof Chunk) {
Chunk chunk1=(Chunk)o1.getValue();
Chunk chunk2=(Chunk)o2.getValue();
return chunk1.compareTo(chunk2);
}
 else return +1;
}
 else return o1.getStatus().compareTo(o2.getStatus());
}","@Override public int compare(Status<T> o1,Status<T> o2){
  if ((o1.isPending() && o2.isPending()) || (!o1.isPending() && !o2.isPending())) {
    if (o1.getValue() instanceof Request) {
      Request rq1=(Request)o1.getValue();
      Request rq2=(Request)o2.getValue();
      if (rq2 == null)       return +1;
      Token rq1t=rq1.getToken();
      Token rq2t=rq2.getToken();
      if (rq1t.compareTo(rq2t) == 0) {
switch (rq1t) {
case ACK:
          return rq2.getLSN().compareTo(rq1.getLSN());
case RQ:
        return rq1.getLSN().compareTo(rq2.getLSN());
case REPLICA_BROADCAST:
      return rq1.getLSN().compareTo(rq2.getLSN());
case CHUNK:
    return rq1.getChunkDetails().compareTo(rq2.getChunkDetails());
case REPLICA:
  return rq1.getLSN().compareTo(rq2.getLSN());
case CHUNK_RP:
return rq1.getChunkDetails().compareTo(rq2.getChunkDetails());
default :
return 0;
}
}
return rq1t.compareTo(rq2t);
}
 else if (o1.getValue() instanceof LSN) {
LSN lsn1=(LSN)o1.getValue();
LSN lsn2=(LSN)o2.getValue();
return lsn1.compareTo(lsn2);
}
 else if (o1.getValue() instanceof Chunk) {
Chunk chunk1=(Chunk)o1.getValue();
Chunk chunk2=(Chunk)o2.getValue();
return chunk1.compareTo(chunk2);
}
 else return +1;
}
 else if (o1.isPending()) return +1;
 else return -1;
}",0.920225624496374
132506,"/** 
 * Insert a full record group. Only to be used by log replay.
 * @param ins
 */
private void insert(InsertRecordGroup ins){
  final LSMDatabase db=databases.get(ins.getDatabaseId());
  if (db == null) {
    return;
  }
  for (  InsertRecord ir : ins.getInserts()) {
    LSMTree tree=db.getIndex(ir.getIndexId());
    Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + new String(ir.getKey()) + ""String_Node_Str""+ new String(ir.getValue())+ ""String_Node_Str""+ db.getDatabaseName()+ ""String_Node_Str""+ ir.getIndexId());
    tree.insert(ir.getKey(),ir.getValue());
  }
}","/** 
 * Insert a full record group. Only to be used by log replay.
 * @param ins
 */
private void insert(InsertRecordGroup ins){
  final LSMDatabase db=databases.get(ins.getDatabaseId());
  if (db == null) {
    return;
  }
  for (  InsertRecord ir : ins.getInserts()) {
    LSMTree tree=db.getIndex(ir.getIndexId());
    Logging.logMessage(Logging.LEVEL_DEBUG,this,""String_Node_Str"" + new String(ir.getKey()) + ""String_Node_Str""+ (ir.getValue() == null ? null : new String(ir.getValue()))+ ""String_Node_Str""+ db.getDatabaseName()+ ""String_Node_Str""+ ir.getIndexId());
    tree.insert(ir.getKey(),ir.getValue());
  }
}",0.972568578553616
132507,"private void notifyChanged(T winnerCanon,T loserCanon){
  for (  ScopedUnionFind<T> subscriber : subscribed.get(loserCanon)) {
    subscriber.merge(winnerCanon,loserCanon);
  }
}","private void notifyChanged(boolean before,T winnerCanon,T loserCanon){
  Pair<T,Boolean> key=Pair.create(loserCanon,before);
  for (  UnionFindSubscriber<T> subscriber : subscribed.get(key)) {
    subscriber.notifyMerge(winnerCanon,loserCanon);
  }
}",0.6822429906542056
132508,"/** 
 * Merge loser into winner. Note: if they are already in same set, no changes, even if loser is canonical
 * @param winner
 * @param loser
 * @return unmodifiable collection of values that changed their canonical member
 */
public Set<T> merge(T winner,T loser){
  T winnerCanon=lookup(winner);
  T loserCanon=lookup(loser);
  Set<T> affectedMembers=members(loserCanon);
  if (affectedMembers.contains(winnerCanon)) {
    return Collections.emptySet();
  }
  notifyChanged(winnerCanon,loserCanon);
  for (  T affectedMember : affectedMembers) {
    canonical.put(affectedMember,winnerCanon);
  }
  subscribeToUpdates(winnerCanon);
  return Collections.unmodifiableSet(affectedMembers);
}","/** 
 * Merge loser into winner. Note: if they are already in same set, no changes, even if loser is canonical
 * @param winner
 * @param loser
 * @return unmodifiable collection of values that changed their canonical member
 */
public Set<T> merge(T winner,T loser){
  T winnerCanon=lookup(winner);
  T loserCanon=lookup(loser);
  Set<T> affectedMembers=members(loserCanon);
  if (affectedMembers.contains(winnerCanon)) {
    return Collections.emptySet();
  }
  notifyChanged(true,winnerCanon,loserCanon);
  for (  T affectedMember : affectedMembers) {
    canonical.put(affectedMember,winnerCanon);
  }
  subscribeToParentUpdates(winnerCanon);
  subscribeToParentUpdates(loserCanon);
  notifyChanged(false,winnerCanon,loserCanon);
  return Collections.unmodifiableSet(affectedMembers);
}",0.9190283400809716
132509,"/** 
 * Test merging
 */
@Test @Ignore public void testPropagateChild3(){
  ScopedUnionFind<Integer> uf=buildBasic();
  ScopedUnionFind<Integer> child=buildChild(uf);
  Set<Integer> affected;
  affected=child.merge(100,200);
  assertEquals(new HashSet<Integer>(Arrays.asList(200)),affected);
  affected=uf.merge(50,200);
  assertEquals(new HashSet<Integer>(Arrays.asList(200)),affected);
  assertEquals(50,(int)uf.lookup(50));
  assertEquals(100,(int)uf.lookup(100));
  assertEquals(50,(int)uf.lookup(200));
  assertEquals(50,(int)child.lookup(50));
  assertEquals(50,(int)child.lookup(100));
  assertEquals(50,(int)child.lookup(200));
}","/** 
 * Test merging
 */
@Test public void testPropagateChild3(){
  ScopedUnionFind<Integer> uf=buildBasic();
  ScopedUnionFind<Integer> child=buildChild(uf);
  Set<Integer> affected;
  affected=child.merge(100,200);
  assertEquals(new HashSet<Integer>(Arrays.asList(200)),affected);
  affected=uf.merge(50,200);
  assertEquals(new HashSet<Integer>(Arrays.asList(200)),affected);
  assertEquals(50,(int)uf.lookup(50));
  assertEquals(100,(int)uf.lookup(100));
  assertEquals(50,(int)uf.lookup(200));
  assertEquals(50,(int)child.lookup(50));
  assertEquals(50,(int)child.lookup(100));
  assertEquals(50,(int)child.lookup(200));
}",0.9936808846761452
132510,"@Override public void declare(List<VarDecl> decls){
}","@Override public void declare(List<VarDecl> decls){
  List<TclList> batchedArgs=new ArrayList<TclList>();
  List<String> batchedVarNames=new ArrayList<String>();
  List<TclList> batchedFileArgs=new ArrayList<TclList>();
  List<String> batchedFileVarNames=new ArrayList<String>();
  List<Boolean> batchedFileIsMappeds=new ArrayList<Boolean>();
  for (  VarDecl decl : decls) {
    Var var=decl.var;
    Arg initReaders=decl.initReaders;
    Arg initWriters=decl.initWriters;
    assert(!var.mappedDecl() || Types.isMappable(var.type()));
    if (var.storage() == Alloc.ALIAS) {
      assert(initReaders == null && initWriters == null);
      continue;
    }
    pointAdd(new Comment(""String_Node_Str"" + var.type().typeName() + ""String_Node_Str""+ prefixVar(var.name())+ ""String_Node_Str""+ var.provenance().logFormat()));
    if (var.storage().isGlobal()) {
      pointAdd(Turbine.makeTCLGlobal(prefixVar(var)));
      continue;
    }
    if (!Settings.getBooleanUnchecked(Settings.ENABLE_REFCOUNTING)) {
      initReaders=Arg.ONE;
    }
    if (Types.isFile(var)) {
      batchedFileArgs.add(createArgs(var,initReaders,initWriters));
      batchedFileVarNames.add(prefixVar(var));
      batchedFileIsMappeds.add(var.mappedDecl());
    }
 else     if (Types.isPrimFuture(var) || Types.isPrimUpdateable(var) || Types.isArray(var)|| Types.isRef(var)|| Types.isBag(var)|| Types.isStruct(var)) {
      batchedArgs.add(createArgs(var,initReaders,initWriters));
      batchedVarNames.add(prefixVar(var));
    }
 else     if (Types.isPrimValue(var) || Types.isContainerLocal(var) || Types.isStructLocal(var)) {
      assert(var.storage() == Alloc.LOCAL);
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + var.type().typeName());
    }
  }
  if (!batchedArgs.isEmpty()) {
    pointAdd(Turbine.batchDeclare(batchedVarNames,batchedArgs));
    pointAdd(logVariableCreation(batchedVarNames));
  }
  if (!batchedFileArgs.isEmpty()) {
    pointAdd(Turbine.batchDeclareFiles(batchedFileVarNames,batchedFileArgs,batchedFileIsMappeds));
  }
}",0.0502131691141639
132511,"/** 
 * A loop over an integer range.  The range can be totally fixed ( the bounds might be literal integers) or can vary at runtime ( ( in which case it is specified by integer value variables ) The loop construct should run immediately, but have the loop iterations run in parallel
 * @param loopName unique name for loop
 * @param loopVar variable (integer value) used to store iteration parameter
 * @param countVar variable (integer value) used to store iteration number startingfrom 0 (optional)
 * @param start start (inclusive) of the loop: should be int or int value var
 * @param end end (inclusive) of the loop: should be int or int value var
 * @param increment increment of the loop: should be int or int value var
 * @param passedVars variables used in loop body
 * @param splitDegree the desired loop split factor (negative if no splitting)
 * @param perIterIncrs per-iteration increments
 * @param constIncrs constant increments
 */
public void startRangeLoop(String loopName,Var loopVar,Var countVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs);","/** 
 * A loop over an integer range.  The range can be totally fixed ( the bounds might be literal integers) or can vary at runtime ( ( in which case it is specified by integer value variables ) The loop construct should run immediately, but have the loop iterations run in parallel
 * @param loopName unique name for loop
 * @param loopVar variable (integer value) used to store iteration parameter
 * @param countVar variable (integer value) used to store iteration number startingfrom 0 (optional)
 * @param start start (inclusive) of the loop: should be int or int value var
 * @param end end (inclusive) of the loop: should be int or int value var
 * @param increment increment of the loop: should be int or int value var
 * @param passedVars variables used in loop body
 * @param splitDegree the desired loop split factor (negative if no splitting)
 * @param perIterIncrs per-iteration increments
 * @param constIncrs constant increments
 */
public void startRangeLoop(String loopName,Var loopVar,Var countVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs,List<RefCount> perIterDecrs);",0.9882154882154882
132512,"/** 
 * Start a parallel foreach loop over an array.
 * @param loopName unique name for loop
 * @param container
 * @param memberVar
 * @param loopCountVar counter variable, can be null
 * @param splitDegree
 * @param leafDegree
 * @param arrayClosed if true, assume array is already closed
 * @param passedVars
 * @param perIterIncrs per-iteration increments
 * @param constIncrs constant increments
 */
public void startForeachLoop(String loopName,Var container,Var memberVar,Var loopCountVar,int splitDegree,int leafDegree,boolean arrayClosed,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs);","/** 
 * Start a parallel foreach loop over an array.
 * @param loopName unique name for loop
 * @param container
 * @param memberVar
 * @param loopCountVar counter variable, can be null
 * @param splitDegree
 * @param leafDegree
 * @param arrayClosed if true, assume array is already closed
 * @param passedVars
 * @param perIterIncrs per-iteration increments
 * @param constIncrs constant increments
 */
public void startForeachLoop(String loopName,Var container,Var memberVar,Var loopCountVar,int splitDegree,int leafDegree,boolean arrayClosed,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs,List<RefCount> perIterDecrs);",0.9785932721712538
132513,"@Override public void generate(Logger logger,CompilerBackend gen,GenInfo info){
  gen.startRangeLoop(loopName,loopVar,loopCounterVar,start,end,increment,splitDegree,leafDegree,passedVars,startIncrements,constStartIncrements);
  this.loopBody.generate(logger,gen,info);
  gen.endRangeLoop(splitDegree,endDecrements);
}","@Override public void generate(Logger logger,CompilerBackend gen,GenInfo info){
  gen.startRangeLoop(loopName,loopVar,loopCounterVar,start,end,increment,splitDegree,leafDegree,passedVars,startIncrements,constStartIncrements,endDecrements);
  this.loopBody.generate(logger,gen,info);
  gen.endRangeLoop(splitDegree,endDecrements);
}",0.9783950617283952
132514,"@Override public void startRangeLoop(String loopName,Var loopVar,Var countVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs){
}","@Override public void startRangeLoop(String loopName,Var loopVar,Var countVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs,List<RefCount> perIterDecrs){
}",0.9442231075697212
132515,"private void startIntRangeLoop2(String loopName,String loopVarName,Expression start,Expression end,Expression incr,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs){
  if (!perIterIncrs.isEmpty()) {
    pointAdd(new SetVariable(TCLTMP_ITERSTOTAL,rangeItersLeft(start,end,incr)));
    Value itersTotal=Value.numericValue(TCLTMP_ITERSTOTAL);
    handleRefcounts(constIncrs,perIterIncrs,itersTotal,false);
  }
  if (splitDegree > 0) {
    startRangeSplit(loopName,passedVars,perIterIncrs,splitDegree,leafDegree,start,end,incr);
    startRangeLoopInner(loopName,loopVarName,TCLTMP_RANGE_LO_V,TCLTMP_RANGE_HI_V,TCLTMP_RANGE_INC_V);
  }
 else {
    startRangeLoopInner(loopName,loopVarName,start,end,incr);
  }
}","private void startIntRangeLoop2(String loopName,String loopVarName,Expression start,Expression end,Expression incr,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs,List<RefCount> perIterDecrs){
  if (!perIterIncrs.isEmpty()) {
    pointAdd(new SetVariable(TCLTMP_ITERSTOTAL,rangeItersLeft(start,end,incr)));
    Value itersTotal=Value.numericValue(TCLTMP_ITERSTOTAL);
    handleRefcounts(constIncrs,perIterIncrs,itersTotal,false);
  }
  if (splitDegree > 0) {
    startRangeSplit(loopName,passedVars,perIterIncrs,splitDegree,leafDegree,start,end,incr,perIterDecrs);
    startRangeLoopInner(loopName,loopVarName,TCLTMP_RANGE_LO_V,TCLTMP_RANGE_HI_V,TCLTMP_RANGE_INC_V);
  }
 else {
    startRangeLoopInner(loopName,loopVarName,start,end,incr);
  }
}",0.974422956955708
132516,"private void startIntRangeLoop(String loopName,String loopVarName,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs){
  assert(start.isImmInt());
  assert(end.isImmInt());
  assert(increment.isImmInt());
  startIntRangeLoop2(loopName,loopVarName,argToExpr(start),argToExpr(end),argToExpr(increment),splitDegree,leafDegree,passedVars,perIterIncrs,constIncrs);
}","private void startIntRangeLoop(String loopName,String loopVarName,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs,List<RefCount> perIterDecrs){
  assert(start.isImmInt());
  assert(end.isImmInt());
  assert(increment.isImmInt());
  startIntRangeLoop2(loopName,loopVarName,argToExpr(start),argToExpr(end),argToExpr(increment),splitDegree,leafDegree,passedVars,perIterIncrs,constIncrs,perIterDecrs);
}",0.9578622816032888
132517,"private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs){
  assert(start.isImmFloat());
  assert(end.isImmFloat());
  assert(increment.isImmFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_FLOAT_RANGE_ITERMAX);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.paren(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}","private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs,List<RefCount> perIterDecrs){
  assert(start.isImmFloat());
  assert(end.isImmFloat());
  assert(increment.isImmFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_FLOAT_RANGE_ITERMAX);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.paren(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs,perIterDecrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}",0.9835539510629764
132518,"/** 
 * After this function is called, in the TCL context at the top of the stack will be available the bottom, top (inclusive) and increment of the split in tcl values: TCLTMP_RANGE_LO TCLTMP_RANGE_HI and TCLTMP_RANGE_INC
 * @param loopName
 * @param splitDegree
 * @param leafDegree
 * @param startE start of range (inclusive)
 * @param endE end of range (inclusive)
 * @param incrE
 * @param usedVariables
 */
private void startRangeSplit(String loopName,List<PassedVar> passedVars,List<RefCount> perIterIncrs,int splitDegree,int leafDegree,Expression startE,Expression endE,Expression incrE){
  List<String> commonFormalArgs=new ArrayList<String>();
  for (  PassedVar pv : passedVars) {
    commonFormalArgs.add(prefixVar(pv.var.name()));
  }
  Value loVal=Value.numericValue(TCLTMP_RANGE_LO);
  Value hiVal=Value.numericValue(TCLTMP_RANGE_HI);
  Value incVal=Value.numericValue(TCLTMP_RANGE_INC);
  commonFormalArgs.add(loVal.variable());
  commonFormalArgs.add(hiVal.variable());
  commonFormalArgs.add(incVal.variable());
  List<String> outerFormalArgs=new ArrayList<String>(commonFormalArgs);
  List<Expression> commonArgs=new ArrayList<Expression>();
  for (  PassedVar pv : passedVars) {
    commonArgs.add(varToExpr(pv.var));
  }
  List<Expression> outerCallArgs=new ArrayList<Expression>(commonArgs);
  outerCallArgs.add(startE);
  outerCallArgs.add(endE);
  outerCallArgs.add(incrE);
  List<Expression> innerCallArgs=new ArrayList<Expression>(commonArgs);
  innerCallArgs.add(loVal);
  innerCallArgs.add(hiVal);
  innerCallArgs.add(incVal);
  Sequence outer=new Sequence();
  String outerProcName=uniqueTCLFunctionName(loopName + ""String_Node_Str"");
  tree.add(new Proc(outerProcName,usedTclFunctionNames,outerFormalArgs,outer));
  Sequence inner=new Sequence();
  String innerProcName=uniqueTCLFunctionName(loopName + ""String_Node_Str"");
  tree.add(new Proc(innerProcName,usedTclFunctionNames,commonFormalArgs,inner));
  pointAdd(new Command(outerProcName,outerCallArgs));
  Expression done=new TclExpr(loVal,TclExpr.GT,hiVal);
  If finishedIf=new If(done,false);
  finishedIf.thenBlock().add(Command.returnCommand());
  outer.add(finishedIf);
  WhileLoop iter=new WhileLoop(LiteralInt.ONE);
  outer.add(iter);
  Value itersLeft=Value.numericValue(TCLTMP_ITERSLEFT);
  iter.loopBody().add(new SetVariable(itersLeft.variable(),rangeItersLeft(loVal,hiVal,incVal)));
  Expression doneSplitting=new TclExpr(itersLeft,TclExpr.LTE,new LiteralInt(leafDegree));
  If splitIf=new If(doneSplitting,true);
  iter.loopBody().add(splitIf);
  splitIf.thenBlock().add(new Command(innerProcName,innerCallArgs));
  splitIf.thenBlock().add(Command.returnCommand());
  splitIf.elseBlock().append(rangeDoSplit(splitDegree,leafDegree,loVal,hiVal,incVal,outerProcName,commonArgs,itersLeft));
  pointPush(inner);
}","/** 
 * After this function is called, in the TCL context at the top of the stack will be available the bottom, top (inclusive) and increment of the split in tcl values: TCLTMP_RANGE_LO TCLTMP_RANGE_HI and TCLTMP_RANGE_INC
 * @param loopName
 * @param splitDegree
 * @param leafDegree
 * @param startE start of range (inclusive)
 * @param endE end of range (inclusive)
 * @param incrE
 * @param perIterDecrs
 * @param usedVariables
 */
private void startRangeSplit(String loopName,List<PassedVar> passedVars,List<RefCount> perIterIncrs,int splitDegree,int leafDegree,Expression startE,Expression endE,Expression incrE,List<RefCount> perIterDecrs){
  List<Var> mustPass=RefCount.extractVars(perIterDecrs);
  List<String> commonFormalArgs=new ArrayList<String>();
  for (  PassedVar pv : passedVars) {
    commonFormalArgs.add(prefixVar(pv.var));
    mustPass.remove(pv.var);
  }
  for (  Var var : mustPass) {
    commonFormalArgs.add(prefixVar(var));
  }
  Value loVal=Value.numericValue(TCLTMP_RANGE_LO);
  Value hiVal=Value.numericValue(TCLTMP_RANGE_HI);
  Value incVal=Value.numericValue(TCLTMP_RANGE_INC);
  commonFormalArgs.add(loVal.variable());
  commonFormalArgs.add(hiVal.variable());
  commonFormalArgs.add(incVal.variable());
  List<String> outerFormalArgs=new ArrayList<String>(commonFormalArgs);
  List<Expression> commonArgs=new ArrayList<Expression>();
  for (  PassedVar pv : passedVars) {
    commonArgs.add(varToExpr(pv.var));
  }
  for (  Var var : mustPass) {
    commonArgs.add(varToExpr(var));
  }
  List<Expression> outerCallArgs=new ArrayList<Expression>(commonArgs);
  outerCallArgs.add(startE);
  outerCallArgs.add(endE);
  outerCallArgs.add(incrE);
  List<Expression> innerCallArgs=new ArrayList<Expression>(commonArgs);
  innerCallArgs.add(loVal);
  innerCallArgs.add(hiVal);
  innerCallArgs.add(incVal);
  Sequence outer=new Sequence();
  String outerProcName=uniqueTCLFunctionName(loopName + ""String_Node_Str"");
  tree.add(new Proc(outerProcName,usedTclFunctionNames,outerFormalArgs,outer));
  Sequence inner=new Sequence();
  String innerProcName=uniqueTCLFunctionName(loopName + ""String_Node_Str"");
  tree.add(new Proc(innerProcName,usedTclFunctionNames,commonFormalArgs,inner));
  pointAdd(new Command(outerProcName,outerCallArgs));
  Expression done=new TclExpr(loVal,TclExpr.GT,hiVal);
  If finishedIf=new If(done,false);
  finishedIf.thenBlock().add(Command.returnCommand());
  outer.add(finishedIf);
  WhileLoop iter=new WhileLoop(LiteralInt.ONE);
  outer.add(iter);
  Value itersLeft=Value.numericValue(TCLTMP_ITERSLEFT);
  iter.loopBody().add(new SetVariable(itersLeft.variable(),rangeItersLeft(loVal,hiVal,incVal)));
  Expression doneSplitting=new TclExpr(itersLeft,TclExpr.LTE,new LiteralInt(leafDegree));
  If splitIf=new If(doneSplitting,true);
  iter.loopBody().add(splitIf);
  splitIf.thenBlock().add(new Command(innerProcName,innerCallArgs));
  splitIf.thenBlock().add(Command.returnCommand());
  splitIf.elseBlock().append(rangeDoSplit(splitDegree,leafDegree,loVal,hiVal,incVal,outerProcName,commonArgs,itersLeft));
  pointPush(inner);
}",0.9504244482173176
132519,"@Override public void startForeachLoop(String loopName,Var container,Var memberVar,Var loopCountVar,int splitDegree,int leafDegree,boolean arrayClosed,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs){
  boolean haveKeys=loopCountVar != null;
  boolean isKVContainer;
  if (Types.isArray(container) || Types.isArrayLocal(container)) {
    assert(!haveKeys || Types.isArrayKeyVal(container,loopCountVar.asArg()));
    isKVContainer=true;
  }
 else {
    assert(Types.isBag(container) || Types.isBagLocal(container));
    assert(!haveKeys);
    isKVContainer=false;
  }
  boolean localContainer=Types.isContainerLocal(container);
  if (localContainer) {
    if (!arrayClosed || splitDegree >= 0) {
      throw new STCRuntimeError(""String_Node_Str"");
    }
    assert(Types.isElemType(container,memberVar));
  }
 else {
    assert(Types.isElemValType(container,memberVar));
  }
  if (!arrayClosed) {
    throw new STCRuntimeError(""String_Node_Str"");
  }
  boolean isDict;
  Value tclContainer;
  if (splitDegree <= 0) {
    if (localContainer) {
      tclContainer=varToExpr(container);
      isDict=isKVContainer;
    }
 else {
      tclContainer=new Value(TCLTMP_ARRAY_CONTENTS);
      pointAdd(Turbine.enumerateAll(tclContainer.variable(),varToExpr(container),haveKeys));
      isDict=haveKeys;
    }
    Expression containerSize;
    if (isDict) {
      containerSize=Turbine.dictSize(tclContainer);
    }
 else {
      containerSize=Turbine.listLength(tclContainer);
    }
    handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  }
 else {
    assert(!localContainer);
    tclContainer=new Value(TCLTMP_ARRAY_CONTENTS);
    startForeachSplit(loopName,container,tclContainer.variable(),splitDegree,leafDegree,haveKeys,passedVars,perIterIncrs,constIncrs);
    isDict=haveKeys;
  }
  startForeachInner(tclContainer,memberVar,loopCountVar,isDict);
}","@Override public void startForeachLoop(String loopName,Var container,Var memberVar,Var loopCountVar,int splitDegree,int leafDegree,boolean arrayClosed,List<PassedVar> passedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs,List<RefCount> perIterDecrs){
  boolean haveKeys=loopCountVar != null;
  boolean isKVContainer;
  if (Types.isArray(container) || Types.isArrayLocal(container)) {
    assert(!haveKeys || Types.isArrayKeyVal(container,loopCountVar.asArg()));
    isKVContainer=true;
  }
 else {
    assert(Types.isBag(container) || Types.isBagLocal(container));
    assert(!haveKeys);
    isKVContainer=false;
  }
  boolean localContainer=Types.isContainerLocal(container);
  if (localContainer) {
    if (!arrayClosed || splitDegree >= 0) {
      throw new STCRuntimeError(""String_Node_Str"");
    }
    assert(Types.isElemType(container,memberVar));
  }
 else {
    assert(Types.isElemValType(container,memberVar));
  }
  if (!arrayClosed) {
    throw new STCRuntimeError(""String_Node_Str"");
  }
  boolean isDict;
  Value tclContainer;
  if (splitDegree <= 0) {
    if (localContainer) {
      tclContainer=varToExpr(container);
      isDict=isKVContainer;
    }
 else {
      tclContainer=new Value(TCLTMP_ARRAY_CONTENTS);
      pointAdd(Turbine.enumerateAll(tclContainer.variable(),varToExpr(container),haveKeys));
      isDict=haveKeys;
    }
    Expression containerSize;
    if (isDict) {
      containerSize=Turbine.dictSize(tclContainer);
    }
 else {
      containerSize=Turbine.listLength(tclContainer);
    }
    handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  }
 else {
    assert(!localContainer);
    tclContainer=new Value(TCLTMP_ARRAY_CONTENTS);
    startForeachSplit(loopName,container,tclContainer.variable(),splitDegree,leafDegree,haveKeys,passedVars,perIterIncrs,constIncrs,perIterDecrs);
    isDict=haveKeys;
  }
  startForeachInner(tclContainer,memberVar,loopCountVar,isDict);
}",0.989391979301423
132520,"private void startForeachSplit(String procName,Var arrayVar,String contentsVar,int splitDegree,int leafDegree,boolean haveKeys,List<PassedVar> usedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs){
  pointAdd(Turbine.containerSize(TCLTMP_CONTAINER_SIZE,varToExpr(arrayVar)));
  Value containerSize=Value.numericValue(TCLTMP_CONTAINER_SIZE);
  Expression lastIndex=TclExpr.minus(containerSize,LiteralInt.ONE);
  handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  ArrayList<PassedVar> splitUsedVars=new ArrayList<PassedVar>(usedVars);
  if (!PassedVar.contains(splitUsedVars,arrayVar)) {
    splitUsedVars.add(new PassedVar(arrayVar,false));
  }
  startRangeSplit(procName,splitUsedVars,perIterIncrs,splitDegree,leafDegree,LiteralInt.ZERO,lastIndex,LiteralInt.ONE);
  pointAdd(new SetVariable(TCLTMP_SPLITLEN,new TclExpr(Value.numericValue(TCLTMP_RANGE_HI),TclExpr.MINUS,Value.numericValue(TCLTMP_RANGE_LO),TclExpr.PLUS,LiteralInt.ONE)));
  pointAdd(Turbine.enumerate(contentsVar,varToExpr(arrayVar),haveKeys,TCLTMP_RANGE_LO_V,Value.numericValue(TCLTMP_SPLITLEN)));
}","private void startForeachSplit(String procName,Var arrayVar,String contentsVar,int splitDegree,int leafDegree,boolean haveKeys,List<PassedVar> usedVars,List<RefCount> perIterIncrs,ListMultimap<Var,RefCount> constIncrs,List<RefCount> perIterDecrs){
  pointAdd(Turbine.containerSize(TCLTMP_CONTAINER_SIZE,varToExpr(arrayVar)));
  Value containerSize=Value.numericValue(TCLTMP_CONTAINER_SIZE);
  Expression lastIndex=TclExpr.minus(containerSize,LiteralInt.ONE);
  handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  ArrayList<PassedVar> splitUsedVars=new ArrayList<PassedVar>(usedVars);
  if (!PassedVar.contains(splitUsedVars,arrayVar)) {
    splitUsedVars.add(new PassedVar(arrayVar,false));
  }
  startRangeSplit(procName,splitUsedVars,perIterIncrs,splitDegree,leafDegree,LiteralInt.ZERO,lastIndex,LiteralInt.ONE,perIterDecrs);
  pointAdd(new SetVariable(TCLTMP_SPLITLEN,new TclExpr(Value.numericValue(TCLTMP_RANGE_HI),TclExpr.MINUS,Value.numericValue(TCLTMP_RANGE_LO),TclExpr.PLUS,LiteralInt.ONE)));
  pointAdd(Turbine.enumerate(contentsVar,varToExpr(arrayVar),haveKeys,TCLTMP_RANGE_LO_V,Value.numericValue(TCLTMP_SPLITLEN)));
}",0.9818664307828394
132521,"private void compileTopLevel(GlobalContext context,LocatedModule mainModule) throws UserException, UndefinedFunctionException, ModuleLoadException {
  LocalContext topLevelContext=LocalContext.topLevelContext(context);
  Pair<ParsedModule,Boolean> loadedMainModule=modules.loadIfNeeded(context,mainModule);
  assert(!loadedMainModule.val2);
  varAnalyzer.walkTopLevel(context,loadedMainModule.val1.ast,moduleIterator(context));
  backend.startFunction(FnID.ENTRY_FUNCTION,Var.NONE,Var.NONE,ExecTarget.syncControl());
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,topLevelContext,FrontendPass.COMPILE_TOPLEVEL,loadedModule);
  }
  List<FnOverload> mainOverloads=context.lookupFunction(Constants.MAIN_FUNCTION);
  if (mainOverloads.size() == 1) {
    FnID mainID=mainOverloads.get(0).id;
    backend.functionCall(mainID,Arg.NONE,Var.NONE,ExecTarget.syncControl(),new TaskProps());
  }
 else   if (mainOverloads.size() >= 2) {
    throw new DoubleDefineException(context,""String_Node_Str"" + Constants.MAIN_FUNCTION);
  }
  backend.endFunction();
}","private void compileTopLevel(GlobalContext context,LocatedModule mainModule) throws UserException, UndefinedFunctionException, ModuleLoadException {
  LocalContext topLevelContext=LocalContext.topLevelContext(context);
  Pair<ParsedModule,Boolean> loadedMainModule=modules.loadIfNeeded(context,mainModule);
  assert(!loadedMainModule.val2);
  varAnalyzer.walkTopLevel(context,loadedMainModule.val1);
  backend.startFunction(FnID.ENTRY_FUNCTION,Var.NONE,Var.NONE,ExecTarget.syncControl());
  loadModule(context,topLevelContext,FrontendPass.COMPILE_TOPLEVEL,mainModule);
  List<FnOverload> mainOverloads=context.lookupFunction(Constants.MAIN_FUNCTION);
  if (mainOverloads.size() == 1) {
    FnID mainID=mainOverloads.get(0).id;
    backend.functionCall(mainID,Arg.NONE,Var.NONE,ExecTarget.syncControl(),new TaskProps());
  }
 else   if (mainOverloads.size() >= 2) {
    throw new DoubleDefineException(context,""String_Node_Str"" + Constants.MAIN_FUNCTION);
  }
  backend.endFunction();
}",0.9396426846933849
132522,"/** 
 * Handle an import statement by loading definitions for, or compiling module as needed.
 * @param context
 * @param tree
 * @param pass
 * @throws UserException
 */
private void importModule(GlobalContext context,SwiftAST tree,FrontendPass pass) throws UserException {
  assert(tree.getType() == ExMParser.IMPORT);
  assert(tree.getChildCount() == 1);
  SwiftAST moduleID=tree.child(0);
  if (pass == FrontendPass.DEFINITIONS) {
    LocatedModule module=LocatedModule.fromModuleNameAST(context,moduleID,false);
    loadModule(context,null,pass,module);
  }
}","/** 
 * Handle an import statement by loading definitions for, or compiling module as needed.
 * @param context
 * @param tree
 * @param topLevelCx required if we're compiling top level statements
 * @param pass
 * @throws UserException
 */
private void importModule(GlobalContext context,LocalContext topLevelCx,SwiftAST tree,FrontendPass pass) throws UserException {
  assert(tree.getType() == ExMParser.IMPORT);
  assert(tree.getChildCount() == 1);
  SwiftAST moduleID=tree.child(0);
  if (pass == FrontendPass.DEFINITIONS || pass == FrontendPass.COMPILE_TOPLEVEL) {
    LocatedModule module=LocatedModule.fromModuleNameAST(context,moduleID,false);
    loadModule(context,topLevelCx,pass,module);
  }
}",0.8778565799842396
132523,"/** 
 * Second pass: - Compile top-level statements
 * @param globalContext
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileStatements(LocalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  List<SwiftAST> stmts=new ArrayList<SwiftAST>();
  for (  SwiftAST stmt : fileTree.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (TopLevel.isStatement(type)) {
      stmts.add(stmt);
    }
 else     if (!TopLevel.isDefinition(type)) {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
  for (  SwiftAST stmt : stmts) {
    walkStatement(context,stmt,WalkMode.NORMAL);
  }
}","/** 
 * Second pass: - Compile top-level statements
 * @param globalContext
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileStatements(LocalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  List<SwiftAST> stmts=new ArrayList<SwiftAST>();
  for (  SwiftAST stmt : fileTree.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (TopLevel.isStatement(type)) {
      stmts.add(stmt);
    }
 else     if (type == ExMParser.IMPORT) {
      importModule(context.getGlobals(),context,stmt,FrontendPass.COMPILE_TOPLEVEL);
    }
 else     if (!TopLevel.isDefinition(type)) {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
  for (  SwiftAST stmt : stmts) {
    walkStatement(context,stmt,WalkMode.NORMAL);
  }
}",0.9215339233038348
132524,"/** 
 * First pass: - Register (but don't compile) all functions and other definitions
 * @param context
 * @param fileTree
 * @throws UserException
 * @throws DoubleDefineException
 * @throws UndefinedTypeException
 */
private void walkTopLevelDefs(GlobalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  for (  SwiftAST stmt : fileTree.children()) {
    int type=stmt.getType();
    syncFilePos(context,stmt);
switch (type) {
case ExMParser.IMPORT:
      importModule(context,stmt,FrontendPass.DEFINITIONS);
    break;
case ExMParser.DEFINE_BUILTIN_FUNCTION:
  defineBuiltinFunction(context,stmt);
break;
case ExMParser.DEFINE_FUNCTION:
defineFunction(context,stmt);
break;
case ExMParser.DEFINE_APP_FUNCTION:
defineAppFunction(context,stmt);
break;
case ExMParser.DEFINE_NEW_STRUCT_TYPE:
defineNewStructType(context,stmt);
break;
case ExMParser.DEFINE_NEW_TYPE:
case ExMParser.TYPEDEF:
defineNewType(context,stmt,type == ExMParser.TYPEDEF);
break;
case ExMParser.GLOBAL_CONST:
globalConst(context,stmt);
break;
case ExMParser.PRAGMA:
pragmaTopLevel(context,stmt);
break;
case ExMParser.EOF:
break;
default :
if (!TopLevel.isStatement(type)) {
throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
}
}
}
}","/** 
 * First pass: - Register (but don't compile) all functions and other definitions
 * @param context
 * @param fileTree
 * @throws UserException
 * @throws DoubleDefineException
 * @throws UndefinedTypeException
 */
private void walkTopLevelDefs(GlobalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  for (  SwiftAST stmt : fileTree.children()) {
    int type=stmt.getType();
    syncFilePos(context,stmt);
switch (type) {
case ExMParser.IMPORT:
      importModule(context,null,stmt,FrontendPass.DEFINITIONS);
    break;
case ExMParser.DEFINE_BUILTIN_FUNCTION:
  defineBuiltinFunction(context,stmt);
break;
case ExMParser.DEFINE_FUNCTION:
defineFunction(context,stmt);
break;
case ExMParser.DEFINE_APP_FUNCTION:
defineAppFunction(context,stmt);
break;
case ExMParser.DEFINE_NEW_STRUCT_TYPE:
defineNewStructType(context,stmt);
break;
case ExMParser.DEFINE_NEW_TYPE:
case ExMParser.TYPEDEF:
defineNewType(context,stmt,type == ExMParser.TYPEDEF);
break;
case ExMParser.GLOBAL_CONST:
globalConst(context,stmt);
break;
case ExMParser.PRAGMA:
pragmaTopLevel(context,stmt);
break;
case ExMParser.EOF:
break;
default :
if (!TopLevel.isStatement(type)) {
throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
}
}
}
}",0.9981364144614238
132525,"/** 
 * Compile the function, assuming it is already defined in context 
 */
private void compileFunction(Context context,SwiftAST tree) throws UserException {
  String function=tree.child(0).getText();
  LogHelper.debug(context,""String_Node_Str"" + function);
  assert(context.isFunction(function));
  FnID id=(FnID)tree.getIdentifier();
  int numOverloads=context.lookupFunction(id.originalName()).size();
  if (numOverloads >= 2 && context.hasFunctionProp(id,FnProp.CHECKPOINTED)) {
    throw new InvalidOverloadException(context,""String_Node_Str"" + ""String_Node_Str"");
  }
  assert(context.hasFunctionProp(id,FnProp.COMPOSITE));
  SwiftAST outputs=tree.child(2);
  SwiftAST inputs=tree.child(3);
  SwiftAST block=tree.child(4);
  FunctionDecl fdecl=FunctionDecl.fromAST(context,varCreator,exprWalker,function,inputs,outputs,Collections.<String>emptySet());
  List<Var> iList=fdecl.getInVars(context);
  List<Var> oList=fdecl.getOutVars(context);
  List<Var> backendIList=VarRepr.backendVars(iList);
  List<Var> backendOList=VarRepr.backendVars(oList);
  syncFilePos(context,tree);
  String moduleName=modules.currentModule().moduleName;
  varAnalyzer.walkFunction(context,lineMap(),moduleName,function,iList,oList,block);
  LocalContext functionContext=LocalContext.fnContext(context,function);
  functionContext.addDeclaredVariables(iList);
  functionContext.addDeclaredVariables(oList);
  ExecTarget mode=context.hasFunctionProp(id,FnProp.SYNC) ? ExecTarget.syncControl() : ExecTarget.dispatchedControl();
  backend.startFunction(id,backendOList,backendIList,mode);
  block(functionContext,block);
  backend.endFunction();
  LogHelper.debug(context,""String_Node_Str"" + function);
}","/** 
 * Compile the function, assuming it is already defined in context 
 */
private void compileFunction(Context context,SwiftAST tree) throws UserException {
  String function=tree.child(0).getText();
  LogHelper.debug(context,""String_Node_Str"" + function);
  assert(context.isFunction(function));
  FnID id=(FnID)tree.getIdentifier();
  int numOverloads=context.lookupFunction(id.originalName()).size();
  if (numOverloads >= 2 && context.hasFunctionProp(id,FnProp.CHECKPOINTED)) {
    throw new InvalidOverloadException(context,""String_Node_Str"" + ""String_Node_Str"");
  }
  assert(context.hasFunctionProp(id,FnProp.COMPOSITE));
  SwiftAST outputs=tree.child(2);
  SwiftAST inputs=tree.child(3);
  SwiftAST block=tree.child(4);
  FunctionDecl fdecl=FunctionDecl.fromAST(context,varCreator,exprWalker,function,inputs,outputs,Collections.<String>emptySet());
  List<Var> iList=fdecl.getInVars(context);
  List<Var> oList=fdecl.getOutVars(context);
  List<Var> backendIList=VarRepr.backendVars(iList);
  List<Var> backendOList=VarRepr.backendVars(oList);
  syncFilePos(context,tree);
  varAnalyzer.walkFunction(context,modules.currentModule(),function,iList,oList,block);
  LocalContext functionContext=LocalContext.fnContext(context,function);
  functionContext.addDeclaredVariables(iList);
  functionContext.addDeclaredVariables(oList);
  ExecTarget mode=context.hasFunctionProp(id,FnProp.SYNC) ? ExecTarget.syncControl() : ExecTarget.dispatchedControl();
  backend.startFunction(id,backendOList,backendIList,mode);
  block(functionContext,block);
  backend.endFunction();
  LogHelper.debug(context,""String_Node_Str"" + function);
}",0.9707743296173548
132526,"public ASTWalker(STCMiddleEnd backend,ForeignFunctions foreignFuncs){
  this.backend=backend;
  this.foreignFuncs=foreignFuncs;
  this.modules=new LoadedModules();
  this.varCreator=new VarCreator(backend);
  this.wrapper=new WrapperGen(backend);
  this.exprWalker=new ExprWalker(wrapper,varCreator,backend,modules);
  this.lValWalker=new LValWalker(backend,varCreator,exprWalker,modules);
  this.varAnalyzer=new VariableUsageAnalyzer();
}","public ASTWalker(STCMiddleEnd backend,ForeignFunctions foreignFuncs){
  this.backend=backend;
  this.foreignFuncs=foreignFuncs;
  this.modules=new LoadedModules();
  this.varCreator=new VarCreator(backend);
  this.wrapper=new WrapperGen(backend);
  this.exprWalker=new ExprWalker(wrapper,varCreator,backend,modules);
  this.lValWalker=new LValWalker(backend,varCreator,exprWalker,modules);
  this.varAnalyzer=new VariableUsageAnalyzer(modules);
}",0.992090395480226
132527,"public Context(Logger logger,int level){
  super();
  this.level=level;
  this.logger=logger;
}","public Context(Context parent,Logger logger,int level){
  super();
  this.parent=parent;
  this.level=level;
  this.logger=logger;
}",0.8370044052863436
132528,"public GlobalContext(String inputFile,Logger logger,ForeignFunctions foreignFuncs){
  super(logger,0);
  this.inputFile=inputFile;
  this.foreignFuncs=foreignFuncs;
  Map<String,Type> builtInTypes=Types.getBuiltInTypes();
  for (  Entry<String,Type> type : builtInTypes.entrySet()) {
    types.put(type.getKey(),type.getValue());
    try {
      addDef(type.getKey(),DefKind.TYPE);
    }
 catch (    DoubleDefineException e) {
      throw new STCRuntimeError(e.getMessage());
    }
  }
  try {
    addExecContexts(ExecContext.builtinExecContexts());
  }
 catch (  DoubleDefineException e) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"",e);
  }
}","public GlobalContext(String inputFile,Logger logger,ForeignFunctions foreignFuncs){
  super(null,logger,0);
  this.inputFile=inputFile;
  this.foreignFuncs=foreignFuncs;
  Map<String,Type> builtInTypes=Types.getBuiltInTypes();
  for (  Entry<String,Type> type : builtInTypes.entrySet()) {
    types.put(type.getKey(),type.getValue());
    try {
      addDef(type.getKey(),DefKind.TYPE);
    }
 catch (    DoubleDefineException e) {
      throw new STCRuntimeError(e.getMessage());
    }
  }
  try {
    addExecContexts(ExecContext.builtinExecContexts());
  }
 catch (  DoubleDefineException e) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"",e);
  }
}",0.9962714392244594
132529,"public LocalContext(Context parent,String functionName,boolean isTopLevel){
  super(parent.getLogger(),parent.getLevel() + 1);
  this.functionContext=functionName != null ? new FunctionContext(functionName) : null;
  this.parent=parent;
  this.globals=parent.getGlobals();
  this.isTopLevel=isTopLevel;
  inputFile=parent.inputFile;
  line=parent.line;
  col=parent.col;
}","public LocalContext(Context parent,String functionName,boolean isTopLevel){
  super(parent,parent.getLogger(),parent.getLevel() + 1);
  this.functionContext=functionName != null ? new FunctionContext(functionName) : null;
  this.globals=parent.getGlobals();
  this.isTopLevel=isTopLevel;
  inputFile=parent.inputFile;
  line=parent.line;
  col=parent.col;
}",0.8532235939643347
132530,"/** 
 * @param filePath
 * @return
 * @throws IOException 
 */
public static String getCanonicalFilePath(String filePath) throws IOException {
  return new File(filePath).getCanonicalPath();
}","/** 
 * @param filePath
 * @return
 * @throws IOException
 */
public static String getCanonicalFilePath(String filePath) throws IOException {
  return new File(filePath).getCanonicalPath();
}",0.9973890339425588
132531,"/** 
 * Use the file and line info from c preprocessor to  update SwiftAST
 * @param lexer
 * @param tree
 */
private static LineMapping parsePreprocOutput(ANTLRInputStream input){
}","/** 
 * Use the file and line info from c preprocessor to update SwiftAST
 * @param lexer
 * @param tree
 */
private static LineMapping parsePreprocOutput(ANTLRInputStream input){
}",0.9972451790633609
132532,"/** 
 * Use ANTLR to parse the input and get the Tree
 * @throws IOException 
 */
private static SwiftAST runANTLR(ANTLRInputStream input,LineMapping lineMap){
  ExMLexer lexer=new ExMLexer(input);
  lexer.lineMap=lineMap;
  CommonTokenStream tokens=new CommonTokenStream(lexer);
  ExMParser parser=new ExMParser(tokens);
  parser.lineMap=lineMap;
  parser.setTreeAdaptor(new SwTreeAdaptor());
  ExMParser.program_return program=null;
  try {
    program=parser.program();
  }
 catch (  RecognitionException e) {
    e.printStackTrace();
    System.out.println(""String_Node_Str"");
    throw new STCFatal(ExitCode.ERROR_INTERNAL.code());
  }
  if (parser.parserError) {
    System.err.println(""String_Node_Str"");
    throw new STCFatal(ExitCode.ERROR_USER.code());
  }
  if (program == null)   throw new STCRuntimeError(""String_Node_Str"");
  SwiftAST tree=(SwiftAST)program.getTree();
  return tree;
}","/** 
 * Use ANTLR to parse the input and get the Tree
 * @throws IOException
 */
private static SwiftAST runANTLR(ANTLRInputStream input,LineMapping lineMap){
  ExMLexer lexer=new ExMLexer(input);
  lexer.lineMap=lineMap;
  CommonTokenStream tokens=new CommonTokenStream(lexer);
  ExMParser parser=new ExMParser(tokens);
  parser.lineMap=lineMap;
  parser.setTreeAdaptor(new SwTreeAdaptor());
  ExMParser.program_return program=null;
  try {
    program=parser.program();
  }
 catch (  RecognitionException e) {
    e.printStackTrace();
    System.out.println(""String_Node_Str"");
    throw new STCFatal(ExitCode.ERROR_INTERNAL.code());
  }
  if (parser.parserError) {
    System.err.println(""String_Node_Str"");
    throw new STCFatal(ExitCode.ERROR_USER.code());
  }
  if (program == null)   throw new STCRuntimeError(""String_Node_Str"");
  SwiftAST tree=(SwiftAST)program.getTree();
  return tree;
}",0.999444135630906
132533,"public static TclTree genLocalOpTcl(BuiltinOpcode op,Var out,List<Arg> in,ArrayList<Expression> argExpr){
  if (op == BuiltinOpcode.ASSERT || op == BuiltinOpcode.ASSERT_EQ) {
    assert(out != null && Types.isVoidVal(out));
    Token tclFn;
switch (op) {
case ASSERT:
      tclFn=Turbine.TURBINE_ASSERT_IMPL;
    break;
case ASSERT_EQ:
  tclFn=Turbine.TURBINE_ASSERT_EQUAL_IMPL;
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + op.toString());
}
return new Command(tclFn,argExpr);
}
 else {
if (op == BuiltinOpcode.SPRINTF) {
Square fmtArgs=new TclList(argExpr);
Square fmt=new Square(new Token(""String_Node_Str""),new Token(""String_Node_Str""),fmtArgs);
return new SetVariable(TclNamer.prefixVar(out.name()),fmt);
}
 else {
assert(out != null);
assert(Types.isPrimValue(out));
Expression rhs;
if (op == BuiltinOpcode.STRCAT) {
rhs=localStrCat(in,argExpr);
}
 else if (op == BuiltinOpcode.DIRCAT) {
rhs=localDirCat(in,argExpr);
}
 else if (op == BuiltinOpcode.EQ_STRING || op == BuiltinOpcode.NEQ_STRING) {
assert(argExpr.size() == 2);
Expression s1=argExpr.get(0);
Expression s2=argExpr.get(1);
rhs=Turbine.stringEqual(s1,s2);
if (op == BuiltinOpcode.NEQ_STRING) {
rhs=new TclExpr(true,TclExpr.NOT,rhs);
}
}
 else if (op == BuiltinOpcode.COPY_BLOB || op == BuiltinOpcode.COPY_BOOL || op == BuiltinOpcode.COPY_INT || op == BuiltinOpcode.COPY_VOID || op == BuiltinOpcode.COPY_FLOAT || op == BuiltinOpcode.COPY_STRING) {
assert(argExpr.size() == 1);
checkCopy(op,out,in.get(0));
rhs=argExpr.get(0);
}
 else if (op == BuiltinOpcode.MOD_INT) {
rhs=Turbine.modInteger(argExpr.get(0),argExpr.get(1));
}
 else if (op == BuiltinOpcode.DIV_INT) {
rhs=Turbine.divideInteger(argExpr.get(0),argExpr.get(1));
}
 else if (op == BuiltinOpcode.POW_INT) {
assert(argExpr.size() == 2);
assert(in.get(0).isImmInt() && in.get(1).isImmInt());
rhs=new Square(Turbine.POW_INTEGER_IMPL,argExpr.get(0),argExpr.get(1));
}
 else if (op == BuiltinOpcode.SUBSTRING) {
assert(argExpr.size() == 3);
rhs=new Square(Turbine.SUBSTRING_IMPL,argExpr.get(0),argExpr.get(1),argExpr.get(2));
}
 else if (op == BuiltinOpcode.INTTOSTR || op == BuiltinOpcode.FLOATTOSTR) {
assert(argExpr.size() == 1);
rhs=argExpr.get(0);
}
 else if (op == BuiltinOpcode.PARSE_INT) {
assert(argExpr.size() == 2);
rhs=Square.fnCall(Turbine.PARSE_INT,argExpr.get(0),argExpr.get(1));
}
 else if (op == BuiltinOpcode.PARSE_FLOAT) {
assert(argExpr.size() == 1);
rhs=Square.fnCall(Turbine.TOFLOAT_IMPL,argExpr.get(0),argExpr.get(1));
}
 else {
Expression exp[]=arithOpExpr(op,argExpr);
rhs=new TclExpr(exp);
}
return new SetVariable(TclNamer.prefixVar(out.name()),rhs);
}
}
}","public static TclTree genLocalOpTcl(BuiltinOpcode op,Var out,List<Arg> in,ArrayList<Expression> argExpr){
  if (op == BuiltinOpcode.ASSERT || op == BuiltinOpcode.ASSERT_EQ) {
    assert(out != null && Types.isVoidVal(out));
    Token tclFn;
switch (op) {
case ASSERT:
      tclFn=Turbine.TURBINE_ASSERT_IMPL;
    break;
case ASSERT_EQ:
  tclFn=Turbine.TURBINE_ASSERT_EQUAL_IMPL;
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + op.toString());
}
return new Command(tclFn,argExpr);
}
 else {
if (op == BuiltinOpcode.SPRINTF) {
Square fmtArgs=new TclList(argExpr);
Square fmt=new Square(new Token(""String_Node_Str""),new Token(""String_Node_Str""),fmtArgs);
return new SetVariable(TclNamer.prefixVar(out.name()),fmt);
}
 else {
assert(out != null);
assert(Types.isPrimValue(out));
Expression rhs;
if (op == BuiltinOpcode.STRCAT) {
rhs=localStrCat(in,argExpr);
}
 else if (op == BuiltinOpcode.DIRCAT) {
rhs=localDirCat(in,argExpr);
}
 else if (op == BuiltinOpcode.EQ_STRING || op == BuiltinOpcode.NEQ_STRING) {
assert(argExpr.size() == 2);
Expression s1=argExpr.get(0);
Expression s2=argExpr.get(1);
rhs=Turbine.stringEqual(s1,s2);
if (op == BuiltinOpcode.NEQ_STRING) {
rhs=new TclExpr(true,TclExpr.NOT,rhs);
}
}
 else if (op == BuiltinOpcode.COPY_BLOB || op == BuiltinOpcode.COPY_BOOL || op == BuiltinOpcode.COPY_INT || op == BuiltinOpcode.COPY_VOID || op == BuiltinOpcode.COPY_FLOAT || op == BuiltinOpcode.COPY_STRING) {
assert(argExpr.size() == 1);
checkCopy(op,out,in.get(0));
rhs=argExpr.get(0);
}
 else if (op == BuiltinOpcode.MOD_INT) {
rhs=Turbine.modInteger(argExpr.get(0),argExpr.get(1));
}
 else if (op == BuiltinOpcode.DIV_INT) {
rhs=Turbine.divideInteger(argExpr.get(0),argExpr.get(1));
}
 else if (op == BuiltinOpcode.POW_INT) {
assert(argExpr.size() == 2);
assert(in.get(0).isImmInt() && in.get(1).isImmInt());
rhs=new Square(Turbine.POW_INTEGER_IMPL,argExpr.get(0),argExpr.get(1));
}
 else if (op == BuiltinOpcode.SUBSTRING) {
assert(argExpr.size() == 3);
rhs=new Square(Turbine.SUBSTRING_IMPL,argExpr.get(0),argExpr.get(1),argExpr.get(2));
}
 else if (op == BuiltinOpcode.INTTOSTR || op == BuiltinOpcode.FLOATTOSTR) {
assert(argExpr.size() == 1);
rhs=argExpr.get(0);
}
 else if (op == BuiltinOpcode.PARSE_INT) {
assert(argExpr.size() == 2);
rhs=Square.fnCall(Turbine.PARSE_INT,argExpr.get(0),argExpr.get(1));
}
 else if (op == BuiltinOpcode.PARSE_FLOAT) {
assert(argExpr.size() == 1);
rhs=Square.fnCall(Turbine.TOFLOAT_IMPL,argExpr.get(0));
}
 else {
Expression exp[]=arithOpExpr(op,argExpr);
rhs=new TclExpr(exp);
}
return new SetVariable(TclNamer.prefixVar(out.name()),rhs);
}
}
}",0.9971313826735514
132534,"private void compileTopLevel(GlobalContext context,LocatedModule mainModule) throws UserException, UndefinedFunctionException, ModuleLoadException {
  LocalContext topLevelContext=LocalContext.topLevelContext(context);
  Pair<ParsedModule,Boolean> loadedMainModule=modules.loadIfNeeded(context,mainModule);
  assert(!loadedMainModule.val2);
  varAnalyzer.walkTopLevel(context,loadedMainModule.val1);
  backend.startFunction(FnID.ENTRY_FUNCTION,Var.NONE,Var.NONE,ExecTarget.syncControl());
  loadModule(context,topLevelContext,FrontendPass.COMPILE_TOPLEVEL,mainModule);
  List<FnOverload> mainOverloads=context.lookupFunction(Constants.MAIN_FUNCTION);
  if (mainOverloads.size() == 1) {
    FnID mainID=mainOverloads.get(0).id;
    backend.functionCall(mainID,Arg.NONE,Var.NONE,ExecTarget.syncControl(),new TaskProps());
  }
 else   if (mainOverloads.size() >= 2) {
    throw new DoubleDefineException(context,""String_Node_Str"" + Constants.MAIN_FUNCTION);
  }
  backend.endFunction();
}","private void compileTopLevel(GlobalContext context,LocatedModule mainModule,LocatedModule builtins) throws UserException {
  LocalContext topLevelContext=LocalContext.topLevelContext(context);
  List<LocatedModule> locatedModules=Arrays.asList(builtins,mainModule);
  List<ParsedModule> parsedModules=new ArrayList<ParsedModule>();
  for (  LocatedModule module : locatedModules) {
    Pair<ParsedModule,Boolean> loaded=modules.loadIfNeeded(context,module);
    assert(!loaded.val2);
    parsedModules.add(loaded.val1);
  }
  for (  ParsedModule module : parsedModules) {
    varAnalyzer.walkTopLevel(context,module);
  }
  backend.startFunction(FnID.ENTRY_FUNCTION,Var.NONE,Var.NONE,ExecTarget.syncControl());
  for (int i=0; i < parsedModules.size(); i++) {
    walkFile(context,topLevelContext,locatedModules.get(i),parsedModules.get(i),FrontendPass.COMPILE_TOPLEVEL);
  }
  List<FnOverload> mainOverloads=context.lookupFunction(Constants.MAIN_FUNCTION);
  if (mainOverloads.size() == 1) {
    FnID mainID=mainOverloads.get(0).id;
    backend.functionCall(mainID,Arg.NONE,Var.NONE,ExecTarget.syncControl(),new TaskProps());
  }
 else   if (mainOverloads.size() >= 2) {
    throw new DoubleDefineException(context,""String_Node_Str"" + Constants.MAIN_FUNCTION);
  }
  backend.endFunction();
}",0.7290294246815986
132535,"private void walkTopLevel(GlobalContext context,LocalContext topLevelContext,SwiftAST fileTree,FrontendPass pass) throws UserException {
  if (pass == FrontendPass.DEFINITIONS) {
    walkTopLevelDefs(context,fileTree);
  }
 else   if (pass == FrontendPass.COMPILE_TOPLEVEL) {
    walkTopLevelCompileStatements(topLevelContext,fileTree);
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS);
    walkTopLevelCompileFunctions(context,fileTree);
  }
}","private void walkTopLevel(GlobalContext context,LocalContext topLevelContext,ParsedModule module,FrontendPass pass) throws UserException {
  if (pass == FrontendPass.DEFINITIONS) {
    walkTopLevelDefs(context,module);
  }
 else   if (pass == FrontendPass.COMPILE_TOPLEVEL) {
    walkTopLevelCompileStatements(topLevelContext,module);
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS);
    walkTopLevelCompileFunctions(context,module);
  }
}",0.9232456140350878
132536,"/** 
 * Second pass: - Compile top-level statements
 * @param globalContext
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileStatements(LocalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  List<SwiftAST> stmts=new ArrayList<SwiftAST>();
  for (  SwiftAST stmt : fileTree.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (TopLevel.isStatement(type)) {
      stmts.add(stmt);
    }
 else     if (type == ExMParser.IMPORT) {
      importModule(context.getGlobals(),context,stmt,FrontendPass.COMPILE_TOPLEVEL);
    }
 else     if (!TopLevel.isDefinition(type)) {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
  for (  SwiftAST stmt : stmts) {
    walkStatement(context,stmt,WalkMode.NORMAL);
  }
}","/** 
 * Second pass: - Compile top-level statements
 * @param context
 * @param module
 * @throws UserException
 */
private void walkTopLevelCompileStatements(LocalContext context,ParsedModule module) throws UserException {
  assert(module.ast.getType() == ExMParser.PROGRAM);
  if (!modules.needToCompileTopLevel(module)) {
    return;
  }
  syncFilePos(context,module.ast);
  List<SwiftAST> stmts=new ArrayList<SwiftAST>();
  for (  SwiftAST stmt : module.ast.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (TopLevel.isStatement(type)) {
      stmts.add(stmt);
    }
 else     if (type == ExMParser.IMPORT) {
      importModule(context.getGlobals(),context,stmt,FrontendPass.COMPILE_TOPLEVEL);
    }
 else     if (!TopLevel.isDefinition(type)) {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
  for (  SwiftAST stmt : stmts) {
    walkStatement(context,stmt,WalkMode.NORMAL);
  }
}",0.9090909090909092
132537,"/** 
 * First pass: - Register (but don't compile) all functions and other definitions
 * @param context
 * @param fileTree
 * @throws UserException
 * @throws DoubleDefineException
 * @throws UndefinedTypeException
 */
private void walkTopLevelDefs(GlobalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  for (  SwiftAST stmt : fileTree.children()) {
    int type=stmt.getType();
    syncFilePos(context,stmt);
switch (type) {
case ExMParser.IMPORT:
      importModule(context,null,stmt,FrontendPass.DEFINITIONS);
    break;
case ExMParser.DEFINE_BUILTIN_FUNCTION:
  defineBuiltinFunction(context,stmt);
break;
case ExMParser.DEFINE_FUNCTION:
defineFunction(context,stmt);
break;
case ExMParser.DEFINE_APP_FUNCTION:
defineAppFunction(context,stmt);
break;
case ExMParser.DEFINE_NEW_STRUCT_TYPE:
defineNewStructType(context,stmt);
break;
case ExMParser.DEFINE_NEW_TYPE:
case ExMParser.TYPEDEF:
defineNewType(context,stmt,type == ExMParser.TYPEDEF);
break;
case ExMParser.GLOBAL_CONST:
globalConst(context,stmt);
break;
case ExMParser.PRAGMA:
pragmaTopLevel(context,stmt);
break;
case ExMParser.EOF:
break;
default :
if (!TopLevel.isStatement(type)) {
throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
}
}
}
}","/** 
 * First pass: - Register (but don't compile) all functions and other definitions
 * @param context
 * @param module
 * @throws UserException
 * @throws DoubleDefineException
 * @throws UndefinedTypeException
 */
private void walkTopLevelDefs(GlobalContext context,ParsedModule module) throws UserException {
  assert(module.ast.getType() == ExMParser.PROGRAM);
  syncFilePos(context,module.ast);
  for (  SwiftAST stmt : module.ast.children()) {
    int type=stmt.getType();
    syncFilePos(context,stmt);
switch (type) {
case ExMParser.IMPORT:
      importModule(context,null,stmt,FrontendPass.DEFINITIONS);
    break;
case ExMParser.DEFINE_BUILTIN_FUNCTION:
  defineBuiltinFunction(context,stmt);
break;
case ExMParser.DEFINE_FUNCTION:
defineFunction(context,stmt);
break;
case ExMParser.DEFINE_APP_FUNCTION:
defineAppFunction(context,stmt);
break;
case ExMParser.DEFINE_NEW_STRUCT_TYPE:
defineNewStructType(context,stmt);
break;
case ExMParser.DEFINE_NEW_TYPE:
case ExMParser.TYPEDEF:
defineNewType(context,stmt,type == ExMParser.TYPEDEF);
break;
case ExMParser.GLOBAL_CONST:
globalConst(context,stmt);
break;
case ExMParser.PRAGMA:
pragmaTopLevel(context,stmt);
break;
case ExMParser.EOF:
break;
default :
if (!TopLevel.isStatement(type)) {
throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
}
}
}
}",0.9576837416481068
132538,"/** 
 * Walk the statements in a file.
 * @param context
 * @param topLevelContext required for some passes
 * @param module the parsed file to compile
 * @param pass controls whether we just load top-level defs, or whetherwe attempt to compile the module
 * @throws UserException
 */
private void walkFile(GlobalContext context,LocalContext topLevelContext,LocatedModule module,ParsedModule parsed,FrontendPass pass) throws UserException {
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
  modules.enterModule(module,parsed);
  walkTopLevel(context,topLevelContext,parsed.ast,pass);
  modules.exitModule();
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
}","/** 
 * Walk the statements in a file.
 * @param context
 * @param topLevelContext required for some passes
 * @param module the parsed file to compile
 * @param pass controls whether we just load top-level defs, or whetherwe attempt to compile the module
 * @throws UserException
 */
private void walkFile(GlobalContext context,LocalContext topLevelContext,LocatedModule module,ParsedModule parsed,FrontendPass pass) throws UserException {
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
  modules.enterModule(module,parsed);
  walkTopLevel(context,topLevelContext,parsed,pass);
  modules.exitModule();
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
}",0.9973297730307076
132539,"/** 
 * Third pass: - Compile composite and app functions, now that all function names and globals are known
 * @param context
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileFunctions(GlobalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  for (  SwiftAST stmt : fileTree.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (type == ExMParser.DEFINE_FUNCTION) {
      compileFunction(context,stmt);
    }
 else     if (type == ExMParser.DEFINE_APP_FUNCTION) {
      compileAppFunction(context,stmt);
    }
 else     if (TopLevel.isStatement(type) || TopLevel.isDefinition(type)) {
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
}","/** 
 * Third pass: - Compile composite and app functions, now that all function names and globals are known
 * @param context
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileFunctions(GlobalContext context,ParsedModule module) throws UserException {
  assert(module.ast.getType() == ExMParser.PROGRAM);
  syncFilePos(context,module.ast);
  for (  SwiftAST stmt : module.ast.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (type == ExMParser.DEFINE_FUNCTION) {
      compileFunction(context,stmt);
    }
 else     if (type == ExMParser.DEFINE_APP_FUNCTION) {
      compileAppFunction(context,stmt);
    }
 else     if (TopLevel.isStatement(type) || TopLevel.isDefinition(type)) {
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
}",0.9409722222222222
132540,"/** 
 * Walk the AST and make calls to backend to generate lower level code. This function is called to start the walk at the top level file
 * @param mainFilePath the main file path to process
 * @param originalMainFilePath original main file, in case a temporary fileis being directly parsed
 * @param preprocessed true if module was preprocessed
 * @throws UserException
 */
public void walk(String mainFilePath,String originalMainFilePath,boolean preprocessed) throws UserException {
  GlobalContext context=new GlobalContext(mainFilePath,Logging.getSTCLogger(),foreignFuncs);
  String mainModuleName=FilenameUtils.getBaseName(originalMainFilePath);
  LocatedModule mainModule=new LocatedModule(mainFilePath,mainModuleName,preprocessed);
  LocatedModule builtins=LocatedModule.fromPath(context,Arrays.asList(""String_Node_Str""),false);
  loadDefinitions(context,mainModule,builtins);
  compileTopLevel(context,mainModule);
  compileFunctions(context);
}","/** 
 * Walk the AST and make calls to backend to generate lower level code. This function is called to start the walk at the top level file
 * @param mainFilePath the main file path to process
 * @param originalMainFilePath original main file, in case a temporary fileis being directly parsed
 * @param preprocessed true if module was preprocessed
 * @throws UserException
 */
public void walk(String mainFilePath,String originalMainFilePath,boolean preprocessed) throws UserException {
  GlobalContext context=new GlobalContext(mainFilePath,Logging.getSTCLogger(),foreignFuncs);
  String mainModuleName=FilenameUtils.getBaseName(originalMainFilePath);
  LocatedModule mainModule=new LocatedModule(mainFilePath,mainModuleName,preprocessed);
  LocatedModule builtins=LocatedModule.fromPath(context,Arrays.asList(""String_Node_Str""),false);
  loadDefinitions(context,mainModule,builtins);
  compileTopLevel(context,mainModule,builtins);
  compileFunctions(context);
}",0.9953149401353462
132541,"private void assignExpression(Context context,VariableUsageInfo vu,SwiftAST tree) throws UserException {
  if (tree.getChildCount() < 2)   throw new STCRuntimeError(""String_Node_Str"");
  Assignment assignments=Assignment.fromAST(context,tree);
  for (  Pair<List<LValue>,SwiftAST> assign : assignments.getMatchedAssignments(context)) {
    List<LValue> lVals=assign.val1;
    SwiftAST rVal=assign.val2;
    Type rValT=TypeChecker.findExprType(context,lVals,rVal);
    List<Type> rValFields=TupleType.getFields(rValT);
    walkExpr(context,vu,rVal);
    Map<String,Type> rValTVBindings=new TreeMap<String,Type>();
    for (int i=0; i < lVals.size(); i++) {
      LValue lVal=lVals.get(i);
      syncFilePos(context,lVal.tree);
      if (lVal.var == null) {
        lVal=lVal.varDeclarationNeeded(context,rValFields.get(i));
        assert(lVal != null);
        vu.declare(context,lVal.var.name(),lVal.var.type(),false);
        context.declareVariable(lVal.var.type(),lVal.var.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.unknown(),false);
      }
 else {
        TypeChecker.checkAssignment(context,assignments.op,rValT,lVal.getType(context),lVal.toString(),rValTVBindings);
      }
      singleAssignment(context,vu,lVal,assignments.op);
    }
  }
}","private void assignExpression(Context context,VariableUsageInfo vu,SwiftAST tree) throws UserException {
  if (tree.getChildCount() < 2)   throw new STCRuntimeError(""String_Node_Str"");
  Assignment assignments=Assignment.fromAST(context,tree);
  for (  Pair<List<LValue>,SwiftAST> assign : assignments.getMatchedAssignments(context)) {
    List<LValue> lVals=assign.val1;
    SwiftAST rVal=assign.val2;
    Type rValT=TypeChecker.findExprType(context,lVals,rVal);
    List<Type> rValFields=TupleType.getFields(rValT);
    walkExpr(context,vu,rVal);
    Map<String,Type> rValTVBindings=new TreeMap<String,Type>();
    for (int i=0; i < lVals.size(); i++) {
      LValue lVal=lVals.get(i);
      Type rValFieldType=rValFields.get(i);
      syncFilePos(context,lVal.tree);
      if (lVal.var == null) {
        lVal=lVal.varDeclarationNeeded(context,rValFieldType);
        assert(lVal != null);
        vu.declare(context,lVal.var.name(),lVal.var.type(),false);
        context.declareVariable(lVal.var.type(),lVal.var.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.unknown(),false);
      }
 else {
        TypeChecker.checkAssignment(context,assignments.op,rValFieldType,lVal.getType(context),lVal.toString(),rValTVBindings);
      }
      singleAssignment(context,vu,lVal,assignments.op);
    }
  }
}",0.9742589703588144
132542,"/** 
 * Match abstract argument types to provided argument expressions, Expand variable-length arguments if needed.  Leaves nulls if optional argument was omitted.
 * @param context
 * @param overload
 * @param argTypes
 * @param kwArgTypes
 * @param throwOnFail on failure, return null if falseor throw exception if true
 * @return
 * @throws TypeMismatchException
 * @throws InvalidConstructException
 */
private static List<MatchedArg> matchArgs(Context context,FnOverload overload,List<Type> argTypes,Map<String,Type> kwArgTypes,boolean throwOnFail) throws TypeMismatchException {
  FunctionType fnType=overload.type;
  DefaultVals<Var> defaultVals=overload.defaultVals;
  assert(!fnType.hasVarargs() || !defaultVals.hasAnyDefaults());
  List<Type> abstractInputs=fnType.getInputs();
  int numPosArgs=argTypes.size();
  int numKwArgs=kwArgTypes.size();
  int numTotalArgs=numPosArgs + numKwArgs;
  boolean fixedLength=!defaultVals.hasAnyDefaults() && !fnType.hasVarargs();
  int minNumArgs;
  if (defaultVals.hasAnyDefaults()) {
    minNumArgs=defaultVals.firstDefault();
  }
 else   if (fnType.hasVarargs()) {
    minNumArgs=fnType.getInputs().size() - 1;
  }
 else {
    minNumArgs=fnType.getInputs().size();
  }
  if (numTotalArgs < minNumArgs) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ (fixedLength ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str""+ minNumArgs+ ""String_Node_Str""+ numTotalArgs);
    }
    return null;
  }
  if (!fnType.hasVarargs() && numPosArgs > abstractInputs.size()) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ abstractInputs.size()+ ""String_Node_Str""+ numPosArgs+ ""String_Node_Str"");
    }
    return null;
  }
  int numMatchedArgs=Math.max(numTotalArgs,fnType.getInputs().size());
  Set<String> unmatchedKwArgs=new HashSet<String>(kwArgTypes.keySet());
  MatchedArg matched[]=new MatchedArg[numMatchedArgs];
  for (int i=0; i < numMatchedArgs; i++) {
    String name;
    Type formalArgType;
    Type argExprType;
    if (i < numPosArgs) {
      argExprType=argTypes.get(i);
      int formalArgIx;
      if (i < abstractInputs.size()) {
        formalArgIx=i;
      }
 else {
        assert(fnType.hasVarargs());
        formalArgIx=abstractInputs.size() - 1;
      }
      name=overload.inArgNames.get(formalArgIx);
      formalArgType=abstractInputs.get(formalArgIx);
    }
 else {
      assert(!fnType.hasVarargs());
      name=overload.inArgNames.get(i);
      boolean hasDefault=overload.defaultVals.defaultVals().get(i) != null;
      if (!hasDefault) {
        throw new TypeMismatchException(context,""String_Node_Str"" + name + ""String_Node_Str""+ overload.id.originalName());
      }
      formalArgType=abstractInputs.get(i);
      argExprType=kwArgTypes.get(name);
      if (argExprType != null) {
        unmatchedKwArgs.remove(name);
      }
    }
    matched[i]=new MatchedArg(name,formalArgType,argExprType);
  }
  if (unmatchedKwArgs.size() > 0) {
    throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ unmatchedKwArgs);
  }
  return Arrays.asList(matched);
}","/** 
 * Match abstract argument types to provided argument expressions, Expand variable-length arguments if needed.  Leaves nulls if optional argument was omitted.
 * @param context
 * @param overload
 * @param argTypes
 * @param kwArgTypes
 * @param throwOnFail on failure, return null if falseor throw exception if true
 * @return
 * @throws TypeMismatchException
 * @throws InvalidConstructException
 */
private static List<MatchedArg> matchArgs(Context context,FnOverload overload,List<Type> argTypes,Map<String,Type> kwArgTypes,boolean throwOnFail) throws TypeMismatchException {
  FunctionType fnType=overload.type;
  DefaultVals<Var> defaultVals=overload.defaultVals;
  assert(!fnType.hasVarargs() || !defaultVals.hasAnyDefaults());
  List<Type> abstractInputs=fnType.getInputs();
  int numPosArgs=argTypes.size();
  int numKwArgs=kwArgTypes.size();
  int numTotalArgs=numPosArgs + numKwArgs;
  boolean fixedLength=!defaultVals.hasAnyDefaults() && !fnType.hasVarargs();
  int minNumArgs;
  if (defaultVals.hasAnyDefaults()) {
    minNumArgs=defaultVals.firstDefault();
  }
 else   if (fnType.hasVarargs()) {
    minNumArgs=fnType.getInputs().size() - 1;
  }
 else {
    minNumArgs=fnType.getInputs().size();
  }
  if (numTotalArgs < minNumArgs) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ (fixedLength ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str""+ minNumArgs+ ""String_Node_Str""+ numTotalArgs);
    }
    return null;
  }
  if (!fnType.hasVarargs() && numPosArgs > abstractInputs.size()) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ abstractInputs.size()+ ""String_Node_Str""+ numPosArgs+ ""String_Node_Str"");
    }
    return null;
  }
  int numMatchedArgs=fnType.hasVarargs() ? numTotalArgs : fnType.getInputs().size();
  Set<String> unmatchedKwArgs=new HashSet<String>(kwArgTypes.keySet());
  MatchedArg matched[]=new MatchedArg[numMatchedArgs];
  for (int i=0; i < numMatchedArgs; i++) {
    String name;
    Type formalArgType;
    Type argExprType;
    if (i < numPosArgs) {
      argExprType=argTypes.get(i);
      int formalArgIx;
      if (i < abstractInputs.size()) {
        formalArgIx=i;
      }
 else {
        assert(fnType.hasVarargs());
        formalArgIx=abstractInputs.size() - 1;
      }
      name=overload.inArgNames.get(formalArgIx);
      formalArgType=abstractInputs.get(formalArgIx);
    }
 else {
      assert(!fnType.hasVarargs());
      name=overload.inArgNames.get(i);
      boolean hasDefault=overload.defaultVals.defaultVals().get(i) != null;
      if (!hasDefault) {
        throw new TypeMismatchException(context,""String_Node_Str"" + name + ""String_Node_Str""+ overload.id.originalName());
      }
      formalArgType=abstractInputs.get(i);
      argExprType=kwArgTypes.get(name);
      if (argExprType != null) {
        unmatchedKwArgs.remove(name);
      }
    }
    matched[i]=new MatchedArg(name,formalArgType,argExprType);
  }
  if (unmatchedKwArgs.size() > 0) {
    throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ unmatchedKwArgs);
  }
  return Arrays.asList(matched);
}",0.9909936955869107
132543,"/** 
 * Evaluate any task properties.  Put values into propVals.
 * @param context
 * @param fc
 * @param propVals
 * @param renames
 * @throws UserException
 * @returns true if a wait statement was opened
 */
private boolean evalCallProperties(Context context,FnID id,FunctionCall fc,TaskProps propVals,Map<String,String> renames) throws UserException {
  if (fc.annotations().isEmpty()) {
    return false;
  }
  List<Pair<TaskPropKey,Var>> propFutures=new ArrayList<Pair<TaskPropKey,Var>>();
  List<Var> waitVars=new ArrayList<Var>();
  Var locationVar=null;
  for (  TaskPropKey ann : fc.annotations().keySet()) {
    checkCallAnnotation(context,id,fc,ann);
    SwiftAST expr=fc.annotations().get(ann);
    Type exprType=TypeChecker.findExprType(context,expr);
    Type concreteType=TaskProp.checkFrontendType(context,ann,exprType);
    Var future=exprWalker.eval(context,expr,concreteType,false,renames);
    waitVars.add(future);
    propFutures.add(Pair.create(ann,future));
  }
  if (fc.location() != null) {
    checkCanTarget(context,id);
    locationVar=exprWalker.eval(context,fc.location(),Types.F_LOCATION,false,renames);
    waitVars.add(locationVar);
  }
  backend.startWaitStatement(context.constructName(""String_Node_Str""),VarRepr.backendVars(waitVars),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedControl());
  for (  Pair<TaskPropKey,Var> x : propFutures) {
    Var value=exprWalker.retrieveToVar(context,x.val2);
    propVals.put(x.val1,value.asArg());
  }
  if (locationVar != null) {
    populateFromLocationStruct(context,locationVar,propVals);
  }
  if (fc.softLocationOverride()) {
    propVals.put(TaskPropKey.LOC_STRICTNESS,TaskProps.LOC_STRICTNESS_SOFT_ARG);
  }
  return true;
}","/** 
 * Evaluate any task properties.  Put values into propVals.
 * @param context
 * @param fc
 * @param propVals
 * @param renames
 * @throws UserException
 * @returns true if a wait statement was opened
 */
private boolean evalCallProperties(Context context,FnID id,FunctionCall fc,TaskProps propVals,Map<String,String> renames) throws UserException {
  List<Pair<TaskPropKey,Var>> propFutures=new ArrayList<Pair<TaskPropKey,Var>>();
  List<Var> waitVars=new ArrayList<Var>();
  Var locationVar=null;
  for (  TaskPropKey ann : fc.annotations().keySet()) {
    checkCallAnnotation(context,id,fc,ann);
    SwiftAST expr=fc.annotations().get(ann);
    Type exprType=TypeChecker.findExprType(context,expr);
    Type concreteType=TaskProp.checkFrontendType(context,ann,exprType);
    Var future=exprWalker.eval(context,expr,concreteType,false,renames);
    waitVars.add(future);
    propFutures.add(Pair.create(ann,future));
  }
  if (fc.location() != null) {
    checkCanTarget(context,id);
    locationVar=exprWalker.eval(context,fc.location(),Types.F_LOCATION,false,renames);
    waitVars.add(locationVar);
  }
  if (waitVars.isEmpty()) {
    return false;
  }
  backend.startWaitStatement(context.constructName(""String_Node_Str""),VarRepr.backendVars(waitVars),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedControl());
  for (  Pair<TaskPropKey,Var> x : propFutures) {
    Var value=exprWalker.retrieveToVar(context,x.val2);
    propVals.put(x.val1,value.asArg());
  }
  if (locationVar != null) {
    populateFromLocationStruct(context,locationVar,propVals);
  }
  if (fc.softLocationOverride()) {
    propVals.put(TaskPropKey.LOC_STRICTNESS,TaskProps.LOC_STRICTNESS_SOFT_ARG);
  }
  return true;
}",0.968494749124854
132544,"/** 
 * Overload resolution test with some matching args
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadPartialMatch() throws TypeMismatchException {
  FunctionType stringFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_STRING);
  FunctionType blobFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_BLOB);
  FnID blobFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_INT,Types.F_FILE,Types.F_STRING),Arrays.asList(Pair.create(blobFnID,blobFn),Pair.create(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.id.equals(stringFnID));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(stringFn));
}","/** 
 * Overload resolution test with some matching args
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadPartialMatch() throws TypeMismatchException {
  FunctionType stringFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_STRING);
  FunctionType blobFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_BLOB);
  FnID blobFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_INT,Types.F_FILE,Types.F_STRING),Arrays.asList(Pair.create(blobFnID,blobFn),Pair.create(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(stringFnID,match.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}",0.834619625137817
132545,"/** 
 * Check that union args are handled correctly when resolving overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion1() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_STRING),Arrays.asList(Pair.create(intFnID,intFn),Pair.create(floatFnID,floatFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.id.equals(intFnID));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(intFn));
}","/** 
 * Check that union args are handled correctly when resolving overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion1() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_STRING),Arrays.asList(Pair.create(intFnID,intFn),Pair.create(floatFnID,floatFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(intFnID,match.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
}",0.8154402895054282
132546,"/** 
 * Check that union args are handled correctly when they could match multiple overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion2() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  Pair<FnID,FunctionType> intPair=Pair.create(intFnID,intFn);
  Pair<FnID,FunctionType> floatPair=Pair.create(floatFnID,floatFn);
  List<Pair<FnID,FunctionType>> overloadList=Arrays.asList(intPair,floatPair);
  List<Pair<FnID,FunctionType>> overloadListRev=Arrays.asList(floatPair,intPair);
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadList);
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.id.equals(intFnID));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(intFn));
  FnCallInfo fc2=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadListRev);
  List<FnMatch> matches2=concretiseInputsOverloaded(FAKE_CONTEXT,fc2,true);
  assert(matches2.size() == 0);
  FnMatch match2=matches2.get(0);
  assert(match2.id.equals(intFnID));
  assert(match2.concreteAlts.size() == 1);
  assert(match2.concreteAlts.get(0).equals(intFn));
  FnCallInfo fc3=makeOverloadedFnCallInfo(Arrays.asList(FLOAT_OR_INT),overloadListRev);
  List<FnMatch> matches3=concretiseInputsOverloaded(FAKE_CONTEXT,fc3,true);
  assert(matches2.size() == 0);
  FnMatch match3=matches3.get(0);
  assert(match3.id.equals(floatFnID));
  assert(match3.concreteAlts.size() == 1);
  assert(match3.concreteAlts.get(0).equals(floatFn));
}","/** 
 * Check that union args are handled correctly when they could match multiple overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion2() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  Pair<FnID,FunctionType> intPair=Pair.create(intFnID,intFn);
  Pair<FnID,FunctionType> floatPair=Pair.create(floatFnID,floatFn);
  List<Pair<FnID,FunctionType>> overloadList=Arrays.asList(intPair,floatPair);
  List<Pair<FnID,FunctionType>> overloadListRev=Arrays.asList(floatPair,intPair);
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadList);
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(intFnID,match.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
  FnCallInfo fc2=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadListRev);
  List<FnMatch> matches2=concretiseInputsOverloaded(FAKE_CONTEXT,fc2,true);
  assertEquals(1,matches2.size());
  FnMatch match2=matches2.get(0);
  assertEquals(intFnID,match2.id);
  assertEquals(1,match2.concreteAlts.size());
  assertEquals(intFn,match2.concreteAlts.get(0));
  FnCallInfo fc3=makeOverloadedFnCallInfo(Arrays.asList(FLOAT_OR_INT),overloadListRev);
  List<FnMatch> matches3=concretiseInputsOverloaded(FAKE_CONTEXT,fc3,true);
  assertEquals(1,matches2.size());
  FnMatch match3=matches3.get(0);
  assertEquals(floatFnID,match3.id);
  assertEquals(1,match3.concreteAlts.size());
  assertEquals(floatFn,match3.concreteAlts.get(0));
}",0.8454293628808864
132547,"/** 
 * Simple overload resolution test
 * @throws TypeMismatchException
 */
@Test public void testSelectOverload() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING),Arrays.asList(Pair.create(intFnID,intFn),Pair.create(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.id.equals(stringFnID));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(stringFn));
}","/** 
 * Simple overload resolution test
 * @throws TypeMismatchException
 */
@Test public void testSelectOverload() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING),Arrays.asList(Pair.create(intFnID,intFn),Pair.create(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(stringFnID,match.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}",0.8007518796992481
132548,"private void defineBuiltinFunction(Context context,SwiftAST tree) throws UserException {
  final int REQUIRED_CHILDREN=5;
  assert(tree.getChildCount() >= REQUIRED_CHILDREN);
  String function=tree.child(0).getText();
  SwiftAST typeParamsT=tree.child(1);
  SwiftAST outputs=tree.child(2);
  SwiftAST inputs=tree.child(3);
  SwiftAST tclPackage=tree.child(4);
  assert(inputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(outputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(tclPackage.getType() == ExMParser.TCL_PACKAGE);
  assert(tclPackage.getChildCount() == 2);
  Set<String> typeParams=extractTypeParams(typeParamsT);
  FunctionDecl fdecl=FunctionDecl.fromAST(context,varCreator,exprWalker,function,inputs,outputs,typeParams);
  FunctionType ft=fdecl.getFunctionType();
  LogHelper.debug(context,""String_Node_Str"" + function + ""String_Node_Str""+ ft);
  FnID fid=context.defineFunction(function,ft,fdecl.defaultVals());
  tree.setIdentifier(fid);
  String pkg=Literals.extractLiteralString(context,tclPackage.child(0));
  String version=Literals.extractLiteralString(context,tclPackage.child(1));
  backend.requirePackage(new TclPackage(pkg,version));
  int pos=REQUIRED_CHILDREN;
  TclFunRef impl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.TCL_FUN_REF) {
    SwiftAST tclImplRef=tree.child(pos);
    String symbol=Literals.extractLiteralString(context,tclImplRef.child(0));
    impl=new TclFunRef(pkg,symbol,version);
    pos++;
  }
  TclOpTemplate inlineTcl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.INLINE_TCL) {
    SwiftAST inlineTclTree=tree.child(pos);
    inlineTcl=wrapper.loadTclTemplate(context,fid,fdecl,ft,inlineTclTree);
    pos++;
  }
  FunctionType backendFT=VarRepr.backendFnType(ft);
  backend.defineBuiltinFunction(fid,backendFT,inlineTcl,impl);
  context.getForeignFunctions().addForeignFunction(fid);
  for (; pos < tree.getChildCount(); pos++) {
    handleBuiltinFunctionAnnotation(context,fid,fdecl,tree.child(pos),inlineTcl != null);
  }
  ExecTarget taskMode=context.getForeignFunctions().getTaskMode(fid);
  boolean isTargetable=false;
  if (taskMode.isDispatched()) {
    isTargetable=true;
    context.setFunctionProperty(fid,FnProp.TARGETABLE);
  }
  if (impl != null) {
    context.setFunctionProperty(fid,FnProp.BUILTIN);
  }
 else {
    if (inlineTcl == null) {
      throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + fid.originalName());
    }
    context.setFunctionProperty(fid,FnProp.WRAPPED_BUILTIN);
    context.setFunctionProperty(fid,FnProp.SYNC);
    boolean isParallel=context.hasFunctionProp(fid,FnProp.PARALLEL);
    if (isParallel && (!taskMode.isAsync() || !taskMode.targetContext().isAnyWorkContext())) {
      throw new UserException(context,""String_Node_Str"");
    }
    wrapper.saveWrapper(context,fid,backendFT,fdecl,taskMode,isParallel,isTargetable);
  }
}","private void defineBuiltinFunction(Context context,SwiftAST tree) throws UserException {
  final int REQUIRED_CHILDREN=5;
  assert(tree.getChildCount() >= REQUIRED_CHILDREN);
  String function=tree.child(0).getText();
  SwiftAST typeParamsT=tree.child(1);
  SwiftAST outputs=tree.child(2);
  SwiftAST inputs=tree.child(3);
  SwiftAST tclPackage=tree.child(4);
  assert(inputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(outputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(tclPackage.getType() == ExMParser.TCL_PACKAGE);
  assert(tclPackage.getChildCount() == 2);
  Set<String> typeParams=extractTypeParams(typeParamsT);
  FunctionDecl fdecl=FunctionDecl.fromAST(context,varCreator,exprWalker,function,inputs,outputs,typeParams);
  FunctionType ft=fdecl.getFunctionType();
  LogHelper.debug(context,""String_Node_Str"" + function + ""String_Node_Str""+ ft);
  FnID fid=context.defineFunction(function,ft,fdecl.defaultVals());
  tree.setIdentifier(fid);
  if (function.equals(""String_Node_Str"")) {
    System.err.println(""String_Node_Str"" + fid);
  }
  String pkg=Literals.extractLiteralString(context,tclPackage.child(0));
  String version=Literals.extractLiteralString(context,tclPackage.child(1));
  backend.requirePackage(new TclPackage(pkg,version));
  int pos=REQUIRED_CHILDREN;
  TclFunRef impl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.TCL_FUN_REF) {
    SwiftAST tclImplRef=tree.child(pos);
    String symbol=Literals.extractLiteralString(context,tclImplRef.child(0));
    impl=new TclFunRef(pkg,symbol,version);
    pos++;
  }
  TclOpTemplate inlineTcl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.INLINE_TCL) {
    SwiftAST inlineTclTree=tree.child(pos);
    inlineTcl=wrapper.loadTclTemplate(context,fid,fdecl,ft,inlineTclTree);
    pos++;
  }
  FunctionType backendFT=VarRepr.backendFnType(ft);
  backend.defineBuiltinFunction(fid,backendFT,inlineTcl,impl);
  context.getForeignFunctions().addForeignFunction(fid);
  for (; pos < tree.getChildCount(); pos++) {
    handleBuiltinFunctionAnnotation(context,fid,fdecl,tree.child(pos),inlineTcl != null);
  }
  ExecTarget taskMode=context.getForeignFunctions().getTaskMode(fid);
  boolean isTargetable=false;
  if (taskMode.isDispatched()) {
    isTargetable=true;
    context.setFunctionProperty(fid,FnProp.TARGETABLE);
  }
  if (impl != null) {
    context.setFunctionProperty(fid,FnProp.BUILTIN);
  }
 else {
    if (inlineTcl == null) {
      throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + fid.originalName());
    }
    context.setFunctionProperty(fid,FnProp.WRAPPED_BUILTIN);
    context.setFunctionProperty(fid,FnProp.SYNC);
    boolean isParallel=context.hasFunctionProp(fid,FnProp.PARALLEL);
    if (isParallel && (!taskMode.isAsync() || !taskMode.targetContext().isAnyWorkContext())) {
      throw new UserException(context,""String_Node_Str"");
    }
    wrapper.saveWrapper(context,fid,backendFT,fdecl,taskMode,isParallel,isTargetable);
  }
}",0.9837166358905488
132549,"/** 
 * Helper to choose variable name.
 * @param prefix Prefix that must be at start
 * @param preferredSuffix Preferred suffix
 * @param counterName name of counter to use to make unique if needed
 * @return
 */
protected String uniqueName(String prefix,String preferredSuffix,String counterName){
  if (preferredSuffix != null) {
    prefix+=preferredSuffix;
    if (lookupDef(prefix) == null) {
      return prefix;
    }
  }
  String name=null;
  do {
    long counter=nextCounterVal(counterName);
    name=prefix + counter;
  }
 while (lookupDef(name) != null);
  return name;
}","/** 
 * Helper to choose variable name.
 * @param prefix Prefix that must be at start
 * @param preferredSuffix Preferred suffix
 * @param counterName name of counter to use to make unique if needed
 * @return
 */
protected String uniqueName(String prefix,String preferredSuffix,String counterName){
  if (preferredSuffix != null) {
    prefix+=preferredSuffix;
  }
  if (lookupDef(prefix) == null) {
    return prefix;
  }
  String name=null;
  do {
    long counter=nextCounterVal(counterName);
    name=prefix + counter;
  }
 while (lookupDef(name) != null);
  return name;
}",0.9672977624784854
132550,"@Override public FnID defineFunction(String name,FunctionType type,DefaultVals<Var> defaultVals) throws UserException {
  DefInfo def=lookupDef(name);
  if (def != null && def.kind == DefKind.FUNCTION) {
    return overloadFunction(name,type,defaultVals);
  }
 else {
    declareVariable(type,name,Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.userVar(getSourceLoc()),false);
    FnID fnID=new FnID(name,name);
    addFunctionOverload(name,fnID,type,defaultVals);
    return fnID;
  }
}","@Override public FnID defineFunction(String name,FunctionType type,DefaultVals<Var> defaultVals) throws UserException {
  FnID fnID;
  DefInfo def=lookupDef(name);
  if (def != null && def.kind == DefKind.FUNCTION) {
    fnID=overloadFunction(name,type,defaultVals);
    declareVariable(type,fnID.uniqueName(),Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.userVar(getSourceLoc()),false);
  }
 else {
    declareVariable(type,name,Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.userVar(getSourceLoc()),false);
    fnID=new FnID(name,name);
    addFunctionOverload(name,fnID,type,defaultVals);
  }
  return fnID;
}",0.7992863514719001
132551,"private static void noMatchingOverloadsException(Context context,FnCallInfo fc) throws TypeMismatchException {
  StringBuilder sb=new StringBuilder();
  sb.append(""String_Node_Str"");
  sb.append(typeList(fc.argTypes));
  sb.append(""String_Node_Str"" + fc.name + ""String_Node_Str"");
  sb.append(""String_Node_Str"");
  for (  FnOverload fnType : fc.fnTypes) {
    sb.append(typeList(fnType.type.getInputs()));
    sb.append('\n');
  }
  throw new TypeMismatchException(context,sb.toString());
}","private static TypeMismatchException noMatchingOverloadsException(Context context,FnCallInfo fc){
  StringBuilder sb=new StringBuilder();
  sb.append(""String_Node_Str"");
  sb.append(typeList(fc.argTypes));
  sb.append(""String_Node_Str"" + fc.name + ""String_Node_Str"");
  sb.append(""String_Node_Str"");
  for (  FnOverload fnType : fc.fnTypes) {
    sb.append(typeList(fnType.type.getInputs()));
    sb.append('\n');
  }
  return new TypeMismatchException(context,sb.toString());
}",0.2045454545454545
132552,"public static ConcreteMatch concretiseFunctionCall(Context context,FunctionCall fc,List<Var> outputs) throws UserException {
}","public static ConcreteMatch concretiseFunctionCall(Context context,FunctionCall fc,List<Var> outputs) throws UserException {
  FnMatch match=concretiseInputs(context,fc);
  List<Type> outTs=Var.extractTypes(outputs);
  FunctionType concreteOutTs=concretiseOutputs(context,match,outputs,outTs);
  return new ConcreteMatch(match.overload.id,concreteOutTs,match.overload.defaultVals);
}",0.4950884086444008
132553,"/** 
 * Narrow possible function call types based on inputs.
 * @param context
 * @param fc
 * @param resolveOverload if true, resolve to a single overload
 * @return list of possible concrete function types with varargs, typevarsand union type args removed.  Grouped by which overload they match.  Each match should have at least one concrete function type associated with it.
 * @throws UserException
 */
private static List<FnMatch> concretiseInputs(Context context,FunctionCall fc,boolean resolveOverload) throws UserException {
  List<Type> argTypes=new ArrayList<Type>(fc.args().size());
  for (  SwiftAST arg : fc.args()) {
    argTypes.add(TypeChecker.findExprType(context,arg));
  }
  FnCallInfo info=new FnCallInfo(fc.originalName(),fc.overloads(),argTypes);
  return concretiseInputsOverloaded(context,info,resolveOverload);
}","/** 
 * Narrow possible function call types based on inputs.
 * @param context
 * @param fc
 * @param resolveOverload if true, resolve to a single overload
 * @return list of possible concrete function types with varargs, typevarsand union type args removed.  Grouped by which overload they match.  Each match should have at least one concrete function type associated with it.
 * @throws UserException
 */
private static FnMatch concretiseInputs(Context context,FunctionCall fc) throws UserException {
  List<Type> argTypes=new ArrayList<Type>(fc.args().size());
  for (  SwiftAST arg : fc.args()) {
    argTypes.add(TypeChecker.findExprType(context,arg));
  }
  FnCallInfo info=new FnCallInfo(fc.originalName(),fc.overloads(),argTypes);
  return concretiseInputsOverloaded(context,info);
}",0.9717444717444718
132554,"static List<FnMatch> concretiseInputsOverloaded(Context context,FnCallInfo fc,boolean resolveOverload) throws TypeMismatchException {
}","/** 
 * Resolve overloaded function call to a single overload based on input arguments (output arguments are *not* used to resolve overloads).
 * @param context
 * @param fc
 * @return
 * @throws TypeMismatchException
 */
static FnMatch concretiseInputsOverloaded(Context context,FnCallInfo fc) throws TypeMismatchException {
}",0.4242424242424242
132555,"public static Type findFuncCallExprType(Context context,SwiftAST tree) throws UndefinedFunctionException, UserException {
  FunctionCall f=FunctionCall.fromAST(context,tree,false);
  return exprTypeFromMatches(concretiseInputs(context,f,false));
}","public static Type findFuncCallExprType(Context context,SwiftAST tree) throws UndefinedFunctionException, UserException {
  FunctionCall f=FunctionCall.fromAST(context,tree,false);
  return exprTypeFromMatch(concretiseInputs(context,f));
}",0.9835390946502056
132556,"@Test public void testSelectOverloadNoMatch2() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_FLOAT,Types.F_FLOAT),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
}","@Test public void testSelectOverloadNoMatch2() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_FLOAT,Types.F_FLOAT),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc);
}",0.995640802092415
132557,"/** 
 * Overload resolution test with some matching args
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadPartialMatch() throws TypeMismatchException {
  FunctionType stringFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_STRING);
  FunctionType blobFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_BLOB);
  FnID blobFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_INT,Types.F_FILE,Types.F_STRING),Arrays.asList(new FnOverload(blobFnID,blobFn),new FnOverload(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(stringFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}","/** 
 * Overload resolution test with some matching args
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadPartialMatch() throws TypeMismatchException {
  FunctionType stringFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_STRING);
  FunctionType blobFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_BLOB);
  FnID blobFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_INT,Types.F_FILE,Types.F_STRING),Arrays.asList(new FnOverload(blobFnID,blobFn),new FnOverload(stringFnID,stringFn)));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(stringFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}",0.8715336728919072
132558,"@Test public void testMatchOptional() throws TypeMismatchException {
  String name=""String_Node_Str"";
  FunctionType ft=makeSimpleFT(Types.F_INT,Types.F_BOOL);
  Var constVar=new Var(Types.F_BOOL,""String_Node_Str"",Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.unknown());
  DefaultVals<Var> defaults=DefaultVals.fromDefaultValVector(Arrays.asList(null,constVar));
  FnOverload fo=new FnOverload(new FnID(name,name),ft,defaults);
  FnCallInfo fc;
  List<FnMatch> matches;
  FnMatch match;
  List<Type> intBoolArgs=Arrays.asList(Types.F_INT,Types.F_BOOL);
  fc=new FnCallInfo(name,fo.asList(),intBoolArgs);
  matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(""String_Node_Str"",1,matches.size());
  match=matches.get(0);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",intBoolArgs,match.concreteAlts.get(0).getInputs());
  List<Type> intArgs=Arrays.asList(Types.F_INT);
  fc=new FnCallInfo(name,fo.asList(),intArgs);
  matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(""String_Node_Str"",1,matches.size());
  match=matches.get(0);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",intArgs,match.concreteAlts.get(0).getInputs());
}","@Test public void testMatchOptional() throws TypeMismatchException {
  String name=""String_Node_Str"";
  FunctionType ft=makeSimpleFT(Types.F_INT,Types.F_BOOL);
  Var constVar=new Var(Types.F_BOOL,""String_Node_Str"",Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.unknown());
  DefaultVals<Var> defaults=DefaultVals.fromDefaultValVector(Arrays.asList(null,constVar));
  FnOverload fo=new FnOverload(new FnID(name,name),ft,defaults);
  List<Type> intBoolArgs=Arrays.asList(Types.F_INT,Types.F_BOOL);
  FnCallInfo fc=new FnCallInfo(name,fo.asList(),intBoolArgs);
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",intBoolArgs,match.concreteAlts.get(0).getInputs());
  List<Type> intArgs=Arrays.asList(Types.F_INT);
  fc=new FnCallInfo(name,fo.asList(),intArgs);
  match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",intArgs,match.concreteAlts.get(0).getInputs());
}",0.6131511528608027
132559,"/** 
 * Check that union args are handled correctly when resolving overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion1() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_STRING),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(floatFnID,floatFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(intFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
}","/** 
 * Check that union args are handled correctly when resolving overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion1() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_STRING),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(floatFnID,floatFn)));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(intFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
}",0.8727498448168839
132560,"/** 
 * Check that union args are handled correctly when they could match multiple overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion2() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnOverload intOverload=new FnOverload(intFnID,intFn);
  FnOverload floatOverload=new FnOverload(floatFnID,floatFn);
  List<FnOverload> overloadList=Arrays.asList(intOverload,floatOverload);
  List<FnOverload> overloadListRev=Arrays.asList(floatOverload,intOverload);
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadList);
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(intFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
  FnCallInfo fc2=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadListRev);
  List<FnMatch> matches2=concretiseInputsOverloaded(FAKE_CONTEXT,fc2,true);
  assertEquals(1,matches2.size());
  FnMatch match2=matches2.get(0);
  assertEquals(intFnID,match2.overload.id);
  assertEquals(1,match2.concreteAlts.size());
  assertEquals(intFn,match2.concreteAlts.get(0));
  FnCallInfo fc3=makeOverloadedFnCallInfo(Arrays.asList(FLOAT_OR_INT),overloadListRev);
  List<FnMatch> matches3=concretiseInputsOverloaded(FAKE_CONTEXT,fc3,true);
  assertEquals(1,matches2.size());
  FnMatch match3=matches3.get(0);
  assertEquals(floatFnID,match3.overload.id);
  assertEquals(1,match3.concreteAlts.size());
  assertEquals(floatFn,match3.concreteAlts.get(0));
}","/** 
 * Check that union args are handled correctly when they could match multiple overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion2() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnOverload intOverload=new FnOverload(intFnID,intFn);
  FnOverload floatOverload=new FnOverload(floatFnID,floatFn);
  List<FnOverload> overloadList=Arrays.asList(intOverload,floatOverload);
  List<FnOverload> overloadListRev=Arrays.asList(floatOverload,intOverload);
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadList);
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(intFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
  FnCallInfo fc2=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadListRev);
  FnMatch match2=concretiseInputsOverloaded(FAKE_CONTEXT,fc2);
  assertEquals(intFnID,match2.overload.id);
  assertEquals(1,match2.concreteAlts.size());
  assertEquals(intFn,match2.concreteAlts.get(0));
  FnCallInfo fc3=makeOverloadedFnCallInfo(Arrays.asList(FLOAT_OR_INT),overloadListRev);
  FnMatch match3=concretiseInputsOverloaded(FAKE_CONTEXT,fc3);
  assertEquals(floatFnID,match3.overload.id);
  assertEquals(1,match3.concreteAlts.size());
  assertEquals(floatFn,match3.concreteAlts.get(0));
}",0.861956843038723
132561,"@Test public void testSelectOverloadVarArgs1() throws TypeMismatchException {
  FunctionType ft1=makeFT(Arrays.asList(Types.F_STRING,Types.F_STRING),true);
  FunctionType ft2=makeFT(Arrays.asList(FLOAT_OR_INT),true);
  FnID fid1=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID fid2=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING,Types.F_STRING),Arrays.asList(new FnOverload(fid1,ft1),new FnOverload(fid2,ft2)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.overload.id.equals(fid1));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(ft1));
}","@Test public void testSelectOverloadVarArgs1() throws TypeMismatchException {
  List<Type> twoStrings=Arrays.asList(Types.F_STRING,Types.F_STRING);
  FunctionType ft1=makeFT(twoStrings,true);
  FunctionType ft1NoVarArgs=makeFT(twoStrings,false);
  FunctionType ft2=makeFT(Arrays.asList(FLOAT_OR_INT),true);
  FnID fid1=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID fid2=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(twoStrings,Arrays.asList(new FnOverload(fid1,ft1),new FnOverload(fid2,ft2)));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(fid1,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(ft1NoVarArgs,match.concreteAlts.get(0));
}",0.6069057104913679
132562,"@Test public void testMatchNoVarArgs() throws TypeMismatchException {
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT,Types.F_FLOAT,Types.F_INT,Types.F_FLOAT));
  List<FnMatch> matches;
  matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,false);
  assertEquals(""String_Node_Str"",1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",Arrays.asList(Types.F_INT,Types.F_FLOAT,Types.F_INT,Types.F_FLOAT),match.concreteAlts.get(0).getInputs());
}","@Test public void testMatchNoVarArgs() throws TypeMismatchException {
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT,Types.F_FLOAT,Types.F_INT,Types.F_FLOAT));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",Arrays.asList(Types.F_INT,Types.F_FLOAT,Types.F_INT,Types.F_FLOAT),match.concreteAlts.get(0).getInputs());
}",0.891542288557214
132563,"@Test public void testSelectOverloadNoMatch() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_FLOAT),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
}","@Test public void testSelectOverloadNoMatch() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_FLOAT),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc);
}",0.9955237242614146
132564,"/** 
 * Simple overload resolution test
 * @throws TypeMismatchException
 */
@Test public void testSelectOverload() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(stringFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}","/** 
 * Simple overload resolution test
 * @throws TypeMismatchException
 */
@Test public void testSelectOverload() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(stringFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}",0.8599096191091027
132565,"@Test public void testMatchMultiVarArgs() throws TypeMismatchException {
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT));
  List<FnMatch> matches;
  matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(""String_Node_Str"",1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",Arrays.asList(Types.F_INT),match.concreteAlts.get(0).getInputs());
}","@Test public void testMatchMultiVarArgs() throws TypeMismatchException {
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",Arrays.asList(Types.F_INT),match.concreteAlts.get(0).getInputs());
}",0.6235294117647059
132566,"@Test public void testMatchVarArgsFail() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT,Types.F_STRING));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc,false);
}","@Test public void testMatchVarArgsFail() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT,Types.F_STRING));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc);
}",0.9884615384615384
132567,"/** 
 * Match abstract argument types to provided argument expressions, producing a list of argument types of the same length as the number of input arguments. Expand variable-length arguments or omit optional arguments if needed.
 * @param context
 * @param overload
 * @param argTypes
 * @param throwOnFail on failure, return null if falseor throw exception if true
 * @return
 * @throws TypeMismatchException
 */
private static List<Type> matchArgs(Context context,FnOverload overload,List<Type> argTypes,boolean throwOnFail) throws TypeMismatchException {
  FunctionType fnType=overload.type;
  DefaultVals<Var> defaultVals=overload.defaultVals;
  List<Type> abstractInputs=fnType.getInputs();
  int numArgs=argTypes.size();
  boolean fixedLength=!defaultVals.hasAnyDefaults() && !fnType.hasVarargs();
  int minNumArgs;
  if (defaultVals.hasAnyDefaults()) {
    minNumArgs=defaultVals.firstDefault();
  }
 else   if (fnType.hasVarargs()) {
    minNumArgs=fnType.getInputs().size() - 1;
  }
 else {
    minNumArgs=fnType.getInputs().size();
  }
  if (numArgs < abstractInputs.size() - 1) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ (fixedLength ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str""+ minNumArgs+ ""String_Node_Str""+ numArgs);
    }
    return null;
  }
  if (!fnType.hasVarargs() && numArgs > abstractInputs.size()) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ abstractInputs.size()+ ""String_Node_Str""+ numArgs);
    }
    return null;
  }
  if (fnType.hasVarargs()) {
    return expandVarArgs(abstractInputs,numArgs);
  }
 else {
    return abstractInputs.subList(0,numArgs);
  }
}","/** 
 * Match abstract argument types to provided argument expressions, producing a list of argument types of the same length as the number of input arguments. Expand variable-length arguments or omit optional arguments if needed.
 * @param context
 * @param overload
 * @param argTypes
 * @param throwOnFail on failure, return null if falseor throw exception if true
 * @return
 * @throws TypeMismatchException
 */
private static List<Type> matchArgs(Context context,FnOverload overload,List<Type> argTypes,boolean throwOnFail) throws TypeMismatchException {
  FunctionType fnType=overload.type;
  DefaultVals<Var> defaultVals=overload.defaultVals;
  List<Type> abstractInputs=fnType.getInputs();
  int numArgs=argTypes.size();
  boolean fixedLength=!defaultVals.hasAnyDefaults() && !fnType.hasVarargs();
  int minNumArgs;
  if (defaultVals.hasAnyDefaults()) {
    minNumArgs=defaultVals.firstDefault();
  }
 else   if (fnType.hasVarargs()) {
    minNumArgs=fnType.getInputs().size() - 1;
  }
 else {
    minNumArgs=fnType.getInputs().size();
  }
  if (numArgs < minNumArgs) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ (fixedLength ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str""+ minNumArgs+ ""String_Node_Str""+ numArgs);
    }
    return null;
  }
  if (!fnType.hasVarargs() && numArgs > abstractInputs.size()) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ abstractInputs.size()+ ""String_Node_Str""+ numArgs);
    }
    return null;
  }
  if (fnType.hasVarargs()) {
    return expandVarArgs(abstractInputs,numArgs);
  }
 else {
    return abstractInputs.subList(0,numArgs);
  }
}",0.9904240766073872
132568,"private static FnMatch concretiseInputsNonOverloaded(Context context,FnOverload overload,List<Type> argTypes,boolean throwOnFail) throws TypeMismatchException {
  List<Type> expandedInArgs=matchArgs(context,overload,argTypes,throwOnFail);
  if (expandedInArgs == null) {
    return null;
  }
  assert(expandedInArgs.size() == argTypes.size());
  List<Type> concreteArgTypes=new ArrayList<Type>(argTypes.size());
  MultiMap<String,Type> tvConstraints=new MultiMap<String,Type>();
  for (int i=0; i < argTypes.size(); i++) {
    Type exp=expandedInArgs.get(i);
    Type act=argTypes.get(i);
    Type exp2=narrowArgType(context,overload.id,i,exp,act,tvConstraints,throwOnFail);
    if (exp2 == null) {
      assert(!throwOnFail);
      return null;
    }
    concreteArgTypes.add(exp2);
  }
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ concreteArgTypes+ ""String_Node_Str""+ tvConstraints);
  Map<String,List<Type>> bindings=unifyTypeVarConstraints(context,overload.id,overload.type.getTypeVars(),tvConstraints,throwOnFail);
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ tvConstraints);
  List<FunctionType> possibilities=findPossibleFunctionTypes(context,overload.id,overload.type,concreteArgTypes,bindings);
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ possibilities);
  if (possibilities.size() == 0) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ overload.type.getInputs()+ ""String_Node_Str""+ ""String_Node_Str""+ argTypes);
    }
    return null;
  }
  return new FnMatch(overload,possibilities);
}","static FnMatch concretiseInputsNonOverloaded(Context context,FnOverload overload,List<Type> argTypes,boolean throwOnFail) throws TypeMismatchException {
  List<Type> expandedInArgs=matchArgs(context,overload,argTypes,throwOnFail);
  if (expandedInArgs == null) {
    return null;
  }
  assert(expandedInArgs.size() == argTypes.size());
  List<Type> concreteArgTypes=new ArrayList<Type>(argTypes.size());
  MultiMap<String,Type> tvConstraints=new MultiMap<String,Type>();
  for (int i=0; i < argTypes.size(); i++) {
    Type exp=expandedInArgs.get(i);
    Type act=argTypes.get(i);
    Type exp2=narrowArgType(context,overload.id,i,exp,act,tvConstraints,throwOnFail);
    if (exp2 == null) {
      assert(!throwOnFail);
      return null;
    }
    concreteArgTypes.add(exp2);
  }
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ concreteArgTypes+ ""String_Node_Str""+ tvConstraints);
  Map<String,List<Type>> bindings=unifyTypeVarConstraints(context,overload.id,overload.type.getTypeVars(),tvConstraints,throwOnFail);
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ tvConstraints);
  List<FunctionType> possibilities=findPossibleFunctionTypes(context,overload.id,overload.type,concreteArgTypes,bindings);
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ possibilities);
  if (possibilities.size() == 0) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ overload.type.getInputs()+ ""String_Node_Str""+ ""String_Node_Str""+ argTypes);
    }
    return null;
  }
  return new FnMatch(overload,possibilities);
}",0.9977553310886644
132569,"private void assignExpression(Context context,VariableUsageInfo vu,SwiftAST tree) throws UserException {
  if (tree.getChildCount() < 2)   throw new STCRuntimeError(""String_Node_Str"");
  Assignment assignments=Assignment.fromAST(context,tree);
  for (  Pair<List<LValue>,SwiftAST> assign : assignments.getMatchedAssignments(context)) {
    List<LValue> lVals=assign.val1;
    SwiftAST rVal=assign.val2;
    Type rValT=TypeChecker.findExprType(context,lVals,rVal);
    List<Type> rValFields=TupleType.getFields(rValT);
    walkExpr(context,vu,rVal);
    Map<String,Type> rValTVBindings=new TreeMap<String,Type>();
    for (int i=0; i < lVals.size(); i++) {
      LValue lVal=lVals.get(i);
      syncFilePos(context,lVal.tree);
      if (lVal.var == null) {
        lVal=lVal.varDeclarationNeeded(context,rValFields.get(i));
        assert(lVal != null);
        vu.declare(context,lVal.var.name(),lVal.var.type(),false);
        context.declareVariable(lVal.var.type(),lVal.var.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.unknown(),false);
      }
 else {
        TypeChecker.checkAssignment(context,assignments.op,rValT,lVal.getType(context),lVal.toString(),rValTVBindings);
      }
      singleAssignment(context,vu,lVal,assignments.op);
    }
  }
}","private void assignExpression(Context context,VariableUsageInfo vu,SwiftAST tree) throws UserException {
  if (tree.getChildCount() < 2)   throw new STCRuntimeError(""String_Node_Str"");
  Assignment assignments=Assignment.fromAST(context,tree);
  for (  Pair<List<LValue>,SwiftAST> assign : assignments.getMatchedAssignments(context)) {
    List<LValue> lVals=assign.val1;
    SwiftAST rVal=assign.val2;
    Type rValT=TypeChecker.findExprType(context,lVals,rVal);
    List<Type> rValFields=TupleType.getFields(rValT);
    walkExpr(context,vu,rVal);
    Map<String,Type> rValTVBindings=new TreeMap<String,Type>();
    for (int i=0; i < lVals.size(); i++) {
      LValue lVal=lVals.get(i);
      Type rValFieldType=rValFields.get(i);
      syncFilePos(context,lVal.tree);
      if (lVal.var == null) {
        lVal=lVal.varDeclarationNeeded(context,rValFieldType);
        assert(lVal != null);
        vu.declare(context,lVal.var.name(),lVal.var.type(),false);
        context.declareVariable(lVal.var.type(),lVal.var.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.unknown(),false);
      }
 else {
        TypeChecker.checkAssignment(context,assignments.op,rValFieldType,lVal.getType(context),lVal.toString(),rValTVBindings);
      }
      singleAssignment(context,vu,lVal,assignments.op);
    }
  }
}",0.9742589703588144
132570,"public void generateWrappedBuiltin(FnID wrapperID,FnID builtinID,FunctionType ft,List<Var> outArgs,List<Var> userInArgs,ExecTarget mode,boolean isParallel,boolean isTargetable) throws UserException {
  List<Var> realInArgs=new ArrayList<Var>();
  realInArgs.addAll(userInArgs);
  TaskProps props=new TaskProps();
  if (isParallel) {
    Var par=new Var(Types.V_INT,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(par);
    props.put(TaskPropKey.PARALLELISM,par.asArg());
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"");
  }
  if (isTargetable) {
    Var location=new Var(Types.V_INT,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(location);
    props.put(TaskPropKey.LOCATION,location.asArg());
    Var softLocation=new Var(Types.V_BOOL,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(softLocation);
    props.put(TaskPropKey.SOFT_LOCATION,softLocation.asArg());
  }
  Function fn=new Function(wrapperID,realInArgs,outArgs,ExecTarget.syncControl());
  this.program.addFunction(fn);
  WaitMode waitMode;
  if (!mode.isDispatched() && !isParallel) {
    waitMode=WaitMode.WAIT_ONLY;
  }
 else {
    waitMode=WaitMode.TASK_DISPATCH;
  }
  Block mainBlock=fn.mainBlock();
  boolean mustMapOutFiles=!program.foreignFunctions().canInitOutputMapping(builtinID);
  Pair<List<WaitVar>,Map<Var,Var>> p;
  p=WrapUtil.buildWaitVars(mainBlock,mainBlock.statementIterator(),userInArgs,Var.NONE,outArgs,mustMapOutFiles);
  List<WaitVar> waitVars=p.val1;
  Map<Var,Var> filenameVars=p.val2;
  WaitStatement wait=new WaitStatement(wrapperID.uniqueName() + ""String_Node_Str"",waitVars,PassedVar.NONE,Var.NONE,waitMode,true,mode,props);
  mainBlock.addContinuation(wait);
  Block waitBlock=wait.getBlock();
  List<Statement> instBuffer=new ArrayList<Statement>();
  List<Arg> inVals=WrapUtil.fetchLocalOpInputs(waitBlock,userInArgs,instBuffer,false);
  List<Var> outVals=WrapUtil.createLocalOpOutputs(waitBlock,outArgs,filenameVars,instBuffer,false,mustMapOutFiles,true);
  instBuffer.add(new LocalFunctionCall(wrapperID,inVals,outVals,program.foreignFunctions()));
  WrapUtil.setLocalOpOutputs(waitBlock,outArgs,outVals,instBuffer,!mustMapOutFiles,true);
  waitBlock.addStatements(instBuffer);
}","public void generateWrappedBuiltin(FnID wrapperID,FnID builtinID,FunctionType ft,List<Var> outArgs,List<Var> userInArgs,ExecTarget mode,boolean isParallel,boolean isTargetable) throws UserException {
  List<Var> realInArgs=new ArrayList<Var>();
  realInArgs.addAll(userInArgs);
  TaskProps props=new TaskProps();
  if (isParallel) {
    Var par=new Var(Types.V_INT,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(par);
    props.put(TaskPropKey.PARALLELISM,par.asArg());
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"");
  }
  if (isTargetable) {
    Var location=new Var(Types.V_INT,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(location);
    props.put(TaskPropKey.LOCATION,location.asArg());
    Var softLocation=new Var(Types.V_BOOL,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(softLocation);
    props.put(TaskPropKey.SOFT_LOCATION,softLocation.asArg());
  }
  Function fn=new Function(wrapperID,realInArgs,outArgs,ExecTarget.syncControl());
  this.program.addFunction(fn);
  WaitMode waitMode;
  if (!mode.isDispatched() && !isParallel) {
    waitMode=WaitMode.WAIT_ONLY;
  }
 else {
    waitMode=WaitMode.TASK_DISPATCH;
  }
  Block mainBlock=fn.mainBlock();
  boolean mustMapOutFiles=!program.foreignFunctions().canInitOutputMapping(builtinID);
  Pair<List<WaitVar>,Map<Var,Var>> p;
  p=WrapUtil.buildWaitVars(mainBlock,mainBlock.statementIterator(),userInArgs,Var.NONE,outArgs,mustMapOutFiles);
  List<WaitVar> waitVars=p.val1;
  Map<Var,Var> filenameVars=p.val2;
  WaitStatement wait=new WaitStatement(wrapperID.uniqueName() + ""String_Node_Str"",waitVars,PassedVar.NONE,Var.NONE,waitMode,true,mode,props);
  mainBlock.addContinuation(wait);
  Block waitBlock=wait.getBlock();
  List<Statement> instBuffer=new ArrayList<Statement>();
  List<Arg> inVals=WrapUtil.fetchLocalOpInputs(waitBlock,userInArgs,instBuffer,false);
  List<Var> outVals=WrapUtil.createLocalOpOutputs(waitBlock,outArgs,filenameVars,instBuffer,false,mustMapOutFiles,true);
  instBuffer.add(new LocalFunctionCall(builtinID,inVals,outVals,program.foreignFunctions()));
  WrapUtil.setLocalOpOutputs(waitBlock,outArgs,outVals,instBuffer,!mustMapOutFiles,true);
  waitBlock.addStatements(instBuffer);
}",0.997096640398175
132571,"/** 
 * Switch to passing values directly. Do this before function inlining since function inlining will clean it up.
 * @param logger
 * @param program
 */
@Override public void optimize(Logger logger,Program program){
  Set<FnID> usedFunctionNames=program.getFunctionIDs();
  Map<FnID,Function> toInline=new HashMap<FnID,Function>();
  ListIterator<Function> fnIt=program.functionIterator();
  while (fnIt.hasNext()) {
    Function fn=fnIt.next();
    Function newFn=switchToValuePassing(logger,program.foreignFunctions(),fn,usedFunctionNames);
    if (newFn != null) {
      fnIt.remove();
      fnIt.add(newFn);
      usedFunctionNames.add(newFn.id());
      toInline.put(fn.id(),fn);
    }
  }
  FunctionInline.inlineAllOccurrences(logger,program,toInline);
}","/** 
 * Switch to passing values directly. Do this before function inlining since function inlining will clean it up.
 * @param logger
 * @param program
 */
@Override public void optimize(Logger logger,Program program){
  Set<FnID> usedFnIDs=new HashSet<FnID>(program.getFunctionMap().keySet());
  Map<FnID,Function> toInline=new HashMap<FnID,Function>();
  ListIterator<Function> fnIt=program.functionIterator();
  while (fnIt.hasNext()) {
    Function fn=fnIt.next();
    Function newFn=switchToValuePassing(logger,program.foreignFunctions(),fn,usedFnIDs);
    if (newFn != null) {
      fnIt.remove();
      fnIt.add(newFn);
      usedFnIDs.add(newFn.id());
      toInline.put(fn.id(),fn);
    }
  }
  FunctionInline.inlineAllOccurrences(logger,program,toInline);
}",0.9281984334203656
132572,"private Function switchToValuePassing(Logger logger,ForeignFunctions foreignFuncs,Function fn,Set<FnID> usedFunctionNames){
  if (fn.blockingInputs().isEmpty())   return null;
  List<Var> switchVars=new ArrayList<Var>();
  for (  WaitVar input : fn.blockingInputs()) {
    if (Types.isPrimFuture(input.var)) {
      Type valueT=Types.retrievedType(input.var.type());
      if (Semantics.canPassToChildTask(valueT)) {
        switchVars.add(input.var);
      }
    }
  }
  if (switchVars.isEmpty())   return null;
  List<Pair<Var,Var>> futValPairs=createValueVars(fn,switchVars);
  Map<Var,Var> switched=new HashMap<Var,Var>();
  for (  Pair<Var,Var> fv : futValPairs) {
    switched.put(fv.val1,fv.val2);
    assert(fv.val2 != null);
  }
  List<Var> newIList=buildNewInputList(fn,switched);
  FnID newID=selectUniqueID(fn.id(),usedFunctionNames);
  Block callNewFunction=callNewFunctionCode(foreignFuncs,fn,newID,switchVars);
  Block newBlock=fn.swapBlock(callNewFunction);
  for (  Pair<Var,Var> fv : futValPairs) {
    Var tmpfuture=new Var(fv.val1.type(),fv.val1.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.renamed(fv.val1));
    newBlock.renameVars(fn.id(),Collections.singletonMap(fv.val1,tmpfuture.asArg()),RenameMode.REPLACE_VAR,true);
    newBlock.addVariable(tmpfuture);
    Instruction store=TurbineOp.storePrim(tmpfuture,fv.val2.asArg());
    newBlock.addInstructionFront(store);
  }
  List<WaitVar> newBlocking=new ArrayList<WaitVar>();
  for (  WaitVar wv : fn.blockingInputs()) {
    if (!switchVars.contains(wv.var)) {
      newBlocking.add(wv);
    }
  }
  return new Function(newID,newIList,newBlocking,fn.getOutputList(),fn.mode(),newBlock);
}","private Function switchToValuePassing(Logger logger,ForeignFunctions foreignFuncs,Function fn,Set<FnID> usedFnIDs){
  if (fn.blockingInputs().isEmpty())   return null;
  List<Var> switchVars=new ArrayList<Var>();
  for (  WaitVar input : fn.blockingInputs()) {
    if (Types.isPrimFuture(input.var)) {
      Type valueT=Types.retrievedType(input.var.type());
      if (Semantics.canPassToChildTask(valueT)) {
        switchVars.add(input.var);
      }
    }
  }
  if (switchVars.isEmpty())   return null;
  List<Pair<Var,Var>> futValPairs=createValueVars(fn,switchVars);
  Map<Var,Var> switched=new HashMap<Var,Var>();
  for (  Pair<Var,Var> fv : futValPairs) {
    switched.put(fv.val1,fv.val2);
    assert(fv.val2 != null);
  }
  List<Var> newIList=buildNewInputList(fn,switched);
  FnID newID=selectUniqueID(fn.id(),usedFnIDs);
  Block callNewFunction=callNewFunctionCode(foreignFuncs,fn,newID,switchVars);
  Block newBlock=fn.swapBlock(callNewFunction);
  for (  Pair<Var,Var> fv : futValPairs) {
    Var tmpfuture=new Var(fv.val1.type(),fv.val1.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.renamed(fv.val1));
    newBlock.renameVars(fn.id(),Collections.singletonMap(fv.val1,tmpfuture.asArg()),RenameMode.REPLACE_VAR,true);
    newBlock.addVariable(tmpfuture);
    Instruction store=TurbineOp.storePrim(tmpfuture,fv.val2.asArg());
    newBlock.addInstructionFront(store);
  }
  List<WaitVar> newBlocking=new ArrayList<WaitVar>();
  for (  WaitVar wv : fn.blockingInputs()) {
    if (!switchVars.contains(wv.var)) {
      newBlocking.add(wv);
    }
  }
  return new Function(newID,newIList,newBlocking,fn.getOutputList(),fn.mode(),newBlock);
}",0.99157134256472
132573,"public void addUnifiedValues(GlobalConstants consts,String errContext,int stmtIndex,UnifiedValues unified) throws OptUnsafeError {
  int closedIndex=stmtIndex + 1;
  for (  Var closed : unified.closed) {
    markClosed(closed,closedIndex,false);
  }
  for (  Var closed : unified.recursivelyClosed) {
    markClosed(closed,closedIndex,true);
  }
  update(consts,errContext,unified.availableVals,null,stmtIndex);
}","public void addUnifiedValues(GlobalConstants consts,String errContext,int stmtIndex,UnifiedValues unified) throws OptUnsafeError {
  for (  Var closed : unified.closed) {
    markClosed(closed,stmtIndex,false);
  }
  for (  Var closed : unified.recursivelyClosed) {
    markClosed(closed,stmtIndex,true);
  }
  update(consts,errContext,unified.availableVals,null,stmtIndex);
}",0.4258555133079847
132574,"private void compileTopLevel(GlobalContext context,LocatedModule mainModule) throws UserException, UndefinedFunctionException, ModuleLoadException {
  Pair<ParsedModule,Boolean> loadedMainModule=modules.loadIfNeeded(context,mainModule);
  assert(!loadedMainModule.val2);
  varAnalyzer.walkTopLevel(context,loadedMainModule.val1.ast,moduleIterator(context));
  backend.startFunction(Constants.ENTRY_FUNCTION,Var.NONE,Var.NONE,ExecTarget.syncControl());
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,FrontendPass.COMPILE_TOPLEVEL,loadedModule);
  }
  FunctionType mainFn=context.lookupFunction(Constants.MAIN_FUNCTION);
  if (mainFn != null) {
    backend.functionCall(Constants.MAIN_FUNCTION,Constants.MAIN_FUNCTION,Arg.NONE,Var.NONE,ExecTarget.syncControl(),new TaskProps());
  }
  backend.endFunction();
}","private void compileTopLevel(GlobalContext context,LocalContext topLevelContext,LocatedModule mainModule) throws UserException, UndefinedFunctionException, ModuleLoadException {
  Pair<ParsedModule,Boolean> loadedMainModule=modules.loadIfNeeded(context,mainModule);
  assert(!loadedMainModule.val2);
  varAnalyzer.walkTopLevel(context,loadedMainModule.val1.ast,moduleIterator(context));
  backend.startFunction(Constants.ENTRY_FUNCTION,Var.NONE,Var.NONE,ExecTarget.syncControl());
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,topLevelContext,FrontendPass.COMPILE_TOPLEVEL,loadedModule);
  }
  FunctionType mainFn=context.lookupFunction(Constants.MAIN_FUNCTION);
  if (mainFn != null) {
    backend.functionCall(Constants.MAIN_FUNCTION,Constants.MAIN_FUNCTION,Arg.NONE,Var.NONE,ExecTarget.syncControl(),new TaskProps());
  }
  backend.endFunction();
}",0.974152785755313
132575,"private void compileFunctions(GlobalContext context) throws ModuleLoadException, UserException {
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,FrontendPass.COMPILE_FUNCTIONS,loadedModule);
  }
}","private void compileFunctions(GlobalContext context,LocalContext topLevelContext) throws ModuleLoadException, UserException {
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,topLevelContext,FrontendPass.COMPILE_FUNCTIONS,loadedModule);
  }
}",0.9129593810444874
132576,"private void walkTopLevel(GlobalContext context,SwiftAST fileTree,FrontendPass pass) throws UserException {
  if (pass == FrontendPass.DEFINITIONS) {
    walkTopLevelDefs(context,fileTree);
  }
 else   if (pass == FrontendPass.COMPILE_TOPLEVEL) {
    walkTopLevelCompileStatements(context,fileTree);
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS);
    walkTopLevelCompileFunctions(context,fileTree);
  }
}","private void walkTopLevel(GlobalContext context,LocalContext topLevelContext,SwiftAST fileTree,FrontendPass pass) throws UserException {
  if (pass == FrontendPass.DEFINITIONS) {
    walkTopLevelDefs(context,topLevelContext,fileTree);
  }
 else   if (pass == FrontendPass.COMPILE_TOPLEVEL) {
    walkTopLevelCompileStatements(topLevelContext,fileTree);
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS);
    walkTopLevelCompileFunctions(context,fileTree);
  }
}",0.9385474860335196
132577,"/** 
 * Handle an import statement by loading definitions for, or compiling module as needed.
 * @param context
 * @param tree
 * @param pass
 * @throws UserException
 */
private void importModule(GlobalContext context,SwiftAST tree,FrontendPass pass) throws UserException {
  assert(tree.getType() == ExMParser.IMPORT);
  assert(tree.getChildCount() == 1);
  SwiftAST moduleID=tree.child(0);
  if (pass == FrontendPass.DEFINITIONS) {
    LocatedModule module=LocatedModule.fromModuleNameAST(context,moduleID,false);
    loadModule(context,pass,module);
  }
}","/** 
 * Handle an import statement by loading definitions for, or compiling module as needed.
 * @param context
 * @param tree
 * @param pass
 * @throws UserException
 */
private void importModule(GlobalContext context,LocalContext topLevelContext,SwiftAST tree,FrontendPass pass) throws UserException {
  assert(tree.getType() == ExMParser.IMPORT);
  assert(tree.getChildCount() == 1);
  SwiftAST moduleID=tree.child(0);
  if (pass == FrontendPass.DEFINITIONS) {
    LocatedModule module=LocatedModule.fromModuleNameAST(context,moduleID,false);
    loadModule(context,topLevelContext,pass,module);
  }
}",0.9613069647463456
132578,"/** 
 * Second pass: - Compile top-level statements
 * @param globalContext
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileStatements(GlobalContext globalContext,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  Context context=LocalContext.topLevelContext(globalContext);
  syncFilePos(context,fileTree);
  List<SwiftAST> stmts=new ArrayList<SwiftAST>();
  for (  SwiftAST stmt : fileTree.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (TopLevel.isStatement(type)) {
      stmts.add(stmt);
    }
 else     if (!TopLevel.isDefinition(type)) {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
  for (  SwiftAST stmt : stmts) {
    walkStatement(context,stmt,WalkMode.NORMAL);
  }
}","/** 
 * Second pass: - Compile top-level statements
 * @param globalContext
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileStatements(LocalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  List<SwiftAST> stmts=new ArrayList<SwiftAST>();
  for (  SwiftAST stmt : fileTree.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (TopLevel.isStatement(type)) {
      stmts.add(stmt);
    }
 else     if (!TopLevel.isDefinition(type)) {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
  for (  SwiftAST stmt : stmts) {
    walkStatement(context,stmt,WalkMode.NORMAL);
  }
}",0.9522058823529412
132579,"/** 
 * First pass: - Register (but don't compile) all functions and other definitions
 * @param context
 * @param fileTree
 * @throws UserException
 * @throws DoubleDefineException
 * @throws UndefinedTypeException
 */
private void walkTopLevelDefs(GlobalContext context,SwiftAST fileTree) throws UserException, DoubleDefineException, UndefinedTypeException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  for (  SwiftAST stmt : fileTree.children()) {
    int type=stmt.getType();
    syncFilePos(context,stmt);
switch (type) {
case ExMParser.IMPORT:
      importModule(context,stmt,FrontendPass.DEFINITIONS);
    break;
case ExMParser.DEFINE_BUILTIN_FUNCTION:
  defineBuiltinFunction(context,stmt);
break;
case ExMParser.DEFINE_FUNCTION:
defineFunction(context,stmt);
break;
case ExMParser.DEFINE_APP_FUNCTION:
defineAppFunction(context,stmt);
break;
case ExMParser.DEFINE_NEW_STRUCT_TYPE:
defineNewStructType(context,stmt);
break;
case ExMParser.DEFINE_NEW_TYPE:
case ExMParser.TYPEDEF:
defineNewType(context,stmt,type == ExMParser.TYPEDEF);
break;
case ExMParser.GLOBAL_CONST:
globalConst(context,stmt);
break;
case ExMParser.PRAGMA:
pragmaTopLevel(context,stmt);
break;
case ExMParser.EOF:
break;
default :
if (!TopLevel.isStatement(type)) {
throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
}
}
}
}","/** 
 * First pass: - Register (but don't compile) all functions and other definitions
 * @param context
 * @param fileTree
 * @throws UserException
 * @throws DoubleDefineException
 * @throws UndefinedTypeException
 */
private void walkTopLevelDefs(GlobalContext context,LocalContext topLevelContext,SwiftAST fileTree) throws UserException, DoubleDefineException, UndefinedTypeException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  for (  SwiftAST stmt : fileTree.children()) {
    int type=stmt.getType();
    syncFilePos(context,stmt);
switch (type) {
case ExMParser.IMPORT:
      importModule(context,topLevelContext,stmt,FrontendPass.DEFINITIONS);
    break;
case ExMParser.DEFINE_BUILTIN_FUNCTION:
  defineBuiltinFunction(context,stmt);
break;
case ExMParser.DEFINE_FUNCTION:
defineFunction(context,stmt);
break;
case ExMParser.DEFINE_APP_FUNCTION:
defineAppFunction(context,stmt);
break;
case ExMParser.DEFINE_NEW_STRUCT_TYPE:
defineNewStructType(context,stmt);
break;
case ExMParser.DEFINE_NEW_TYPE:
case ExMParser.TYPEDEF:
defineNewType(context,stmt,type == ExMParser.TYPEDEF);
break;
case ExMParser.GLOBAL_CONST:
globalConst(context,stmt);
break;
case ExMParser.PRAGMA:
pragmaTopLevel(context,stmt);
break;
case ExMParser.EOF:
break;
default :
if (!TopLevel.isStatement(type)) {
throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
}
}
}
}",0.9840255591054312
132580,"/** 
 * Walk the statements in a file.
 * @param context
 * @param module the parsed file to compile
 * @param pass controls whether we just load top-level defs, or whetherwe attempt to compile the module
 * @throws UserException
 */
private void walkFile(GlobalContext context,LocatedModule module,ParsedModule parsed,FrontendPass pass) throws UserException {
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
  modules.enterModule(module,parsed);
  walkTopLevel(context,parsed.ast,pass);
  modules.exitModule();
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
}","/** 
 * Walk the statements in a file.
 * @param context
 * @param module the parsed file to compile
 * @param pass controls whether we just load top-level defs, or whetherwe attempt to compile the module
 * @throws UserException
 */
private void walkFile(GlobalContext context,LocalContext topLevelContext,LocatedModule module,ParsedModule parsed,FrontendPass pass) throws UserException {
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
  modules.enterModule(module,parsed);
  walkTopLevel(context,topLevelContext,parsed.ast,pass);
  modules.exitModule();
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
}",0.966789667896679
132581,"/** 
 * Compile or load definitions for a module (depending on pass), if needed. Avoids double-compiling or double loading a module.
 * @param context
 * @param pass
 * @param module
 * @throws ModuleLoadException
 * @throws UserException
 */
private void loadModule(GlobalContext context,FrontendPass pass,LocatedModule module) throws ModuleLoadException, UserException {
  Pair<ParsedModule,Boolean> loaded=modules.loadIfNeeded(context,module);
  ParsedModule parsed=loaded.val1;
  boolean newlyLoaded=loaded.val2;
  if (pass == FrontendPass.DEFINITIONS) {
    if (newlyLoaded) {
      if (LogHelper.isDebugEnabled()) {
        LogHelper.debug(context,""String_Node_Str"" + module.canonicalName);
        LogHelper.debug(context,parsed.ast.printTree());
      }
      walkFile(context,module,parsed,pass);
    }
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS || pass == FrontendPass.COMPILE_TOPLEVEL);
    assert(!newlyLoaded);
    walkFile(context,module,parsed,pass);
  }
}","/** 
 * Compile or load definitions for a module (depending on pass), if needed. Avoids double-compiling or double loading a module.
 * @param context
 * @param pass
 * @param module
 * @throws ModuleLoadException
 * @throws UserException
 */
private void loadModule(GlobalContext context,LocalContext topLevelContext,FrontendPass pass,LocatedModule module) throws UserException {
  Pair<ParsedModule,Boolean> loaded=modules.loadIfNeeded(context,module);
  ParsedModule parsed=loaded.val1;
  boolean newlyLoaded=loaded.val2;
  if (pass == FrontendPass.DEFINITIONS) {
    if (newlyLoaded) {
      if (LogHelper.isDebugEnabled()) {
        LogHelper.debug(context,""String_Node_Str"" + module.canonicalName);
        LogHelper.debug(context,parsed.ast.printTree());
      }
      walkFile(context,topLevelContext,module,parsed,pass);
    }
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS || pass == FrontendPass.COMPILE_TOPLEVEL);
    assert(!newlyLoaded);
    walkFile(context,topLevelContext,module,parsed,pass);
  }
}",0.9594059405940594
132582,"private void loadDefinitions(GlobalContext context,LocatedModule mainModule,LocatedModule builtins) throws ModuleLoadException, UserException {
  loadModule(context,FrontendPass.DEFINITIONS,builtins);
  loadModule(context,FrontendPass.DEFINITIONS,mainModule);
}","private void loadDefinitions(GlobalContext context,LocalContext topLevelContext,LocatedModule mainModule,LocatedModule builtins) throws ModuleLoadException, UserException {
  loadModule(context,topLevelContext,FrontendPass.DEFINITIONS,builtins);
  loadModule(context,topLevelContext,FrontendPass.DEFINITIONS,mainModule);
}",0.8953687821612349
132583,"/** 
 * Walk the AST and make calls to backend to generate lower level code. This function is called to start the walk at the top level file
 * @param mainFilePath the main file path to process
 * @param originalMainFilePath original main file, in case a temporary fileis being directly parsed
 * @param preprocessed true if module was preprocessed
 * @throws UserException
 */
public void walk(String mainFilePath,String originalMainFilePath,boolean preprocessed) throws UserException {
  GlobalContext context=new GlobalContext(mainFilePath,Logging.getSTCLogger(),foreignFuncs);
  String mainModuleName=FilenameUtils.getBaseName(originalMainFilePath);
  LocatedModule mainModule=new LocatedModule(mainFilePath,mainModuleName,preprocessed);
  LocatedModule builtins=LocatedModule.fromPath(context,Arrays.asList(""String_Node_Str""),false);
  loadDefinitions(context,mainModule,builtins);
  compileTopLevel(context,mainModule);
  compileFunctions(context);
}","/** 
 * Walk the AST and make calls to backend to generate lower level code. This function is called to start the walk at the top level file
 * @param mainFilePath the main file path to process
 * @param originalMainFilePath original main file, in case a temporary fileis being directly parsed
 * @param preprocessed true if module was preprocessed
 * @throws UserException
 */
public void walk(String mainFilePath,String originalMainFilePath,boolean preprocessed) throws UserException {
  GlobalContext context=new GlobalContext(mainFilePath,Logging.getSTCLogger(),foreignFuncs);
  LocalContext topLevelContext=LocalContext.topLevelContext(context);
  String mainModuleName=FilenameUtils.getBaseName(originalMainFilePath);
  LocatedModule mainModule=new LocatedModule(mainFilePath,mainModuleName,preprocessed);
  LocatedModule builtins=LocatedModule.fromPath(context,Arrays.asList(""String_Node_Str""),false);
  loadDefinitions(context,topLevelContext,mainModule,builtins);
  compileTopLevel(context,topLevelContext,mainModule);
  compileFunctions(context,topLevelContext);
}",0.941871921182266
132584,"public void addUnifiedValues(GlobalConstants consts,String errContext,int stmtIndex,UnifiedValues unified) throws OptUnsafeError {
  for (  Var closed : unified.closed) {
    markClosed(closed,stmtIndex,false);
  }
  for (  Var closed : unified.recursivelyClosed) {
    markClosed(closed,stmtIndex,true);
  }
  update(consts,errContext,unified.availableVals,null,stmtIndex);
}","public void addUnifiedValues(GlobalConstants consts,String errContext,int stmtIndex,UnifiedValues unified) throws OptUnsafeError {
  int closedIndex=stmtIndex + 1;
  for (  Var closed : unified.closed) {
    markClosed(closed,closedIndex,false);
  }
  for (  Var closed : unified.recursivelyClosed) {
    markClosed(closed,closedIndex,true);
  }
  update(consts,errContext,unified.availableVals,null,stmtIndex);
}",0.5348542458808618
132585,"public static FunctionCall fromAST(Context context,SwiftAST tree,boolean doWarn) throws UserException {
  assert(tree.getChildCount() >= 2);
  SwiftAST fTree=tree.child(0);
  String f;
  if (fTree.getType() == ExMParser.DEPRECATED) {
    fTree=fTree.child(0);
    assert(fTree.getType() == ExMParser.ID);
    f=fTree.getText();
    if (doWarn) {
      LogHelper.warn(context,""String_Node_Str"" + f + ""String_Node_Str"");
    }
  }
 else {
    assert(fTree.getType() == ExMParser.ID);
    f=fTree.getText();
  }
  SwiftAST arglist=tree.child(1);
  DefInfo def=context.lookupDef(f);
  List<SwiftAST> annotations=tree.children(2);
  if (def.kind == DefKind.FUNCTION) {
    FunctionType ftype=context.lookupFunction(f);
    assert(ftype != null);
    return regularFunctionFromAST(context,annotations,f,arglist,ftype);
  }
 else   if (def.kind == DefKind.TYPE) {
    Type type=context.lookupTypeUnsafe(f);
    assert(type != null);
    if (Types.isStruct(type)) {
      return structConstructorFromAST(context,annotations,f,arglist,type);
    }
  }
  throw UndefinedFunctionException.unknownFunction(context,f);
}","public static FunctionCall fromAST(Context context,SwiftAST tree,boolean doWarn) throws UserException {
  assert(tree.getChildCount() >= 2);
  SwiftAST fTree=tree.child(0);
  String f;
  if (fTree.getType() == ExMParser.DEPRECATED) {
    fTree=fTree.child(0);
    assert(fTree.getType() == ExMParser.ID);
    f=fTree.getText();
    if (doWarn) {
      LogHelper.warn(context,""String_Node_Str"" + f + ""String_Node_Str"");
    }
  }
 else {
    assert(fTree.getType() == ExMParser.ID);
    f=fTree.getText();
  }
  SwiftAST arglist=tree.child(1);
  DefInfo def=context.lookupDef(f);
  List<SwiftAST> annotations=tree.children(2);
  if (def == null) {
    throw UndefinedFunctionException.unknownFunction(context,f);
  }
 else   if (def.kind == DefKind.FUNCTION) {
    FunctionType ftype=context.lookupFunction(f);
    assert(ftype != null);
    return regularFunctionFromAST(context,annotations,f,arglist,ftype);
  }
 else   if (def.kind == DefKind.TYPE) {
    Type type=context.lookupTypeUnsafe(f);
    assert(type != null);
    if (Types.isStruct(type)) {
      return structConstructorFromAST(context,annotations,f,arglist,type);
    }
  }
  throw new TypeMismatchException(f + ""String_Node_Str"" + ""String_Node_Str"");
}",0.9152688172043012
132586,"/** 
 * Fix up any variables missing from the usedVariables passed through continuations. This is useful because it is easier to write other optimizations if they are allowed to mess up the usedVariables
 * @param updateLists modify tree and update pass and keep open lists
 */
public static void fixupProgram(Logger logger,Program prog,boolean updateLists){
  Set<Var> referencedGlobals=new HashSet<Var>();
  for (  Function fn : prog.getFunctions()) {
    if (updateLists) {
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.REBUILD);
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.ADD);
    }
 else {
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.NO_UPDATE);
    }
  }
  if (updateLists)   removeUnusedGlobals(prog.constants(),referencedGlobals);
}","/** 
 * Fix up any variables missing from the usedVariables passed through continuations. This is useful because it is easier to write other optimizations if they are allowed to mess up the usedVariables
 * @param updateLists modify tree and update pass and keep open lists
 */
public static void fixupProgram(Logger logger,Program prog,boolean updateLists){
  Set<Var> referencedGlobals=new HashSet<Var>();
  for (  Function fn : prog.getFunctions()) {
    if (updateLists) {
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.REBUILD);
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.ADD);
    }
 else {
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.NO_UPDATE);
    }
  }
  if (updateLists)   removeUnusedGlobals(prog.constants(),prog.globalVars(),referencedGlobals);
}",0.9900552486187846
132587,"private static void removeUnusedGlobals(GlobalConstants constants,Set<Var> referencedGlobals){
  Set<Var> globNames=new HashSet<Var>(constants.map().keySet());
  globNames.removeAll(referencedGlobals);
  for (  Var unused : globNames) {
    constants.remove(unused);
  }
}","private static void removeUnusedGlobals(GlobalConstants constants,GlobalVars globalVars,Set<Var> referencedGlobals){
  Set<Var> globalsToRemove=new HashSet<Var>();
  globalsToRemove.addAll(constants.map().keySet());
  globalsToRemove.addAll(globalVars.vars());
  globalsToRemove.removeAll(referencedGlobals);
  for (  Var unused : globalsToRemove) {
    if (unused.storage() == Alloc.GLOBAL_CONST) {
      constants.remove(unused);
    }
 else {
      assert(unused.storage() == Alloc.GLOBAL_VAR);
      globalVars.removeVariable(unused);
    }
  }
}",0.4257907542579075
132588,"/** 
 * Try to hoist this instruction
 * @param logger
 * @param inst
 * @param state
 * @return true if hoisted
 */
private boolean tryHoist(Logger logger,Instruction inst,HoistTracking state){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ state.maxHoist+ ""String_Node_Str""+ state.maxLoopHoist);
  }
  if (!inst.canChangeTiming()) {
    logger.trace(""String_Node_Str"");
    return false;
  }
  int maxHoist=state.maxHoist;
  for (  Arg in : inst.getInputs()) {
    if (maxHoist <= 0)     return false;
    if (in.isVar()) {
      Var inVar=in.getVar();
      maxHoist=Math.min(maxHoist,maxInputHoist(logger,state,inVar));
      if (!Semantics.canPassToChildTask(inVar)) {
        return false;
      }
    }
  }
  for (  Var readOutput : inst.getReadOutputs()) {
    int inputHoist=maxInputHoist(logger,state,readOutput);
    maxHoist=Math.min(maxHoist,inputHoist);
    logger.trace(""String_Node_Str"" + inputHoist + ""String_Node_Str""+ readOutput.name());
  }
  for (  Var out : inst.getOutputs()) {
    if (!Semantics.canPassToChildTask(out)) {
      return false;
    }
    if (trackDeclares(out)) {
      int declareDepth=state.declareMap.getDepth(out);
      if (logger.isTraceEnabled())       logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ declareDepth);
      assert(declareDepth >= 0);
      if (declareDepth > state.maxLoopHoist && !inst.isIdempotent()) {
        maxHoist=Math.min(maxHoist,state.maxLoopHoist);
        if (logger.isTraceEnabled())         logger.trace(""String_Node_Str"" + state.maxLoopHoist);
      }
    }
  }
  for (  Var out : inst.getPiecewiseAssignedOutputs()) {
    if (!inst.isIdempotent()) {
      maxHoist=Math.min(maxHoist,state.maxLoopHoist);
      if (logger.isTraceEnabled())       logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ ""String_Node_Str""+ state.maxLoopHoist);
    }
  }
  for (  Var out : inst.getOutputs()) {
    if (Types.outputRequiresInitialization(out)) {
      if (!inst.isInitialized(out)) {
        int initDepth=state.initializedMap.getDepth(out);
        if (logger.isTraceEnabled())         logger.trace(""String_Node_Str"" + initDepth + ""String_Node_Str""+ ""String_Node_Str""+ out);
        maxHoist=Math.min(maxHoist,initDepth);
        return false;
      }
    }
  }
  if (logger.isTraceEnabled())   logger.trace(""String_Node_Str"" + maxHoist);
  if (maxHoist <= 0) {
    return false;
  }
  int maxCorrectContext=maxHoistContext(logger,state,inst.execMode(),maxHoist);
  maxHoist=Math.max(maxHoist,maxCorrectContext);
  if (maxHoist <= 0) {
    return false;
  }
  doHoist(logger,inst,maxHoist,state);
  return true;
}","/** 
 * Try to hoist this instruction
 * @param logger
 * @param inst
 * @param state
 * @return true if hoisted
 */
private boolean tryHoist(Logger logger,Instruction inst,HoistTracking state){
}",0.1377855887521968
132589,"/** 
 * Match abstract argument types to provided argument expressions, Expand variable-length arguments if needed.  Leaves nulls if optional argument was omitted.
 * @param context
 * @param overload
 * @param argTypes
 * @param kwArgTypes
 * @param throwOnFail on failure, return null if falseor throw exception if true
 * @return
 * @throws TypeMismatchException
 * @throws InvalidConstructException
 */
private static List<MatchedArg> matchArgs(Context context,FnOverload overload,List<Type> argTypes,Map<String,Type> kwArgTypes,boolean throwOnFail) throws TypeMismatchException {
  FunctionType fnType=overload.type;
  DefaultVals<Var> defaultVals=overload.defaultVals;
  assert(!fnType.hasVarargs() || !defaultVals.hasAnyDefaults());
  List<Type> abstractInputs=fnType.getInputs();
  int numPosArgs=argTypes.size();
  int numKwArgs=kwArgTypes.size();
  int numTotalArgs=numPosArgs + numKwArgs;
  boolean fixedLength=!defaultVals.hasAnyDefaults() && !fnType.hasVarargs();
  int minNumArgs;
  if (defaultVals.hasAnyDefaults()) {
    minNumArgs=defaultVals.firstDefault();
  }
 else   if (fnType.hasVarargs()) {
    minNumArgs=fnType.getInputs().size() - 1;
  }
 else {
    minNumArgs=fnType.getInputs().size();
  }
  if (numTotalArgs < minNumArgs) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ (fixedLength ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str""+ minNumArgs+ ""String_Node_Str""+ numTotalArgs);
    }
    return null;
  }
  if (!fnType.hasVarargs() && numPosArgs > abstractInputs.size()) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ abstractInputs.size()+ ""String_Node_Str""+ numPosArgs+ ""String_Node_Str"");
    }
    return null;
  }
  int numMatchedArgs=Math.max(numTotalArgs,fnType.getInputs().size());
  Set<String> unmatchedKwArgs=new HashSet<String>(kwArgTypes.keySet());
  MatchedArg matched[]=new MatchedArg[numMatchedArgs];
  for (int i=0; i < numMatchedArgs; i++) {
    String name;
    Type formalArgType;
    Type argExprType;
    if (i < numPosArgs) {
      argExprType=argTypes.get(i);
      int formalArgIx;
      if (i < abstractInputs.size()) {
        formalArgIx=i;
      }
 else {
        assert(fnType.hasVarargs());
        formalArgIx=abstractInputs.size() - 1;
      }
      name=overload.inArgNames.get(formalArgIx);
      formalArgType=abstractInputs.get(formalArgIx);
    }
 else {
      assert(!fnType.hasVarargs());
      name=overload.inArgNames.get(i);
      boolean hasDefault=overload.defaultVals.defaultVals().get(i) != null;
      if (!hasDefault) {
        throw new TypeMismatchException(context,""String_Node_Str"" + name + ""String_Node_Str""+ overload.id.originalName());
      }
      formalArgType=abstractInputs.get(i);
      argExprType=kwArgTypes.get(name);
      if (argExprType != null) {
        unmatchedKwArgs.remove(name);
      }
    }
    matched[i]=new MatchedArg(name,formalArgType,argExprType);
  }
  if (unmatchedKwArgs.size() > 0) {
    throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ unmatchedKwArgs);
  }
  return Arrays.asList(matched);
}","/** 
 * Match abstract argument types to provided argument expressions, Expand variable-length arguments if needed.  Leaves nulls if optional argument was omitted.
 * @param context
 * @param overload
 * @param argTypes
 * @param kwArgTypes
 * @param throwOnFail on failure, return null if falseor throw exception if true
 * @return
 * @throws TypeMismatchException
 * @throws InvalidConstructException
 */
private static List<MatchedArg> matchArgs(Context context,FnOverload overload,List<Type> argTypes,Map<String,Type> kwArgTypes,boolean throwOnFail) throws TypeMismatchException {
  FunctionType fnType=overload.type;
  DefaultVals<Var> defaultVals=overload.defaultVals;
  assert(!fnType.hasVarargs() || !defaultVals.hasAnyDefaults());
  List<Type> abstractInputs=fnType.getInputs();
  int numPosArgs=argTypes.size();
  int numKwArgs=kwArgTypes.size();
  int numTotalArgs=numPosArgs + numKwArgs;
  boolean fixedLength=!defaultVals.hasAnyDefaults() && !fnType.hasVarargs();
  int minNumArgs;
  if (defaultVals.hasAnyDefaults()) {
    minNumArgs=defaultVals.firstDefault();
  }
 else   if (fnType.hasVarargs()) {
    minNumArgs=fnType.getInputs().size() - 1;
  }
 else {
    minNumArgs=fnType.getInputs().size();
  }
  if (numTotalArgs < minNumArgs) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ (fixedLength ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str""+ minNumArgs+ ""String_Node_Str""+ numTotalArgs);
    }
    return null;
  }
  if (!fnType.hasVarargs() && numPosArgs > abstractInputs.size()) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ abstractInputs.size()+ ""String_Node_Str""+ numPosArgs+ ""String_Node_Str"");
    }
    return null;
  }
  int numMatchedArgs=fnType.hasVarargs() ? numTotalArgs : fnType.getInputs().size();
  Set<String> unmatchedKwArgs=new HashSet<String>(kwArgTypes.keySet());
  MatchedArg matched[]=new MatchedArg[numMatchedArgs];
  for (int i=0; i < numMatchedArgs; i++) {
    String name;
    Type formalArgType;
    Type argExprType;
    if (i < numPosArgs) {
      argExprType=argTypes.get(i);
      int formalArgIx;
      if (i < abstractInputs.size()) {
        formalArgIx=i;
      }
 else {
        assert(fnType.hasVarargs());
        formalArgIx=abstractInputs.size() - 1;
      }
      name=overload.inArgNames.get(formalArgIx);
      formalArgType=abstractInputs.get(formalArgIx);
    }
 else {
      assert(!fnType.hasVarargs());
      name=overload.inArgNames.get(i);
      boolean hasDefault=overload.defaultVals.defaultVals().get(i) != null;
      if (!hasDefault) {
        throw new TypeMismatchException(context,""String_Node_Str"" + name + ""String_Node_Str""+ overload.id.originalName());
      }
      formalArgType=abstractInputs.get(i);
      argExprType=kwArgTypes.get(name);
      if (argExprType != null) {
        unmatchedKwArgs.remove(name);
      }
    }
    matched[i]=new MatchedArg(name,formalArgType,argExprType);
  }
  if (unmatchedKwArgs.size() > 0) {
    throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ unmatchedKwArgs);
  }
  return Arrays.asList(matched);
}",0.9909936955869107
132590,"/** 
 * Match abstract argument types to provided argument expressions, producing a list of argument types of the same length as the number of input arguments. Expand variable-length arguments or omit optional arguments if needed.
 * @param context
 * @param overload
 * @param argTypes
 * @param throwOnFail on failure, return null if falseor throw exception if true
 * @return
 * @throws TypeMismatchException
 */
private static List<Type> matchArgs(Context context,FnOverload overload,List<Type> argTypes,boolean throwOnFail) throws TypeMismatchException {
  FunctionType fnType=overload.type;
  DefaultVals<Var> defaultVals=overload.defaultVals;
  List<Type> abstractInputs=fnType.getInputs();
  int numArgs=argTypes.size();
  boolean fixedLength=!defaultVals.hasAnyDefaults() && !fnType.hasVarargs();
  int minNumArgs;
  if (defaultVals.hasAnyDefaults()) {
    minNumArgs=defaultVals.firstDefault();
  }
 else   if (fnType.hasVarargs()) {
    minNumArgs=fnType.getInputs().size() - 1;
  }
 else {
    minNumArgs=fnType.getInputs().size();
  }
  if (numArgs < abstractInputs.size() - 1) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ (fixedLength ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str""+ minNumArgs+ ""String_Node_Str""+ numArgs);
    }
    return null;
  }
  if (!fnType.hasVarargs() && numArgs > abstractInputs.size()) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ abstractInputs.size()+ ""String_Node_Str""+ numArgs);
    }
    return null;
  }
  if (fnType.hasVarargs()) {
    return expandVarArgs(abstractInputs,numArgs);
  }
 else {
    return abstractInputs.subList(0,numArgs);
  }
}","/** 
 * Match abstract argument types to provided argument expressions, producing a list of argument types of the same length as the number of input arguments. Expand variable-length arguments or omit optional arguments if needed.
 * @param context
 * @param overload
 * @param argTypes
 * @param throwOnFail on failure, return null if falseor throw exception if true
 * @return
 * @throws TypeMismatchException
 */
private static List<Type> matchArgs(Context context,FnOverload overload,List<Type> argTypes,boolean throwOnFail) throws TypeMismatchException {
  FunctionType fnType=overload.type;
  DefaultVals<Var> defaultVals=overload.defaultVals;
  List<Type> abstractInputs=fnType.getInputs();
  int numArgs=argTypes.size();
  boolean fixedLength=!defaultVals.hasAnyDefaults() && !fnType.hasVarargs();
  int minNumArgs;
  if (defaultVals.hasAnyDefaults()) {
    minNumArgs=defaultVals.firstDefault();
  }
 else   if (fnType.hasVarargs()) {
    minNumArgs=fnType.getInputs().size() - 1;
  }
 else {
    minNumArgs=fnType.getInputs().size();
  }
  if (numArgs < minNumArgs) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ (fixedLength ? ""String_Node_Str"" : ""String_Node_Str"")+ ""String_Node_Str""+ minNumArgs+ ""String_Node_Str""+ numArgs);
    }
    return null;
  }
  if (!fnType.hasVarargs() && numArgs > abstractInputs.size()) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ abstractInputs.size()+ ""String_Node_Str""+ numArgs);
    }
    return null;
  }
  if (fnType.hasVarargs()) {
    return expandVarArgs(abstractInputs,numArgs);
  }
 else {
    return abstractInputs.subList(0,numArgs);
  }
}",0.9904240766073872
132591,"private static FnMatch concretiseInputsNonOverloaded(Context context,FnOverload overload,List<Type> argTypes,boolean throwOnFail) throws TypeMismatchException {
  List<Type> expandedInArgs=matchArgs(context,overload,argTypes,throwOnFail);
  if (expandedInArgs == null) {
    return null;
  }
  assert(expandedInArgs.size() == argTypes.size());
  List<Type> concreteArgTypes=new ArrayList<Type>(argTypes.size());
  MultiMap<String,Type> tvConstraints=new MultiMap<String,Type>();
  for (int i=0; i < argTypes.size(); i++) {
    Type exp=expandedInArgs.get(i);
    Type act=argTypes.get(i);
    Type exp2=narrowArgType(context,overload.id,i,exp,act,tvConstraints,throwOnFail);
    if (exp2 == null) {
      assert(!throwOnFail);
      return null;
    }
    concreteArgTypes.add(exp2);
  }
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ concreteArgTypes+ ""String_Node_Str""+ tvConstraints);
  Map<String,List<Type>> bindings=unifyTypeVarConstraints(context,overload.id,overload.type.getTypeVars(),tvConstraints,throwOnFail);
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ tvConstraints);
  List<FunctionType> possibilities=findPossibleFunctionTypes(context,overload.id,overload.type,concreteArgTypes,bindings);
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ possibilities);
  if (possibilities.size() == 0) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ overload.type.getInputs()+ ""String_Node_Str""+ ""String_Node_Str""+ argTypes);
    }
    return null;
  }
  return new FnMatch(overload,possibilities);
}","static FnMatch concretiseInputsNonOverloaded(Context context,FnOverload overload,List<Type> argTypes,boolean throwOnFail) throws TypeMismatchException {
  List<Type> expandedInArgs=matchArgs(context,overload,argTypes,throwOnFail);
  if (expandedInArgs == null) {
    return null;
  }
  assert(expandedInArgs.size() == argTypes.size());
  List<Type> concreteArgTypes=new ArrayList<Type>(argTypes.size());
  MultiMap<String,Type> tvConstraints=new MultiMap<String,Type>();
  for (int i=0; i < argTypes.size(); i++) {
    Type exp=expandedInArgs.get(i);
    Type act=argTypes.get(i);
    Type exp2=narrowArgType(context,overload.id,i,exp,act,tvConstraints,throwOnFail);
    if (exp2 == null) {
      assert(!throwOnFail);
      return null;
    }
    concreteArgTypes.add(exp2);
  }
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ concreteArgTypes+ ""String_Node_Str""+ tvConstraints);
  Map<String,List<Type>> bindings=unifyTypeVarConstraints(context,overload.id,overload.type.getTypeVars(),tvConstraints,throwOnFail);
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ tvConstraints);
  List<FunctionType> possibilities=findPossibleFunctionTypes(context,overload.id,overload.type,concreteArgTypes,bindings);
  LogHelper.trace(context,""String_Node_Str"" + overload.id.uniqueName() + ""String_Node_Str""+ possibilities);
  if (possibilities.size() == 0) {
    if (throwOnFail) {
      throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + overload.id.originalName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ overload.type.getInputs()+ ""String_Node_Str""+ ""String_Node_Str""+ argTypes);
    }
    return null;
  }
  return new FnMatch(overload,possibilities);
}",0.9977553310886644
132592,"private void defineBuiltinFunction(Context context,SwiftAST tree) throws UserException {
  final int REQUIRED_CHILDREN=5;
  assert(tree.getChildCount() >= REQUIRED_CHILDREN);
  String function=tree.child(0).getText();
  SwiftAST typeParamsT=tree.child(1);
  SwiftAST outputs=tree.child(2);
  SwiftAST inputs=tree.child(3);
  SwiftAST tclPackage=tree.child(4);
  assert(inputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(outputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(tclPackage.getType() == ExMParser.TCL_PACKAGE);
  assert(tclPackage.getChildCount() == 2);
  Set<String> typeParams=extractTypeParams(typeParamsT);
  FunctionDecl fdecl=FunctionDecl.fromAST(context,varCreator,exprWalker,function,inputs,outputs,typeParams);
  FunctionType ft=fdecl.getFunctionType();
  LogHelper.debug(context,""String_Node_Str"" + function + ""String_Node_Str""+ ft);
  FnID fid=context.defineFunction(function,ft,fdecl.defaultVals());
  tree.setIdentifier(fid);
  String pkg=Literals.extractLiteralString(context,tclPackage.child(0));
  String version=Literals.extractLiteralString(context,tclPackage.child(1));
  backend.requirePackage(new TclPackage(pkg,version));
  int pos=REQUIRED_CHILDREN;
  TclFunRef impl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.TCL_FUN_REF) {
    SwiftAST tclImplRef=tree.child(pos);
    String symbol=Literals.extractLiteralString(context,tclImplRef.child(0));
    impl=new TclFunRef(pkg,symbol,version);
    pos++;
  }
  TclOpTemplate inlineTcl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.INLINE_TCL) {
    SwiftAST inlineTclTree=tree.child(pos);
    inlineTcl=wrapper.loadTclTemplate(context,fid,fdecl,ft,inlineTclTree);
    pos++;
  }
  FunctionType backendFT=VarRepr.backendFnType(ft);
  backend.defineBuiltinFunction(fid,backendFT,inlineTcl,impl);
  context.getForeignFunctions().addForeignFunction(fid);
  for (; pos < tree.getChildCount(); pos++) {
    handleBuiltinFunctionAnnotation(context,fid,fdecl,tree.child(pos),inlineTcl != null);
  }
  ExecTarget taskMode=context.getForeignFunctions().getTaskMode(fid);
  boolean isTargetable=false;
  if (taskMode.isDispatched()) {
    isTargetable=true;
    context.setFunctionProperty(fid,FnProp.TARGETABLE);
  }
  if (impl != null) {
    context.setFunctionProperty(fid,FnProp.BUILTIN);
  }
 else {
    if (inlineTcl == null) {
      throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + fid.originalName());
    }
    context.setFunctionProperty(fid,FnProp.WRAPPED_BUILTIN);
    context.setFunctionProperty(fid,FnProp.SYNC);
    boolean isParallel=context.hasFunctionProp(fid,FnProp.PARALLEL);
    if (isParallel && (!taskMode.isAsync() || !taskMode.targetContext().isAnyWorkContext())) {
      throw new UserException(context,""String_Node_Str"");
    }
    wrapper.saveWrapper(context,fid,backendFT,fdecl,taskMode,isParallel,isTargetable);
  }
}","private void defineBuiltinFunction(Context context,SwiftAST tree) throws UserException {
  final int REQUIRED_CHILDREN=5;
  assert(tree.getChildCount() >= REQUIRED_CHILDREN);
  String function=tree.child(0).getText();
  SwiftAST typeParamsT=tree.child(1);
  SwiftAST outputs=tree.child(2);
  SwiftAST inputs=tree.child(3);
  SwiftAST tclPackage=tree.child(4);
  assert(inputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(outputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(tclPackage.getType() == ExMParser.TCL_PACKAGE);
  assert(tclPackage.getChildCount() == 2);
  Set<String> typeParams=extractTypeParams(typeParamsT);
  FunctionDecl fdecl=FunctionDecl.fromAST(context,varCreator,exprWalker,function,inputs,outputs,typeParams);
  FunctionType ft=fdecl.getFunctionType();
  LogHelper.debug(context,""String_Node_Str"" + function + ""String_Node_Str""+ ft);
  FnID fid=context.defineFunction(function,ft,fdecl.defaultVals());
  tree.setIdentifier(fid);
  if (function.equals(""String_Node_Str"")) {
    System.err.println(""String_Node_Str"" + fid);
  }
  String pkg=Literals.extractLiteralString(context,tclPackage.child(0));
  String version=Literals.extractLiteralString(context,tclPackage.child(1));
  backend.requirePackage(new TclPackage(pkg,version));
  int pos=REQUIRED_CHILDREN;
  TclFunRef impl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.TCL_FUN_REF) {
    SwiftAST tclImplRef=tree.child(pos);
    String symbol=Literals.extractLiteralString(context,tclImplRef.child(0));
    impl=new TclFunRef(pkg,symbol,version);
    pos++;
  }
  TclOpTemplate inlineTcl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.INLINE_TCL) {
    SwiftAST inlineTclTree=tree.child(pos);
    inlineTcl=wrapper.loadTclTemplate(context,fid,fdecl,ft,inlineTclTree);
    pos++;
  }
  FunctionType backendFT=VarRepr.backendFnType(ft);
  backend.defineBuiltinFunction(fid,backendFT,inlineTcl,impl);
  context.getForeignFunctions().addForeignFunction(fid);
  for (; pos < tree.getChildCount(); pos++) {
    handleBuiltinFunctionAnnotation(context,fid,fdecl,tree.child(pos),inlineTcl != null);
  }
  ExecTarget taskMode=context.getForeignFunctions().getTaskMode(fid);
  boolean isTargetable=false;
  if (taskMode.isDispatched()) {
    isTargetable=true;
    context.setFunctionProperty(fid,FnProp.TARGETABLE);
  }
  if (impl != null) {
    context.setFunctionProperty(fid,FnProp.BUILTIN);
  }
 else {
    if (inlineTcl == null) {
      throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + fid.originalName());
    }
    context.setFunctionProperty(fid,FnProp.WRAPPED_BUILTIN);
    context.setFunctionProperty(fid,FnProp.SYNC);
    boolean isParallel=context.hasFunctionProp(fid,FnProp.PARALLEL);
    if (isParallel && (!taskMode.isAsync() || !taskMode.targetContext().isAnyWorkContext())) {
      throw new UserException(context,""String_Node_Str"");
    }
    wrapper.saveWrapper(context,fid,backendFT,fdecl,taskMode,isParallel,isTargetable);
  }
}",0.9837166358905488
132593,"/** 
 * Helper to choose variable name.
 * @param prefix Prefix that must be at start
 * @param preferredSuffix Preferred suffix
 * @param counterName name of counter to use to make unique if needed
 * @return
 */
protected String uniqueName(String prefix,String preferredSuffix,String counterName){
  if (preferredSuffix != null) {
    prefix+=preferredSuffix;
    if (lookupDef(prefix) == null) {
      return prefix;
    }
  }
  String name=null;
  do {
    long counter=nextCounterVal(counterName);
    name=prefix + counter;
  }
 while (lookupDef(name) != null);
  return name;
}","/** 
 * Helper to choose variable name.
 * @param prefix Prefix that must be at start
 * @param preferredSuffix Preferred suffix
 * @param counterName name of counter to use to make unique if needed
 * @return
 */
protected String uniqueName(String prefix,String preferredSuffix,String counterName){
  if (preferredSuffix != null) {
    prefix+=preferredSuffix;
  }
  if (lookupDef(prefix) == null) {
    return prefix;
  }
  String name=null;
  do {
    long counter=nextCounterVal(counterName);
    name=prefix + counter;
  }
 while (lookupDef(name) != null);
  return name;
}",0.9672977624784854
132594,"@Override public FnID defineFunction(String name,FunctionType type,DefaultVals<Var> defaultVals) throws UserException {
  DefInfo def=lookupDef(name);
  if (def != null && def.kind == DefKind.FUNCTION) {
    return overloadFunction(name,type,defaultVals);
  }
 else {
    declareVariable(type,name,Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.userVar(getSourceLoc()),false);
    FnID fnID=new FnID(name,name);
    addFunctionOverload(name,fnID,type,defaultVals);
    return fnID;
  }
}","@Override public FnID defineFunction(String name,FunctionType type,DefaultVals<Var> defaultVals) throws UserException {
  FnID fnID;
  DefInfo def=lookupDef(name);
  if (def != null && def.kind == DefKind.FUNCTION) {
    fnID=overloadFunction(name,type,defaultVals);
    declareVariable(type,fnID.uniqueName(),Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.userVar(getSourceLoc()),false);
  }
 else {
    declareVariable(type,name,Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.userVar(getSourceLoc()),false);
    fnID=new FnID(name,name);
    addFunctionOverload(name,fnID,type,defaultVals);
  }
  return fnID;
}",0.7992863514719001
132595,"private static void noMatchingOverloadsException(Context context,FnCallInfo fc) throws TypeMismatchException {
  StringBuilder sb=new StringBuilder();
  sb.append(""String_Node_Str"");
  sb.append(typeList(fc.argTypes));
  sb.append(""String_Node_Str"" + fc.name + ""String_Node_Str"");
  sb.append(""String_Node_Str"");
  for (  FnOverload fnType : fc.fnTypes) {
    sb.append(typeList(fnType.type.getInputs()));
    sb.append('\n');
  }
  throw new TypeMismatchException(context,sb.toString());
}","private static TypeMismatchException noMatchingOverloadsException(Context context,FnCallInfo fc){
  StringBuilder sb=new StringBuilder();
  sb.append(""String_Node_Str"");
  sb.append(typeList(fc.argTypes));
  sb.append(""String_Node_Str"" + fc.name + ""String_Node_Str"");
  sb.append(""String_Node_Str"");
  for (  FnOverload fnType : fc.fnTypes) {
    sb.append(typeList(fnType.type.getInputs()));
    sb.append('\n');
  }
  return new TypeMismatchException(context,sb.toString());
}",0.2045454545454545
132596,"public static ConcreteMatch concretiseFunctionCall(Context context,FunctionCall fc,List<Var> outputs) throws UserException {
}","public static ConcreteMatch concretiseFunctionCall(Context context,FunctionCall fc,List<Var> outputs) throws UserException {
  FnMatch match=concretiseInputs(context,fc);
  List<Type> outTs=Var.extractTypes(outputs);
  FunctionType concreteOutTs=concretiseOutputs(context,match,outputs,outTs);
  return new ConcreteMatch(match.overload.id,concreteOutTs,match.overload.defaultVals);
}",0.4950884086444008
132597,"/** 
 * Narrow possible function call types based on inputs.
 * @param context
 * @param fc
 * @param resolveOverload if true, resolve to a single overload
 * @return list of possible concrete function types with varargs, typevarsand union type args removed.  Grouped by which overload they match.  Each match should have at least one concrete function type associated with it.
 * @throws UserException
 */
private static List<FnMatch> concretiseInputs(Context context,FunctionCall fc,boolean resolveOverload) throws UserException {
  List<Type> argTypes=new ArrayList<Type>(fc.args().size());
  for (  SwiftAST arg : fc.args()) {
    argTypes.add(TypeChecker.findExprType(context,arg));
  }
  FnCallInfo info=new FnCallInfo(fc.originalName(),fc.overloads(),argTypes);
  return concretiseInputsOverloaded(context,info,resolveOverload);
}","/** 
 * Narrow possible function call types based on inputs.
 * @param context
 * @param fc
 * @param resolveOverload if true, resolve to a single overload
 * @return list of possible concrete function types with varargs, typevarsand union type args removed.  Grouped by which overload they match.  Each match should have at least one concrete function type associated with it.
 * @throws UserException
 */
private static FnMatch concretiseInputs(Context context,FunctionCall fc) throws UserException {
  List<Type> argTypes=new ArrayList<Type>(fc.args().size());
  for (  SwiftAST arg : fc.args()) {
    argTypes.add(TypeChecker.findExprType(context,arg));
  }
  FnCallInfo info=new FnCallInfo(fc.originalName(),fc.overloads(),argTypes);
  return concretiseInputsOverloaded(context,info);
}",0.9717444717444718
132598,"static List<FnMatch> concretiseInputsOverloaded(Context context,FnCallInfo fc,boolean resolveOverload) throws TypeMismatchException {
}","/** 
 * Resolve overloaded function call to a single overload based on input arguments (output arguments are *not* used to resolve overloads).
 * @param context
 * @param fc
 * @return
 * @throws TypeMismatchException
 */
static FnMatch concretiseInputsOverloaded(Context context,FnCallInfo fc) throws TypeMismatchException {
}",0.4242424242424242
132599,"public static Type findFuncCallExprType(Context context,SwiftAST tree) throws UndefinedFunctionException, UserException {
  FunctionCall f=FunctionCall.fromAST(context,tree,false);
  return exprTypeFromMatches(concretiseInputs(context,f,false));
}","public static Type findFuncCallExprType(Context context,SwiftAST tree) throws UndefinedFunctionException, UserException {
  FunctionCall f=FunctionCall.fromAST(context,tree,false);
  return exprTypeFromMatch(concretiseInputs(context,f));
}",0.9835390946502056
132600,"@Test public void testSelectOverloadNoMatch2() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_FLOAT,Types.F_FLOAT),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
}","@Test public void testSelectOverloadNoMatch2() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_FLOAT,Types.F_FLOAT),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc);
}",0.995640802092415
132601,"/** 
 * Overload resolution test with some matching args
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadPartialMatch() throws TypeMismatchException {
  FunctionType stringFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_STRING);
  FunctionType blobFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_BLOB);
  FnID blobFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_INT,Types.F_FILE,Types.F_STRING),Arrays.asList(new FnOverload(blobFnID,blobFn),new FnOverload(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(stringFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}","/** 
 * Overload resolution test with some matching args
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadPartialMatch() throws TypeMismatchException {
  FunctionType stringFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_STRING);
  FunctionType blobFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_BLOB);
  FnID blobFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_INT,Types.F_FILE,Types.F_STRING),Arrays.asList(new FnOverload(blobFnID,blobFn),new FnOverload(stringFnID,stringFn)));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(stringFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}",0.8715336728919072
132602,"@Test public void testMatchOptional() throws TypeMismatchException {
  String name=""String_Node_Str"";
  FunctionType ft=makeSimpleFT(Types.F_INT,Types.F_BOOL);
  Var constVar=new Var(Types.F_BOOL,""String_Node_Str"",Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.unknown());
  DefaultVals<Var> defaults=DefaultVals.fromDefaultValVector(Arrays.asList(null,constVar));
  FnOverload fo=new FnOverload(new FnID(name,name),ft,defaults);
  FnCallInfo fc;
  List<FnMatch> matches;
  FnMatch match;
  List<Type> intBoolArgs=Arrays.asList(Types.F_INT,Types.F_BOOL);
  fc=new FnCallInfo(name,fo.asList(),intBoolArgs);
  matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(""String_Node_Str"",1,matches.size());
  match=matches.get(0);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",intBoolArgs,match.concreteAlts.get(0).getInputs());
  List<Type> intArgs=Arrays.asList(Types.F_INT);
  fc=new FnCallInfo(name,fo.asList(),intArgs);
  matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(""String_Node_Str"",1,matches.size());
  match=matches.get(0);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",intArgs,match.concreteAlts.get(0).getInputs());
}","@Test public void testMatchOptional() throws TypeMismatchException {
  String name=""String_Node_Str"";
  FunctionType ft=makeSimpleFT(Types.F_INT,Types.F_BOOL);
  Var constVar=new Var(Types.F_BOOL,""String_Node_Str"",Alloc.GLOBAL_CONST,DefType.GLOBAL_CONST,VarProvenance.unknown());
  DefaultVals<Var> defaults=DefaultVals.fromDefaultValVector(Arrays.asList(null,constVar));
  FnOverload fo=new FnOverload(new FnID(name,name),ft,defaults);
  List<Type> intBoolArgs=Arrays.asList(Types.F_INT,Types.F_BOOL);
  FnCallInfo fc=new FnCallInfo(name,fo.asList(),intBoolArgs);
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",intBoolArgs,match.concreteAlts.get(0).getInputs());
  List<Type> intArgs=Arrays.asList(Types.F_INT);
  fc=new FnCallInfo(name,fo.asList(),intArgs);
  match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",intArgs,match.concreteAlts.get(0).getInputs());
}",0.6131511528608027
132603,"/** 
 * Check that union args are handled correctly when resolving overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion1() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_STRING),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(floatFnID,floatFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(intFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
}","/** 
 * Check that union args are handled correctly when resolving overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion1() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_STRING),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(floatFnID,floatFn)));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(intFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
}",0.8727498448168839
132604,"/** 
 * Check that union args are handled correctly when they could match multiple overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion2() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnOverload intOverload=new FnOverload(intFnID,intFn);
  FnOverload floatOverload=new FnOverload(floatFnID,floatFn);
  List<FnOverload> overloadList=Arrays.asList(intOverload,floatOverload);
  List<FnOverload> overloadListRev=Arrays.asList(floatOverload,intOverload);
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadList);
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(intFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
  FnCallInfo fc2=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadListRev);
  List<FnMatch> matches2=concretiseInputsOverloaded(FAKE_CONTEXT,fc2,true);
  assertEquals(1,matches2.size());
  FnMatch match2=matches2.get(0);
  assertEquals(intFnID,match2.overload.id);
  assertEquals(1,match2.concreteAlts.size());
  assertEquals(intFn,match2.concreteAlts.get(0));
  FnCallInfo fc3=makeOverloadedFnCallInfo(Arrays.asList(FLOAT_OR_INT),overloadListRev);
  List<FnMatch> matches3=concretiseInputsOverloaded(FAKE_CONTEXT,fc3,true);
  assertEquals(1,matches2.size());
  FnMatch match3=matches3.get(0);
  assertEquals(floatFnID,match3.overload.id);
  assertEquals(1,match3.concreteAlts.size());
  assertEquals(floatFn,match3.concreteAlts.get(0));
}","/** 
 * Check that union args are handled correctly when they could match multiple overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion2() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnOverload intOverload=new FnOverload(intFnID,intFn);
  FnOverload floatOverload=new FnOverload(floatFnID,floatFn);
  List<FnOverload> overloadList=Arrays.asList(intOverload,floatOverload);
  List<FnOverload> overloadListRev=Arrays.asList(floatOverload,intOverload);
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadList);
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(intFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
  FnCallInfo fc2=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadListRev);
  FnMatch match2=concretiseInputsOverloaded(FAKE_CONTEXT,fc2);
  assertEquals(intFnID,match2.overload.id);
  assertEquals(1,match2.concreteAlts.size());
  assertEquals(intFn,match2.concreteAlts.get(0));
  FnCallInfo fc3=makeOverloadedFnCallInfo(Arrays.asList(FLOAT_OR_INT),overloadListRev);
  FnMatch match3=concretiseInputsOverloaded(FAKE_CONTEXT,fc3);
  assertEquals(floatFnID,match3.overload.id);
  assertEquals(1,match3.concreteAlts.size());
  assertEquals(floatFn,match3.concreteAlts.get(0));
}",0.861956843038723
132605,"@Test public void testSelectOverloadVarArgs1() throws TypeMismatchException {
  FunctionType ft1=makeFT(Arrays.asList(Types.F_STRING,Types.F_STRING),true);
  FunctionType ft2=makeFT(Arrays.asList(FLOAT_OR_INT),true);
  FnID fid1=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID fid2=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING,Types.F_STRING),Arrays.asList(new FnOverload(fid1,ft1),new FnOverload(fid2,ft2)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.overload.id.equals(fid1));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(ft1));
}","@Test public void testSelectOverloadVarArgs1() throws TypeMismatchException {
  List<Type> twoStrings=Arrays.asList(Types.F_STRING,Types.F_STRING);
  FunctionType ft1=makeFT(twoStrings,true);
  FunctionType ft1NoVarArgs=makeFT(twoStrings,false);
  FunctionType ft2=makeFT(Arrays.asList(FLOAT_OR_INT),true);
  FnID fid1=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID fid2=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(twoStrings,Arrays.asList(new FnOverload(fid1,ft1),new FnOverload(fid2,ft2)));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(fid1,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(ft1NoVarArgs,match.concreteAlts.get(0));
}",0.6069057104913679
132606,"@Test public void testMatchNoVarArgs() throws TypeMismatchException {
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT,Types.F_FLOAT,Types.F_INT,Types.F_FLOAT));
  List<FnMatch> matches;
  matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,false);
  assertEquals(""String_Node_Str"",1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",Arrays.asList(Types.F_INT,Types.F_FLOAT,Types.F_INT,Types.F_FLOAT),match.concreteAlts.get(0).getInputs());
}","@Test public void testMatchNoVarArgs() throws TypeMismatchException {
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT,Types.F_FLOAT,Types.F_INT,Types.F_FLOAT));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",Arrays.asList(Types.F_INT,Types.F_FLOAT,Types.F_INT,Types.F_FLOAT),match.concreteAlts.get(0).getInputs());
}",0.891542288557214
132607,"@Test public void testSelectOverloadNoMatch() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_FLOAT),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
}","@Test public void testSelectOverloadNoMatch() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_FLOAT),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc);
}",0.9955237242614146
132608,"/** 
 * Simple overload resolution test
 * @throws TypeMismatchException
 */
@Test public void testSelectOverload() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(stringFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}","/** 
 * Simple overload resolution test
 * @throws TypeMismatchException
 */
@Test public void testSelectOverload() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING),Arrays.asList(new FnOverload(intFnID,intFn),new FnOverload(stringFnID,stringFn)));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(stringFnID,match.overload.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}",0.8599096191091027
132609,"@Test public void testMatchMultiVarArgs() throws TypeMismatchException {
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT));
  List<FnMatch> matches;
  matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(""String_Node_Str"",1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",Arrays.asList(Types.F_INT),match.concreteAlts.get(0).getInputs());
}","@Test public void testMatchMultiVarArgs() throws TypeMismatchException {
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT));
  FnMatch match=concretiseInputsOverloaded(FAKE_CONTEXT,fc);
  assertEquals(""String_Node_Str"",1,match.concreteAlts.size());
  assertEquals(""String_Node_Str"",Arrays.asList(Types.F_INT),match.concreteAlts.get(0).getInputs());
}",0.6235294117647059
132610,"@Test public void testMatchVarArgsFail() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT,Types.F_STRING));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc,false);
}","@Test public void testMatchVarArgsFail() throws TypeMismatchException {
  exception.expect(TypeMismatchException.class);
  FnCallInfo fc=makeFnCallInfo(VARARGS_TYPE,Arrays.asList(Types.F_INT,Types.F_STRING));
  concretiseInputsOverloaded(FAKE_CONTEXT,fc);
}",0.9884615384615384
132611,"/** 
 * Overload resolution test with some matching args
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadPartialMatch() throws TypeMismatchException {
  FunctionType stringFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_STRING);
  FunctionType blobFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_BLOB);
  FnID blobFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_INT,Types.F_FILE,Types.F_STRING),Arrays.asList(Pair.create(blobFnID,blobFn),Pair.create(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.id.equals(stringFnID));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(stringFn));
}","/** 
 * Overload resolution test with some matching args
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadPartialMatch() throws TypeMismatchException {
  FunctionType stringFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_STRING);
  FunctionType blobFn=makeSimpleFT(Types.F_INT,Types.F_FILE,Types.F_BLOB);
  FnID blobFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_INT,Types.F_FILE,Types.F_STRING),Arrays.asList(Pair.create(blobFnID,blobFn),Pair.create(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(stringFnID,match.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}",0.834619625137817
132612,"/** 
 * Check that union args are handled correctly when resolving overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion1() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_STRING),Arrays.asList(Pair.create(intFnID,intFn),Pair.create(floatFnID,floatFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.id.equals(intFnID));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(intFn));
}","/** 
 * Check that union args are handled correctly when resolving overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion1() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_STRING),Arrays.asList(Pair.create(intFnID,intFn),Pair.create(floatFnID,floatFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(intFnID,match.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
}",0.8154402895054282
132613,"/** 
 * Check that union args are handled correctly when they could match multiple overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion2() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  Pair<FnID,FunctionType> intPair=Pair.create(intFnID,intFn);
  Pair<FnID,FunctionType> floatPair=Pair.create(floatFnID,floatFn);
  List<Pair<FnID,FunctionType>> overloadList=Arrays.asList(intPair,floatPair);
  List<Pair<FnID,FunctionType>> overloadListRev=Arrays.asList(floatPair,intPair);
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadList);
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.id.equals(intFnID));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(intFn));
  FnCallInfo fc2=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadListRev);
  List<FnMatch> matches2=concretiseInputsOverloaded(FAKE_CONTEXT,fc2,true);
  assert(matches2.size() == 0);
  FnMatch match2=matches2.get(0);
  assert(match2.id.equals(intFnID));
  assert(match2.concreteAlts.size() == 1);
  assert(match2.concreteAlts.get(0).equals(intFn));
  FnCallInfo fc3=makeOverloadedFnCallInfo(Arrays.asList(FLOAT_OR_INT),overloadListRev);
  List<FnMatch> matches3=concretiseInputsOverloaded(FAKE_CONTEXT,fc3,true);
  assert(matches2.size() == 0);
  FnMatch match3=matches3.get(0);
  assert(match3.id.equals(floatFnID));
  assert(match3.concreteAlts.size() == 1);
  assert(match3.concreteAlts.get(0).equals(floatFn));
}","/** 
 * Check that union args are handled correctly when they could match multiple overloads
 * @throws TypeMismatchException
 */
@Test public void testSelectOverloadUnion2() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType floatFn=makeSimpleFT(Types.F_FLOAT);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID floatFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  Pair<FnID,FunctionType> intPair=Pair.create(intFnID,intFn);
  Pair<FnID,FunctionType> floatPair=Pair.create(floatFnID,floatFn);
  List<Pair<FnID,FunctionType>> overloadList=Arrays.asList(intPair,floatPair);
  List<Pair<FnID,FunctionType>> overloadListRev=Arrays.asList(floatPair,intPair);
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadList);
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(intFnID,match.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(intFn,match.concreteAlts.get(0));
  FnCallInfo fc2=makeOverloadedFnCallInfo(Arrays.asList(INT_OR_FLOAT),overloadListRev);
  List<FnMatch> matches2=concretiseInputsOverloaded(FAKE_CONTEXT,fc2,true);
  assertEquals(1,matches2.size());
  FnMatch match2=matches2.get(0);
  assertEquals(intFnID,match2.id);
  assertEquals(1,match2.concreteAlts.size());
  assertEquals(intFn,match2.concreteAlts.get(0));
  FnCallInfo fc3=makeOverloadedFnCallInfo(Arrays.asList(FLOAT_OR_INT),overloadListRev);
  List<FnMatch> matches3=concretiseInputsOverloaded(FAKE_CONTEXT,fc3,true);
  assertEquals(1,matches2.size());
  FnMatch match3=matches3.get(0);
  assertEquals(floatFnID,match3.id);
  assertEquals(1,match3.concreteAlts.size());
  assertEquals(floatFn,match3.concreteAlts.get(0));
}",0.8454293628808864
132614,"/** 
 * Simple overload resolution test
 * @throws TypeMismatchException
 */
@Test public void testSelectOverload() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING),Arrays.asList(Pair.create(intFnID,intFn),Pair.create(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assert(matches.size() == 0);
  FnMatch match=matches.get(0);
  assert(match.id.equals(stringFnID));
  assert(match.concreteAlts.size() == 1);
  assert(match.concreteAlts.get(0).equals(stringFn));
}","/** 
 * Simple overload resolution test
 * @throws TypeMismatchException
 */
@Test public void testSelectOverload() throws TypeMismatchException {
  FunctionType intFn=makeSimpleFT(Types.F_INT);
  FunctionType stringFn=makeSimpleFT(Types.F_STRING);
  FnID intFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnID stringFnID=new FnID(""String_Node_Str"",""String_Node_Str"");
  FnCallInfo fc=makeOverloadedFnCallInfo(Arrays.asList(Types.F_STRING),Arrays.asList(Pair.create(intFnID,intFn),Pair.create(stringFnID,stringFn)));
  List<FnMatch> matches=concretiseInputsOverloaded(FAKE_CONTEXT,fc,true);
  assertEquals(1,matches.size());
  FnMatch match=matches.get(0);
  assertEquals(stringFnID,match.id);
  assertEquals(1,match.concreteAlts.size());
  assertEquals(stringFn,match.concreteAlts.get(0));
}",0.8007518796992481
132615,"/** 
 * Evaluate any task properties.  Put values into propVals.
 * @param context
 * @param fc
 * @param propVals
 * @param renames
 * @throws UserException
 * @returns true if a wait statement was opened
 */
private boolean evalCallProperties(Context context,FunctionCall fc,TaskProps propVals,Map<String,String> renames) throws UserException {
  if (fc.annotations().isEmpty()) {
    return false;
  }
  List<Pair<TaskPropKey,Var>> propFutures=new ArrayList<Pair<TaskPropKey,Var>>();
  List<Var> waitVars=new ArrayList<Var>();
  Var locationVar=null;
  for (  TaskPropKey ann : fc.annotations().keySet()) {
    checkCallAnnotation(context,fc,ann);
    SwiftAST expr=fc.annotations().get(ann);
    Type exprType=TypeChecker.findExprType(context,expr);
    Type concreteType=TaskProp.checkFrontendType(context,ann,exprType);
    Var future=exprWalker.eval(context,expr,concreteType,false,renames);
    waitVars.add(future);
    propFutures.add(Pair.create(ann,future));
  }
  if (fc.location() != null) {
    checkCanTarget(context,fc);
    locationVar=exprWalker.eval(context,fc.location(),Types.F_LOCATION,false,renames);
    waitVars.add(locationVar);
  }
  backend.startWaitStatement(context.constructName(""String_Node_Str""),VarRepr.backendVars(waitVars),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedControl());
  for (  Pair<TaskPropKey,Var> x : propFutures) {
    Var value=exprWalker.retrieveToVar(context,x.val2);
    propVals.put(x.val1,value.asArg());
  }
  if (locationVar != null) {
    populateFromLocationStruct(context,locationVar,propVals);
  }
  if (f.softLocationOverride()) {
    propVals.put(TaskPropKey.LOC_STRICTNESS,TaskProps.LOC_STRICTNESS_SOFT_ARG);
  }
  return true;
}","/** 
 * Evaluate any task properties.  Put values into propVals.
 * @param context
 * @param fc
 * @param propVals
 * @param renames
 * @throws UserException
 * @returns true if a wait statement was opened
 */
private boolean evalCallProperties(Context context,FunctionCall fc,TaskProps propVals,Map<String,String> renames) throws UserException {
  if (fc.annotations().isEmpty()) {
    return false;
  }
  List<Pair<TaskPropKey,Var>> propFutures=new ArrayList<Pair<TaskPropKey,Var>>();
  List<Var> waitVars=new ArrayList<Var>();
  Var locationVar=null;
  for (  TaskPropKey ann : fc.annotations().keySet()) {
    checkCallAnnotation(context,fc,ann);
    SwiftAST expr=fc.annotations().get(ann);
    Type exprType=TypeChecker.findExprType(context,expr);
    Type concreteType=TaskProp.checkFrontendType(context,ann,exprType);
    Var future=exprWalker.eval(context,expr,concreteType,false,renames);
    waitVars.add(future);
    propFutures.add(Pair.create(ann,future));
  }
  if (fc.location() != null) {
    checkCanTarget(context,fc);
    locationVar=exprWalker.eval(context,fc.location(),Types.F_LOCATION,false,renames);
    waitVars.add(locationVar);
  }
  backend.startWaitStatement(context.constructName(""String_Node_Str""),VarRepr.backendVars(waitVars),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedControl());
  for (  Pair<TaskPropKey,Var> x : propFutures) {
    Var value=exprWalker.retrieveToVar(context,x.val2);
    propVals.put(x.val1,value.asArg());
  }
  if (locationVar != null) {
    populateFromLocationStruct(context,locationVar,propVals);
  }
  if (fc.softLocationOverride()) {
    propVals.put(TaskPropKey.LOC_STRICTNESS,TaskProps.LOC_STRICTNESS_SOFT_ARG);
  }
  return true;
}",0.9997070026369764
132616,"/** 
 * Generate backend instruction for function call
 * @param context
 * @param function name of function
 * @param oList list of output variables
 * @param iList list of input variables (with correct types)
 * @param priorityVal optional priority value (can be null)
 * @throws UserException
 */
private void backendFunctionCall(Context context,String function,FunctionCallKind kind,FunctionType concrete,List<Var> oList,List<Var> iList,TaskProps props) throws UserException {
  props.assertInternalTypesValid();
  if (kind == FunctionCallKind.STRUCT_CONSTRUCTOR) {
    backendStructConstructor(context,function,concrete,oList,iList);
    return;
  }
  assert(kind == FunctionCallKind.REGULAR_FUNCTION);
  FunctionType def=context.lookupFunction(function);
  if (def == null) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + function);
  }
  if (context.hasFunctionProp(function,FnProp.DEPRECATED)) {
    LogHelper.warn(context,""String_Node_Str"" + function);
  }
  List<Var> backendIList=VarRepr.backendVars(iList);
  List<Var> backendOList=VarRepr.backendVars(oList);
  TaskProps backendProps=VarRepr.backendProps(props);
  if (context.isIntrinsic(function)) {
    IntrinsicFunction intF=context.lookupIntrinsic(function);
    backend.intrinsicCall(intF,backendIList,backendOList,backendProps);
  }
 else   if (context.hasFunctionProp(function,FnProp.BUILTIN)) {
    ForeignFunctions foreignFuncs=context.getForeignFunctions();
    if (foreignFuncs.hasOpEquiv(function)) {
      assert(oList.size() <= 1);
      Var backendOut=(backendOList.size() == 0 ? null : backendOList.get(0));
      backend.asyncOp(foreignFuncs.getOpEquiv(function),backendOut,Arg.fromVarList(backendIList),backendProps);
    }
 else {
      backend.builtinFunctionCall(function,function,backendIList,backendOList,backendProps);
    }
  }
 else   if (context.hasFunctionProp(function,FnProp.COMPOSITE)) {
    ExecTarget mode;
    if (context.hasFunctionProp(function,FnProp.SYNC)) {
      mode=ExecTarget.syncControl();
    }
 else {
      mode=ExecTarget.dispatchedControl();
    }
    backend.functionCall(function,function,Var.asArgList(backendIList),backendOList,mode,backendProps);
  }
 else {
    backendCallWrapped(context,function,concrete,backendOList,backendIList,backendProps);
  }
}","/** 
 * Generate backend instruction for function call
 * @param context
 * @param function name of function
 * @param oList list of output variables
 * @param iList list of input variables (with correct types)
 * @param priorityVal optional priority value (can be null)
 * @throws UserException
 */
private void backendFunctionCall(Context context,String function,FunctionCallKind kind,FunctionType concrete,List<Var> oList,List<Var> iList,TaskProps props) throws UserException {
  props.assertInternalTypesValid();
  if (kind == FunctionCallKind.STRUCT_CONSTRUCTOR) {
    backendStructConstructor(context,function,concrete,oList,iList);
    return;
  }
 else   if (kind == FunctionCallKind.SUBTYPE_CONSTRUCTOR) {
    backendSubtypeConstructor(context,function,concrete,oList,iList);
    return;
  }
  assert(kind == FunctionCallKind.REGULAR_FUNCTION);
  FunctionType def=context.lookupFunction(function);
  if (def == null) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + function);
  }
  if (context.hasFunctionProp(function,FnProp.DEPRECATED)) {
    LogHelper.warn(context,""String_Node_Str"" + function);
  }
  List<Var> backendIList=VarRepr.backendVars(iList);
  List<Var> backendOList=VarRepr.backendVars(oList);
  TaskProps backendProps=VarRepr.backendProps(props);
  if (context.isIntrinsic(function)) {
    IntrinsicFunction intF=context.lookupIntrinsic(function);
    backend.intrinsicCall(intF,backendIList,backendOList,backendProps);
  }
 else   if (context.hasFunctionProp(function,FnProp.BUILTIN)) {
    ForeignFunctions foreignFuncs=context.getForeignFunctions();
    if (foreignFuncs.hasOpEquiv(function)) {
      assert(oList.size() <= 1);
      Var backendOut=(backendOList.size() == 0 ? null : backendOList.get(0));
      backend.asyncOp(foreignFuncs.getOpEquiv(function),backendOut,Arg.fromVarList(backendIList),backendProps);
    }
 else {
      backend.builtinFunctionCall(function,function,backendIList,backendOList,backendProps);
    }
  }
 else   if (context.hasFunctionProp(function,FnProp.COMPOSITE)) {
    ExecTarget mode;
    if (context.hasFunctionProp(function,FnProp.SYNC)) {
      mode=ExecTarget.syncControl();
    }
 else {
      mode=ExecTarget.dispatchedControl();
    }
    backend.functionCall(function,function,Var.asArgList(backendIList),backendOList,mode,backendProps);
  }
 else {
    backendCallWrapped(context,function,concrete,backendOList,backendIList,backendProps);
  }
}",0.9691853102574928
132617,"/** 
 * Call wrapper function for app or wrapped builtin
 * @param context
 * @param function
 * @param concrete
 * @param backendOList
 * @param backendIList
 * @param props
 * @throws UserException
 */
private void backendCallWrapped(Context context,String function,FunctionType concrete,List<Var> backendOList,List<Var> backendIList,TaskProps props) throws UserException {
  String wrapperFnName;
  if (context.hasFunctionProp(function,FnProp.WRAPPED_BUILTIN)) {
    wrapperFnName=wrappers.generateWrapper(context,function,concrete);
  }
 else {
    assert(context.hasFunctionProp(function,FnProp.APP));
    wrapperFnName=function;
  }
  List<Arg> realInputs=new ArrayList<Arg>();
  for (  Var in : backendIList) {
    realInputs.add(in.asArg());
  }
  if (context.hasFunctionProp(function,FnProp.PARALLEL)) {
    Arg par=props.get(TaskPropKey.PARALLELISM);
    if (par == null) {
      throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + function);
    }
    realInputs.add(VarRepr.backendArg(par));
  }
  if (context.hasFunctionProp(function,FnProp.TARGETABLE)) {
    Arg location=props.getWithDefault(TaskPropKey.LOCATION);
    realInputs.add(VarRepr.backendArg(location));
    Arg softLocation=props.getWithDefault(TaskPropKey.SOFT_LOCATION);
    realInputs.add(VarRepr.backendArg(softLocation));
  }
  assert(context.hasFunctionProp(function,FnProp.SYNC));
  ExecTarget mode=ExecTarget.syncControl();
  backend.functionCall(wrapperFnName,function,realInputs,backendOList,mode,VarRepr.backendProps(props));
}","/** 
 * Call wrapper function for app or wrapped builtin
 * @param context
 * @param function
 * @param concrete
 * @param backendOList
 * @param backendIList
 * @param props
 * @throws UserException
 */
private void backendCallWrapped(Context context,String function,FunctionType concrete,List<Var> backendOList,List<Var> backendIList,TaskProps props) throws UserException {
  String wrapperFnName;
  if (context.hasFunctionProp(function,FnProp.WRAPPED_BUILTIN)) {
    wrapperFnName=wrappers.generateWrapper(context,function,concrete);
  }
 else {
    assert(context.hasFunctionProp(function,FnProp.APP));
    wrapperFnName=function;
  }
  List<Arg> realInputs=new ArrayList<Arg>();
  for (  Var in : backendIList) {
    realInputs.add(in.asArg());
  }
  if (context.hasFunctionProp(function,FnProp.PARALLEL)) {
    Arg par=props.get(TaskPropKey.PARALLELISM);
    if (par == null) {
      throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + function);
    }
    realInputs.add(VarRepr.backendArg(par));
  }
  if (context.hasFunctionProp(function,FnProp.TARGETABLE)) {
    Arg location=props.getWithDefault(TaskPropKey.LOC_RANK);
    realInputs.add(VarRepr.backendArg(location));
    Arg locStrictness=props.getWithDefault(TaskPropKey.LOC_STRICTNESS);
    realInputs.add(VarRepr.backendArg(locStrictness));
    Arg locAccuracy=props.getWithDefault(TaskPropKey.LOC_ACCURACY);
    realInputs.add(VarRepr.backendArg(locAccuracy));
  }
  assert(context.hasFunctionProp(function,FnProp.SYNC));
  ExecTarget mode=ExecTarget.syncControl();
  backend.functionCall(wrapperFnName,function,realInputs,backendOList,mode,VarRepr.backendProps(props));
}",0.9317470256731372
132618,"/** 
 * Switch to passing values directly. Do this before function inlining since function inlining will clean it up.
 * @param logger
 * @param program
 */
@Override public void optimize(Logger logger,Program program){
  Set<FnID> usedFunctionNames=program.getFunctionIDs();
  Map<FnID,Function> toInline=new HashMap<FnID,Function>();
  ListIterator<Function> fnIt=program.functionIterator();
  while (fnIt.hasNext()) {
    Function fn=fnIt.next();
    Function newFn=switchToValuePassing(logger,program.foreignFunctions(),fn,usedFunctionNames);
    if (newFn != null) {
      fnIt.remove();
      fnIt.add(newFn);
      usedFunctionNames.add(newFn.id());
      toInline.put(fn.id(),fn);
    }
  }
  FunctionInline.inlineAllOccurrences(logger,program,toInline);
}","/** 
 * Switch to passing values directly. Do this before function inlining since function inlining will clean it up.
 * @param logger
 * @param program
 */
@Override public void optimize(Logger logger,Program program){
  Set<FnID> usedFnIDs=new HashSet<FnID>(program.getFunctionMap().keySet());
  Map<FnID,Function> toInline=new HashMap<FnID,Function>();
  ListIterator<Function> fnIt=program.functionIterator();
  while (fnIt.hasNext()) {
    Function fn=fnIt.next();
    Function newFn=switchToValuePassing(logger,program.foreignFunctions(),fn,usedFnIDs);
    if (newFn != null) {
      fnIt.remove();
      fnIt.add(newFn);
      usedFnIDs.add(newFn.id());
      toInline.put(fn.id(),fn);
    }
  }
  FunctionInline.inlineAllOccurrences(logger,program,toInline);
}",0.9281984334203656
132619,"private Function switchToValuePassing(Logger logger,ForeignFunctions foreignFuncs,Function fn,Set<FnID> usedFunctionNames){
  if (fn.blockingInputs().isEmpty())   return null;
  List<Var> switchVars=new ArrayList<Var>();
  for (  WaitVar input : fn.blockingInputs()) {
    if (Types.isPrimFuture(input.var)) {
      Type valueT=Types.retrievedType(input.var.type());
      if (Semantics.canPassToChildTask(valueT)) {
        switchVars.add(input.var);
      }
    }
  }
  if (switchVars.isEmpty())   return null;
  List<Pair<Var,Var>> futValPairs=createValueVars(fn,switchVars);
  Map<Var,Var> switched=new HashMap<Var,Var>();
  for (  Pair<Var,Var> fv : futValPairs) {
    switched.put(fv.val1,fv.val2);
    assert(fv.val2 != null);
  }
  List<Var> newIList=buildNewInputList(fn,switched);
  FnID newID=selectUniqueID(fn.id(),usedFunctionNames);
  Block callNewFunction=callNewFunctionCode(foreignFuncs,fn,newID,switchVars);
  Block newBlock=fn.swapBlock(callNewFunction);
  for (  Pair<Var,Var> fv : futValPairs) {
    Var tmpfuture=new Var(fv.val1.type(),fv.val1.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.renamed(fv.val1));
    newBlock.renameVars(fn.id(),Collections.singletonMap(fv.val1,tmpfuture.asArg()),RenameMode.REPLACE_VAR,true);
    newBlock.addVariable(tmpfuture);
    Instruction store=TurbineOp.storePrim(tmpfuture,fv.val2.asArg());
    newBlock.addInstructionFront(store);
  }
  List<WaitVar> newBlocking=new ArrayList<WaitVar>();
  for (  WaitVar wv : fn.blockingInputs()) {
    if (!switchVars.contains(wv.var)) {
      newBlocking.add(wv);
    }
  }
  return new Function(newID,newIList,newBlocking,fn.getOutputList(),fn.mode(),newBlock);
}","private Function switchToValuePassing(Logger logger,ForeignFunctions foreignFuncs,Function fn,Set<FnID> usedFnIDs){
  if (fn.blockingInputs().isEmpty())   return null;
  List<Var> switchVars=new ArrayList<Var>();
  for (  WaitVar input : fn.blockingInputs()) {
    if (Types.isPrimFuture(input.var)) {
      Type valueT=Types.retrievedType(input.var.type());
      if (Semantics.canPassToChildTask(valueT)) {
        switchVars.add(input.var);
      }
    }
  }
  if (switchVars.isEmpty())   return null;
  List<Pair<Var,Var>> futValPairs=createValueVars(fn,switchVars);
  Map<Var,Var> switched=new HashMap<Var,Var>();
  for (  Pair<Var,Var> fv : futValPairs) {
    switched.put(fv.val1,fv.val2);
    assert(fv.val2 != null);
  }
  List<Var> newIList=buildNewInputList(fn,switched);
  FnID newID=selectUniqueID(fn.id(),usedFnIDs);
  Block callNewFunction=callNewFunctionCode(foreignFuncs,fn,newID,switchVars);
  Block newBlock=fn.swapBlock(callNewFunction);
  for (  Pair<Var,Var> fv : futValPairs) {
    Var tmpfuture=new Var(fv.val1.type(),fv.val1.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.renamed(fv.val1));
    newBlock.renameVars(fn.id(),Collections.singletonMap(fv.val1,tmpfuture.asArg()),RenameMode.REPLACE_VAR,true);
    newBlock.addVariable(tmpfuture);
    Instruction store=TurbineOp.storePrim(tmpfuture,fv.val2.asArg());
    newBlock.addInstructionFront(store);
  }
  List<WaitVar> newBlocking=new ArrayList<WaitVar>();
  for (  WaitVar wv : fn.blockingInputs()) {
    if (!switchVars.contains(wv.var)) {
      newBlocking.add(wv);
    }
  }
  return new Function(newID,newIList,newBlocking,fn.getOutputList(),fn.mode(),newBlock);
}",0.99157134256472
132620,"public void generateWrappedBuiltin(FnID wrapperID,FnID builtinID,FunctionType ft,List<Var> outArgs,List<Var> userInArgs,ExecTarget mode,boolean isParallel,boolean isTargetable) throws UserException {
  List<Var> realInArgs=new ArrayList<Var>();
  realInArgs.addAll(userInArgs);
  TaskProps props=new TaskProps();
  if (isParallel) {
    Var par=new Var(Types.V_INT,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(par);
    props.put(TaskPropKey.PARALLELISM,par.asArg());
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"");
  }
  if (isTargetable) {
    Var location=new Var(Types.V_INT,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(location);
    props.put(TaskPropKey.LOCATION,location.asArg());
    Var softLocation=new Var(Types.V_BOOL,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(softLocation);
    props.put(TaskPropKey.SOFT_LOCATION,softLocation.asArg());
  }
  Function fn=new Function(wrapperID,realInArgs,outArgs,ExecTarget.syncControl());
  this.program.addFunction(fn);
  WaitMode waitMode;
  if (!mode.isDispatched() && !isParallel) {
    waitMode=WaitMode.WAIT_ONLY;
  }
 else {
    waitMode=WaitMode.TASK_DISPATCH;
  }
  Block mainBlock=fn.mainBlock();
  boolean mustMapOutFiles=!program.foreignFunctions().canInitOutputMapping(builtinID);
  Pair<List<WaitVar>,Map<Var,Var>> p;
  p=WrapUtil.buildWaitVars(mainBlock,mainBlock.statementIterator(),userInArgs,Var.NONE,outArgs,mustMapOutFiles);
  List<WaitVar> waitVars=p.val1;
  Map<Var,Var> filenameVars=p.val2;
  WaitStatement wait=new WaitStatement(wrapperID.uniqueName() + ""String_Node_Str"",waitVars,PassedVar.NONE,Var.NONE,waitMode,true,mode,props);
  mainBlock.addContinuation(wait);
  Block waitBlock=wait.getBlock();
  List<Statement> instBuffer=new ArrayList<Statement>();
  List<Arg> inVals=WrapUtil.fetchLocalOpInputs(waitBlock,userInArgs,instBuffer,false);
  List<Var> outVals=WrapUtil.createLocalOpOutputs(waitBlock,outArgs,filenameVars,instBuffer,false,mustMapOutFiles,true);
  instBuffer.add(new LocalFunctionCall(wrapperID,inVals,outVals,program.foreignFunctions()));
  WrapUtil.setLocalOpOutputs(waitBlock,outArgs,outVals,instBuffer,!mustMapOutFiles,true);
  waitBlock.addStatements(instBuffer);
}","public void generateWrappedBuiltin(FnID wrapperID,FnID builtinID,FunctionType ft,List<Var> outArgs,List<Var> userInArgs,ExecTarget mode,boolean isParallel,boolean isTargetable) throws UserException {
  List<Var> realInArgs=new ArrayList<Var>();
  realInArgs.addAll(userInArgs);
  TaskProps props=new TaskProps();
  if (isParallel) {
    Var par=new Var(Types.V_INT,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(par);
    props.put(TaskPropKey.PARALLELISM,par.asArg());
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"");
  }
  if (isTargetable) {
    Var location=new Var(Types.V_INT,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(location);
    props.put(TaskPropKey.LOCATION,location.asArg());
    Var softLocation=new Var(Types.V_BOOL,Var.VALUEOF_VAR_PREFIX + ""String_Node_Str"",Alloc.LOCAL,DefType.INARG,VarProvenance.optimizerTmp());
    realInArgs.add(softLocation);
    props.put(TaskPropKey.SOFT_LOCATION,softLocation.asArg());
  }
  Function fn=new Function(wrapperID,realInArgs,outArgs,ExecTarget.syncControl());
  this.program.addFunction(fn);
  WaitMode waitMode;
  if (!mode.isDispatched() && !isParallel) {
    waitMode=WaitMode.WAIT_ONLY;
  }
 else {
    waitMode=WaitMode.TASK_DISPATCH;
  }
  Block mainBlock=fn.mainBlock();
  boolean mustMapOutFiles=!program.foreignFunctions().canInitOutputMapping(builtinID);
  Pair<List<WaitVar>,Map<Var,Var>> p;
  p=WrapUtil.buildWaitVars(mainBlock,mainBlock.statementIterator(),userInArgs,Var.NONE,outArgs,mustMapOutFiles);
  List<WaitVar> waitVars=p.val1;
  Map<Var,Var> filenameVars=p.val2;
  WaitStatement wait=new WaitStatement(wrapperID.uniqueName() + ""String_Node_Str"",waitVars,PassedVar.NONE,Var.NONE,waitMode,true,mode,props);
  mainBlock.addContinuation(wait);
  Block waitBlock=wait.getBlock();
  List<Statement> instBuffer=new ArrayList<Statement>();
  List<Arg> inVals=WrapUtil.fetchLocalOpInputs(waitBlock,userInArgs,instBuffer,false);
  List<Var> outVals=WrapUtil.createLocalOpOutputs(waitBlock,outArgs,filenameVars,instBuffer,false,mustMapOutFiles,true);
  instBuffer.add(new LocalFunctionCall(builtinID,inVals,outVals,program.foreignFunctions()));
  WrapUtil.setLocalOpOutputs(waitBlock,outArgs,outVals,instBuffer,!mustMapOutFiles,true);
  waitBlock.addStatements(instBuffer);
}",0.997096640398175
132621,"public void addUnifiedValues(GlobalConstants consts,String errContext,int stmtIndex,UnifiedValues unified) throws OptUnsafeError {
  int closedIndex=stmtIndex + 1;
  for (  Var closed : unified.closed) {
    markClosed(closed,closedIndex,false);
  }
  for (  Var closed : unified.recursivelyClosed) {
    markClosed(closed,closedIndex,true);
  }
  update(consts,errContext,unified.availableVals,null,stmtIndex);
}","public void addUnifiedValues(GlobalConstants consts,String errContext,int stmtIndex,UnifiedValues unified) throws OptUnsafeError {
  for (  Var closed : unified.closed) {
    markClosed(closed,stmtIndex,false);
  }
  for (  Var closed : unified.recursivelyClosed) {
    markClosed(closed,stmtIndex,true);
  }
  update(consts,errContext,unified.availableVals,null,stmtIndex);
}",0.4258555133079847
132622,"public void addUnifiedValues(GlobalConstants consts,String errContext,int stmtIndex,UnifiedValues unified) throws OptUnsafeError {
  for (  Var closed : unified.closed) {
    markClosed(closed,stmtIndex,false);
  }
  for (  Var closed : unified.recursivelyClosed) {
    markClosed(closed,stmtIndex,true);
  }
  update(consts,errContext,unified.availableVals,null,stmtIndex);
}","public void addUnifiedValues(GlobalConstants consts,String errContext,int stmtIndex,UnifiedValues unified) throws OptUnsafeError {
  int closedIndex=stmtIndex + 1;
  for (  Var closed : unified.closed) {
    markClosed(closed,closedIndex,false);
  }
  for (  Var closed : unified.recursivelyClosed) {
    markClosed(closed,closedIndex,true);
  }
  update(consts,errContext,unified.availableVals,null,stmtIndex);
}",0.5348542458808618
132623,"private void compileTopLevel(GlobalContext context,LocatedModule mainModule) throws UserException, UndefinedFunctionException, ModuleLoadException {
  Pair<ParsedModule,Boolean> loadedMainModule=modules.loadIfNeeded(context,mainModule);
  assert(!loadedMainModule.val2);
  varAnalyzer.walkTopLevel(context,loadedMainModule.val1.ast,moduleIterator(context));
  backend.startFunction(Constants.ENTRY_FUNCTION,Var.NONE,Var.NONE,ExecTarget.syncControl());
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,FrontendPass.COMPILE_TOPLEVEL,loadedModule);
  }
  FunctionType mainFn=context.lookupFunction(Constants.MAIN_FUNCTION);
  if (mainFn != null) {
    backend.functionCall(Constants.MAIN_FUNCTION,Constants.MAIN_FUNCTION,Arg.NONE,Var.NONE,ExecTarget.syncControl(),new TaskProps());
  }
  backend.endFunction();
}","private void compileTopLevel(GlobalContext context,LocalContext topLevelContext,LocatedModule mainModule) throws UserException, UndefinedFunctionException, ModuleLoadException {
  Pair<ParsedModule,Boolean> loadedMainModule=modules.loadIfNeeded(context,mainModule);
  assert(!loadedMainModule.val2);
  varAnalyzer.walkTopLevel(context,loadedMainModule.val1.ast,moduleIterator(context));
  backend.startFunction(Constants.ENTRY_FUNCTION,Var.NONE,Var.NONE,ExecTarget.syncControl());
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,topLevelContext,FrontendPass.COMPILE_TOPLEVEL,loadedModule);
  }
  FunctionType mainFn=context.lookupFunction(Constants.MAIN_FUNCTION);
  if (mainFn != null) {
    backend.functionCall(Constants.MAIN_FUNCTION,Constants.MAIN_FUNCTION,Arg.NONE,Var.NONE,ExecTarget.syncControl(),new TaskProps());
  }
  backend.endFunction();
}",0.974152785755313
132624,"private void compileFunctions(GlobalContext context) throws ModuleLoadException, UserException {
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,FrontendPass.COMPILE_FUNCTIONS,loadedModule);
  }
}","private void compileFunctions(GlobalContext context,LocalContext topLevelContext) throws ModuleLoadException, UserException {
  for (  LocatedModule loadedModule : modules.loadedModules()) {
    loadModule(context,topLevelContext,FrontendPass.COMPILE_FUNCTIONS,loadedModule);
  }
}",0.9129593810444874
132625,"private void walkTopLevel(GlobalContext context,SwiftAST fileTree,FrontendPass pass) throws UserException {
  if (pass == FrontendPass.DEFINITIONS) {
    walkTopLevelDefs(context,fileTree);
  }
 else   if (pass == FrontendPass.COMPILE_TOPLEVEL) {
    walkTopLevelCompileStatements(context,fileTree);
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS);
    walkTopLevelCompileFunctions(context,fileTree);
  }
}","private void walkTopLevel(GlobalContext context,LocalContext topLevelContext,SwiftAST fileTree,FrontendPass pass) throws UserException {
  if (pass == FrontendPass.DEFINITIONS) {
    walkTopLevelDefs(context,topLevelContext,fileTree);
  }
 else   if (pass == FrontendPass.COMPILE_TOPLEVEL) {
    walkTopLevelCompileStatements(topLevelContext,fileTree);
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS);
    walkTopLevelCompileFunctions(context,fileTree);
  }
}",0.9385474860335196
132626,"/** 
 * Handle an import statement by loading definitions for, or compiling module as needed.
 * @param context
 * @param tree
 * @param pass
 * @throws UserException
 */
private void importModule(GlobalContext context,SwiftAST tree,FrontendPass pass) throws UserException {
  assert(tree.getType() == ExMParser.IMPORT);
  assert(tree.getChildCount() == 1);
  SwiftAST moduleID=tree.child(0);
  if (pass == FrontendPass.DEFINITIONS) {
    LocatedModule module=LocatedModule.fromModuleNameAST(context,moduleID,false);
    loadModule(context,pass,module);
  }
}","/** 
 * Handle an import statement by loading definitions for, or compiling module as needed.
 * @param context
 * @param tree
 * @param pass
 * @throws UserException
 */
private void importModule(GlobalContext context,LocalContext topLevelContext,SwiftAST tree,FrontendPass pass) throws UserException {
  assert(tree.getType() == ExMParser.IMPORT);
  assert(tree.getChildCount() == 1);
  SwiftAST moduleID=tree.child(0);
  if (pass == FrontendPass.DEFINITIONS) {
    LocatedModule module=LocatedModule.fromModuleNameAST(context,moduleID,false);
    loadModule(context,topLevelContext,pass,module);
  }
}",0.9613069647463456
132627,"/** 
 * Second pass: - Compile top-level statements
 * @param globalContext
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileStatements(GlobalContext globalContext,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  Context context=LocalContext.topLevelContext(globalContext);
  syncFilePos(context,fileTree);
  List<SwiftAST> stmts=new ArrayList<SwiftAST>();
  for (  SwiftAST stmt : fileTree.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (TopLevel.isStatement(type)) {
      stmts.add(stmt);
    }
 else     if (!TopLevel.isDefinition(type)) {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
  for (  SwiftAST stmt : stmts) {
    walkStatement(context,stmt,WalkMode.NORMAL);
  }
}","/** 
 * Second pass: - Compile top-level statements
 * @param globalContext
 * @param fileTree
 * @throws UserException
 */
private void walkTopLevelCompileStatements(LocalContext context,SwiftAST fileTree) throws UserException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  List<SwiftAST> stmts=new ArrayList<SwiftAST>();
  for (  SwiftAST stmt : fileTree.children()) {
    syncFilePos(context,stmt);
    int type=stmt.getType();
    if (TopLevel.isStatement(type)) {
      stmts.add(stmt);
    }
 else     if (!TopLevel.isDefinition(type)) {
      throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
    }
  }
  for (  SwiftAST stmt : stmts) {
    walkStatement(context,stmt,WalkMode.NORMAL);
  }
}",0.9522058823529412
132628,"/** 
 * First pass: - Register (but don't compile) all functions and other definitions
 * @param context
 * @param fileTree
 * @throws UserException
 * @throws DoubleDefineException
 * @throws UndefinedTypeException
 */
private void walkTopLevelDefs(GlobalContext context,SwiftAST fileTree) throws UserException, DoubleDefineException, UndefinedTypeException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  for (  SwiftAST stmt : fileTree.children()) {
    int type=stmt.getType();
    syncFilePos(context,stmt);
switch (type) {
case ExMParser.IMPORT:
      importModule(context,stmt,FrontendPass.DEFINITIONS);
    break;
case ExMParser.DEFINE_BUILTIN_FUNCTION:
  defineBuiltinFunction(context,stmt);
break;
case ExMParser.DEFINE_FUNCTION:
defineFunction(context,stmt);
break;
case ExMParser.DEFINE_APP_FUNCTION:
defineAppFunction(context,stmt);
break;
case ExMParser.DEFINE_NEW_STRUCT_TYPE:
defineNewStructType(context,stmt);
break;
case ExMParser.DEFINE_NEW_TYPE:
case ExMParser.TYPEDEF:
defineNewType(context,stmt,type == ExMParser.TYPEDEF);
break;
case ExMParser.GLOBAL_CONST:
globalConst(context,stmt);
break;
case ExMParser.PRAGMA:
pragmaTopLevel(context,stmt);
break;
case ExMParser.EOF:
break;
default :
if (!TopLevel.isStatement(type)) {
throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
}
}
}
}","/** 
 * First pass: - Register (but don't compile) all functions and other definitions
 * @param context
 * @param fileTree
 * @throws UserException
 * @throws DoubleDefineException
 * @throws UndefinedTypeException
 */
private void walkTopLevelDefs(GlobalContext context,LocalContext topLevelContext,SwiftAST fileTree) throws UserException, DoubleDefineException, UndefinedTypeException {
  assert(fileTree.getType() == ExMParser.PROGRAM);
  syncFilePos(context,fileTree);
  for (  SwiftAST stmt : fileTree.children()) {
    int type=stmt.getType();
    syncFilePos(context,stmt);
switch (type) {
case ExMParser.IMPORT:
      importModule(context,topLevelContext,stmt,FrontendPass.DEFINITIONS);
    break;
case ExMParser.DEFINE_BUILTIN_FUNCTION:
  defineBuiltinFunction(context,stmt);
break;
case ExMParser.DEFINE_FUNCTION:
defineFunction(context,stmt);
break;
case ExMParser.DEFINE_APP_FUNCTION:
defineAppFunction(context,stmt);
break;
case ExMParser.DEFINE_NEW_STRUCT_TYPE:
defineNewStructType(context,stmt);
break;
case ExMParser.DEFINE_NEW_TYPE:
case ExMParser.TYPEDEF:
defineNewType(context,stmt,type == ExMParser.TYPEDEF);
break;
case ExMParser.GLOBAL_CONST:
globalConst(context,stmt);
break;
case ExMParser.PRAGMA:
pragmaTopLevel(context,stmt);
break;
case ExMParser.EOF:
break;
default :
if (!TopLevel.isStatement(type)) {
throw new STCRuntimeError(""String_Node_Str"" + LogHelper.tokName(type) + ""String_Node_Str"");
}
}
}
}",0.9840255591054312
132629,"/** 
 * Walk the statements in a file.
 * @param context
 * @param module the parsed file to compile
 * @param pass controls whether we just load top-level defs, or whetherwe attempt to compile the module
 * @throws UserException
 */
private void walkFile(GlobalContext context,LocatedModule module,ParsedModule parsed,FrontendPass pass) throws UserException {
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
  modules.enterModule(module,parsed);
  walkTopLevel(context,parsed.ast,pass);
  modules.exitModule();
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
}","/** 
 * Walk the statements in a file.
 * @param context
 * @param module the parsed file to compile
 * @param pass controls whether we just load top-level defs, or whetherwe attempt to compile the module
 * @throws UserException
 */
private void walkFile(GlobalContext context,LocalContext topLevelContext,LocatedModule module,ParsedModule parsed,FrontendPass pass) throws UserException {
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
  modules.enterModule(module,parsed);
  walkTopLevel(context,topLevelContext,parsed.ast,pass);
  modules.exitModule();
  LogHelper.debug(context,""String_Node_Str"" + module.canonicalName + ""String_Node_Str""+ pass);
}",0.966789667896679
132630,"/** 
 * Compile or load definitions for a module (depending on pass), if needed. Avoids double-compiling or double loading a module.
 * @param context
 * @param pass
 * @param module
 * @throws ModuleLoadException
 * @throws UserException
 */
private void loadModule(GlobalContext context,FrontendPass pass,LocatedModule module) throws ModuleLoadException, UserException {
  Pair<ParsedModule,Boolean> loaded=modules.loadIfNeeded(context,module);
  ParsedModule parsed=loaded.val1;
  boolean newlyLoaded=loaded.val2;
  if (pass == FrontendPass.DEFINITIONS) {
    if (newlyLoaded) {
      if (LogHelper.isDebugEnabled()) {
        LogHelper.debug(context,""String_Node_Str"" + module.canonicalName);
        LogHelper.debug(context,parsed.ast.printTree());
      }
      walkFile(context,module,parsed,pass);
    }
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS || pass == FrontendPass.COMPILE_TOPLEVEL);
    assert(!newlyLoaded);
    walkFile(context,module,parsed,pass);
  }
}","/** 
 * Compile or load definitions for a module (depending on pass), if needed. Avoids double-compiling or double loading a module.
 * @param context
 * @param pass
 * @param module
 * @throws ModuleLoadException
 * @throws UserException
 */
private void loadModule(GlobalContext context,LocalContext topLevelContext,FrontendPass pass,LocatedModule module) throws UserException {
  Pair<ParsedModule,Boolean> loaded=modules.loadIfNeeded(context,module);
  ParsedModule parsed=loaded.val1;
  boolean newlyLoaded=loaded.val2;
  if (pass == FrontendPass.DEFINITIONS) {
    if (newlyLoaded) {
      if (LogHelper.isDebugEnabled()) {
        LogHelper.debug(context,""String_Node_Str"" + module.canonicalName);
        LogHelper.debug(context,parsed.ast.printTree());
      }
      walkFile(context,topLevelContext,module,parsed,pass);
    }
  }
 else {
    assert(pass == FrontendPass.COMPILE_FUNCTIONS || pass == FrontendPass.COMPILE_TOPLEVEL);
    assert(!newlyLoaded);
    walkFile(context,topLevelContext,module,parsed,pass);
  }
}",0.9594059405940594
132631,"private void loadDefinitions(GlobalContext context,LocatedModule mainModule,LocatedModule builtins) throws ModuleLoadException, UserException {
  loadModule(context,FrontendPass.DEFINITIONS,builtins);
  loadModule(context,FrontendPass.DEFINITIONS,mainModule);
}","private void loadDefinitions(GlobalContext context,LocalContext topLevelContext,LocatedModule mainModule,LocatedModule builtins) throws ModuleLoadException, UserException {
  loadModule(context,topLevelContext,FrontendPass.DEFINITIONS,builtins);
  loadModule(context,topLevelContext,FrontendPass.DEFINITIONS,mainModule);
}",0.8953687821612349
132632,"/** 
 * Walk the AST and make calls to backend to generate lower level code. This function is called to start the walk at the top level file
 * @param mainFilePath the main file path to process
 * @param originalMainFilePath original main file, in case a temporary fileis being directly parsed
 * @param preprocessed true if module was preprocessed
 * @throws UserException
 */
public void walk(String mainFilePath,String originalMainFilePath,boolean preprocessed) throws UserException {
  GlobalContext context=new GlobalContext(mainFilePath,Logging.getSTCLogger(),foreignFuncs);
  String mainModuleName=FilenameUtils.getBaseName(originalMainFilePath);
  LocatedModule mainModule=new LocatedModule(mainFilePath,mainModuleName,preprocessed);
  LocatedModule builtins=LocatedModule.fromPath(context,Arrays.asList(""String_Node_Str""),false);
  loadDefinitions(context,mainModule,builtins);
  compileTopLevel(context,mainModule);
  compileFunctions(context);
}","/** 
 * Walk the AST and make calls to backend to generate lower level code. This function is called to start the walk at the top level file
 * @param mainFilePath the main file path to process
 * @param originalMainFilePath original main file, in case a temporary fileis being directly parsed
 * @param preprocessed true if module was preprocessed
 * @throws UserException
 */
public void walk(String mainFilePath,String originalMainFilePath,boolean preprocessed) throws UserException {
  GlobalContext context=new GlobalContext(mainFilePath,Logging.getSTCLogger(),foreignFuncs);
  LocalContext topLevelContext=LocalContext.topLevelContext(context);
  String mainModuleName=FilenameUtils.getBaseName(originalMainFilePath);
  LocatedModule mainModule=new LocatedModule(mainFilePath,mainModuleName,preprocessed);
  LocatedModule builtins=LocatedModule.fromPath(context,Arrays.asList(""String_Node_Str""),false);
  loadDefinitions(context,topLevelContext,mainModule,builtins);
  compileTopLevel(context,topLevelContext,mainModule);
  compileFunctions(context,topLevelContext);
}",0.941871921182266
132633,"public static FunctionCall fromAST(Context context,SwiftAST tree,boolean doWarn) throws UserException {
  assert(tree.getChildCount() >= 2);
  SwiftAST fTree=tree.child(0);
  String f;
  if (fTree.getType() == ExMParser.DEPRECATED) {
    fTree=fTree.child(0);
    assert(fTree.getType() == ExMParser.ID);
    f=fTree.getText();
    if (doWarn) {
      LogHelper.warn(context,""String_Node_Str"" + f + ""String_Node_Str"");
    }
  }
 else {
    assert(fTree.getType() == ExMParser.ID);
    f=fTree.getText();
  }
  SwiftAST arglist=tree.child(1);
  DefInfo def=context.lookupDef(f);
  List<SwiftAST> annotations=tree.children(2);
  if (def.kind == DefKind.FUNCTION) {
    FunctionType ftype=context.lookupFunction(f);
    assert(ftype != null);
    return regularFunctionFromAST(context,annotations,f,arglist,ftype);
  }
 else   if (def.kind == DefKind.TYPE) {
    Type type=context.lookupTypeUnsafe(f);
    assert(type != null);
    if (Types.isStruct(type)) {
      return structConstructorFromAST(context,annotations,f,arglist,type);
    }
  }
  throw UndefinedFunctionException.unknownFunction(context,f);
}","public static FunctionCall fromAST(Context context,SwiftAST tree,boolean doWarn) throws UserException {
  assert(tree.getChildCount() >= 2);
  SwiftAST fTree=tree.child(0);
  String f;
  if (fTree.getType() == ExMParser.DEPRECATED) {
    fTree=fTree.child(0);
    assert(fTree.getType() == ExMParser.ID);
    f=fTree.getText();
    if (doWarn) {
      LogHelper.warn(context,""String_Node_Str"" + f + ""String_Node_Str"");
    }
  }
 else {
    assert(fTree.getType() == ExMParser.ID);
    f=fTree.getText();
  }
  SwiftAST arglist=tree.child(1);
  DefInfo def=context.lookupDef(f);
  List<SwiftAST> annotations=tree.children(2);
  if (def == null) {
    throw UndefinedFunctionException.unknownFunction(context,f);
  }
 else   if (def.kind == DefKind.FUNCTION) {
    FunctionType ftype=context.lookupFunction(f);
    assert(ftype != null);
    return regularFunctionFromAST(context,annotations,f,arglist,ftype);
  }
 else   if (def.kind == DefKind.TYPE) {
    Type type=context.lookupTypeUnsafe(f);
    assert(type != null);
    if (Types.isStruct(type)) {
      return structConstructorFromAST(context,annotations,f,arglist,type);
    }
  }
  throw new TypeMismatchException(f + ""String_Node_Str"" + ""String_Node_Str"");
}",0.9152688172043012
132634,"/** 
 * Fix up any variables missing from the usedVariables passed through continuations. This is useful because it is easier to write other optimizations if they are allowed to mess up the usedVariables
 * @param updateLists modify tree and update pass and keep open lists
 */
public static void fixupProgram(Logger logger,Program prog,boolean updateLists){
  Set<Var> referencedGlobals=new HashSet<Var>();
  for (  Function fn : prog.getFunctions()) {
    if (updateLists) {
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.REBUILD);
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.ADD);
    }
 else {
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.NO_UPDATE);
    }
  }
  if (updateLists)   removeUnusedGlobals(prog.constants(),referencedGlobals);
}","/** 
 * Fix up any variables missing from the usedVariables passed through continuations. This is useful because it is easier to write other optimizations if they are allowed to mess up the usedVariables
 * @param updateLists modify tree and update pass and keep open lists
 */
public static void fixupProgram(Logger logger,Program prog,boolean updateLists){
  Set<Var> referencedGlobals=new HashSet<Var>();
  for (  Function fn : prog.getFunctions()) {
    if (updateLists) {
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.REBUILD);
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.ADD);
    }
 else {
      fixupFunction(logger,prog.constants(),prog.globalVars(),fn,referencedGlobals,FixupVarMode.NO_UPDATE);
    }
  }
  if (updateLists)   removeUnusedGlobals(prog.constants(),prog.globalVars(),referencedGlobals);
}",0.9900552486187846
132635,"private static void removeUnusedGlobals(GlobalConstants constants,Set<Var> referencedGlobals){
  Set<Var> globNames=new HashSet<Var>(constants.map().keySet());
  globNames.removeAll(referencedGlobals);
  for (  Var unused : globNames) {
    constants.remove(unused);
  }
}","private static void removeUnusedGlobals(GlobalConstants constants,GlobalVars globalVars,Set<Var> referencedGlobals){
  Set<Var> globalsToRemove=new HashSet<Var>();
  globalsToRemove.addAll(constants.map().keySet());
  globalsToRemove.addAll(globalVars.vars());
  globalsToRemove.removeAll(referencedGlobals);
  for (  Var unused : globalsToRemove) {
    if (unused.storage() == Alloc.GLOBAL_CONST) {
      constants.remove(unused);
    }
 else {
      assert(unused.storage() == Alloc.GLOBAL_VAR);
      globalVars.removeVariable(unused);
    }
  }
}",0.4257907542579075
132636,"/** 
 * Try to hoist this instruction
 * @param logger
 * @param inst
 * @param state
 * @return true if hoisted
 */
private boolean tryHoist(Logger logger,Instruction inst,HoistTracking state){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ state.maxHoist+ ""String_Node_Str""+ state.maxLoopHoist);
  }
  if (!inst.canChangeTiming()) {
    logger.trace(""String_Node_Str"");
    return false;
  }
  int maxHoist=state.maxHoist;
  for (  Arg in : inst.getInputs()) {
    if (maxHoist <= 0)     return false;
    if (in.isVar()) {
      Var inVar=in.getVar();
      maxHoist=Math.min(maxHoist,maxInputHoist(logger,state,inVar));
      if (!Semantics.canPassToChildTask(inVar)) {
        return false;
      }
    }
  }
  for (  Var readOutput : inst.getReadOutputs()) {
    int inputHoist=maxInputHoist(logger,state,readOutput);
    maxHoist=Math.min(maxHoist,inputHoist);
    logger.trace(""String_Node_Str"" + inputHoist + ""String_Node_Str""+ readOutput.name());
  }
  for (  Var out : inst.getOutputs()) {
    if (!Semantics.canPassToChildTask(out)) {
      return false;
    }
    if (trackDeclares(out)) {
      int declareDepth=state.declareMap.getDepth(out);
      if (logger.isTraceEnabled())       logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ declareDepth);
      assert(declareDepth >= 0);
      if (declareDepth > state.maxLoopHoist && !inst.isIdempotent()) {
        maxHoist=Math.min(maxHoist,state.maxLoopHoist);
        if (logger.isTraceEnabled())         logger.trace(""String_Node_Str"" + state.maxLoopHoist);
      }
    }
  }
  for (  Var out : inst.getPiecewiseAssignedOutputs()) {
    if (!inst.isIdempotent()) {
      maxHoist=Math.min(maxHoist,state.maxLoopHoist);
      if (logger.isTraceEnabled())       logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ ""String_Node_Str""+ state.maxLoopHoist);
    }
  }
  for (  Var out : inst.getOutputs()) {
    if (Types.outputRequiresInitialization(out)) {
      if (!inst.isInitialized(out)) {
        int initDepth=state.initializedMap.getDepth(out);
        if (logger.isTraceEnabled())         logger.trace(""String_Node_Str"" + initDepth + ""String_Node_Str""+ ""String_Node_Str""+ out);
        maxHoist=Math.min(maxHoist,initDepth);
        return false;
      }
    }
  }
  if (logger.isTraceEnabled())   logger.trace(""String_Node_Str"" + maxHoist);
  if (maxHoist <= 0) {
    return false;
  }
  int maxCorrectContext=maxHoistContext(logger,state,inst.execMode(),maxHoist);
  maxHoist=Math.max(maxHoist,maxCorrectContext);
  if (maxHoist <= 0) {
    return false;
  }
  doHoist(logger,inst,maxHoist,state);
  return true;
}","/** 
 * Try to hoist this instruction
 * @param logger
 * @param inst
 * @param state
 * @return true if hoisted
 */
private boolean tryHoist(Logger logger,Instruction inst,HoistTracking state){
}",0.1377855887521968
132637,"/** 
 * Called once all usage info is collected
 * @return violations if this is a struct and it is incompletely specified
 */
public List<Violation> isIncompletelyDefinedStruct(Context context){
  if (Types.isStruct(this.type)) {
    ArrayList<Violation> result=new ArrayList<Violation>();
    Ternary assigned=isAssigned();
    if (assigned == Ternary.TRUE) {
      return result;
    }
 else     if (assigned == Ternary.MAYBE && isRead() != Ternary.FALSE) {
      result.add(new Violation(ViolationType.WARNING,this.getName() + ""String_Node_Str"",context));
    }
    for (    VInfo vi : structFields.values()) {
      if (vi.isAssigned() != Ternary.TRUE) {
        if (vi.isAssigned() == Ternary.FALSE && vi.isRead() == Ternary.TRUE) {
          result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + vi.getName() + ""String_Node_Str""+ ""String_Node_Str"",context));
        }
 else         if (assigned != Ternary.FALSE) {
          result.add(new Violation(ViolationType.WARNING,vi.getName() + ""String_Node_Str"" + ""String_Node_Str"",context));
        }
      }
      List<Violation> more=vi.isIncompletelyDefinedStruct(context);
      if (more != null)       result.addAll(more);
    }
    return result;
  }
 else {
    return null;
  }
}","/** 
 * Called once all usage info is collected
 * @return violations if this is a struct and it is incompletely specified
 */
public List<Violation> isIncompletelyDefinedStruct(Context context){
  if (Types.isStruct(this.type)) {
    ArrayList<Violation> result=new ArrayList<Violation>();
    Ternary assigned=isAssigned();
    if (assigned == Ternary.TRUE) {
      return result;
    }
 else     if (assigned == Ternary.MAYBE && isRead() != Ternary.FALSE) {
      result.add(new Violation(ViolationType.WARNING,this.getName() + ""String_Node_Str"",context));
    }
    for (    VInfo vi : structFields.values()) {
      Ternary somehowAssigned=Ternary.or(vi.isAssigned(),vi.isPartAssigned());
      if (somehowAssigned != Ternary.TRUE) {
        if (somehowAssigned == Ternary.FALSE && vi.isRead() == Ternary.TRUE) {
          result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + vi.getName() + ""String_Node_Str""+ ""String_Node_Str"",context));
        }
 else         if (somehowAssigned != Ternary.FALSE) {
          result.add(new Violation(ViolationType.WARNING,vi.getName() + ""String_Node_Str"" + ""String_Node_Str"",context));
        }
      }
      List<Violation> more=vi.isIncompletelyDefinedStruct(context);
      if (more != null)       result.addAll(more);
    }
    return result;
  }
 else {
    return null;
  }
}",0.9428129829984544
132638,"private List<Violation> structAssign(Context context,List<String> fieldPath,int arrayDepth,AssignOp op){
  VInfo fieldVInfo;
  List<Violation> errs=new ArrayList<Violation>();
  if (structFields != null) {
    if (this.assigned == Ternary.TRUE) {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
 else     if (this.assigned == Ternary.MAYBE) {
      errs.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
    String field=fieldPath.get(0);
    if (structFields.containsKey(field)) {
      fieldVInfo=structFields.get(field);
    }
 else {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + field + ""String_Node_Str""+ this.name+ ""String_Node_Str""+ ((StructType)type).getStructTypeName(),context));
    }
  }
 else {
    return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + this.name + ""String_Node_Str"",context));
  }
  fieldPath.remove(0);
  errs.addAll(fieldVInfo.assign(context,fieldPath,arrayDepth,op));
  mergeStructFieldWriteInfo(context);
  return errs;
}","private List<Violation> structAssign(Context context,List<String> fieldPath,int arrayDepth,AssignOp op){
  VInfo fieldVInfo;
  List<Violation> errs=new ArrayList<Violation>();
  if (structFields != null) {
    if (this.assigned == Ternary.TRUE) {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
 else     if (this.assigned == Ternary.MAYBE) {
      errs.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
    String field=fieldPath.get(0);
    if (structFields.containsKey(field)) {
      fieldVInfo=structFields.get(field);
    }
 else {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + field + ""String_Node_Str""+ this.name+ ""String_Node_Str""+ ((StructType)type).getStructTypeName(),context));
    }
  }
 else {
    return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + this.name + ""String_Node_Str"",context));
  }
  fieldPath.remove(0);
  errs.addAll(fieldVInfo.assign(context,fieldPath,arrayDepth,op));
  return errs;
}",0.9756302521008404
132639,"private static Type unpackedStructType(Type type,StructType structType){
  List<StructField> packedFields=structType.getFields();
  List<StructField> unpackedFields=new ArrayList<StructField>(packedFields.size());
  boolean differences=false;
  for (  StructField packedField : packedFields) {
    Type packedFieldType=packedField.getType();
    Type unpackedFieldType=unpackedType(packedFieldType);
    if (unpackedFieldType.equals(packedFieldType)) {
      unpackedFields.add(packedField);
    }
 else {
      StructField unpackedField=new StructField(unpackedFieldType,packedField.getName());
      unpackedFields.add(unpackedField);
      differences=true;
    }
  }
  if (differences) {
    return new StructType(true,""String_Node_Str"" + type.typeName(),unpackedFields);
  }
 else {
    return StructType.localStruct(structType);
  }
}","private static Type unpackedStructType(StructType structType){
  List<StructField> packedFields=structType.getFields();
  List<StructField> unpackedFields=new ArrayList<StructField>(packedFields.size());
  boolean differences=false;
  for (  StructField packedField : packedFields) {
    Type packedFieldType=packedField.getType();
    Type unpackedFieldType;
    if (Types.isRef(packedFieldType)) {
      unpackedFieldType=unpackedType(packedFieldType);
    }
 else     if (Types.isStruct(packedFieldType)) {
      unpackedFieldType=unpackedStructType((StructType)packedFieldType);
    }
 else {
      unpackedFieldType=packedFieldType;
    }
    if (unpackedFieldType.equals(packedFieldType)) {
      unpackedFields.add(packedField);
    }
 else {
      StructField unpackedField=new StructField(unpackedFieldType,packedField.getName());
      unpackedFields.add(unpackedField);
      System.err.println(unpackedField + ""String_Node_Str"" + packedField);
      differences=true;
    }
  }
  if (differences) {
    return new StructType(true,""String_Node_Str"" + structType.typeName(),unpackedFields);
  }
 else {
    return StructType.localStruct(structType);
  }
}",0.7391521197007481
132640,"/** 
 * Work out type of variable if we recursively extract all values into local variables
 * @param t
 * @return
 */
public static Type unpackedType(Typed t){
  Type type=stripRefs(t.type());
  if (Types.isContainer(type) || Types.isContainerLocal(type)) {
    return unpackedContainerType(type);
  }
 else   if (Types.isStruct(type) || Types.isStructLocal(type)) {
    return unpackedStructType(type,(StructType)type);
  }
 else {
    while (Types.canRetrieve(type)) {
      type=retrievedType(type);
    }
    return type;
  }
}","/** 
 * Work out type of variable if we recursively extract all values into local variables
 * @param t
 * @return
 */
public static Type unpackedType(Typed t){
  Type type=stripRefs(t.type());
  if (Types.isContainer(type) || Types.isContainerLocal(type)) {
    return unpackedContainerType(type);
  }
 else   if (Types.isStruct(type) || Types.isStructLocal(type)) {
    return unpackedStructType((StructType)type);
  }
 else {
    while (Types.canRetrieve(type)) {
      type=retrievedType(type);
    }
    return type;
  }
}",0.9952785646836638
132641,"/** 
 * Retrieve a struct directly to a local struct, without following any references
 * @param dst
 * @param src non-recursively closed struct
 * @return
 */
public static Instruction retrieveStruct(Var dst,Var src){
  assert(Types.isStruct(src.type()));
  assert(Types.isStructLocal(dst));
  assert(StructType.localStruct((StructType)src.type().getImplType()).assignableTo(dst.type()));
  return new TurbineOp(Opcode.LOAD_STRUCT,dst,src.asArg());
}","/** 
 * Retrieve a struct directly to a local struct, without following any references
 * @param dst
 * @param src non-recursively closed struct
 * @return
 */
public static Instruction retrieveStruct(Var dst,Var src){
}",0.6557377049180327
132642,"private static Type unpackedStructType(Type type,StructType structType){
  List<StructField> packedFields=structType.getFields();
  List<StructField> unpackedFields=new ArrayList<StructField>(packedFields.size());
  boolean differences=false;
  for (  StructField packedField : packedFields) {
    Type packedFieldType=packedField.getType();
    Type unpackedFieldType=unpackedType(packedFieldType);
    if (unpackedFieldType.equals(packedFieldType)) {
      unpackedFields.add(packedField);
    }
 else {
      StructField unpackedField=new StructField(unpackedFieldType,packedField.getName());
      unpackedFields.add(unpackedField);
      differences=true;
    }
  }
  if (differences) {
    return new StructType(true,""String_Node_Str"" + type.typeName(),unpackedFields);
  }
 else {
    return structType;
  }
}","private static Type unpackedStructType(Type type,StructType structType){
  List<StructField> packedFields=structType.getFields();
  List<StructField> unpackedFields=new ArrayList<StructField>(packedFields.size());
  boolean differences=false;
  for (  StructField packedField : packedFields) {
    Type packedFieldType=packedField.getType();
    Type unpackedFieldType=unpackedType(packedFieldType);
    if (unpackedFieldType.equals(packedFieldType)) {
      unpackedFields.add(packedField);
    }
 else {
      StructField unpackedField=new StructField(unpackedFieldType,packedField.getName());
      unpackedFields.add(unpackedField);
      differences=true;
    }
  }
  if (differences) {
    return new StructType(true,""String_Node_Str"" + type.typeName(),unpackedFields);
  }
 else {
    return StructType.localStruct(structType);
  }
}",0.9734299516908212
132643,"/** 
 * The type that would result from a retrieve operation
 * @param t
 * @return
 */
public static Type retrievedType(Typed t,boolean recursive){
  if (isScalarFuture(t) || isScalarUpdateable(t)) {
    return new ScalarValueType(t.type().primType());
  }
 else   if (isFile(t)) {
    return new FileValueType(t.type().fileKind());
  }
 else   if (isRef(t)) {
    return t.type().baseType().memberType();
  }
 else   if (recursive && (isContainer(t) || isContainerLocal(t))) {
    return unpackedType(t);
  }
 else   if (isArray(t)) {
    assert(!recursive);
    ArrayType at=(ArrayType)t.type().getImplType();
    Type retrievedMemberType=retrievedType(at.memberType(),false);
    return ArrayType.localArray(at.keyType(),retrievedMemberType);
  }
 else   if (isBag(t)) {
    assert(!recursive);
    BagType bt=(BagType)t.type().getImplType();
    Type retrievedMemberType=retrievedType(bt.memberType(),false);
    return BagType.localBag(retrievedMemberType);
  }
 else   if (isStruct(t)) {
    StructType st=(StructType)t.type().getImplType();
    if (recursive) {
      for (      StructField f : st.getFields()) {
        if (Types.isRef(f.getType())) {
          throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + st);
        }
      }
    }
    return StructType.localStruct(st);
  }
 else {
    throw new STCRuntimeError(t.type() + ""String_Node_Str"");
  }
}","/** 
 * The type that would result from a retrieve operation
 * @param t
 * @return
 */
public static Type retrievedType(Typed t,boolean recursive){
  if (isScalarFuture(t) || isScalarUpdateable(t)) {
    return new ScalarValueType(t.type().primType());
  }
 else   if (isFile(t)) {
    return new FileValueType(t.type().fileKind());
  }
 else   if (isRef(t)) {
    return t.type().baseType().memberType();
  }
 else   if (recursive && (isContainer(t) || isContainerLocal(t))) {
    return unpackedType(t);
  }
 else   if (isArray(t)) {
    assert(!recursive);
    ArrayType at=(ArrayType)t.type().getImplType();
    Type retrievedMemberType=retrievedType(at.memberType(),false);
    return ArrayType.localArray(at.keyType(),retrievedMemberType);
  }
 else   if (isBag(t)) {
    assert(!recursive);
    BagType bt=(BagType)t.type().getImplType();
    Type retrievedMemberType=retrievedType(bt.memberType(),false);
    return BagType.localBag(retrievedMemberType);
  }
 else   if (isStruct(t)) {
    StructType st=(StructType)t.type().getImplType();
    if (recursive) {
      return unpackedType(st);
    }
 else {
      return StructType.localStruct(st);
    }
  }
 else {
    throw new STCRuntimeError(t.type() + ""String_Node_Str"");
  }
}",0.8677087304613038
132644,"/** 
 * Fetch the value of a variable
 * @param block
 * @param instBuffer append fetch instruction to this list
 * @param var the variable to fetch the value of
 * @param acquireWrite if the var is a reference, do we acquirewrite refcounts?
 * @return variable holding value
 */
public static Var fetchValueOf(Block block,List<? super Instruction> instBuffer,Var var,String valName,boolean recursive,boolean acquireWrite){
  Type valueT;
  if (recursive && Types.isContainer(var)) {
    valueT=Types.unpackedType(var);
  }
 else {
    valueT=Types.retrievedType(var);
  }
  if (Types.isPrimUpdateable(var)) {
    Var value_v=createValueVar(valName,valueT,var);
    block.addVariable(value_v);
    instBuffer.add(TurbineOp.latestValue(value_v,var));
    return value_v;
  }
 else   if (Types.isPrimFuture(var)) {
    Var value_v=createValueVar(valName,valueT,var);
    block.addVariable(value_v);
    instBuffer.add(ICInstructions.retrievePrim(value_v,var));
    if (Types.isBlobVal(valueT)) {
      block.addCleanup(value_v,TurbineOp.freeBlob(value_v));
    }
    return value_v;
  }
 else   if (Types.isRef(var)) {
    Var deref=new Var(valueT,valName,Alloc.ALIAS,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    instBuffer.add(TurbineOp.retrieveRef(deref,var,acquireWrite));
    return deref;
  }
 else   if (Types.isContainer(var) && recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    instBuffer.add(TurbineOp.retrieveRecursive(deref,var));
    return deref;
  }
 else   if (Types.isStruct(var) && recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    Instruction structRetrieve=TurbineOp.retrieveStruct(deref,var);
    for (    StructField f : ((StructType)var.type().getImplType()).getFields()) {
      if (Types.isRef(f.getType())) {
        structRetrieve=TurbineOp.retrieveRecursive(deref,var);
      }
    }
    instBuffer.add(structRetrieve);
    return deref;
  }
 else   if ((Types.isContainer(var) || Types.isStruct(var)) && !recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    if (Types.isArray(var)) {
      instBuffer.add(TurbineOp.retrieveArray(deref,var));
    }
 else     if (Types.isBag(var)) {
      instBuffer.add(TurbineOp.retrieveBag(deref,var));
    }
 else {
      assert(Types.isStruct(var));
      instBuffer.add(TurbineOp.retrieveStruct(deref,var));
    }
    return deref;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"");
  }
}","/** 
 * Fetch the value of a variable
 * @param block
 * @param instBuffer append fetch instruction to this list
 * @param var the variable to fetch the value of
 * @param acquireWrite if the var is a reference, do we acquirewrite refcounts?
 * @return variable holding value
 */
public static Var fetchValueOf(Block block,List<? super Instruction> instBuffer,Var var,String valName,boolean recursive,boolean acquireWrite){
  Type valueT=Types.retrievedType(var,recursive);
  if (Types.isPrimUpdateable(var)) {
    Var value_v=createValueVar(valName,valueT,var);
    block.addVariable(value_v);
    instBuffer.add(TurbineOp.latestValue(value_v,var));
    return value_v;
  }
 else   if (Types.isPrimFuture(var)) {
    Var value_v=createValueVar(valName,valueT,var);
    block.addVariable(value_v);
    instBuffer.add(ICInstructions.retrievePrim(value_v,var));
    if (Types.isBlobVal(valueT)) {
      block.addCleanup(value_v,TurbineOp.freeBlob(value_v));
    }
    return value_v;
  }
 else   if (Types.isRef(var)) {
    Var deref=new Var(valueT,valName,Alloc.ALIAS,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    instBuffer.add(TurbineOp.retrieveRef(deref,var,acquireWrite));
    return deref;
  }
 else   if (Types.isContainer(var) && recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    instBuffer.add(TurbineOp.retrieveRecursive(deref,var));
    return deref;
  }
 else   if (Types.isStruct(var) && recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    boolean mustUseRecursive=false;
    for (    StructField f : ((StructType)var.type().getImplType()).getFields()) {
      if (Types.isRef(f.getType())) {
        mustUseRecursive=true;
      }
    }
    Instruction structRetrieve=mustUseRecursive ? TurbineOp.retrieveRecursive(deref,var) : TurbineOp.retrieveStruct(deref,var);
    instBuffer.add(structRetrieve);
    return deref;
  }
 else   if ((Types.isContainer(var) || Types.isStruct(var)) && !recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    if (Types.isArray(var)) {
      instBuffer.add(TurbineOp.retrieveArray(deref,var));
    }
 else     if (Types.isBag(var)) {
      instBuffer.add(TurbineOp.retrieveBag(deref,var));
    }
 else {
      assert(Types.isStruct(var));
      instBuffer.add(TurbineOp.retrieveStruct(deref,var));
    }
    return deref;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"");
  }
}",0.9292250233426704
132645,"private List<Violation> arrayAssign(Context context,int arrayDepth,AssignOp op){
  if (assigned != Ternary.FALSE) {
    if (arrayAssignDepth != arrayDepth) {
      return Arrays.asList(makeArrDepthViolation(context,arrayDepth));
    }
    this.assigned=Ternary.TRUE;
  }
 else {
    this.assigned=Ternary.TRUE;
    arrayAssignDepth=arrayDepth;
  }
  return Collections.emptyList();
}","private List<Violation> arrayAssign(Context context,int arrayDepth,AssignOp op){
  this.assigned=Ternary.TRUE;
  return Collections.emptyList();
}",0.5519848771266541
132646,"/** 
 * make a copy, except it might be in a different scope All of the usage tracking is reset
 */
public VInfo makeEmptyCopy(boolean locallyDeclared){
  HashMap<String,VInfo> structFieldsNew=null;
  if (structFields != null) {
    structFieldsNew=new HashMap<String,VInfo>();
    for (    Entry<String,VInfo> e : structFields.entrySet()) {
      structFieldsNew.put(e.getKey(),e.getValue().makeEmptyCopy(locallyDeclared));
    }
  }
  return new VInfo(type,hasMapping,structFieldsNew,locallyDeclared,name,Ternary.FALSE,Ternary.FALSE,Ternary.FALSE,0,-1);
}","/** 
 * make a copy, except it might be in a different scope All of the usage tracking is reset
 */
public VInfo makeEmptyCopy(boolean locallyDeclared){
  HashMap<String,VInfo> structFieldsNew=null;
  if (structFields != null) {
    structFieldsNew=new HashMap<String,VInfo>();
    for (    Entry<String,VInfo> e : structFields.entrySet()) {
      structFieldsNew.put(e.getKey(),e.getValue().makeEmptyCopy(locallyDeclared));
    }
  }
  return new VInfo(type,hasMapping,structFieldsNew,locallyDeclared,name,Ternary.FALSE,Ternary.FALSE,Ternary.FALSE,-1);
}",0.9982014388489208
132647,"private VInfo(Type type,boolean mapped,boolean locallyDeclared,String name,Ternary assigned,Ternary read,Ternary appended,int arrayAssignDepth,int maxReadDepth){
  this.type=type;
  this.hasMapping=mapped;
  if (Types.isStruct(type)) {
    structFields=new HashMap<String,VInfo>();
    for (    StructField f : ((StructType)type.getImplType()).getFields()) {
      VInfo fieldVInfo=new VInfo(f.getType(),mapped,name + ""String_Node_Str"" + f.getName(),locallyDeclared);
      structFields.put(f.getName(),fieldVInfo);
    }
  }
 else {
    structFields=null;
  }
  this.declaredInCurrentScope=locallyDeclared;
  this.name=name;
  this.assigned=assigned;
  this.appended=appended;
  this.read=read;
  this.arrayAssignDepth=arrayAssignDepth;
  this.maxReadDepth=maxReadDepth;
}","private VInfo(Type type,boolean mapped,boolean locallyDeclared,String name,Ternary assigned,Ternary read,Ternary appended,int maxReadDepth){
  this.type=type;
  this.hasMapping=mapped;
  if (Types.isStruct(type)) {
    structFields=new HashMap<String,VInfo>();
    for (    StructField f : ((StructType)type.getImplType()).getFields()) {
      VInfo fieldVInfo=new VInfo(f.getType(),mapped,name + ""String_Node_Str"" + f.getName(),locallyDeclared);
      structFields.put(f.getName(),fieldVInfo);
    }
  }
 else {
    structFields=null;
  }
  this.declaredInCurrentScope=locallyDeclared;
  this.name=name;
  this.assigned=assigned;
  this.appended=appended;
  this.read=read;
  this.maxReadDepth=maxReadDepth;
}",0.9575185434929198
132648,"/** 
 * Merge in information about writes to this variable that can occur in nested contexts
 * @param file
 * @param line
 * @param branches
 * @param exhaustive
 * @return
 */
private ArrayList<Violation> mergeBranchWriteInfo(Context context,List<VInfo> branches,boolean exhaustive){
  ArrayList<Violation> result=new ArrayList<Violation>();
  Ternary assignedInBranch;
  Ternary appendedInBranch;
  int expectAssignedDepth;
  if (exhaustive) {
    VInfo firstBranch=branches.get(0);
    assignedInBranch=firstBranch.assigned;
    appendedInBranch=firstBranch.appended;
    expectAssignedDepth=firstBranch.arrayAssignDepth;
  }
 else {
    assignedInBranch=Ternary.FALSE;
    appendedInBranch=Ternary.FALSE;
    expectAssignedDepth=0;
  }
  for (int i=0; i < branches.size(); i++) {
    VInfo currBr=branches.get(i);
    if (assignedInBranch == Ternary.FALSE) {
      expectAssignedDepth=currBr.arrayAssignDepth;
    }
    if (assignedInBranch != Ternary.FALSE && currBr.assigned != Ternary.FALSE) {
      if (currBr.arrayAssignDepth != expectAssignedDepth) {
        result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str""+ ""String_Node_Str"",context));
      }
    }
    assignedInBranch=Ternary.consensus(assignedInBranch,currBr.assigned);
    appendedInBranch=Ternary.consensus(appendedInBranch,currBr.appended);
  }
  this.appended=Ternary.or(appended,appendedInBranch);
  if (assignedInBranch == Ternary.FALSE) {
    return result;
  }
 else   if (this.assigned == Ternary.FALSE) {
    this.assigned=assignedInBranch;
    this.arrayAssignDepth=expectAssignedDepth;
    return result;
  }
 else   if (Types.isArray(this.type)) {
    this.assigned=Ternary.or(assigned,assignedInBranch);
    if (this.arrayAssignDepth != expectAssignedDepth) {
      result.add(makeArrDepthViolation(context,expectAssignedDepth));
    }
    return result;
  }
 else {
    if (Ternary.and(assignedInBranch,this.assigned) == Ternary.TRUE) {
      result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str"",context));
      return result;
    }
    if (assignedInBranch == Ternary.MAYBE) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    if ((this.assigned != Ternary.FALSE && assignedInBranch == Ternary.MAYBE) || (assignedInBranch != Ternary.FALSE && this.assigned == Ternary.MAYBE)) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    this.assigned=Ternary.or(this.assigned,assignedInBranch);
    return result;
  }
}","/** 
 * Merge in information about writes to this variable that can occur in nested contexts
 * @param file
 * @param line
 * @param branches
 * @param exhaustive
 * @return
 */
private ArrayList<Violation> mergeBranchWriteInfo(Context context,List<VInfo> branches,boolean exhaustive){
  ArrayList<Violation> result=new ArrayList<Violation>();
  Ternary assignedInBranch;
  Ternary appendedInBranch;
  if (exhaustive) {
    VInfo firstBranch=branches.get(0);
    assignedInBranch=firstBranch.assigned;
    appendedInBranch=firstBranch.appended;
  }
 else {
    assignedInBranch=Ternary.FALSE;
    appendedInBranch=Ternary.FALSE;
  }
  for (int i=0; i < branches.size(); i++) {
    VInfo currBr=branches.get(i);
    assignedInBranch=Ternary.consensus(assignedInBranch,currBr.assigned);
    appendedInBranch=Ternary.consensus(appendedInBranch,currBr.appended);
  }
  this.appended=Ternary.or(appended,appendedInBranch);
  if (assignedInBranch == Ternary.FALSE) {
    return result;
  }
 else   if (this.assigned == Ternary.FALSE) {
    this.assigned=assignedInBranch;
    return result;
  }
 else   if (Types.isArray(this.type)) {
    this.assigned=Ternary.or(assigned,assignedInBranch);
    return result;
  }
 else {
    if (Ternary.and(assignedInBranch,this.assigned) == Ternary.TRUE) {
      result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str"",context));
      return result;
    }
    if (assignedInBranch == Ternary.MAYBE) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    if ((this.assigned != Ternary.FALSE && assignedInBranch == Ternary.MAYBE) || (assignedInBranch != Ternary.FALSE && this.assigned == Ternary.MAYBE)) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    this.assigned=Ternary.or(this.assigned,assignedInBranch);
    return result;
  }
}",0.4425868895732512
132649,"private Var derefOrCopyResult(Context context,Var lookupResult,Var outVar) throws UndefinedTypeException, UserException {
  try {
    if (outVar == null) {
      return lookupResult;
    }
 else     if (Types.isAssignableRefTo(lookupResult.type(),outVar.type())) {
      dereference(context,outVar,lookupResult);
      return outVar;
    }
 else {
      copyByValue(context,outVar,lookupResult);
      return outVar;
    }
  }
 catch (  RuntimeException e) {
    Logging.getSTCLogger().debug(""String_Node_Str"" + lookupResult + ""String_Node_Str""+ outVar);
    throw e;
  }
}","private Var derefOrCopyResult(Context context,Var lookupResult,Var outVar) throws UndefinedTypeException, UserException {
  try {
    if (outVar == null) {
      return lookupResult;
    }
 else     if (Types.isRefTo(lookupResult.type(),outVar.type())) {
      dereference(context,outVar,lookupResult);
      return outVar;
    }
 else {
      copyByValue(context,outVar,lookupResult);
      return outVar;
    }
  }
 catch (  RuntimeException e) {
    Logging.getSTCLogger().debug(""String_Node_Str"" + lookupResult + ""String_Node_Str""+ outVar);
    throw e;
  }
}",0.9911971830985916
132650,"public static void checkCopy(Context context,Type srctype,Type dsttype) throws TypeMismatchException {
  if (!(srctype.assignableTo(dsttype) && srctype.getImplType().equals(dsttype.getImplType()))) {
    new Exception().printStackTrace();
    throw new TypeMismatchException(context,""String_Node_Str"" + srctype.toString() + ""String_Node_Str""+ dsttype.toString());
  }
}","public static void checkCopy(Context context,Type srctype,Type dsttype) throws TypeMismatchException {
  if (!(srctype.assignableTo(dsttype) && srctype.getImplType().equals(dsttype.getImplType()))) {
    throw new TypeMismatchException(context,""String_Node_Str"" + srctype.toString() + ""String_Node_Str""+ dsttype.toString());
  }
}",0.944206008583691
132651,"private static Type structLoad(Context context,SwiftAST tree) throws UserException, TypeMismatchException {
  Type structType=findExprType(context,tree.child(0));
  String fieldName=tree.child(1).getText();
  Type fieldType;
  fieldType=findStructFieldType(context,fieldName,structType);
  if (fieldType == null) {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ ((StructType)structType).getStructTypeName());
  }
  if (Types.isStruct(structType)) {
    return fieldType;
  }
 else {
    assert(Types.isStructRef(structType));
    return VarRepr.containerElemRepr(fieldType,false);
  }
}","private static Type structLoad(Context context,SwiftAST tree) throws UserException, TypeMismatchException {
  Type structType=findExprType(context,tree.child(0));
  String fieldName=tree.child(1).getText();
  Type fieldType;
  fieldType=findStructFieldType(context,fieldName,structType);
  if (fieldType == null) {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ ((StructType)structType).getStructTypeName());
  }
  return structLoadResultType(structType,fieldType);
}",0.8111401218450827
132652,"/** 
 * @param context
 * @param branchVUs The variable usage info for all branches
 * @param writtenVars All vars that might be written are added here
 * @throws UserException
 * @throws UndefinedTypeException
 */
private void summariseBranchVariableUsage(Context context,List<VariableUsageInfo> branchVUs,List<Var> writtenVars) throws UndefinedTypeException, UserException {
  for (  Var v : context.getVisibleVariables()) {
    if (Types.isArray(v.type())) {
      for (      VariableUsageInfo bvu : branchVUs) {
        VInfo vi=bvu.lookupVariableInfo(v.name());
        if (vi != null && vi.isAssigned() != Ternary.FALSE) {
          writtenVars.add(v);
          break;
        }
      }
    }
 else     if (Types.isStruct(v.type())) {
      ArrayList<Pair<Var,VInfo>> arrs=new ArrayList<Pair<Var,VInfo>>();
      HashSet<Var> alreadyFound=new HashSet<Var>();
      for (      VariableUsageInfo bvu : branchVUs) {
        arrs.clear();
        exprWalker.findArraysInStruct(context,v,bvu.lookupVariableInfo(v.name()),arrs);
        for (        Pair<Var,VInfo> p : arrs) {
          if (p.val2.isAssigned() != Ternary.FALSE) {
            alreadyFound.add(p.val1);
          }
        }
      }
      writtenVars.addAll(alreadyFound);
    }
  }
}","/** 
 * @param context
 * @param branchVUs The variable usage info for all branches
 * @param writtenVars All vars that might be written are added here
 * @throws UserException
 * @throws UndefinedTypeException
 */
private void summariseBranchVariableUsage(Context context,List<VariableUsageInfo> branchVUs,List<Var> writtenVars) throws UndefinedTypeException, UserException {
  for (  Var v : context.getVisibleVariables()) {
    if (Types.isArray(v.type())) {
      for (      VariableUsageInfo bvu : branchVUs) {
        VInfo vi=bvu.lookupVariableInfo(v.name());
        if (vi != null && vi.isAssigned() != Ternary.FALSE) {
          writtenVars.add(v);
          break;
        }
      }
    }
 else     if (Types.isStruct(v.type())) {
      ArrayList<Pair<Var,VInfo>> arrs=new ArrayList<Pair<Var,VInfo>>();
      HashSet<Var> alreadyFound=new HashSet<Var>();
      for (      VariableUsageInfo bvu : branchVUs) {
        arrs.clear();
        VInfo vi=bvu.lookupVariableInfo(v.name());
        if (vi != null) {
          exprWalker.findArraysInStruct(context,v,vi,arrs);
          for (          Pair<Var,VInfo> p : arrs) {
            if (p.val2.isAssigned() != Ternary.FALSE) {
              alreadyFound.add(p.val1);
            }
          }
        }
      }
      writtenVars.addAll(alreadyFound);
    }
  }
}",0.9172815533980584
132653,"private void findArraysInStructToClose(Context context,Var struct,VInfo structVInfo,StackLite<String> fieldPath,List<Pair<Var,VInfo>> arrays) throws UndefinedTypeException, UserException {
  assert(structVInfo != null);
  StructType vtype=(StructType)struct.type();
  for (  StructField f : vtype.getFields()) {
    fieldPath.push(f.getName());
    if (Types.isArray(f.getType())) {
      Var fieldVar=structLookup(context,struct,fieldPath,false);
      VInfo fieldInfo=structVInfo != null ? structVInfo.getFieldVInfo(f.getName()) : null;
      arrays.add(Pair.create(fieldVar,fieldInfo));
    }
 else     if (Types.isStruct(f.getType())) {
      VInfo nestedVInfo=structVInfo.getFieldVInfo(f.getName());
      findArraysInStructToClose(context,struct,nestedVInfo,fieldPath,arrays);
    }
    fieldPath.pop();
  }
}","/** 
 * @param context
 * @param rootStruct root struct
 * @param currStructType current structure type (of root of sub-structure)
 * @param currStructVInfo
 * @param fieldPath
 * @param arrays
 * @throws UndefinedTypeException
 * @throws UserException
 */
private void findArraysInStructToClose(Context context,Var rootStruct,StructType currStructType,VInfo currStructVInfo,StackLite<String> fieldPath,List<Pair<Var,VInfo>> arrays) throws UndefinedTypeException, UserException {
  assert(currStructVInfo != null);
  for (  StructField f : currStructType.getFields()) {
    fieldPath.push(f.getName());
    if (Types.isArray(f.getType())) {
      Var fieldVar=structLookup(context,rootStruct,fieldPath,false);
      VInfo fieldInfo=currStructVInfo != null ? currStructVInfo.getFieldVInfo(f.getName()) : null;
      arrays.add(Pair.create(fieldVar,fieldInfo));
    }
 else     if (Types.isStruct(f.getType())) {
      VInfo nestedVInfo=currStructVInfo.getFieldVInfo(f.getName());
      findArraysInStructToClose(context,rootStruct,(StructType)f.getType(),nestedVInfo,fieldPath,arrays);
    }
    fieldPath.pop();
  }
}",0.5693581780538303
132654,"void findArraysInStruct(Context context,Var root,VInfo structVInfo,List<Pair<Var,VInfo>> arrays) throws UndefinedTypeException, UserException {
  findArraysInStructToClose(context,root,structVInfo,new StackLite<String>(),arrays);
}","void findArraysInStruct(Context context,Var root,VInfo structVInfo,List<Pair<Var,VInfo>> arrays) throws UndefinedTypeException, UserException {
  assert(Types.isStruct(root));
  findArraysInStructToClose(context,root,(StructType)root.type(),structVInfo,new StackLite<String>(),arrays);
}",0.8918918918918919
132655,"/** 
 * Find the type of a particular field
 * @param context
 * @param fieldName
 * @param type a struct or a reference to a struct
 * @return
 * @throws TypeMismatchException if the type isn't a struct or struct ref,or the field doesn't exist in the type
 */
public static Type findStructFieldType(Context context,String fieldName,Type type) throws TypeMismatchException {
  StructType structType;
  if (Types.isStruct(type)) {
    structType=((StructType)type);
  }
 else   if (Types.isStructRef(type)) {
    structType=((StructType)(type.memberType()));
  }
 else {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ type.toString());
  }
  Type fieldType=structType.getFieldTypeByName(fieldName);
  if (fieldType == null) {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ structType.typeName());
  }
  return fieldType;
}","/** 
 * Find the type of a particular field
 * @param context
 * @param fieldName
 * @param type a struct or a reference to a struct
 * @return
 * @throws TypeMismatchException if the type isn't a struct or struct ref,or the field doesn't exist in the type
 */
public static Type findStructFieldType(Context context,String fieldName,Type type) throws TypeMismatchException {
  StructType structType;
  if (Types.isStruct(type)) {
    structType=((StructType)type);
  }
 else   if (Types.isStructRef(type)) {
    structType=((StructType)(type.memberType()));
  }
 else {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ type.toString());
  }
  Type fieldType=structType.getFieldTypeByName(fieldName);
  if (fieldType == null) {
    new Exception().printStackTrace();
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ structType.typeName()+ ""String_Node_Str""+ ""String_Node_Str""+ structType.getFields());
  }
  return fieldType;
}",0.9219638242894056
132656,"private List<Arg> canonicalizeAssignValue(GlobalConstants consts,ArgCV val){
  if (val.isArrayMemberVal() || val.isArrayMember()) {
    Arg arr=byAlias.findCanonical(val.getInput(0));
    Arg ix=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(arr,ix);
  }
 else   if (val.isStructFieldVal() || val.isStructFieldAlias() || val.isStructFieldCopy()|| val.isStructFieldValRef()) {
    Arg struct=byAlias.findCanonical(val.getInput(0));
    Arg fieldName=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(struct,fieldName);
  }
 else   if (val.isFilenameValCV() || val.isFilenameAliasCV() || val.isLocalFilenameCV()) {
    return Arrays.asList(Arg.createStringLit(""String_Node_Str""),byAlias.findCanonical(val.getInput(0)));
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + val + ""String_Node_Str"");
  }
}","private List<Arg> canonicalizeAssignValue(GlobalConstants consts,ArgCV val){
  if (val.isArrayMemberVal() || val.isArrayMember()) {
    Arg arr=byAlias.findCanonical(val.getInput(0));
    Arg ix=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(arr,ix);
  }
 else   if (val.isStructFieldVal() || val.isStructFieldAlias() || val.isStructFieldCopy()|| val.isStructFieldValRef()) {
    List<Arg> inputs=val.getInputs();
    List<Arg> result=new ArrayList<Arg>(inputs.size());
    assert(inputs.size() >= 2);
    Arg struct=inputs.get(0);
    result.add(byAlias.findCanonical(struct));
    for (int i=1; i < inputs.size(); i++) {
      Arg field=inputs.get(i);
      result.add(byValue.findCanonical(field));
    }
    return result;
  }
 else   if (val.isFilenameValCV() || val.isFilenameAliasCV() || val.isLocalFilenameCV()) {
    return Arrays.asList(Arg.createStringLit(""String_Node_Str""),byAlias.findCanonical(val.getInput(0)));
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + val + ""String_Node_Str"");
  }
}",0.7853347502656748
132657,"/** 
 * Called once all usage info is collected
 * @return violations if this is a struct and it is incompletely specified
 */
public List<Violation> isIncompletelyDefinedStruct(Context context){
  if (Types.isStruct(this.type)) {
    ArrayList<Violation> result=new ArrayList<Violation>();
    Ternary assigned=isAssigned();
    if (assigned == Ternary.TRUE) {
      return result;
    }
 else     if (assigned == Ternary.MAYBE && isRead() != Ternary.FALSE) {
      result.add(new Violation(ViolationType.WARNING,this.getName() + ""String_Node_Str"",context));
    }
    for (    VInfo vi : structFields.values()) {
      if (vi.isAssigned() != Ternary.TRUE) {
        if (vi.isAssigned() == Ternary.FALSE && vi.isRead() == Ternary.TRUE) {
          result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + vi.getName() + ""String_Node_Str""+ ""String_Node_Str"",context));
        }
 else         if (assigned != Ternary.FALSE) {
          result.add(new Violation(ViolationType.WARNING,vi.getName() + ""String_Node_Str"" + ""String_Node_Str"",context));
        }
      }
      List<Violation> more=vi.isIncompletelyDefinedStruct(context);
      if (more != null)       result.addAll(more);
    }
    return result;
  }
 else {
    return null;
  }
}","/** 
 * Called once all usage info is collected
 * @return violations if this is a struct and it is incompletely specified
 */
public List<Violation> isIncompletelyDefinedStruct(Context context){
  if (Types.isStruct(this.type)) {
    ArrayList<Violation> result=new ArrayList<Violation>();
    Ternary assigned=isAssigned();
    if (assigned == Ternary.TRUE) {
      return result;
    }
 else     if (assigned == Ternary.MAYBE && isRead() != Ternary.FALSE) {
      result.add(new Violation(ViolationType.WARNING,this.getName() + ""String_Node_Str"",context));
    }
    for (    VInfo vi : structFields.values()) {
      Ternary somehowAssigned=Ternary.or(vi.isAssigned(),vi.isPartAssigned());
      if (somehowAssigned != Ternary.TRUE) {
        if (somehowAssigned == Ternary.FALSE && vi.isRead() == Ternary.TRUE) {
          result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + vi.getName() + ""String_Node_Str""+ ""String_Node_Str"",context));
        }
 else         if (somehowAssigned != Ternary.FALSE) {
          result.add(new Violation(ViolationType.WARNING,vi.getName() + ""String_Node_Str"" + ""String_Node_Str"",context));
        }
      }
      List<Violation> more=vi.isIncompletelyDefinedStruct(context);
      if (more != null)       result.addAll(more);
    }
    return result;
  }
 else {
    return null;
  }
}",0.9428129829984544
132658,"private List<Violation> structAssign(Context context,List<String> fieldPath,int arrayDepth,AssignOp op){
  VInfo fieldVInfo;
  List<Violation> errs=new ArrayList<Violation>();
  if (structFields != null) {
    if (this.assigned == Ternary.TRUE) {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
 else     if (this.assigned == Ternary.MAYBE) {
      errs.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
    String field=fieldPath.get(0);
    if (structFields.containsKey(field)) {
      fieldVInfo=structFields.get(field);
    }
 else {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + field + ""String_Node_Str""+ this.name+ ""String_Node_Str""+ ((StructType)type).getStructTypeName(),context));
    }
  }
 else {
    return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + this.name + ""String_Node_Str"",context));
  }
  fieldPath.remove(0);
  errs.addAll(fieldVInfo.assign(context,fieldPath,arrayDepth,op));
  mergeStructFieldWriteInfo(context);
  return errs;
}","private List<Violation> structAssign(Context context,List<String> fieldPath,int arrayDepth,AssignOp op){
  VInfo fieldVInfo;
  List<Violation> errs=new ArrayList<Violation>();
  if (structFields != null) {
    if (this.assigned == Ternary.TRUE) {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
 else     if (this.assigned == Ternary.MAYBE) {
      errs.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
    String field=fieldPath.get(0);
    if (structFields.containsKey(field)) {
      fieldVInfo=structFields.get(field);
    }
 else {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + field + ""String_Node_Str""+ this.name+ ""String_Node_Str""+ ((StructType)type).getStructTypeName(),context));
    }
  }
 else {
    return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + this.name + ""String_Node_Str"",context));
  }
  fieldPath.remove(0);
  errs.addAll(fieldVInfo.assign(context,fieldPath,arrayDepth,op));
  return errs;
}",0.9756302521008404
132659,"private Var derefOrCopyResult(Context context,Var lookupResult,Var outVar) throws UndefinedTypeException, UserException {
  try {
    if (outVar == null) {
      return lookupResult;
    }
 else     if (Types.isAssignableRefTo(lookupResult.type(),outVar.type())) {
      dereference(context,outVar,lookupResult);
      return outVar;
    }
 else {
      copyByValue(context,outVar,lookupResult);
      return outVar;
    }
  }
 catch (  RuntimeException e) {
    Logging.getSTCLogger().debug(""String_Node_Str"" + lookupResult + ""String_Node_Str""+ outVar);
    throw e;
  }
}","private Var derefOrCopyResult(Context context,Var lookupResult,Var outVar) throws UndefinedTypeException, UserException {
  try {
    if (outVar == null) {
      return lookupResult;
    }
 else     if (Types.isRefTo(lookupResult.type(),outVar.type())) {
      dereference(context,outVar,lookupResult);
      return outVar;
    }
 else {
      copyByValue(context,outVar,lookupResult);
      return outVar;
    }
  }
 catch (  RuntimeException e) {
    Logging.getSTCLogger().debug(""String_Node_Str"" + lookupResult + ""String_Node_Str""+ outVar);
    throw e;
  }
}",0.9911971830985916
132660,"public static void checkCopy(Context context,Type srctype,Type dsttype) throws TypeMismatchException {
  if (!(srctype.assignableTo(dsttype) && srctype.getImplType().equals(dsttype.getImplType()))) {
    new Exception().printStackTrace();
    throw new TypeMismatchException(context,""String_Node_Str"" + srctype.toString() + ""String_Node_Str""+ dsttype.toString());
  }
}","public static void checkCopy(Context context,Type srctype,Type dsttype) throws TypeMismatchException {
  if (!(srctype.assignableTo(dsttype) && srctype.getImplType().equals(dsttype.getImplType()))) {
    throw new TypeMismatchException(context,""String_Node_Str"" + srctype.toString() + ""String_Node_Str""+ dsttype.toString());
  }
}",0.944206008583691
132661,"private static Type structLoad(Context context,SwiftAST tree) throws UserException, TypeMismatchException {
  Type structType=findExprType(context,tree.child(0));
  String fieldName=tree.child(1).getText();
  Type fieldType;
  fieldType=findStructFieldType(context,fieldName,structType);
  if (fieldType == null) {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ ((StructType)structType).getStructTypeName());
  }
  if (Types.isStruct(structType)) {
    return fieldType;
  }
 else {
    assert(Types.isStructRef(structType));
    return VarRepr.containerElemRepr(fieldType,false);
  }
}","private static Type structLoad(Context context,SwiftAST tree) throws UserException, TypeMismatchException {
  Type structType=findExprType(context,tree.child(0));
  String fieldName=tree.child(1).getText();
  Type fieldType;
  fieldType=findStructFieldType(context,fieldName,structType);
  if (fieldType == null) {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ ((StructType)structType).getStructTypeName());
  }
  return structLoadResultType(structType,fieldType);
}",0.8111401218450827
132662,"private static Type unpackedStructType(Type type,StructType structType){
  List<StructField> packedFields=structType.getFields();
  List<StructField> unpackedFields=new ArrayList<StructField>(packedFields.size());
  boolean differences=false;
  for (  StructField packedField : packedFields) {
    Type packedFieldType=packedField.getType();
    Type unpackedFieldType=unpackedType(packedFieldType);
    if (unpackedFieldType.equals(packedFieldType)) {
      unpackedFields.add(packedField);
    }
 else {
      StructField unpackedField=new StructField(unpackedFieldType,packedField.getName());
      unpackedFields.add(unpackedField);
      differences=true;
    }
  }
  if (differences) {
    return new StructType(true,""String_Node_Str"" + type.typeName(),unpackedFields);
  }
 else {
    return structType;
  }
}","private static Type unpackedStructType(Type type,StructType structType){
  List<StructField> packedFields=structType.getFields();
  List<StructField> unpackedFields=new ArrayList<StructField>(packedFields.size());
  boolean differences=false;
  for (  StructField packedField : packedFields) {
    Type packedFieldType=packedField.getType();
    Type unpackedFieldType=unpackedType(packedFieldType);
    if (unpackedFieldType.equals(packedFieldType)) {
      unpackedFields.add(packedField);
    }
 else {
      StructField unpackedField=new StructField(unpackedFieldType,packedField.getName());
      unpackedFields.add(unpackedField);
      differences=true;
    }
  }
  if (differences) {
    return new StructType(true,""String_Node_Str"" + type.typeName(),unpackedFields);
  }
 else {
    return StructType.localStruct(structType);
  }
}",0.9734299516908212
132663,"/** 
 * The type that would result from a retrieve operation
 * @param t
 * @return
 */
public static Type retrievedType(Typed t,boolean recursive){
  if (isScalarFuture(t) || isScalarUpdateable(t)) {
    return new ScalarValueType(t.type().primType());
  }
 else   if (isFile(t)) {
    return new FileValueType(t.type().fileKind());
  }
 else   if (isRef(t)) {
    return t.type().baseType().memberType();
  }
 else   if (recursive && (isContainer(t) || isContainerLocal(t))) {
    return unpackedType(t);
  }
 else   if (isArray(t)) {
    assert(!recursive);
    ArrayType at=(ArrayType)t.type().getImplType();
    Type retrievedMemberType=retrievedType(at.memberType(),false);
    return ArrayType.localArray(at.keyType(),retrievedMemberType);
  }
 else   if (isBag(t)) {
    assert(!recursive);
    BagType bt=(BagType)t.type().getImplType();
    Type retrievedMemberType=retrievedType(bt.memberType(),false);
    return BagType.localBag(retrievedMemberType);
  }
 else   if (isStruct(t)) {
    StructType st=(StructType)t.type().getImplType();
    if (recursive) {
      for (      StructField f : st.getFields()) {
        if (Types.isRef(f.getType())) {
          throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + st);
        }
      }
    }
    return StructType.localStruct(st);
  }
 else {
    throw new STCRuntimeError(t.type() + ""String_Node_Str"");
  }
}","/** 
 * The type that would result from a retrieve operation
 * @param t
 * @return
 */
public static Type retrievedType(Typed t,boolean recursive){
  if (isScalarFuture(t) || isScalarUpdateable(t)) {
    return new ScalarValueType(t.type().primType());
  }
 else   if (isFile(t)) {
    return new FileValueType(t.type().fileKind());
  }
 else   if (isRef(t)) {
    return t.type().baseType().memberType();
  }
 else   if (recursive && (isContainer(t) || isContainerLocal(t))) {
    return unpackedType(t);
  }
 else   if (isArray(t)) {
    assert(!recursive);
    ArrayType at=(ArrayType)t.type().getImplType();
    Type retrievedMemberType=retrievedType(at.memberType(),false);
    return ArrayType.localArray(at.keyType(),retrievedMemberType);
  }
 else   if (isBag(t)) {
    assert(!recursive);
    BagType bt=(BagType)t.type().getImplType();
    Type retrievedMemberType=retrievedType(bt.memberType(),false);
    return BagType.localBag(retrievedMemberType);
  }
 else   if (isStruct(t)) {
    StructType st=(StructType)t.type().getImplType();
    if (recursive) {
      return unpackedType(st);
    }
 else {
      return StructType.localStruct(st);
    }
  }
 else {
    throw new STCRuntimeError(t.type() + ""String_Node_Str"");
  }
}",0.8677087304613038
132664,"/** 
 * Fetch the value of a variable
 * @param block
 * @param instBuffer append fetch instruction to this list
 * @param var the variable to fetch the value of
 * @param acquireWrite if the var is a reference, do we acquirewrite refcounts?
 * @return variable holding value
 */
public static Var fetchValueOf(Block block,List<? super Instruction> instBuffer,Var var,String valName,boolean recursive,boolean acquireWrite){
  Type valueT;
  if (recursive && Types.isContainer(var)) {
    valueT=Types.unpackedType(var);
  }
 else {
    valueT=Types.retrievedType(var);
  }
  if (Types.isPrimUpdateable(var)) {
    Var value_v=createValueVar(valName,valueT,var);
    block.addVariable(value_v);
    instBuffer.add(TurbineOp.latestValue(value_v,var));
    return value_v;
  }
 else   if (Types.isPrimFuture(var)) {
    Var value_v=createValueVar(valName,valueT,var);
    block.addVariable(value_v);
    instBuffer.add(ICInstructions.retrievePrim(value_v,var));
    if (Types.isBlobVal(valueT)) {
      block.addCleanup(value_v,TurbineOp.freeBlob(value_v));
    }
    return value_v;
  }
 else   if (Types.isRef(var)) {
    Var deref=new Var(valueT,valName,Alloc.ALIAS,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    instBuffer.add(TurbineOp.retrieveRef(deref,var,acquireWrite));
    return deref;
  }
 else   if (Types.isContainer(var) && recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    instBuffer.add(TurbineOp.retrieveRecursive(deref,var));
    return deref;
  }
 else   if (Types.isStruct(var) && recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    Instruction structRetrieve=TurbineOp.retrieveStruct(deref,var);
    for (    StructField f : ((StructType)var.type().getImplType()).getFields()) {
      if (Types.isRef(f.getType())) {
        structRetrieve=TurbineOp.retrieveRecursive(deref,var);
      }
    }
    instBuffer.add(structRetrieve);
    return deref;
  }
 else   if ((Types.isContainer(var) || Types.isStruct(var)) && !recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    if (Types.isArray(var)) {
      instBuffer.add(TurbineOp.retrieveArray(deref,var));
    }
 else     if (Types.isBag(var)) {
      instBuffer.add(TurbineOp.retrieveBag(deref,var));
    }
 else {
      assert(Types.isStruct(var));
      instBuffer.add(TurbineOp.retrieveStruct(deref,var));
    }
    return deref;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"");
  }
}","/** 
 * Fetch the value of a variable
 * @param block
 * @param instBuffer append fetch instruction to this list
 * @param var the variable to fetch the value of
 * @param acquireWrite if the var is a reference, do we acquirewrite refcounts?
 * @return variable holding value
 */
public static Var fetchValueOf(Block block,List<? super Instruction> instBuffer,Var var,String valName,boolean recursive,boolean acquireWrite){
  Type valueT=Types.retrievedType(var,recursive);
  if (Types.isPrimUpdateable(var)) {
    Var value_v=createValueVar(valName,valueT,var);
    block.addVariable(value_v);
    instBuffer.add(TurbineOp.latestValue(value_v,var));
    return value_v;
  }
 else   if (Types.isPrimFuture(var)) {
    Var value_v=createValueVar(valName,valueT,var);
    block.addVariable(value_v);
    instBuffer.add(ICInstructions.retrievePrim(value_v,var));
    if (Types.isBlobVal(valueT)) {
      block.addCleanup(value_v,TurbineOp.freeBlob(value_v));
    }
    return value_v;
  }
 else   if (Types.isRef(var)) {
    Var deref=new Var(valueT,valName,Alloc.ALIAS,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    instBuffer.add(TurbineOp.retrieveRef(deref,var,acquireWrite));
    return deref;
  }
 else   if (Types.isContainer(var) && recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    instBuffer.add(TurbineOp.retrieveRecursive(deref,var));
    return deref;
  }
 else   if (Types.isStruct(var) && recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    boolean mustUseRecursive=false;
    for (    StructField f : ((StructType)var.type().getImplType()).getFields()) {
      if (Types.isRef(f.getType())) {
        mustUseRecursive=true;
      }
    }
    Instruction structRetrieve=mustUseRecursive ? TurbineOp.retrieveRecursive(deref,var) : TurbineOp.retrieveStruct(deref,var);
    instBuffer.add(structRetrieve);
    return deref;
  }
 else   if ((Types.isContainer(var) || Types.isStruct(var)) && !recursive) {
    Var deref=new Var(valueT,valName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.valueOf(var));
    block.addVariable(deref);
    if (Types.isArray(var)) {
      instBuffer.add(TurbineOp.retrieveArray(deref,var));
    }
 else     if (Types.isBag(var)) {
      instBuffer.add(TurbineOp.retrieveBag(deref,var));
    }
 else {
      assert(Types.isStruct(var));
      instBuffer.add(TurbineOp.retrieveStruct(deref,var));
    }
    return deref;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"");
  }
}",0.9292250233426704
132665,"private static Type unpackedStructType(Type type,StructType structType){
  List<StructField> packedFields=structType.getFields();
  List<StructField> unpackedFields=new ArrayList<StructField>(packedFields.size());
  boolean differences=false;
  for (  StructField packedField : packedFields) {
    Type packedFieldType=packedField.getType();
    Type unpackedFieldType=unpackedType(packedFieldType);
    if (unpackedFieldType.equals(packedFieldType)) {
      unpackedFields.add(packedField);
    }
 else {
      StructField unpackedField=new StructField(unpackedFieldType,packedField.getName());
      unpackedFields.add(unpackedField);
      differences=true;
    }
  }
  if (differences) {
    return new StructType(true,""String_Node_Str"" + type.typeName(),unpackedFields);
  }
 else {
    return StructType.localStruct(structType);
  }
}","private static Type unpackedStructType(StructType structType){
  List<StructField> packedFields=structType.getFields();
  List<StructField> unpackedFields=new ArrayList<StructField>(packedFields.size());
  boolean differences=false;
  for (  StructField packedField : packedFields) {
    Type packedFieldType=packedField.getType();
    Type unpackedFieldType;
    if (Types.isRef(packedFieldType)) {
      unpackedFieldType=unpackedType(packedFieldType);
    }
 else     if (Types.isStruct(packedFieldType)) {
      unpackedFieldType=unpackedStructType((StructType)packedFieldType);
    }
 else {
      unpackedFieldType=packedFieldType;
    }
    if (unpackedFieldType.equals(packedFieldType)) {
      unpackedFields.add(packedField);
    }
 else {
      StructField unpackedField=new StructField(unpackedFieldType,packedField.getName());
      unpackedFields.add(unpackedField);
      System.err.println(unpackedField + ""String_Node_Str"" + packedField);
      differences=true;
    }
  }
  if (differences) {
    return new StructType(true,""String_Node_Str"" + structType.typeName(),unpackedFields);
  }
 else {
    return StructType.localStruct(structType);
  }
}",0.7391521197007481
132666,"/** 
 * Work out type of variable if we recursively extract all values into local variables
 * @param t
 * @return
 */
public static Type unpackedType(Typed t){
  Type type=stripRefs(t.type());
  if (Types.isContainer(type) || Types.isContainerLocal(type)) {
    return unpackedContainerType(type);
  }
 else   if (Types.isStruct(type) || Types.isStructLocal(type)) {
    return unpackedStructType(type,(StructType)type);
  }
 else {
    while (Types.canRetrieve(type)) {
      type=retrievedType(type);
    }
    return type;
  }
}","/** 
 * Work out type of variable if we recursively extract all values into local variables
 * @param t
 * @return
 */
public static Type unpackedType(Typed t){
  Type type=stripRefs(t.type());
  if (Types.isContainer(type) || Types.isContainerLocal(type)) {
    return unpackedContainerType(type);
  }
 else   if (Types.isStruct(type) || Types.isStructLocal(type)) {
    return unpackedStructType((StructType)type);
  }
 else {
    while (Types.canRetrieve(type)) {
      type=retrievedType(type);
    }
    return type;
  }
}",0.9952785646836638
132667,"/** 
 * Retrieve a struct directly to a local struct, without following any references
 * @param dst
 * @param src non-recursively closed struct
 * @return
 */
public static Instruction retrieveStruct(Var dst,Var src){
  assert(Types.isStruct(src.type()));
  assert(Types.isStructLocal(dst));
  assert(StructType.localStruct((StructType)src.type().getImplType()).assignableTo(dst.type()));
  return new TurbineOp(Opcode.LOAD_STRUCT,dst,src.asArg());
}","/** 
 * Retrieve a struct directly to a local struct, without following any references
 * @param dst
 * @param src non-recursively closed struct
 * @return
 */
public static Instruction retrieveStruct(Var dst,Var src){
}",0.6557377049180327
132668,"private List<Violation> arrayAssign(Context context,int arrayDepth,AssignOp op){
  if (assigned != Ternary.FALSE) {
    if (arrayAssignDepth != arrayDepth) {
      return Arrays.asList(makeArrDepthViolation(context,arrayDepth));
    }
    this.assigned=Ternary.TRUE;
  }
 else {
    this.assigned=Ternary.TRUE;
    arrayAssignDepth=arrayDepth;
  }
  return Collections.emptyList();
}","private List<Violation> arrayAssign(Context context,int arrayDepth,AssignOp op){
  this.assigned=Ternary.TRUE;
  return Collections.emptyList();
}",0.5519848771266541
132669,"/** 
 * make a copy, except it might be in a different scope All of the usage tracking is reset
 */
public VInfo makeEmptyCopy(boolean locallyDeclared){
  HashMap<String,VInfo> structFieldsNew=null;
  if (structFields != null) {
    structFieldsNew=new HashMap<String,VInfo>();
    for (    Entry<String,VInfo> e : structFields.entrySet()) {
      structFieldsNew.put(e.getKey(),e.getValue().makeEmptyCopy(locallyDeclared));
    }
  }
  return new VInfo(type,hasMapping,structFieldsNew,locallyDeclared,name,Ternary.FALSE,Ternary.FALSE,Ternary.FALSE,0,-1);
}","/** 
 * make a copy, except it might be in a different scope All of the usage tracking is reset
 */
public VInfo makeEmptyCopy(boolean locallyDeclared){
  HashMap<String,VInfo> structFieldsNew=null;
  if (structFields != null) {
    structFieldsNew=new HashMap<String,VInfo>();
    for (    Entry<String,VInfo> e : structFields.entrySet()) {
      structFieldsNew.put(e.getKey(),e.getValue().makeEmptyCopy(locallyDeclared));
    }
  }
  return new VInfo(type,hasMapping,structFieldsNew,locallyDeclared,name,Ternary.FALSE,Ternary.FALSE,Ternary.FALSE,-1);
}",0.9982014388489208
132670,"private VInfo(Type type,boolean mapped,boolean locallyDeclared,String name,Ternary assigned,Ternary read,Ternary appended,int arrayAssignDepth,int maxReadDepth){
  this.type=type;
  this.hasMapping=mapped;
  if (Types.isStruct(type)) {
    structFields=new HashMap<String,VInfo>();
    for (    StructField f : ((StructType)type.getImplType()).getFields()) {
      VInfo fieldVInfo=new VInfo(f.getType(),mapped,name + ""String_Node_Str"" + f.getName(),locallyDeclared);
      structFields.put(f.getName(),fieldVInfo);
    }
  }
 else {
    structFields=null;
  }
  this.declaredInCurrentScope=locallyDeclared;
  this.name=name;
  this.assigned=assigned;
  this.appended=appended;
  this.read=read;
  this.arrayAssignDepth=arrayAssignDepth;
  this.maxReadDepth=maxReadDepth;
}","private VInfo(Type type,boolean mapped,boolean locallyDeclared,String name,Ternary assigned,Ternary read,Ternary appended,int maxReadDepth){
  this.type=type;
  this.hasMapping=mapped;
  if (Types.isStruct(type)) {
    structFields=new HashMap<String,VInfo>();
    for (    StructField f : ((StructType)type.getImplType()).getFields()) {
      VInfo fieldVInfo=new VInfo(f.getType(),mapped,name + ""String_Node_Str"" + f.getName(),locallyDeclared);
      structFields.put(f.getName(),fieldVInfo);
    }
  }
 else {
    structFields=null;
  }
  this.declaredInCurrentScope=locallyDeclared;
  this.name=name;
  this.assigned=assigned;
  this.appended=appended;
  this.read=read;
  this.maxReadDepth=maxReadDepth;
}",0.9575185434929198
132671,"/** 
 * Merge in information about writes to this variable that can occur in nested contexts
 * @param file
 * @param line
 * @param branches
 * @param exhaustive
 * @return
 */
private ArrayList<Violation> mergeBranchWriteInfo(Context context,List<VInfo> branches,boolean exhaustive){
  ArrayList<Violation> result=new ArrayList<Violation>();
  Ternary assignedInBranch;
  Ternary appendedInBranch;
  int expectAssignedDepth;
  if (exhaustive) {
    VInfo firstBranch=branches.get(0);
    assignedInBranch=firstBranch.assigned;
    appendedInBranch=firstBranch.appended;
    expectAssignedDepth=firstBranch.arrayAssignDepth;
  }
 else {
    assignedInBranch=Ternary.FALSE;
    appendedInBranch=Ternary.FALSE;
    expectAssignedDepth=0;
  }
  for (int i=0; i < branches.size(); i++) {
    VInfo currBr=branches.get(i);
    if (assignedInBranch == Ternary.FALSE) {
      expectAssignedDepth=currBr.arrayAssignDepth;
    }
    if (assignedInBranch != Ternary.FALSE && currBr.assigned != Ternary.FALSE) {
      if (currBr.arrayAssignDepth != expectAssignedDepth) {
        result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str""+ ""String_Node_Str"",context));
      }
    }
    assignedInBranch=Ternary.consensus(assignedInBranch,currBr.assigned);
    appendedInBranch=Ternary.consensus(appendedInBranch,currBr.appended);
  }
  this.appended=Ternary.or(appended,appendedInBranch);
  if (assignedInBranch == Ternary.FALSE) {
    return result;
  }
 else   if (this.assigned == Ternary.FALSE) {
    this.assigned=assignedInBranch;
    this.arrayAssignDepth=expectAssignedDepth;
    return result;
  }
 else   if (Types.isArray(this.type)) {
    this.assigned=Ternary.or(assigned,assignedInBranch);
    if (this.arrayAssignDepth != expectAssignedDepth) {
      result.add(makeArrDepthViolation(context,expectAssignedDepth));
    }
    return result;
  }
 else {
    if (Ternary.and(assignedInBranch,this.assigned) == Ternary.TRUE) {
      result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str"",context));
      return result;
    }
    if (assignedInBranch == Ternary.MAYBE) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    if ((this.assigned != Ternary.FALSE && assignedInBranch == Ternary.MAYBE) || (assignedInBranch != Ternary.FALSE && this.assigned == Ternary.MAYBE)) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    this.assigned=Ternary.or(this.assigned,assignedInBranch);
    return result;
  }
}","/** 
 * Merge in information about writes to this variable that can occur in nested contexts
 * @param file
 * @param line
 * @param branches
 * @param exhaustive
 * @return
 */
private ArrayList<Violation> mergeBranchWriteInfo(Context context,List<VInfo> branches,boolean exhaustive){
  ArrayList<Violation> result=new ArrayList<Violation>();
  Ternary assignedInBranch;
  Ternary appendedInBranch;
  if (exhaustive) {
    VInfo firstBranch=branches.get(0);
    assignedInBranch=firstBranch.assigned;
    appendedInBranch=firstBranch.appended;
  }
 else {
    assignedInBranch=Ternary.FALSE;
    appendedInBranch=Ternary.FALSE;
  }
  for (int i=0; i < branches.size(); i++) {
    VInfo currBr=branches.get(i);
    assignedInBranch=Ternary.consensus(assignedInBranch,currBr.assigned);
    appendedInBranch=Ternary.consensus(appendedInBranch,currBr.appended);
  }
  this.appended=Ternary.or(appended,appendedInBranch);
  if (assignedInBranch == Ternary.FALSE) {
    return result;
  }
 else   if (this.assigned == Ternary.FALSE) {
    this.assigned=assignedInBranch;
    return result;
  }
 else   if (Types.isArray(this.type)) {
    this.assigned=Ternary.or(assigned,assignedInBranch);
    return result;
  }
 else {
    if (Ternary.and(assignedInBranch,this.assigned) == Ternary.TRUE) {
      result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str"",context));
      return result;
    }
    if (assignedInBranch == Ternary.MAYBE) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    if ((this.assigned != Ternary.FALSE && assignedInBranch == Ternary.MAYBE) || (assignedInBranch != Ternary.FALSE && this.assigned == Ternary.MAYBE)) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    this.assigned=Ternary.or(this.assigned,assignedInBranch);
    return result;
  }
}",0.4425868895732512
132672,"/** 
 * @param context
 * @param branchVUs The variable usage info for all branches
 * @param writtenVars All vars that might be written are added here
 * @throws UserException
 * @throws UndefinedTypeException
 */
private void summariseBranchVariableUsage(Context context,List<VariableUsageInfo> branchVUs,List<Var> writtenVars) throws UndefinedTypeException, UserException {
  for (  Var v : context.getVisibleVariables()) {
    if (Types.isArray(v.type())) {
      for (      VariableUsageInfo bvu : branchVUs) {
        VInfo vi=bvu.lookupVariableInfo(v.name());
        if (vi != null && vi.isAssigned() != Ternary.FALSE) {
          writtenVars.add(v);
          break;
        }
      }
    }
 else     if (Types.isStruct(v.type())) {
      ArrayList<Pair<Var,VInfo>> arrs=new ArrayList<Pair<Var,VInfo>>();
      HashSet<Var> alreadyFound=new HashSet<Var>();
      for (      VariableUsageInfo bvu : branchVUs) {
        arrs.clear();
        exprWalker.findArraysInStruct(context,v,bvu.lookupVariableInfo(v.name()),arrs);
        for (        Pair<Var,VInfo> p : arrs) {
          if (p.val2.isAssigned() != Ternary.FALSE) {
            alreadyFound.add(p.val1);
          }
        }
      }
      writtenVars.addAll(alreadyFound);
    }
  }
}","/** 
 * @param context
 * @param branchVUs The variable usage info for all branches
 * @param writtenVars All vars that might be written are added here
 * @throws UserException
 * @throws UndefinedTypeException
 */
private void summariseBranchVariableUsage(Context context,List<VariableUsageInfo> branchVUs,List<Var> writtenVars) throws UndefinedTypeException, UserException {
  for (  Var v : context.getVisibleVariables()) {
    if (Types.isArray(v.type())) {
      for (      VariableUsageInfo bvu : branchVUs) {
        VInfo vi=bvu.lookupVariableInfo(v.name());
        if (vi != null && vi.isAssigned() != Ternary.FALSE) {
          writtenVars.add(v);
          break;
        }
      }
    }
 else     if (Types.isStruct(v.type())) {
      ArrayList<Pair<Var,VInfo>> arrs=new ArrayList<Pair<Var,VInfo>>();
      HashSet<Var> alreadyFound=new HashSet<Var>();
      for (      VariableUsageInfo bvu : branchVUs) {
        arrs.clear();
        VInfo vi=bvu.lookupVariableInfo(v.name());
        if (vi != null) {
          exprWalker.findArraysInStruct(context,v,vi,arrs);
          for (          Pair<Var,VInfo> p : arrs) {
            if (p.val2.isAssigned() != Ternary.FALSE) {
              alreadyFound.add(p.val1);
            }
          }
        }
      }
      writtenVars.addAll(alreadyFound);
    }
  }
}",0.9172815533980584
132673,"private void findArraysInStructToClose(Context context,Var struct,VInfo structVInfo,StackLite<String> fieldPath,List<Pair<Var,VInfo>> arrays) throws UndefinedTypeException, UserException {
  assert(structVInfo != null);
  StructType vtype=(StructType)struct.type();
  for (  StructField f : vtype.getFields()) {
    fieldPath.push(f.getName());
    if (Types.isArray(f.getType())) {
      Var fieldVar=structLookup(context,struct,fieldPath,false);
      VInfo fieldInfo=structVInfo != null ? structVInfo.getFieldVInfo(f.getName()) : null;
      arrays.add(Pair.create(fieldVar,fieldInfo));
    }
 else     if (Types.isStruct(f.getType())) {
      VInfo nestedVInfo=structVInfo.getFieldVInfo(f.getName());
      findArraysInStructToClose(context,struct,nestedVInfo,fieldPath,arrays);
    }
    fieldPath.pop();
  }
}","/** 
 * @param context
 * @param rootStruct root struct
 * @param currStructType current structure type (of root of sub-structure)
 * @param currStructVInfo
 * @param fieldPath
 * @param arrays
 * @throws UndefinedTypeException
 * @throws UserException
 */
private void findArraysInStructToClose(Context context,Var rootStruct,StructType currStructType,VInfo currStructVInfo,StackLite<String> fieldPath,List<Pair<Var,VInfo>> arrays) throws UndefinedTypeException, UserException {
  assert(currStructVInfo != null);
  for (  StructField f : currStructType.getFields()) {
    fieldPath.push(f.getName());
    if (Types.isArray(f.getType())) {
      Var fieldVar=structLookup(context,rootStruct,fieldPath,false);
      VInfo fieldInfo=currStructVInfo != null ? currStructVInfo.getFieldVInfo(f.getName()) : null;
      arrays.add(Pair.create(fieldVar,fieldInfo));
    }
 else     if (Types.isStruct(f.getType())) {
      VInfo nestedVInfo=currStructVInfo.getFieldVInfo(f.getName());
      findArraysInStructToClose(context,rootStruct,(StructType)f.getType(),nestedVInfo,fieldPath,arrays);
    }
    fieldPath.pop();
  }
}",0.5693581780538303
132674,"void findArraysInStruct(Context context,Var root,VInfo structVInfo,List<Pair<Var,VInfo>> arrays) throws UndefinedTypeException, UserException {
  findArraysInStructToClose(context,root,structVInfo,new StackLite<String>(),arrays);
}","void findArraysInStruct(Context context,Var root,VInfo structVInfo,List<Pair<Var,VInfo>> arrays) throws UndefinedTypeException, UserException {
  assert(Types.isStruct(root));
  findArraysInStructToClose(context,root,(StructType)root.type(),structVInfo,new StackLite<String>(),arrays);
}",0.8918918918918919
132675,"/** 
 * Find the type of a particular field
 * @param context
 * @param fieldName
 * @param type a struct or a reference to a struct
 * @return
 * @throws TypeMismatchException if the type isn't a struct or struct ref,or the field doesn't exist in the type
 */
public static Type findStructFieldType(Context context,String fieldName,Type type) throws TypeMismatchException {
  StructType structType;
  if (Types.isStruct(type)) {
    structType=((StructType)type);
  }
 else   if (Types.isStructRef(type)) {
    structType=((StructType)(type.memberType()));
  }
 else {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ type.toString());
  }
  Type fieldType=structType.getFieldTypeByName(fieldName);
  if (fieldType == null) {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ structType.typeName());
  }
  return fieldType;
}","/** 
 * Find the type of a particular field
 * @param context
 * @param fieldName
 * @param type a struct or a reference to a struct
 * @return
 * @throws TypeMismatchException if the type isn't a struct or struct ref,or the field doesn't exist in the type
 */
public static Type findStructFieldType(Context context,String fieldName,Type type) throws TypeMismatchException {
  StructType structType;
  if (Types.isStruct(type)) {
    structType=((StructType)type);
  }
 else   if (Types.isStructRef(type)) {
    structType=((StructType)(type.memberType()));
  }
 else {
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ type.toString());
  }
  Type fieldType=structType.getFieldTypeByName(fieldName);
  if (fieldType == null) {
    new Exception().printStackTrace();
    throw new TypeMismatchException(context,""String_Node_Str"" + fieldName + ""String_Node_Str""+ structType.typeName()+ ""String_Node_Str""+ ""String_Node_Str""+ structType.getFields());
  }
  return fieldType;
}",0.9219638242894056
132676,"private List<Arg> canonicalizeAssignValue(GlobalConstants consts,ArgCV val){
  if (val.isArrayMemberVal() || val.isArrayMember()) {
    Arg arr=byAlias.findCanonical(val.getInput(0));
    Arg ix=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(arr,ix);
  }
 else   if (val.isStructFieldVal() || val.isStructFieldAlias() || val.isStructFieldCopy()|| val.isStructFieldValRef()) {
    Arg struct=byAlias.findCanonical(val.getInput(0));
    Arg fieldName=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(struct,fieldName);
  }
 else   if (val.isFilenameValCV() || val.isFilenameAliasCV() || val.isLocalFilenameCV()) {
    return Arrays.asList(Arg.createStringLit(""String_Node_Str""),byAlias.findCanonical(val.getInput(0)));
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + val + ""String_Node_Str"");
  }
}","private List<Arg> canonicalizeAssignValue(GlobalConstants consts,ArgCV val){
  if (val.isArrayMemberVal() || val.isArrayMember()) {
    Arg arr=byAlias.findCanonical(val.getInput(0));
    Arg ix=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(arr,ix);
  }
 else   if (val.isStructFieldVal() || val.isStructFieldAlias() || val.isStructFieldCopy()|| val.isStructFieldValRef()) {
    List<Arg> inputs=val.getInputs();
    List<Arg> result=new ArrayList<Arg>(inputs.size());
    assert(inputs.size() >= 2);
    Arg struct=inputs.get(0);
    result.add(byAlias.findCanonical(struct));
    for (int i=1; i < inputs.size(); i++) {
      Arg field=inputs.get(i);
      result.add(byValue.findCanonical(field));
    }
    return result;
  }
 else   if (val.isFilenameValCV() || val.isFilenameAliasCV() || val.isLocalFilenameCV()) {
    return Arrays.asList(Arg.createStringLit(""String_Node_Str""),byAlias.findCanonical(val.getInput(0)));
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + val + ""String_Node_Str"");
  }
}",0.7853347502656748
132677,"private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  assert(start.isImmediateFloat());
  assert(end.isImmediateFloat());
  assert(increment.isImmediateFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_ITERSTOTAL);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.paren(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}","private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  assert(start.isImmediateFloat());
  assert(end.isImmediateFloat());
  assert(increment.isImmediateFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_FLOAT_RANGE_ITERMAX);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.paren(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}",0.7389720760825577
132678,"private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  assert(start.isImmediateFloat());
  assert(end.isImmediateFloat());
  assert(increment.isImmediateFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_ITERSTOTAL);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.group(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}","private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  assert(start.isImmediateFloat());
  assert(end.isImmediateFloat());
  assert(increment.isImmediateFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_ITERSTOTAL);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.paren(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}",0.9959382615759546
132679,"private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  assert(start.isImmediateFloat());
  assert(end.isImmediateFloat());
  assert(increment.isImmediateFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_ITERSTOTAL);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.paren(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}","private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  assert(start.isImmediateFloat());
  assert(end.isImmediateFloat());
  assert(increment.isImmediateFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_FLOAT_RANGE_ITERMAX);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.paren(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}",0.7389720760825577
132680,"private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  assert(start.isImmediateFloat());
  assert(end.isImmediateFloat());
  assert(increment.isImmediateFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_ITERSTOTAL);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.group(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}","private void startFloatRangeLoop(String loopName,Var loopVar,Arg start,Arg end,Arg increment,int splitDegree,int leafDegree,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  assert(start.isImmediateFloat());
  assert(end.isImmediateFloat());
  assert(increment.isImmediateFloat());
  assert(Types.isFloatVal(loopVar));
  Expression startE=argToExpr(start);
  Expression endE=argToExpr(end);
  Expression incrE=argToExpr(increment);
  Value iterLimitVar=new Value(TCLTMP_ITERSTOTAL);
  pointAdd(new SetVariable(iterLimitVar.variable(),new TclExpr(TclExpr.exprFn(TclExpr.INT_CONV,TclExpr.group(TclExpr.exprFn(TclExpr.FLOOR,TclExpr.group(TclExpr.paren(endE,TclExpr.MINUS,startE,TclExpr.PLUS,incrE),TclExpr.DIV,incrE)))),TclExpr.MINUS,LiteralInt.ONE)));
  Value dummyLoopVar=new Value(TCLTMP_FLOAT_RANGE_ITER);
  List<PassedVar> passedVars2=PassedVar.mergeLists(passedVars,PassedVar.fromArgs(false,start,increment));
  startIntRangeLoop2(loopName,dummyLoopVar.variable(),LiteralInt.ZERO,iterLimitVar,LiteralInt.ONE,splitDegree,leafDegree,passedVars2,perIterIncrs,constIncrs);
  pointAdd(new SetVariable(prefixVar(loopVar),new TclExpr(startE,TclExpr.PLUS,incrE,TclExpr.TIMES,dummyLoopVar)));
}",0.9959382615759546
132681,"/** 
 * Helper function to insert element into bag, selecting appropriate backend implementation.
 * @param context
 * @param bag
 * @param elem
 * @return
 * @throws UserException
 * @throws UndefinedTypeException
 */
private Var backendBagAppend(Context context,Var bag,Var elem) throws UserException {
  Var stmtResultVar=elem;
  Type elemType=Types.containerElemType(bag);
  boolean bagRef=Types.isBagRef(bag);
  boolean elemRef=Types.isRefTo(elem,elemType);
  boolean openWait1=bagRef || elemRef;
  if (openWait1) {
    String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
    List<Var> waitVars=new ArrayList<Var>(2);
    if (bagRef) {
      waitVars.add(bag);
    }
    if (elemRef) {
      waitVars.add(elem);
    }
    backend.startWaitStatement(waitName,VarRepr.backendVars(waitVars),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedAny());
    if (bagRef) {
      Var derefedBag=varCreator.createTmpAlias(context,Types.retrievedType(bag));
      exprWalker.retrieveRef(VarRepr.backendVar(derefedBag),VarRepr.backendVar(bag),true);
      bag=derefedBag;
    }
    if (elemRef) {
      stmtResultVar=elem;
      Var derefedElem=varCreator.createTmpAlias(context,elemType);
      exprWalker.retrieveRef(VarRepr.backendVar(derefedElem),VarRepr.backendVar(elem),true);
      elem=derefedElem;
    }
  }
  Var elemVal;
  boolean openWait2=!VarRepr.storeRefInContainer(elem);
  if (openWait2) {
    String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
    backend.startWaitStatement(waitName,VarRepr.backendVars(elem),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedAny());
    elemVal=exprWalker.retrieveToVar(context,elem);
  }
 else {
    elemVal=elem;
  }
  backend.bagInsert(VarRepr.backendVar(bag),VarRepr.backendArg(elemVal));
  if (openWait2) {
    backend.endWaitStatement();
  }
  if (openWait1) {
    backend.endWaitStatement();
  }
  return stmtResultVar;
}","/** 
 * Helper function to insert element into bag, selecting appropriate backend implementation.
 * @param context
 * @param bag
 * @param elem
 * @return
 * @throws UserException
 * @throws UndefinedTypeException
 */
private Var backendBagAppend(Context context,Var bag,Var elem) throws UserException {
  Var stmtResultVar=elem;
  Type elemType=Types.containerElemType(bag);
  boolean bagRef=Types.isBagRef(bag);
  assert(!bagRef || Types.isRef(bag,true));
  boolean elemRef=Types.isRefTo(elem,elemType);
  boolean openWait1=bagRef || elemRef;
  if (openWait1) {
    String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
    List<Var> waitVars=new ArrayList<Var>(2);
    if (bagRef) {
      waitVars.add(bag);
    }
    if (elemRef) {
      waitVars.add(elem);
    }
    backend.startWaitStatement(waitName,VarRepr.backendVars(waitVars),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedAny());
    if (bagRef) {
      Var derefedBag=varCreator.createTmpAlias(context,Types.retrievedType(bag));
      exprWalker.retrieveRef(VarRepr.backendVar(derefedBag),VarRepr.backendVar(bag),true);
      bag=derefedBag;
    }
    if (elemRef) {
      stmtResultVar=elem;
      Var derefedElem=varCreator.createTmpAlias(context,elemType);
      exprWalker.retrieveRef(VarRepr.backendVar(derefedElem),VarRepr.backendVar(elem),false);
      elem=derefedElem;
    }
  }
  Var elemVal;
  boolean openWait2=!VarRepr.storeRefInContainer(elem);
  if (openWait2) {
    String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
    backend.startWaitStatement(waitName,VarRepr.backendVars(elem),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedAny());
    elemVal=exprWalker.retrieveToVar(context,elem);
  }
 else {
    elemVal=elem;
  }
  backend.bagInsert(VarRepr.backendVar(bag),VarRepr.backendArg(elemVal));
  if (openWait2) {
    backend.endWaitStatement();
  }
  if (openWait1) {
    backend.endWaitStatement();
  }
  return stmtResultVar;
}",0.9869931140015302
132682,"private static TypeMismatchException argumentTypeException(Context context,int argPos,Type expType,Type actType,String errContext){
  new Exception().printStackTrace();
  return new TypeMismatchException(context,""String_Node_Str"" + (argPos + 1) + ""String_Node_Str""+ expType.typeName()+ ""String_Node_Str""+ actType.typeName()+ errContext);
}","private static TypeMismatchException argumentTypeException(Context context,int argPos,Type expType,Type actType,String errContext){
  return new TypeMismatchException(context,""String_Node_Str"" + (argPos + 1) + ""String_Node_Str""+ expType.typeName()+ ""String_Node_Str""+ actType.typeName()+ errContext);
}",0.9422776911076444
132683,"/** 
 * Retrieve a reference to a local handle
 * @param dst alias variable to hold handle to referenced data
 * @param src Closed reference
 * @param acquireRead num of read refcounts to acquire
 * @param acquireWrite num of write refcounts to acquire
 * @param decrRead num of read refcounts to decr on input
 * @return
 */
public static Instruction retrieveRef(Var dst,Var src,long acquireRead,long acquireWrite,Arg decrRead){
  assert(Types.isRef(src.type()));
  assert(acquireRead >= 0);
  assert(acquireWrite >= 0);
  assert(decrRead.isImmediateInt());
  if (acquireWrite > 0) {
    assert(Types.isAssignableRefTo(src.type(),dst.type(),true));
  }
 else {
    assert(Types.isAssignableRefTo(src.type(),dst.type()));
  }
  assert(dst.storage() == Alloc.ALIAS);
  return new TurbineOp(Opcode.LOAD_REF,dst,src.asArg(),Arg.createIntLit(acquireRead),Arg.createIntLit(acquireWrite),decrRead);
}","/** 
 * Retrieve a reference to a local handle
 * @param dst alias variable to hold handle to referenced data
 * @param src Closed reference
 * @param acquireRead num of read refcounts to acquire
 * @param acquireWrite num of write refcounts to acquire
 * @param decrRead num of read refcounts to decr on input
 * @return
 */
public static Instruction retrieveRef(Var dst,Var src,long acquireRead,long acquireWrite,Arg decrRead){
}",0.6505660377358491
132684,"/** 
 * Helper function to insert element into bag, selecting appropriate backend implementation.
 * @param context
 * @param bag
 * @param elem
 * @return
 * @throws UserException
 * @throws UndefinedTypeException
 */
private Var backendBagAppend(Context context,Var bag,Var elem) throws UserException {
  Var stmtResultVar=elem;
  Type elemType=Types.containerElemType(bag);
  boolean bagRef=Types.isBagRef(bag);
  boolean elemRef=Types.isRefTo(elem,elemType);
  boolean openWait1=bagRef || elemRef;
  if (openWait1) {
    String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
    List<Var> waitVars=new ArrayList<Var>(2);
    if (bagRef) {
      waitVars.add(bag);
    }
    if (elemRef) {
      waitVars.add(elem);
    }
    backend.startWaitStatement(waitName,VarRepr.backendVars(waitVars),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedAny());
    if (bagRef) {
      Var derefedBag=varCreator.createTmpAlias(context,Types.retrievedType(bag));
      exprWalker.retrieveRef(VarRepr.backendVar(derefedBag),VarRepr.backendVar(bag),true);
      bag=derefedBag;
    }
    if (elemRef) {
      stmtResultVar=elem;
      Var derefedElem=varCreator.createTmpAlias(context,elemType);
      exprWalker.retrieveRef(VarRepr.backendVar(derefedElem),VarRepr.backendVar(elem),true);
      elem=derefedElem;
    }
  }
  Var elemVal;
  boolean openWait2=!VarRepr.storeRefInContainer(elem);
  if (openWait2) {
    String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
    backend.startWaitStatement(waitName,VarRepr.backendVars(elem),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedAny());
    elemVal=exprWalker.retrieveToVar(context,elem);
  }
 else {
    elemVal=elem;
  }
  backend.bagInsert(VarRepr.backendVar(bag),VarRepr.backendArg(elemVal));
  if (openWait2) {
    backend.endWaitStatement();
  }
  if (openWait1) {
    backend.endWaitStatement();
  }
  return stmtResultVar;
}","/** 
 * Helper function to insert element into bag, selecting appropriate backend implementation.
 * @param context
 * @param bag
 * @param elem
 * @return
 * @throws UserException
 * @throws UndefinedTypeException
 */
private Var backendBagAppend(Context context,Var bag,Var elem) throws UserException {
  Var stmtResultVar=elem;
  Type elemType=Types.containerElemType(bag);
  boolean bagRef=Types.isBagRef(bag);
  assert(!bagRef || Types.isRef(bag,true));
  boolean elemRef=Types.isRefTo(elem,elemType);
  boolean openWait1=bagRef || elemRef;
  if (openWait1) {
    String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
    List<Var> waitVars=new ArrayList<Var>(2);
    if (bagRef) {
      waitVars.add(bag);
    }
    if (elemRef) {
      waitVars.add(elem);
    }
    backend.startWaitStatement(waitName,VarRepr.backendVars(waitVars),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedAny());
    if (bagRef) {
      Var derefedBag=varCreator.createTmpAlias(context,Types.retrievedType(bag));
      exprWalker.retrieveRef(VarRepr.backendVar(derefedBag),VarRepr.backendVar(bag),true);
      bag=derefedBag;
    }
    if (elemRef) {
      stmtResultVar=elem;
      Var derefedElem=varCreator.createTmpAlias(context,elemType);
      exprWalker.retrieveRef(VarRepr.backendVar(derefedElem),VarRepr.backendVar(elem),false);
      elem=derefedElem;
    }
  }
  Var elemVal;
  boolean openWait2=!VarRepr.storeRefInContainer(elem);
  if (openWait2) {
    String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
    backend.startWaitStatement(waitName,VarRepr.backendVars(elem),WaitMode.WAIT_ONLY,false,false,ExecTarget.nonDispatchedAny());
    elemVal=exprWalker.retrieveToVar(context,elem);
  }
 else {
    elemVal=elem;
  }
  backend.bagInsert(VarRepr.backendVar(bag),VarRepr.backendArg(elemVal));
  if (openWait2) {
    backend.endWaitStatement();
  }
  if (openWait1) {
    backend.endWaitStatement();
  }
  return stmtResultVar;
}",0.9869931140015302
132685,"private static TypeMismatchException argumentTypeException(Context context,int argPos,Type expType,Type actType,String errContext){
  new Exception().printStackTrace();
  return new TypeMismatchException(context,""String_Node_Str"" + (argPos + 1) + ""String_Node_Str""+ expType.typeName()+ ""String_Node_Str""+ actType.typeName()+ errContext);
}","private static TypeMismatchException argumentTypeException(Context context,int argPos,Type expType,Type actType,String errContext){
  return new TypeMismatchException(context,""String_Node_Str"" + (argPos + 1) + ""String_Node_Str""+ expType.typeName()+ ""String_Node_Str""+ actType.typeName()+ errContext);
}",0.9422776911076444
132686,"/** 
 * Retrieve a reference to a local handle
 * @param dst alias variable to hold handle to referenced data
 * @param src Closed reference
 * @param acquireRead num of read refcounts to acquire
 * @param acquireWrite num of write refcounts to acquire
 * @param decrRead num of read refcounts to decr on input
 * @return
 */
public static Instruction retrieveRef(Var dst,Var src,long acquireRead,long acquireWrite,Arg decrRead){
  assert(Types.isRef(src.type()));
  assert(acquireRead >= 0);
  assert(acquireWrite >= 0);
  assert(decrRead.isImmediateInt());
  if (acquireWrite > 0) {
    assert(Types.isAssignableRefTo(src.type(),dst.type(),true));
  }
 else {
    assert(Types.isAssignableRefTo(src.type(),dst.type()));
  }
  assert(dst.storage() == Alloc.ALIAS);
  return new TurbineOp(Opcode.LOAD_REF,dst,src.asArg(),Arg.createIntLit(acquireRead),Arg.createIntLit(acquireWrite),decrRead);
}","/** 
 * Retrieve a reference to a local handle
 * @param dst alias variable to hold handle to referenced data
 * @param src Closed reference
 * @param acquireRead num of read refcounts to acquire
 * @param acquireWrite num of write refcounts to acquire
 * @param decrRead num of read refcounts to decr on input
 * @return
 */
public static Instruction retrieveRef(Var dst,Var src,long acquireRead,long acquireWrite,Arg decrRead){
}",0.6505660377358491
132687,"private UnifiedValues tryUnifyBranches(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result,List<BlockingVar> contClosedVars) throws OptUnsafeError {
  Conditional cond=((Conditional)cont);
  Arg condValue=cond.conditionArg();
  if (condValue.isVar()) {
    condValue=state.findValue(condValue.getVar());
  }
  Block predicted=cond.branchPredict(condValue);
  boolean unifyBranches;
  List<Block> branchBlocks;
  if (predicted != null) {
    unifyBranches=true;
    branchBlocks=Collections.singletonList(predicted);
  }
 else {
    unifyBranches=cont.isExhaustiveSyncConditional();
    branchBlocks=cont.getBlocks();
  }
  List<Congruences> branchStates=unifyBranches ? new ArrayList<Congruences>() : null;
  for (  Block contBlock : branchBlocks) {
    Congruences blockState=state.enterContBlock(cont.inheritsParentVars(),stmtIndex);
    if (contClosedVars != null) {
      for (      BlockingVar bv : contClosedVars) {
        blockState.markClosedBlockStart(bv.var,bv.recursive);
      }
    }
    findCongruencesRec(prog,fn,contBlock,cont.childContext(execCx),blockState,result);
    if (unifyBranches) {
      branchStates.add(blockState);
    }
  }
  if (unifyBranches) {
    return UnifiedValues.unify(logger,prog.constants(),fn,reorderingAllowed,stmtIndex,state,cont,branchStates,branchBlocks);
  }
 else {
    return UnifiedValues.EMPTY;
  }
}","private UnifiedValues tryUnifyBranches(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result) throws OptUnsafeError {
  Conditional cond=((Conditional)cont);
  Arg condValue=cond.conditionArg();
  if (condValue.isVar()) {
    condValue=state.findValue(condValue.getVar());
  }
  Block predicted=cond.branchPredict(condValue);
  boolean unifyBranches;
  List<Block> branchBlocks;
  if (predicted != null) {
    unifyBranches=true;
    branchBlocks=Collections.singletonList(predicted);
  }
 else {
    unifyBranches=cont.isExhaustiveSyncConditional();
    branchBlocks=cont.getBlocks();
  }
  List<Congruences> branchStates=unifyBranches ? new ArrayList<Congruences>() : null;
  findCongruencesBranchesRec(prog,fn,execCx,cont,stmtIndex,state,result,branchBlocks,branchStates);
  if (unifyBranches) {
    return UnifiedValues.unify(logger,prog.constants(),fn,reorderingAllowed,stmtIndex,state,cont,branchStates,branchBlocks);
  }
 else {
    return UnifiedValues.EMPTY;
  }
}",0.7803889789303079
132688,"private UnifiedValues findCongruencesContRec(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result) throws OptUnsafeError {
  logger.trace(""String_Node_Str"" + cont.getType());
  if (finalizedVarEnabled) {
    if (cont.getType() == ContinuationType.FOREACH_LOOP) {
      ForeachLoop foreach=(ForeachLoop)cont;
      Arg arrayVal=state.findRetrieveResult(foreach.getArrayVar(),false);
      logger.trace(""String_Node_Str"" + arrayVal);
      if (arrayVal != null) {
        foreach.switchToLocalForeach(arrayVal.getVar());
      }
    }
  }
  List<BlockingVar> contClosedVars=null;
  if (finalizedVarEnabled) {
    contClosedVars=cont.closedVars(state.getClosed(stmtIndex),state.getRecursivelyClosed(stmtIndex));
  }
  if (cont.isConditional()) {
    return tryUnifyBranches(prog,fn,execCx,cont,stmtIndex,state,result,contClosedVars);
  }
 else {
    return UnifiedValues.EMPTY;
  }
}","private UnifiedValues findCongruencesContRec(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result) throws OptUnsafeError {
  logger.trace(""String_Node_Str"" + cont.getType());
  if (finalizedVarEnabled) {
    if (cont.getType() == ContinuationType.FOREACH_LOOP) {
      ForeachLoop foreach=(ForeachLoop)cont;
      Arg arrayVal=state.findRetrieveResult(foreach.getArrayVar(),false);
      logger.trace(""String_Node_Str"" + arrayVal);
      if (arrayVal != null) {
        foreach.switchToLocalForeach(arrayVal.getVar());
      }
    }
  }
  if (cont.isConditional()) {
    return tryUnifyBranches(prog,fn,execCx,cont,stmtIndex,state,result);
  }
 else {
    findCongruencesBranchesRec(prog,fn,execCx,cont,stmtIndex,state,result,cont.getBlocks(),null);
    return UnifiedValues.EMPTY;
  }
}",0.7230514096185738
132689,"private UnifiedValues tryUnifyBranches(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result,List<BlockingVar> contClosedVars) throws OptUnsafeError {
  Conditional cond=((Conditional)cont);
  Arg condValue=cond.conditionArg();
  if (condValue.isVar()) {
    condValue=state.findValue(condValue.getVar());
  }
  Block predicted=cond.branchPredict(condValue);
  boolean unifyBranches;
  List<Block> branchBlocks;
  if (predicted != null) {
    unifyBranches=true;
    branchBlocks=Collections.singletonList(predicted);
  }
 else {
    unifyBranches=cont.isExhaustiveSyncConditional();
    branchBlocks=cont.getBlocks();
  }
  List<Congruences> branchStates=unifyBranches ? new ArrayList<Congruences>() : null;
  for (  Block contBlock : branchBlocks) {
    Congruences blockState=state.enterContBlock(cont.inheritsParentVars(),stmtIndex);
    if (contClosedVars != null) {
      for (      BlockingVar bv : contClosedVars) {
        blockState.markClosedBlockStart(bv.var,bv.recursive);
      }
    }
    findCongruencesRec(prog,fn,contBlock,cont.childContext(execCx),blockState,result);
    if (unifyBranches) {
      branchStates.add(blockState);
    }
  }
  if (unifyBranches) {
    return UnifiedValues.unify(logger,prog.constants(),fn,reorderingAllowed,stmtIndex,state,cont,branchStates,branchBlocks);
  }
 else {
    return UnifiedValues.EMPTY;
  }
}","private UnifiedValues tryUnifyBranches(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result) throws OptUnsafeError {
  Conditional cond=((Conditional)cont);
  Arg condValue=cond.conditionArg();
  if (condValue.isVar()) {
    condValue=state.findValue(condValue.getVar());
  }
  Block predicted=cond.branchPredict(condValue);
  boolean unifyBranches;
  List<Block> branchBlocks;
  if (predicted != null) {
    unifyBranches=true;
    branchBlocks=Collections.singletonList(predicted);
  }
 else {
    unifyBranches=cont.isExhaustiveSyncConditional();
    branchBlocks=cont.getBlocks();
  }
  List<Congruences> branchStates=unifyBranches ? new ArrayList<Congruences>() : null;
  findCongruencesBranchesRec(prog,fn,execCx,cont,stmtIndex,state,result,branchBlocks,branchStates);
  if (unifyBranches) {
    return UnifiedValues.unify(logger,prog.constants(),fn,reorderingAllowed,stmtIndex,state,cont,branchStates,branchBlocks);
  }
 else {
    return UnifiedValues.EMPTY;
  }
}",0.7803889789303079
132690,"private UnifiedValues findCongruencesContRec(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result) throws OptUnsafeError {
  logger.trace(""String_Node_Str"" + cont.getType());
  if (finalizedVarEnabled) {
    if (cont.getType() == ContinuationType.FOREACH_LOOP) {
      ForeachLoop foreach=(ForeachLoop)cont;
      Arg arrayVal=state.findRetrieveResult(foreach.getArrayVar(),false);
      logger.trace(""String_Node_Str"" + arrayVal);
      if (arrayVal != null) {
        foreach.switchToLocalForeach(arrayVal.getVar());
      }
    }
  }
  List<BlockingVar> contClosedVars=null;
  if (finalizedVarEnabled) {
    contClosedVars=cont.closedVars(state.getClosed(stmtIndex),state.getRecursivelyClosed(stmtIndex));
  }
  if (cont.isConditional()) {
    return tryUnifyBranches(prog,fn,execCx,cont,stmtIndex,state,result,contClosedVars);
  }
 else {
    return UnifiedValues.EMPTY;
  }
}","private UnifiedValues findCongruencesContRec(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result) throws OptUnsafeError {
  logger.trace(""String_Node_Str"" + cont.getType());
  if (finalizedVarEnabled) {
    if (cont.getType() == ContinuationType.FOREACH_LOOP) {
      ForeachLoop foreach=(ForeachLoop)cont;
      Arg arrayVal=state.findRetrieveResult(foreach.getArrayVar(),false);
      logger.trace(""String_Node_Str"" + arrayVal);
      if (arrayVal != null) {
        foreach.switchToLocalForeach(arrayVal.getVar());
      }
    }
  }
  if (cont.isConditional()) {
    return tryUnifyBranches(prog,fn,execCx,cont,stmtIndex,state,result);
  }
 else {
    findCongruencesBranchesRec(prog,fn,execCx,cont,stmtIndex,state,result,cont.getBlocks(),null);
    return UnifiedValues.EMPTY;
  }
}",0.7230514096185738
132691,"/** 
 * Merge computed values from different conditional branches
 * @param parent
 * @param branchStates
 * @param branchBlocks
 * @param allBranchCVs
 * @param unifiedLocs 
 * @return
 * @throws OptUnsafeError 
 */
private static Pair<List<ValLoc>,Boolean> unifyCVs(GlobalConstants consts,Function fn,boolean reorderingAllowed,Block parent,int parentStmtIndex,CongruenceType congType,List<Congruences> branchStates,List<Block> branchBlocks,List<ArgCV> allBranchCVs,Map<List<Arg>,Var> unifiedLocs) throws OptUnsafeError {
  List<ValLoc> availVals=new ArrayList<ValLoc>();
  boolean createdNewBranchCVs=false;
  for (  ArgCV cv : allBranchCVs) {
    boolean allVals=true;
    boolean allSameLocation=true;
    Closed allClosed=Closed.YES_RECURSIVE;
    IsValCopy anyValCopy=IsValCopy.NO;
    List<Arg> branchLocs=new ArrayList<Arg>(branchStates.size());
    Arg firstLoc=branchStates.get(0).findCanonical(cv,congType);
    for (int i=0; i < branchStates.size(); i++) {
      Congruences bs=branchStates.get(i);
      Arg loc=bs.findCanonical(cv,congType);
      assert(loc != null);
      if (loc != firstLoc && !loc.equals(firstLoc)) {
        allSameLocation=false;
      }
      if (!Types.isPrimValue(loc.type())) {
        allVals=false;
      }
      int branchStmtCount=branchBlocks.get(i).getStatements().size();
      if (allClosed == Closed.YES_RECURSIVE && !bs.isRecClosed(loc,branchStmtCount)) {
        if (bs.isClosed(loc,branchStmtCount)) {
          allClosed=Closed.MAYBE_NOT;
        }
 else {
          allClosed=Closed.YES_NOT_RECURSIVE;
        }
      }
 else       if (allClosed == Closed.YES_NOT_RECURSIVE && !bs.isClosed(loc,branchStmtCount)) {
        allClosed=Closed.MAYBE_NOT;
      }
      branchLocs.add(loc);
    }
    if (Logging.getSTCLogger().isTraceEnabled()) {
      Logging.getSTCLogger().trace(cv + ""String_Node_Str"" + ""String_Node_Str""+ allSameLocation+ ""String_Node_Str""+ allVals);
    }
    IsAssign isAssign=IsAssign.NO;
    if (allSameLocation) {
      availVals.add(createUnifiedCV(cv,firstLoc,allClosed,anyValCopy,isAssign));
    }
 else     if (unifiedLocs.containsKey(branchLocs)) {
      Var unifiedLoc=unifiedLocs.get(branchLocs);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
 else {
      Var unifiedLoc=createUnifyingVar(consts,fn,parent,branchStates,branchBlocks,branchLocs,firstLoc.type());
      createdNewBranchCVs=true;
      unifiedLocs.put(branchLocs,unifiedLoc);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
  }
  return Pair.create(availVals,createdNewBranchCVs);
}","/** 
 * Merge computed values from different conditional branches
 * @param parent
 * @param branchStates
 * @param branchBlocks
 * @param allBranchCVs
 * @param unifiedLocs
 * @return
 * @throws OptUnsafeError
 */
private static Pair<List<ValLoc>,Boolean> unifyCVs(GlobalConstants consts,Function fn,boolean reorderingAllowed,Block parent,int parentStmtIndex,CongruenceType congType,List<Congruences> branchStates,List<Block> branchBlocks,List<ArgCV> allBranchCVs,Map<List<Arg>,Var> unifiedLocs) throws OptUnsafeError {
  List<ValLoc> availVals=new ArrayList<ValLoc>();
  boolean createdNewBranchCVs=false;
  for (  ArgCV cv : allBranchCVs) {
    boolean allVals=true;
    boolean allSameLocation=true;
    Closed allClosed=Closed.YES_RECURSIVE;
    IsValCopy anyValCopy=IsValCopy.NO;
    List<Arg> branchLocs=new ArrayList<Arg>(branchStates.size());
    Arg firstLoc=branchStates.get(0).findCanonical(cv,congType);
    for (int i=0; i < branchStates.size(); i++) {
      Congruences bs=branchStates.get(i);
      Arg loc=bs.findCanonical(cv,congType);
      assert(loc != null);
      if (loc != firstLoc && !loc.equals(firstLoc)) {
        allSameLocation=false;
      }
      if (!Types.isPrimValue(loc.type())) {
        allVals=false;
      }
      int branchStmtCount=branchBlocks.get(i).getStatements().size();
      if (allClosed.isRecClosed() && bs.isRecClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_RECURSIVE;
      }
 else       if (allClosed.isClosed() && bs.isClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_NOT_RECURSIVE;
      }
 else {
        allClosed=Closed.MAYBE_NOT;
      }
      branchLocs.add(loc);
    }
    if (Logging.getSTCLogger().isTraceEnabled()) {
      Logging.getSTCLogger().trace(cv + ""String_Node_Str"" + ""String_Node_Str""+ allSameLocation+ ""String_Node_Str""+ allVals);
    }
    IsAssign isAssign=IsAssign.NO;
    if (allSameLocation) {
      availVals.add(createUnifiedCV(cv,firstLoc,allClosed,anyValCopy,isAssign));
    }
 else     if (unifiedLocs.containsKey(branchLocs)) {
      Var unifiedLoc=unifiedLocs.get(branchLocs);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
 else {
      Var unifiedLoc=createUnifyingVar(consts,fn,parent,branchStates,branchBlocks,branchLocs,firstLoc.type());
      createdNewBranchCVs=true;
      unifiedLocs.put(branchLocs,unifiedLoc);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
  }
  return Pair.create(availVals,createdNewBranchCVs);
}",0.8345908299477656
132692,"/** 
 * Assuming that branches are exhaustive, work out the set of variables closed after the conditional has executed.
 * @param state
 * @param branchStates
 * @return
 * @throws OptUnsafeError 
 */
public static UnifiedValues unify(Logger logger,GlobalConstants consts,Function fn,boolean reorderingAllowed,int parentStmtIndex,Congruences state,Continuation cont,List<Congruences> branchStates,List<Block> branchBlocks) throws OptUnsafeError {
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + branchBlocks.size() + ""String_Node_Str""+ cont.getType());
    for (int i=0; i < branchBlocks.size(); i++) {
      logger.trace(""String_Node_Str"" + (i + 1) + ""String_Node_Str""+ branchBlocks.get(i).getType());
    }
    logger.trace(cont.toString());
  }
  if (branchStates.isEmpty()) {
    return EMPTY;
  }
 else {
    Set<Var> closed=new HashSet<Var>();
    Set<Var> recClosed=new HashSet<Var>();
    unifyClosed(branchStates,closed,recClosed,parentStmtIndex);
    List<ValLoc> availVals=new ArrayList<ValLoc>();
    List<ArgCV> allUnifiedCVs=new ArrayList<ArgCV>();
    Map<List<Arg>,Var> unifiedVars=new HashMap<List<Arg>,Var>();
    int iter=1;
    boolean newCVs;
    do {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + iter + ""String_Node_Str"");
      }
      newCVs=false;
      for (      CongruenceType congType : Arrays.asList(CongruenceType.VALUE,CongruenceType.ALIAS)) {
        List<ArgCV> newAllBranchCVs=findAllBranchCVs(state,congType,branchStates,allUnifiedCVs);
        Pair<List<ValLoc>,Boolean> result=unifyCVs(consts,fn,reorderingAllowed,cont.parent(),parentStmtIndex,congType,branchStates,branchBlocks,newAllBranchCVs,unifiedVars);
        availVals.addAll(result.val1);
        if (result.val2) {
          newCVs=true;
        }
        allUnifiedCVs.addAll(newAllBranchCVs);
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + iter + ""String_Node_Str""+ congType+ ""String_Node_Str""+ result.val1);
        }
      }
      if (iter >= MAX_UNIFY_ITERATIONS) {
        logger.debug(""String_Node_Str"");
        if (logger.isTraceEnabled()) {
          logger.trace(cont);
        }
        break;
      }
      iter++;
    }
 while (newCVs);
    return new UnifiedValues(closed,recClosed,availVals);
  }
}","/** 
 * Assuming that branches are exhaustive, work out the set of variables closed after the conditional has executed.
 * @param state
 * @param branchStates
 * @return
 * @throws OptUnsafeError
 */
public static UnifiedValues unify(Logger logger,GlobalConstants consts,Function fn,boolean reorderingAllowed,int parentStmtIndex,Congruences state,Continuation cont,List<Congruences> branchStates,List<Block> branchBlocks) throws OptUnsafeError {
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + branchBlocks.size() + ""String_Node_Str""+ cont.getType());
    for (int i=0; i < branchBlocks.size(); i++) {
      logger.trace(""String_Node_Str"" + (i + 1) + ""String_Node_Str""+ branchBlocks.get(i).getType());
    }
    logger.trace(cont.toString());
  }
  if (branchStates.isEmpty()) {
    return EMPTY;
  }
 else {
    Set<Var> closed=new HashSet<Var>();
    Set<Var> recClosed=new HashSet<Var>();
    unifyClosed(branchStates,closed,recClosed,parentStmtIndex);
    List<ValLoc> availVals=new ArrayList<ValLoc>();
    List<ArgCV> allUnifiedCVs=new ArrayList<ArgCV>();
    Map<List<Arg>,Var> unifiedVars=new HashMap<List<Arg>,Var>();
    int iter=1;
    boolean newCVs;
    do {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + iter + ""String_Node_Str"");
      }
      newCVs=false;
      for (      CongruenceType congType : Arrays.asList(CongruenceType.VALUE,CongruenceType.ALIAS)) {
        List<ArgCV> newAllBranchCVs=findAllBranchCVs(state,congType,branchStates,allUnifiedCVs);
        Pair<List<ValLoc>,Boolean> result=unifyCVs(consts,fn,reorderingAllowed,cont.parent(),parentStmtIndex,congType,branchStates,branchBlocks,newAllBranchCVs,unifiedVars);
        availVals.addAll(result.val1);
        if (result.val2) {
          newCVs=true;
        }
        allUnifiedCVs.addAll(newAllBranchCVs);
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + iter + ""String_Node_Str""+ congType+ ""String_Node_Str""+ result.val1);
        }
      }
      if (iter >= MAX_UNIFY_ITERATIONS) {
        logger.debug(""String_Node_Str"");
        if (logger.isTraceEnabled()) {
          logger.trace(cont);
        }
        break;
      }
      iter++;
    }
 while (newCVs);
    return new UnifiedValues(closed,recClosed,availVals);
  }
}",0.9997827503801868
132693,"/** 
 * Find computed values that appear in all branches but not parent
 * @param parentState
 * @param congType 
 * @param branchStates
 * @param alreadyAdded ignore these
 * @return
 */
private static List<ArgCV> findAllBranchCVs(Congruences parentState,CongruenceType congType,List<Congruences> branchStates,List<ArgCV> alreadyAdded){
  List<ArgCV> allBranchCVs=new ArrayList<ArgCV>();
  Congruences firstState=branchStates.get(0);
  for (  ArgOrCV val : firstState.availableThisScope(congType)) {
    ArgCV convertedVal=parentState.convertToArgs(val,congType);
    if (convertedVal != null) {
      if (!alreadyAdded.contains(convertedVal) && !parentState.isAvailable(convertedVal,congType)) {
        int nBranches=branchStates.size();
        boolean presentInAll=true;
        for (        Congruences otherState : branchStates.subList(1,nBranches)) {
          if (!otherState.isAvailable(convertedVal,congType)) {
            presentInAll=false;
            break;
          }
        }
        if (presentInAll) {
          allBranchCVs.add(convertedVal);
        }
      }
    }
  }
  return allBranchCVs;
}","/** 
 * Find computed values that appear in all branches but not parent
 * @param parentState
 * @param congType
 * @param branchStates
 * @param alreadyAdded ignore these
 * @return
 */
private static List<ArgCV> findAllBranchCVs(Congruences parentState,CongruenceType congType,List<Congruences> branchStates,List<ArgCV> alreadyAdded){
  List<ArgCV> allBranchCVs=new ArrayList<ArgCV>();
  Congruences firstState=branchStates.get(0);
  for (  ArgOrCV val : firstState.availableThisScope(congType)) {
    ArgCV convertedVal=parentState.convertToArgs(val,congType);
    if (convertedVal != null) {
      if (!alreadyAdded.contains(convertedVal) && !parentState.isAvailable(convertedVal,congType)) {
        int nBranches=branchStates.size();
        boolean presentInAll=true;
        for (        Congruences otherState : branchStates.subList(1,nBranches)) {
          if (!otherState.isAvailable(convertedVal,congType)) {
            presentInAll=false;
            break;
          }
        }
        if (presentInAll) {
          allBranchCVs.add(convertedVal);
        }
      }
    }
  }
  return allBranchCVs;
}",0.9995525727069352
132694,"/** 
 * Merge computed values from different conditional branches
 * @param parent
 * @param branchStates
 * @param branchBlocks
 * @param allBranchCVs
 * @param unifiedLocs 
 * @return
 * @throws OptUnsafeError 
 */
private static Pair<List<ValLoc>,Boolean> unifyCVs(GlobalConstants consts,Function fn,boolean reorderingAllowed,Block parent,int parentStmtIndex,CongruenceType congType,List<Congruences> branchStates,List<Block> branchBlocks,List<ArgCV> allBranchCVs,Map<List<Arg>,Var> unifiedLocs) throws OptUnsafeError {
  List<ValLoc> availVals=new ArrayList<ValLoc>();
  boolean createdNewBranchCVs=false;
  for (  ArgCV cv : allBranchCVs) {
    boolean allVals=true;
    boolean allSameLocation=true;
    Closed allClosed=Closed.YES_RECURSIVE;
    IsValCopy anyValCopy=IsValCopy.NO;
    List<Arg> branchLocs=new ArrayList<Arg>(branchStates.size());
    Arg firstLoc=branchStates.get(0).findCanonical(cv,congType);
    for (int i=0; i < branchStates.size(); i++) {
      Congruences bs=branchStates.get(i);
      Arg loc=bs.findCanonical(cv,congType);
      assert(loc != null);
      if (loc != firstLoc && !loc.equals(firstLoc)) {
        allSameLocation=false;
      }
      if (!Types.isPrimValue(loc.type())) {
        allVals=false;
      }
      int branchStmtCount=branchBlocks.get(i).getStatements().size();
      if (allClosed == Closed.YES_RECURSIVE && !bs.isRecClosed(loc,branchStmtCount)) {
        if (bs.isClosed(loc,branchStmtCount)) {
          allClosed=Closed.MAYBE_NOT;
        }
 else {
          allClosed=Closed.YES_NOT_RECURSIVE;
        }
      }
 else       if (allClosed == Closed.YES_NOT_RECURSIVE && !bs.isClosed(loc,branchStmtCount)) {
        allClosed=Closed.MAYBE_NOT;
      }
      branchLocs.add(loc);
    }
    if (Logging.getSTCLogger().isTraceEnabled()) {
      Logging.getSTCLogger().trace(cv + ""String_Node_Str"" + ""String_Node_Str""+ allSameLocation+ ""String_Node_Str""+ allVals);
    }
    IsAssign isAssign=IsAssign.NO;
    if (allSameLocation) {
      availVals.add(createUnifiedCV(cv,firstLoc,allClosed,anyValCopy,isAssign));
    }
 else     if (unifiedLocs.containsKey(branchLocs)) {
      Var unifiedLoc=unifiedLocs.get(branchLocs);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
 else {
      Var unifiedLoc=createUnifyingVar(consts,fn,parent,branchStates,branchBlocks,branchLocs,firstLoc.type());
      createdNewBranchCVs=true;
      unifiedLocs.put(branchLocs,unifiedLoc);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
  }
  return Pair.create(availVals,createdNewBranchCVs);
}","/** 
 * Merge computed values from different conditional branches
 * @param parent
 * @param branchStates
 * @param branchBlocks
 * @param allBranchCVs
 * @param unifiedLocs
 * @return
 * @throws OptUnsafeError
 */
private static Pair<List<ValLoc>,Boolean> unifyCVs(GlobalConstants consts,Function fn,boolean reorderingAllowed,Block parent,int parentStmtIndex,CongruenceType congType,List<Congruences> branchStates,List<Block> branchBlocks,List<ArgCV> allBranchCVs,Map<List<Arg>,Var> unifiedLocs) throws OptUnsafeError {
  List<ValLoc> availVals=new ArrayList<ValLoc>();
  boolean createdNewBranchCVs=false;
  for (  ArgCV cv : allBranchCVs) {
    boolean allVals=true;
    boolean allSameLocation=true;
    Closed allClosed=Closed.YES_RECURSIVE;
    IsValCopy anyValCopy=IsValCopy.NO;
    List<Arg> branchLocs=new ArrayList<Arg>(branchStates.size());
    Arg firstLoc=branchStates.get(0).findCanonical(cv,congType);
    for (int i=0; i < branchStates.size(); i++) {
      Congruences bs=branchStates.get(i);
      Arg loc=bs.findCanonical(cv,congType);
      assert(loc != null);
      if (loc != firstLoc && !loc.equals(firstLoc)) {
        allSameLocation=false;
      }
      if (!Types.isPrimValue(loc.type())) {
        allVals=false;
      }
      int branchStmtCount=branchBlocks.get(i).getStatements().size();
      if (allClosed.isRecClosed() && bs.isRecClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_RECURSIVE;
      }
 else       if (allClosed.isClosed() && bs.isClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_NOT_RECURSIVE;
      }
 else {
        allClosed=Closed.MAYBE_NOT;
      }
      branchLocs.add(loc);
    }
    if (Logging.getSTCLogger().isTraceEnabled()) {
      Logging.getSTCLogger().trace(cv + ""String_Node_Str"" + ""String_Node_Str""+ allSameLocation+ ""String_Node_Str""+ allVals);
    }
    IsAssign isAssign=IsAssign.NO;
    if (allSameLocation) {
      availVals.add(createUnifiedCV(cv,firstLoc,allClosed,anyValCopy,isAssign));
    }
 else     if (unifiedLocs.containsKey(branchLocs)) {
      Var unifiedLoc=unifiedLocs.get(branchLocs);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
 else {
      Var unifiedLoc=createUnifyingVar(consts,fn,parent,branchStates,branchBlocks,branchLocs,firstLoc.type());
      createdNewBranchCVs=true;
      unifiedLocs.put(branchLocs,unifiedLoc);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
  }
  return Pair.create(availVals,createdNewBranchCVs);
}",0.8345908299477656
132695,"/** 
 * Assuming that branches are exhaustive, work out the set of variables closed after the conditional has executed.
 * @param state
 * @param branchStates
 * @return
 * @throws OptUnsafeError 
 */
public static UnifiedValues unify(Logger logger,GlobalConstants consts,Function fn,boolean reorderingAllowed,int parentStmtIndex,Congruences state,Continuation cont,List<Congruences> branchStates,List<Block> branchBlocks) throws OptUnsafeError {
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + branchBlocks.size() + ""String_Node_Str""+ cont.getType());
    for (int i=0; i < branchBlocks.size(); i++) {
      logger.trace(""String_Node_Str"" + (i + 1) + ""String_Node_Str""+ branchBlocks.get(i).getType());
    }
    logger.trace(cont.toString());
  }
  if (branchStates.isEmpty()) {
    return EMPTY;
  }
 else {
    Set<Var> closed=new HashSet<Var>();
    Set<Var> recClosed=new HashSet<Var>();
    unifyClosed(branchStates,closed,recClosed,parentStmtIndex);
    List<ValLoc> availVals=new ArrayList<ValLoc>();
    List<ArgCV> allUnifiedCVs=new ArrayList<ArgCV>();
    Map<List<Arg>,Var> unifiedVars=new HashMap<List<Arg>,Var>();
    int iter=1;
    boolean newCVs;
    do {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + iter + ""String_Node_Str"");
      }
      newCVs=false;
      for (      CongruenceType congType : Arrays.asList(CongruenceType.VALUE,CongruenceType.ALIAS)) {
        List<ArgCV> newAllBranchCVs=findAllBranchCVs(state,congType,branchStates,allUnifiedCVs);
        Pair<List<ValLoc>,Boolean> result=unifyCVs(consts,fn,reorderingAllowed,cont.parent(),parentStmtIndex,congType,branchStates,branchBlocks,newAllBranchCVs,unifiedVars);
        availVals.addAll(result.val1);
        if (result.val2) {
          newCVs=true;
        }
        allUnifiedCVs.addAll(newAllBranchCVs);
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + iter + ""String_Node_Str""+ congType+ ""String_Node_Str""+ result.val1);
        }
      }
      if (iter >= MAX_UNIFY_ITERATIONS) {
        logger.debug(""String_Node_Str"");
        if (logger.isTraceEnabled()) {
          logger.trace(cont);
        }
        break;
      }
      iter++;
    }
 while (newCVs);
    return new UnifiedValues(closed,recClosed,availVals);
  }
}","/** 
 * Assuming that branches are exhaustive, work out the set of variables closed after the conditional has executed.
 * @param state
 * @param branchStates
 * @return
 * @throws OptUnsafeError
 */
public static UnifiedValues unify(Logger logger,GlobalConstants consts,Function fn,boolean reorderingAllowed,int parentStmtIndex,Congruences state,Continuation cont,List<Congruences> branchStates,List<Block> branchBlocks) throws OptUnsafeError {
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + branchBlocks.size() + ""String_Node_Str""+ cont.getType());
    for (int i=0; i < branchBlocks.size(); i++) {
      logger.trace(""String_Node_Str"" + (i + 1) + ""String_Node_Str""+ branchBlocks.get(i).getType());
    }
    logger.trace(cont.toString());
  }
  if (branchStates.isEmpty()) {
    return EMPTY;
  }
 else {
    Set<Var> closed=new HashSet<Var>();
    Set<Var> recClosed=new HashSet<Var>();
    unifyClosed(branchStates,closed,recClosed,parentStmtIndex);
    List<ValLoc> availVals=new ArrayList<ValLoc>();
    List<ArgCV> allUnifiedCVs=new ArrayList<ArgCV>();
    Map<List<Arg>,Var> unifiedVars=new HashMap<List<Arg>,Var>();
    int iter=1;
    boolean newCVs;
    do {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + iter + ""String_Node_Str"");
      }
      newCVs=false;
      for (      CongruenceType congType : Arrays.asList(CongruenceType.VALUE,CongruenceType.ALIAS)) {
        List<ArgCV> newAllBranchCVs=findAllBranchCVs(state,congType,branchStates,allUnifiedCVs);
        Pair<List<ValLoc>,Boolean> result=unifyCVs(consts,fn,reorderingAllowed,cont.parent(),parentStmtIndex,congType,branchStates,branchBlocks,newAllBranchCVs,unifiedVars);
        availVals.addAll(result.val1);
        if (result.val2) {
          newCVs=true;
        }
        allUnifiedCVs.addAll(newAllBranchCVs);
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + iter + ""String_Node_Str""+ congType+ ""String_Node_Str""+ result.val1);
        }
      }
      if (iter >= MAX_UNIFY_ITERATIONS) {
        logger.debug(""String_Node_Str"");
        if (logger.isTraceEnabled()) {
          logger.trace(cont);
        }
        break;
      }
      iter++;
    }
 while (newCVs);
    return new UnifiedValues(closed,recClosed,availVals);
  }
}",0.9997827503801868
132696,"/** 
 * Find computed values that appear in all branches but not parent
 * @param parentState
 * @param congType 
 * @param branchStates
 * @param alreadyAdded ignore these
 * @return
 */
private static List<ArgCV> findAllBranchCVs(Congruences parentState,CongruenceType congType,List<Congruences> branchStates,List<ArgCV> alreadyAdded){
  List<ArgCV> allBranchCVs=new ArrayList<ArgCV>();
  Congruences firstState=branchStates.get(0);
  for (  ArgOrCV val : firstState.availableThisScope(congType)) {
    ArgCV convertedVal=parentState.convertToArgs(val,congType);
    if (convertedVal != null) {
      if (!alreadyAdded.contains(convertedVal) && !parentState.isAvailable(convertedVal,congType)) {
        int nBranches=branchStates.size();
        boolean presentInAll=true;
        for (        Congruences otherState : branchStates.subList(1,nBranches)) {
          if (!otherState.isAvailable(convertedVal,congType)) {
            presentInAll=false;
            break;
          }
        }
        if (presentInAll) {
          allBranchCVs.add(convertedVal);
        }
      }
    }
  }
  return allBranchCVs;
}","/** 
 * Find computed values that appear in all branches but not parent
 * @param parentState
 * @param congType
 * @param branchStates
 * @param alreadyAdded ignore these
 * @return
 */
private static List<ArgCV> findAllBranchCVs(Congruences parentState,CongruenceType congType,List<Congruences> branchStates,List<ArgCV> alreadyAdded){
  List<ArgCV> allBranchCVs=new ArrayList<ArgCV>();
  Congruences firstState=branchStates.get(0);
  for (  ArgOrCV val : firstState.availableThisScope(congType)) {
    ArgCV convertedVal=parentState.convertToArgs(val,congType);
    if (convertedVal != null) {
      if (!alreadyAdded.contains(convertedVal) && !parentState.isAvailable(convertedVal,congType)) {
        int nBranches=branchStates.size();
        boolean presentInAll=true;
        for (        Congruences otherState : branchStates.subList(1,nBranches)) {
          if (!otherState.isAvailable(convertedVal,congType)) {
            presentInAll=false;
            break;
          }
        }
        if (presentInAll) {
          allBranchCVs.add(convertedVal);
        }
      }
    }
  }
  return allBranchCVs;
}",0.9995525727069352
132697,"/** 
 * Finish adding an updated CV obtained by substituting components
 * @param oldComponent original component
 * @param newComponent replaced with
 * @param newCV canonicalized computed value
 * @param canonical existing set that it is a member of
 */
private void addUpdatedCV(Arg oldComponent,Arg newComponent,ArgOrCV newCV,Arg canonical){
  Arg newCanonical=findCanonicalInternal(newCV);
  if (newCanonical != null) {
    if (newCanonical.equals(canonical)) {
      addSetEntry(newCV,canonical);
    }
 else {
      mergeQueue.add(new ToMerge(canonical,newCanonical));
      if (logger.isTraceEnabled()) {
        if (newComponent != null) {
          logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ newComponent+ ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
        }
 else {
          logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
        }
      }
    }
  }
}","/** 
 * Finish adding an updated CV obtained by substituting components
 * @param oldComponent original component
 * @param newComponent replaced with
 * @param newCV canonicalized computed value
 * @param canonical existing set that it is a member of
 */
private void addUpdatedCV(Arg oldComponent,Arg newComponent,ArgOrCV newCV,Arg canonical){
  Arg newCanonical=findCanonicalInternal(newCV);
  if (newCanonical == null || newCanonical.equals(canonical)) {
    addSetEntry(newCV,canonical);
  }
 else {
    mergeQueue.add(new ToMerge(canonical,newCanonical));
    if (logger.isTraceEnabled()) {
      if (newComponent != null) {
        logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ newComponent+ ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
      }
 else {
        logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
      }
    }
  }
}",0.4884937238493724
132698,"/** 
 * Merge computed values from different conditional branches
 * @param parent
 * @param branchStates
 * @param branchBlocks
 * @param allBranchCVs
 * @param unifiedLocs
 * @return
 * @throws OptUnsafeError
 */
private static Pair<List<ValLoc>,Boolean> unifyCVs(GlobalConstants consts,Function fn,boolean reorderingAllowed,Block parent,int parentStmtIndex,CongruenceType congType,List<Congruences> branchStates,List<Block> branchBlocks,List<ArgCV> allBranchCVs,Map<List<Arg>,Var> unifiedLocs) throws OptUnsafeError {
  List<ValLoc> availVals=new ArrayList<ValLoc>();
  boolean createdNewBranchCVs=false;
  for (  ArgCV cv : allBranchCVs) {
    boolean allVals=true;
    boolean allSameLocation=true;
    Closed allClosed=Closed.YES_RECURSIVE;
    IsValCopy anyValCopy=IsValCopy.NO;
    boolean skip=false;
    List<Arg> branchLocs=new ArrayList<Arg>(branchStates.size());
    Arg firstLoc=branchStates.get(0).findCanonical(cv,congType);
    for (int i=0; i < branchStates.size(); i++) {
      Congruences bs=branchStates.get(i);
      Arg loc=bs.findCanonical(cv,congType);
      if (loc == null) {
        Logging.getSTCLogger().debug(""String_Node_Str"" + cv + ""String_Node_Str""+ congType+ ""String_Node_Str""+ i);
        skip=true;
        continue;
      }
      if (loc != firstLoc && !loc.equals(firstLoc)) {
        allSameLocation=false;
      }
      if (!Types.isPrimValue(loc.type())) {
        allVals=false;
      }
      int branchStmtCount=branchBlocks.get(i).getStatements().size();
      if (allClosed.isRecClosed() && bs.isRecClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_RECURSIVE;
      }
 else       if (allClosed.isClosed() && bs.isClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_NOT_RECURSIVE;
      }
 else {
        allClosed=Closed.MAYBE_NOT;
      }
      branchLocs.add(loc);
    }
    if (Logging.getSTCLogger().isTraceEnabled()) {
      Logging.getSTCLogger().trace(cv + ""String_Node_Str"" + ""String_Node_Str""+ allSameLocation+ ""String_Node_Str""+ allVals);
    }
    IsAssign isAssign=IsAssign.NO;
    if (skip) {
    }
 else     if (allSameLocation) {
      availVals.add(createUnifiedCV(cv,firstLoc,allClosed,anyValCopy,isAssign));
    }
 else     if (unifiedLocs.containsKey(branchLocs)) {
      Var unifiedLoc=unifiedLocs.get(branchLocs);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
 else {
      Var unifiedLoc=createUnifyingVar(consts,fn,parent,branchStates,branchBlocks,branchLocs,firstLoc.type());
      createdNewBranchCVs=true;
      unifiedLocs.put(branchLocs,unifiedLoc);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
  }
  return Pair.create(availVals,createdNewBranchCVs);
}","/** 
 * Merge computed values from different conditional branches
 * @param parent
 * @param branchStates
 * @param branchBlocks
 * @param allBranchCVs
 * @param unifiedLocs
 * @return
 * @throws OptUnsafeError
 */
private static Pair<List<ValLoc>,Boolean> unifyCVs(GlobalConstants consts,Function fn,boolean reorderingAllowed,Block parent,int parentStmtIndex,CongruenceType congType,List<Congruences> branchStates,List<Block> branchBlocks,List<ArgCV> allBranchCVs,Map<List<Arg>,Var> unifiedLocs) throws OptUnsafeError {
  List<ValLoc> availVals=new ArrayList<ValLoc>();
  boolean createdNewBranchCVs=false;
  for (  ArgCV cv : allBranchCVs) {
    boolean allVals=true;
    boolean allSameLocation=true;
    Closed allClosed=Closed.YES_RECURSIVE;
    IsValCopy anyValCopy=IsValCopy.NO;
    boolean skip=false;
    List<Arg> branchLocs=new ArrayList<Arg>(branchStates.size());
    Arg firstLoc=branchStates.get(0).findCanonical(cv,congType);
    for (int i=0; i < branchStates.size(); i++) {
      Congruences bs=branchStates.get(i);
      Arg loc=bs.findCanonical(cv,congType);
      if (loc == null) {
        Logging.getSTCLogger().warn(""String_Node_Str"" + cv + ""String_Node_Str""+ congType+ ""String_Node_Str""+ i,new Exception());
        skip=true;
        continue;
      }
      if (loc != firstLoc && !loc.equals(firstLoc)) {
        allSameLocation=false;
      }
      if (!Types.isPrimValue(loc.type())) {
        allVals=false;
      }
      int branchStmtCount=branchBlocks.get(i).getStatements().size();
      if (allClosed.isRecClosed() && bs.isRecClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_RECURSIVE;
      }
 else       if (allClosed.isClosed() && bs.isClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_NOT_RECURSIVE;
      }
 else {
        allClosed=Closed.MAYBE_NOT;
      }
      branchLocs.add(loc);
    }
    if (Logging.getSTCLogger().isTraceEnabled()) {
      Logging.getSTCLogger().trace(cv + ""String_Node_Str"" + ""String_Node_Str""+ allSameLocation+ ""String_Node_Str""+ allVals);
    }
    IsAssign isAssign=IsAssign.NO;
    if (skip) {
    }
 else     if (allSameLocation) {
      availVals.add(createUnifiedCV(cv,firstLoc,allClosed,anyValCopy,isAssign));
    }
 else     if (unifiedLocs.containsKey(branchLocs)) {
      Var unifiedLoc=unifiedLocs.get(branchLocs);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
 else {
      Var unifiedLoc=createUnifyingVar(consts,fn,parent,branchStates,branchBlocks,branchLocs,firstLoc.type());
      createdNewBranchCVs=true;
      unifiedLocs.put(branchLocs,unifiedLoc);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
  }
  return Pair.create(availVals,createdNewBranchCVs);
}",0.9954636182181092
132699,"/** 
 * Finish adding an updated CV obtained by substituting components
 * @param oldComponent original component
 * @param newComponent replaced with
 * @param newCV canonicalized computed value
 * @param canonical existing set that it is a member of
 */
private void addUpdatedCV(Arg oldComponent,Arg newComponent,ArgOrCV newCV,Arg canonical){
  Arg newCanonical=findCanonicalInternal(newCV);
  if (newCanonical != null) {
    if (newCanonical.equals(canonical)) {
      addSetEntry(newCV,canonical);
    }
 else {
      mergeQueue.add(new ToMerge(canonical,newCanonical));
      if (logger.isTraceEnabled()) {
        if (newComponent != null) {
          logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ newComponent+ ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
        }
 else {
          logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
        }
      }
    }
  }
}","/** 
 * Finish adding an updated CV obtained by substituting components
 * @param oldComponent original component
 * @param newComponent replaced with
 * @param newCV canonicalized computed value
 * @param canonical existing set that it is a member of
 */
private void addUpdatedCV(Arg oldComponent,Arg newComponent,ArgOrCV newCV,Arg canonical){
  Arg newCanonical=findCanonicalInternal(newCV);
  if (newCanonical == null || newCanonical.equals(canonical)) {
    addSetEntry(newCV,canonical);
  }
 else {
    mergeQueue.add(new ToMerge(canonical,newCanonical));
    if (logger.isTraceEnabled()) {
      if (newComponent != null) {
        logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ newComponent+ ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
      }
 else {
        logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
      }
    }
  }
}",0.4884937238493724
132700,"/** 
 * Merge computed values from different conditional branches
 * @param parent
 * @param branchStates
 * @param branchBlocks
 * @param allBranchCVs
 * @param unifiedLocs
 * @return
 * @throws OptUnsafeError
 */
private static Pair<List<ValLoc>,Boolean> unifyCVs(GlobalConstants consts,Function fn,boolean reorderingAllowed,Block parent,int parentStmtIndex,CongruenceType congType,List<Congruences> branchStates,List<Block> branchBlocks,List<ArgCV> allBranchCVs,Map<List<Arg>,Var> unifiedLocs) throws OptUnsafeError {
  List<ValLoc> availVals=new ArrayList<ValLoc>();
  boolean createdNewBranchCVs=false;
  for (  ArgCV cv : allBranchCVs) {
    boolean allVals=true;
    boolean allSameLocation=true;
    Closed allClosed=Closed.YES_RECURSIVE;
    IsValCopy anyValCopy=IsValCopy.NO;
    boolean skip=false;
    List<Arg> branchLocs=new ArrayList<Arg>(branchStates.size());
    Arg firstLoc=branchStates.get(0).findCanonical(cv,congType);
    for (int i=0; i < branchStates.size(); i++) {
      Congruences bs=branchStates.get(i);
      Arg loc=bs.findCanonical(cv,congType);
      if (loc == null) {
        Logging.getSTCLogger().debug(""String_Node_Str"" + cv + ""String_Node_Str""+ congType+ ""String_Node_Str""+ i);
        skip=true;
        continue;
      }
      if (loc != firstLoc && !loc.equals(firstLoc)) {
        allSameLocation=false;
      }
      if (!Types.isPrimValue(loc.type())) {
        allVals=false;
      }
      int branchStmtCount=branchBlocks.get(i).getStatements().size();
      if (allClosed.isRecClosed() && bs.isRecClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_RECURSIVE;
      }
 else       if (allClosed.isClosed() && bs.isClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_NOT_RECURSIVE;
      }
 else {
        allClosed=Closed.MAYBE_NOT;
      }
      branchLocs.add(loc);
    }
    if (Logging.getSTCLogger().isTraceEnabled()) {
      Logging.getSTCLogger().trace(cv + ""String_Node_Str"" + ""String_Node_Str""+ allSameLocation+ ""String_Node_Str""+ allVals);
    }
    IsAssign isAssign=IsAssign.NO;
    if (skip) {
    }
 else     if (allSameLocation) {
      availVals.add(createUnifiedCV(cv,firstLoc,allClosed,anyValCopy,isAssign));
    }
 else     if (unifiedLocs.containsKey(branchLocs)) {
      Var unifiedLoc=unifiedLocs.get(branchLocs);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
 else {
      Var unifiedLoc=createUnifyingVar(consts,fn,parent,branchStates,branchBlocks,branchLocs,firstLoc.type());
      createdNewBranchCVs=true;
      unifiedLocs.put(branchLocs,unifiedLoc);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
  }
  return Pair.create(availVals,createdNewBranchCVs);
}","/** 
 * Merge computed values from different conditional branches
 * @param parent
 * @param branchStates
 * @param branchBlocks
 * @param allBranchCVs
 * @param unifiedLocs
 * @return
 * @throws OptUnsafeError
 */
private static Pair<List<ValLoc>,Boolean> unifyCVs(GlobalConstants consts,Function fn,boolean reorderingAllowed,Block parent,int parentStmtIndex,CongruenceType congType,List<Congruences> branchStates,List<Block> branchBlocks,List<ArgCV> allBranchCVs,Map<List<Arg>,Var> unifiedLocs) throws OptUnsafeError {
  List<ValLoc> availVals=new ArrayList<ValLoc>();
  boolean createdNewBranchCVs=false;
  for (  ArgCV cv : allBranchCVs) {
    boolean allVals=true;
    boolean allSameLocation=true;
    Closed allClosed=Closed.YES_RECURSIVE;
    IsValCopy anyValCopy=IsValCopy.NO;
    boolean skip=false;
    List<Arg> branchLocs=new ArrayList<Arg>(branchStates.size());
    Arg firstLoc=branchStates.get(0).findCanonical(cv,congType);
    for (int i=0; i < branchStates.size(); i++) {
      Congruences bs=branchStates.get(i);
      Arg loc=bs.findCanonical(cv,congType);
      if (loc == null) {
        Logging.getSTCLogger().warn(""String_Node_Str"" + cv + ""String_Node_Str""+ congType+ ""String_Node_Str""+ i,new Exception());
        skip=true;
        continue;
      }
      if (loc != firstLoc && !loc.equals(firstLoc)) {
        allSameLocation=false;
      }
      if (!Types.isPrimValue(loc.type())) {
        allVals=false;
      }
      int branchStmtCount=branchBlocks.get(i).getStatements().size();
      if (allClosed.isRecClosed() && bs.isRecClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_RECURSIVE;
      }
 else       if (allClosed.isClosed() && bs.isClosed(loc,branchStmtCount)) {
        allClosed=Closed.YES_NOT_RECURSIVE;
      }
 else {
        allClosed=Closed.MAYBE_NOT;
      }
      branchLocs.add(loc);
    }
    if (Logging.getSTCLogger().isTraceEnabled()) {
      Logging.getSTCLogger().trace(cv + ""String_Node_Str"" + ""String_Node_Str""+ allSameLocation+ ""String_Node_Str""+ allVals);
    }
    IsAssign isAssign=IsAssign.NO;
    if (skip) {
    }
 else     if (allSameLocation) {
      availVals.add(createUnifiedCV(cv,firstLoc,allClosed,anyValCopy,isAssign));
    }
 else     if (unifiedLocs.containsKey(branchLocs)) {
      Var unifiedLoc=unifiedLocs.get(branchLocs);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
 else {
      Var unifiedLoc=createUnifyingVar(consts,fn,parent,branchStates,branchBlocks,branchLocs,firstLoc.type());
      createdNewBranchCVs=true;
      unifiedLocs.put(branchLocs,unifiedLoc);
      availVals.add(createUnifiedCV(cv,unifiedLoc.asArg(),allClosed,anyValCopy,isAssign));
    }
  }
  return Pair.create(availVals,createdNewBranchCVs);
}",0.9954636182181092
132701,"/** 
 * @param logger
 * @param f
 * @param curr
 * @param cx current exec context
 * @param maybeInLoop if there's maybe a loop between the currentcontext and the root of the task we're in
 */
private static void pipelineTasks(Logger logger,Function f,Block curr,ExecContext cx,boolean maybeInLoop){
  for (  Continuation cont : curr.allComplexStatements()) {
    boolean contInLoop;
    if (cont.isAsync()) {
      contInLoop=false;
    }
 else     if (cont.isLoop()) {
      contInLoop=true;
    }
 else {
      contInLoop=maybeInLoop;
    }
    ExecContext childCx=cont.childContext(cx);
    for (    Block childBlock : cont.getBlocks()) {
      pipelineTasks(logger,f,childBlock,childCx,contInLoop);
    }
  }
  if (maybeInLoop) {
    return;
  }
  List<WaitStatement> candidates=new ArrayList<WaitStatement>();
  for (  Continuation cont : curr.getContinuations()) {
    if (cont.getType() == ContinuationType.WAIT_STATEMENT) {
      WaitStatement w=(WaitStatement)cont;
      ExecContext waitChildContext=w.childContext(cx);
      boolean compatible=true;
      if (!w.getWaitVars().isEmpty()) {
        compatible=false;
      }
 else       if (!waitChildContext.equals(cx) && !waitChildContext.isWildcardContext()) {
        compatible=false;
      }
 else       if (w.isParallel()) {
        compatible=false;
      }
 else       if (w.targetLocation() != null && !w.targetLocation().equals(Location.ANY_LOCATION)) {
        compatible=false;
      }
      if (compatible) {
        candidates.add(w);
      }
    }
  }
  if (candidates.isEmpty()) {
    return;
  }
  logger.trace(""String_Node_Str"" + candidates.size() + ""String_Node_Str""+ ""String_Node_Str"");
  WaitStatement bestCand=candidates.get(0);
  if (candidates.size() > 1) {
    int bestCost=heuristicCost(logger,f,curr,bestCand);
    for (int i=1; i < candidates.size(); i++) {
      WaitStatement cand=candidates.get(i);
      int cost=heuristicCost(logger,f,curr,cand);
      if (cost < bestCost) {
        bestCost=cost;
        bestCand=cand;
      }
    }
  }
  if (candidates.size() == 1) {
    bestCand.inlineInto(curr);
  }
 else {
    NestedBlock nested=new NestedBlock();
    bestCand.inlineInto(nested.getBlock());
    nested.setRunLast(true);
    curr.addContinuation(nested);
  }
}","/** 
 * @param logger
 * @param f
 * @param curr
 * @param cx current exec context
 * @param maybeInLoop if there's maybe a loop between the currentcontext and the root of the task we're in
 */
private static void pipelineTasks(Logger logger,Function f,Block curr,ExecContext cx,boolean maybeInLoop){
  for (  Continuation cont : curr.allComplexStatements()) {
    boolean contInLoop;
    if (cont.isAsync()) {
      contInLoop=false;
    }
 else     if (cont.isLoop()) {
      contInLoop=true;
    }
 else {
      contInLoop=maybeInLoop;
    }
    ExecContext childCx=cont.childContext(cx);
    for (    Block childBlock : cont.getBlocks()) {
      pipelineTasks(logger,f,childBlock,childCx,contInLoop);
    }
  }
  if (maybeInLoop) {
    return;
  }
  List<WaitStatement> candidates=new ArrayList<WaitStatement>();
  for (  Continuation cont : curr.getContinuations()) {
    if (cont.getType() == ContinuationType.WAIT_STATEMENT) {
      WaitStatement w=(WaitStatement)cont;
      ExecContext waitChildContext=w.childContext(cx);
      boolean compatible=true;
      if (!w.getWaitVars().isEmpty()) {
        compatible=false;
      }
 else       if (!waitChildContext.equals(cx) && !waitChildContext.isWildcardContext()) {
        compatible=false;
      }
 else       if (w.isParallel()) {
        compatible=false;
      }
 else       if (!Location.isAnyLocation(w.targetLocation(),true)) {
        compatible=false;
      }
      if (compatible) {
        candidates.add(w);
      }
    }
  }
  if (candidates.isEmpty()) {
    return;
  }
  logger.trace(""String_Node_Str"" + candidates.size() + ""String_Node_Str""+ ""String_Node_Str"");
  WaitStatement bestCand=candidates.get(0);
  if (candidates.size() > 1) {
    int bestCost=heuristicCost(logger,f,curr,bestCand);
    for (int i=1; i < candidates.size(); i++) {
      WaitStatement cand=candidates.get(i);
      int cost=heuristicCost(logger,f,curr,cand);
      if (cost < bestCost) {
        bestCost=cost;
        bestCand=cand;
      }
    }
  }
  if (candidates.size() == 1) {
    bestCand.inlineInto(curr);
  }
 else {
    NestedBlock nested=new NestedBlock();
    bestCand.inlineInto(nested.getBlock());
    nested.setRunLast(true);
    curr.addContinuation(nested);
  }
}",0.9802178261835964
132702,"private boolean compatibleLocPar(Arg location1,Arg location2,Arg par1,Arg par2){
  if (location1 != null && location2 != null) {
    if (!location1.equals(location2)) {
      return false;
    }
  }
 else   if (location1 != null || location2 != null) {
    return false;
  }
  if (par1 != null || par2 != null) {
    if (!par1.equals(par2)) {
      return false;
    }
  }
  return true;
}","private boolean compatibleLocPar(Arg location1,Arg location2,Arg par1,Arg par2){
  Logger logger=Logging.getSTCLogger();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + location1 + ""String_Node_Str""+ location2+ ""String_Node_Str""+ par1+ ""String_Node_Str""+ par2+ ""String_Node_Str"");
  }
  if (location1 != null && location2 != null) {
    if (!location1.equals(location2)) {
      return false;
    }
  }
 else   if (location1 != null || location2 != null) {
    return false;
  }
  if (par1 != null || par2 != null) {
    if (!par1.equals(par2)) {
      return false;
    }
  }
  return true;
}",0.7749003984063745
132703,"public Arg targetLocation(){
  return props.get(TaskPropKey.LOCATION);
}","/** 
 * @return target location.  Non-null.
 */
public Arg targetLocation(){
  return props.getWithDefault(TaskPropKey.LOCATION);
}",0.7093596059113301
132704,"private void callTclFunction(String function,TclFunRef tclf,List<Arg> inputs,List<Var> outputs,TaskProps props){
  assert(props != null);
  props.assertInternalTypesValid();
  TclList iList=TclUtil.tclListOfArgs(inputs);
  TclList oList=TclUtil.tclListOfVariables(outputs);
  if (tclf == null) {
    throw new STCRuntimeError(""String_Node_Str"" + function);
  }
  Arg priority=props.get(TaskPropKey.PRIORITY);
  TclTarget target=TclTarget.fromArg(props.get(TaskPropKey.LOCATION));
  Expression parExpr=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  setPriority(priority);
  Token tclFunction=new Token(tclf.pkg + ""String_Node_Str"" + tclf.symbol);
  List<Expression> funcArgs=new ArrayList<Expression>();
  funcArgs.add(oList);
  funcArgs.add(iList);
  funcArgs.addAll(Turbine.ruleKeywordArgs(target,parExpr));
  Command c=new Command(tclFunction,funcArgs);
  pointAdd(c);
  clearPriority(priority);
}","private void callTclFunction(String function,TclFunRef tclf,List<Arg> inputs,List<Var> outputs,TaskProps props){
  assert(props != null);
  props.assertInternalTypesValid();
  TclList iList=TclUtil.tclListOfArgs(inputs);
  TclList oList=TclUtil.tclListOfVariables(outputs);
  if (tclf == null) {
    throw new STCRuntimeError(""String_Node_Str"" + function);
  }
  Arg priority=props.get(TaskPropKey.PRIORITY);
  TclTarget target=TclTarget.fromArg(props.getWithDefault(TaskPropKey.LOCATION));
  Expression parExpr=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  setPriority(priority);
  Token tclFunction=new Token(tclf.pkg + ""String_Node_Str"" + tclf.symbol);
  List<Expression> funcArgs=new ArrayList<Expression>();
  funcArgs.add(oList);
  funcArgs.add(iList);
  funcArgs.addAll(Turbine.ruleKeywordArgs(target,parExpr));
  Command c=new Command(tclFunction,funcArgs);
  pointAdd(c);
  clearPriority(priority);
}",0.9940184883088636
132705,"private RuleProps buildRuleProps(TaskProps props){
  Expression priority=TclUtil.argToExpr(props.get(TaskPropKey.PRIORITY),true);
  TclTarget target=TclTarget.fromArg(props.get(TaskPropKey.LOCATION));
  Expression parallelism=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  RuleProps ruleProps=new RuleProps(target,parallelism,priority);
  return ruleProps;
}","private RuleProps buildRuleProps(TaskProps props){
  Expression priority=TclUtil.argToExpr(props.get(TaskPropKey.PRIORITY),true);
  TclTarget target=TclTarget.fromArg(props.getWithDefault(TaskPropKey.LOCATION));
  Expression parallelism=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  RuleProps ruleProps=new RuleProps(target,parallelism,priority);
  return ruleProps;
}",0.9854689564068692
132706,"public static TclTarget fromArg(Arg arg){
  if (arg == null) {
    return RANK_ANY;
  }
 else   if (arg.equals(Location.ANY_LOCATION)) {
    return RANK_ANY;
  }
 else {
    return TclTarget.rank(TclUtil.argToExpr(arg));
  }
}","public static TclTarget fromArg(Arg arg){
  assert(arg != null);
  if (Location.isAnyLocation(arg,true)) {
    return RANK_ANY;
  }
 else {
    return TclTarget.rank(TclUtil.argToExpr(arg));
  }
}",0.8009478672985783
132707,"/** 
 * @param logger
 * @param f
 * @param curr
 * @param cx current exec context
 * @param maybeInLoop if there's maybe a loop between the currentcontext and the root of the task we're in
 */
private static void pipelineTasks(Logger logger,Function f,Block curr,ExecContext cx,boolean maybeInLoop){
  for (  Continuation cont : curr.allComplexStatements()) {
    boolean contInLoop;
    if (cont.isAsync()) {
      contInLoop=false;
    }
 else     if (cont.isLoop()) {
      contInLoop=true;
    }
 else {
      contInLoop=maybeInLoop;
    }
    ExecContext childCx=cont.childContext(cx);
    for (    Block childBlock : cont.getBlocks()) {
      pipelineTasks(logger,f,childBlock,childCx,contInLoop);
    }
  }
  if (maybeInLoop) {
    return;
  }
  List<WaitStatement> candidates=new ArrayList<WaitStatement>();
  for (  Continuation cont : curr.getContinuations()) {
    if (cont.getType() == ContinuationType.WAIT_STATEMENT) {
      WaitStatement w=(WaitStatement)cont;
      ExecContext waitChildContext=w.childContext(cx);
      boolean compatible=true;
      if (!w.getWaitVars().isEmpty()) {
        compatible=false;
      }
 else       if (!waitChildContext.equals(cx) && !waitChildContext.isWildcardContext()) {
        compatible=false;
      }
 else       if (w.isParallel()) {
        compatible=false;
      }
 else       if (w.targetLocation() != null && !w.targetLocation().equals(Location.ANY_LOCATION)) {
        compatible=false;
      }
      if (compatible) {
        candidates.add(w);
      }
    }
  }
  if (candidates.isEmpty()) {
    return;
  }
  logger.trace(""String_Node_Str"" + candidates.size() + ""String_Node_Str""+ ""String_Node_Str"");
  WaitStatement bestCand=candidates.get(0);
  if (candidates.size() > 1) {
    int bestCost=heuristicCost(logger,f,curr,bestCand);
    for (int i=1; i < candidates.size(); i++) {
      WaitStatement cand=candidates.get(i);
      int cost=heuristicCost(logger,f,curr,cand);
      if (cost < bestCost) {
        bestCost=cost;
        bestCand=cand;
      }
    }
  }
  if (candidates.size() == 1) {
    bestCand.inlineInto(curr);
  }
 else {
    NestedBlock nested=new NestedBlock();
    bestCand.inlineInto(nested.getBlock());
    nested.setRunLast(true);
    curr.addContinuation(nested);
  }
}","/** 
 * @param logger
 * @param f
 * @param curr
 * @param cx current exec context
 * @param maybeInLoop if there's maybe a loop between the currentcontext and the root of the task we're in
 */
private static void pipelineTasks(Logger logger,Function f,Block curr,ExecContext cx,boolean maybeInLoop){
  for (  Continuation cont : curr.allComplexStatements()) {
    boolean contInLoop;
    if (cont.isAsync()) {
      contInLoop=false;
    }
 else     if (cont.isLoop()) {
      contInLoop=true;
    }
 else {
      contInLoop=maybeInLoop;
    }
    ExecContext childCx=cont.childContext(cx);
    for (    Block childBlock : cont.getBlocks()) {
      pipelineTasks(logger,f,childBlock,childCx,contInLoop);
    }
  }
  if (maybeInLoop) {
    return;
  }
  List<WaitStatement> candidates=new ArrayList<WaitStatement>();
  for (  Continuation cont : curr.getContinuations()) {
    if (cont.getType() == ContinuationType.WAIT_STATEMENT) {
      WaitStatement w=(WaitStatement)cont;
      ExecContext waitChildContext=w.childContext(cx);
      boolean compatible=true;
      if (!w.getWaitVars().isEmpty()) {
        compatible=false;
      }
 else       if (!waitChildContext.equals(cx) && !waitChildContext.isWildcardContext()) {
        compatible=false;
      }
 else       if (w.isParallel()) {
        compatible=false;
      }
 else       if (!Location.isAnyLocation(w.targetLocation(),true)) {
        compatible=false;
      }
      if (compatible) {
        candidates.add(w);
      }
    }
  }
  if (candidates.isEmpty()) {
    return;
  }
  logger.trace(""String_Node_Str"" + candidates.size() + ""String_Node_Str""+ ""String_Node_Str"");
  WaitStatement bestCand=candidates.get(0);
  if (candidates.size() > 1) {
    int bestCost=heuristicCost(logger,f,curr,bestCand);
    for (int i=1; i < candidates.size(); i++) {
      WaitStatement cand=candidates.get(i);
      int cost=heuristicCost(logger,f,curr,cand);
      if (cost < bestCost) {
        bestCost=cost;
        bestCand=cand;
      }
    }
  }
  if (candidates.size() == 1) {
    bestCand.inlineInto(curr);
  }
 else {
    NestedBlock nested=new NestedBlock();
    bestCand.inlineInto(nested.getBlock());
    nested.setRunLast(true);
    curr.addContinuation(nested);
  }
}",0.9802178261835964
132708,"private boolean compatibleLocPar(Arg location1,Arg location2,Arg par1,Arg par2){
  if (location1 != null && location2 != null) {
    if (!location1.equals(location2)) {
      return false;
    }
  }
 else   if (location1 != null || location2 != null) {
    return false;
  }
  if (par1 != null || par2 != null) {
    if (!par1.equals(par2)) {
      return false;
    }
  }
  return true;
}","private boolean compatibleLocPar(Arg location1,Arg location2,Arg par1,Arg par2){
  Logger logger=Logging.getSTCLogger();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + location1 + ""String_Node_Str""+ location2+ ""String_Node_Str""+ par1+ ""String_Node_Str""+ par2+ ""String_Node_Str"");
  }
  if (location1 != null && location2 != null) {
    if (!location1.equals(location2)) {
      return false;
    }
  }
 else   if (location1 != null || location2 != null) {
    return false;
  }
  if (par1 != null || par2 != null) {
    if (!par1.equals(par2)) {
      return false;
    }
  }
  return true;
}",0.7749003984063745
132709,"public Arg targetLocation(){
  return props.get(TaskPropKey.LOCATION);
}","/** 
 * @return target location.  Non-null.
 */
public Arg targetLocation(){
  return props.getWithDefault(TaskPropKey.LOCATION);
}",0.7093596059113301
132710,"private void callTclFunction(String function,TclFunRef tclf,List<Arg> inputs,List<Var> outputs,TaskProps props){
  assert(props != null);
  props.assertInternalTypesValid();
  TclList iList=TclUtil.tclListOfArgs(inputs);
  TclList oList=TclUtil.tclListOfVariables(outputs);
  if (tclf == null) {
    throw new STCRuntimeError(""String_Node_Str"" + function);
  }
  Arg priority=props.get(TaskPropKey.PRIORITY);
  TclTarget target=TclTarget.fromArg(props.get(TaskPropKey.LOCATION));
  Expression parExpr=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  setPriority(priority);
  Token tclFunction=new Token(tclf.pkg + ""String_Node_Str"" + tclf.symbol);
  List<Expression> funcArgs=new ArrayList<Expression>();
  funcArgs.add(oList);
  funcArgs.add(iList);
  funcArgs.addAll(Turbine.ruleKeywordArgs(target,parExpr));
  Command c=new Command(tclFunction,funcArgs);
  pointAdd(c);
  clearPriority(priority);
}","private void callTclFunction(String function,TclFunRef tclf,List<Arg> inputs,List<Var> outputs,TaskProps props){
  assert(props != null);
  props.assertInternalTypesValid();
  TclList iList=TclUtil.tclListOfArgs(inputs);
  TclList oList=TclUtil.tclListOfVariables(outputs);
  if (tclf == null) {
    throw new STCRuntimeError(""String_Node_Str"" + function);
  }
  Arg priority=props.get(TaskPropKey.PRIORITY);
  TclTarget target=TclTarget.fromArg(props.getWithDefault(TaskPropKey.LOCATION));
  Expression parExpr=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  setPriority(priority);
  Token tclFunction=new Token(tclf.pkg + ""String_Node_Str"" + tclf.symbol);
  List<Expression> funcArgs=new ArrayList<Expression>();
  funcArgs.add(oList);
  funcArgs.add(iList);
  funcArgs.addAll(Turbine.ruleKeywordArgs(target,parExpr));
  Command c=new Command(tclFunction,funcArgs);
  pointAdd(c);
  clearPriority(priority);
}",0.9940184883088636
132711,"private RuleProps buildRuleProps(TaskProps props){
  Expression priority=TclUtil.argToExpr(props.get(TaskPropKey.PRIORITY),true);
  TclTarget target=TclTarget.fromArg(props.get(TaskPropKey.LOCATION));
  Expression parallelism=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  RuleProps ruleProps=new RuleProps(target,parallelism,priority);
  return ruleProps;
}","private RuleProps buildRuleProps(TaskProps props){
  Expression priority=TclUtil.argToExpr(props.get(TaskPropKey.PRIORITY),true);
  TclTarget target=TclTarget.fromArg(props.getWithDefault(TaskPropKey.LOCATION));
  Expression parallelism=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  RuleProps ruleProps=new RuleProps(target,parallelism,priority);
  return ruleProps;
}",0.9854689564068692
132712,"public static TclTarget fromArg(Arg arg){
  if (arg == null) {
    return RANK_ANY;
  }
 else   if (arg.equals(Location.ANY_LOCATION)) {
    return RANK_ANY;
  }
 else {
    return TclTarget.rank(TclUtil.argToExpr(arg));
  }
}","public static TclTarget fromArg(Arg arg){
  assert(arg != null);
  if (Location.isAnyLocation(arg,true)) {
    return RANK_ANY;
  }
 else {
    return TclTarget.rank(TclUtil.argToExpr(arg));
  }
}",0.8009478672985783
132713,"private static void fuseNonRecursive(Block block){
  Iterator<Continuation> it=block.continuationIterator();
  assert(block.getContinuations().size() > 1);
  LinkedList<Continuation> mergeCands=new LinkedList<Continuation>(block.getContinuations());
  mergeCands.removeFirst();
  while (it.hasNext()) {
    if (mergeCands.isEmpty()) {
      break;
    }
    Continuation c=it.next();
switch (c.getType()) {
case IF_STATEMENT:
      fuseIfStatement(it,mergeCands,(IfStatement)c);
    break;
case FOREACH_LOOP:
  fuseForeachLoop(it,mergeCands,(ForeachLoop)c);
break;
case RANGE_LOOP:
fuseRangeLoop(it,mergeCands,(RangeLoop)c);
break;
default :
break;
}
mergeCands.removeFirst();
}
}","private static void fuseNonRecursive(String function,Block block){
  Iterator<Continuation> it=block.continuationIterator();
  assert(block.getContinuations().size() > 1);
  LinkedList<Continuation> mergeCands=new LinkedList<Continuation>(block.getContinuations());
  mergeCands.removeFirst();
  while (it.hasNext()) {
    if (mergeCands.isEmpty()) {
      break;
    }
    Continuation c=it.next();
switch (c.getType()) {
case IF_STATEMENT:
      fuseIfStatement(it,mergeCands,(IfStatement)c);
    break;
case FOREACH_LOOP:
  fuseForeachLoop(function,it,mergeCands,(ForeachLoop)c);
break;
case RANGE_LOOP:
fuseRangeLoop(function,it,mergeCands,(RangeLoop)c);
break;
default :
break;
}
mergeCands.removeFirst();
}
}",0.975609756097561
132714,"private static void fuseRecursive(Logger logger,Function f,Block block){
  if (block.getContinuations().size() > 1) {
    fuseNonRecursive(block);
  }
  for (  Continuation c : block.allComplexStatements()) {
    for (    Block child : c.getBlocks()) {
      fuseRecursive(logger,f,child);
    }
  }
}","private static void fuseRecursive(Logger logger,Function f,Block block){
  if (block.getContinuations().size() > 1) {
    fuseNonRecursive(f.getName(),block);
  }
  for (  Continuation c : block.allComplexStatements()) {
    for (    Block child : c.getBlocks()) {
      fuseRecursive(logger,f,child);
    }
  }
}",0.980456026058632
132715,"/** 
 * Try and merge if1 into one of statements in mergeCands If successful, call it.remove()
 * @param it
 * @param mergeCands
 * @param loop1
 */
private static void fuseForeachLoop(Iterator<Continuation> it,LinkedList<Continuation> mergeCands,ForeachLoop loop1){
  for (  Continuation c2 : mergeCands) {
    if (c2.getType() == ContinuationType.FOREACH_LOOP) {
      ForeachLoop loop2=(ForeachLoop)c2;
      if (loop2.fuseable(loop1)) {
        loop2.fuseInto(loop1,true);
        it.remove();
        return;
      }
    }
  }
}","/** 
 * Try and merge if1 into one of statements in mergeCands If successful, call it.remove()
 * @param it
 * @param mergeCands
 * @param loop1
 */
private static void fuseForeachLoop(String function,Iterator<Continuation> it,LinkedList<Continuation> mergeCands,ForeachLoop loop1){
  for (  Continuation c2 : mergeCands) {
    if (c2.getType() == ContinuationType.FOREACH_LOOP) {
      ForeachLoop loop2=(ForeachLoop)c2;
      if (loop2.fuseable(loop1)) {
        loop2.fuseInto(function,loop1,true);
        it.remove();
        return;
      }
    }
  }
}",0.9770852428964252
132716,"/** 
 * Try and merge if1 into one of statements in mergeCands If successful, call it.remove()
 * @param it
 * @param mergeCands
 * @param loop1
 */
private static void fuseRangeLoop(Iterator<Continuation> it,LinkedList<Continuation> mergeCands,RangeLoop loop1){
  for (  Continuation c2 : mergeCands) {
    if (c2.getType() == ContinuationType.RANGE_LOOP) {
      RangeLoop loop2=(RangeLoop)c2;
      if (loop2.fuseable(loop1)) {
        assert(loop2 != loop1);
        loop2.fuseInto(loop1,true);
        it.remove();
        return;
      }
    }
  }
}","/** 
 * Try and merge if1 into one of statements in mergeCands If successful, call it.remove()
 * @param it
 * @param mergeCands
 * @param loop1
 */
private static void fuseRangeLoop(String function,Iterator<Continuation> it,LinkedList<Continuation> mergeCands,RangeLoop loop1){
  for (  Continuation c2 : mergeCands) {
    if (c2.getType() == ContinuationType.RANGE_LOOP) {
      RangeLoop loop2=(RangeLoop)c2;
      if (loop2.fuseable(loop1)) {
        assert(loop2 != loop1);
        loop2.fuseInto(function,loop1,true);
        it.remove();
        return;
      }
    }
  }
}",0.9779735682819384
132717,"/** 
 * Attempt to explode individual instruction
 * @param logger
 * @param fn
 * @param execCx
 * @param block
 * @param it
 * @param inst
 * @param waitedFor any vars waited for, e.g. in outer exploded.  This preventsinfinite cycles of exploding if we didn't change instruction
 * @return true if exploded
 */
private static boolean tryExplode(Logger logger,Function fn,ExecContext execCx,Block block,ListIterator<Statement> it,Instruction inst,HierarchicalSet<Var> waitedFor){
  MakeImmRequest req=inst.canMakeImmediate(waitedFor,Collections.<ArgCV>emptySet(),Collections.<Var>emptySet(),true);
  if (req != null && req.in.size() > 0) {
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ fn.getName());
    }
    it.remove();
    List<Pair<Var,Ternary>> waitVars=new ArrayList<Pair<Var,Ternary>>();
    Map<Var,Var> filenameMap=new HashMap<Var,Var>();
    OptUtil.buildWaitVars(block,it,req.in,req.out,filenameMap,waitVars);
    Block insideWaitBlock=enterWaits(fn,execCx,block,inst,req,waitVars);
    List<Statement> instBuffer=new ArrayList<Statement>();
    List<Arg> inVals=OptUtil.fetchMakeImmInputs(insideWaitBlock,req.in,instBuffer);
    List<Var> localOutputs=OptUtil.createMakeImmOutputs(insideWaitBlock,req.out,filenameMap,instBuffer);
    MakeImmChange change=inst.makeImmediate(new OptVarCreator(insideWaitBlock),Fetched.makeList(req.out,localOutputs,true),Fetched.makeList(req.in,inVals,false));
    OptUtil.fixupImmChange(block,insideWaitBlock,inst,change,instBuffer,localOutputs,req.out);
    insideWaitBlock.addStatements(instBuffer);
    return true;
  }
  return false;
}","/** 
 * Attempt to explode individual instruction
 * @param logger
 * @param fn
 * @param execCx
 * @param block
 * @param it
 * @param inst
 * @param waitedFor any vars waited for, e.g. in outer exploded.  This preventsinfinite cycles of exploding if we didn't change instruction
 * @return true if exploded
 */
private static boolean tryExplode(Logger logger,Function fn,ExecContext execCx,Block block,ListIterator<Statement> it,Instruction inst,HierarchicalSet<Var> waitedFor){
  MakeImmRequest req=inst.canMakeImmediate(waitedFor,Collections.<ArgCV>emptySet(),Collections.<Var>emptySet(),true);
  if (req != null && req.in.size() > 0) {
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ fn.getName());
    }
    it.remove();
    List<Pair<Var,Ternary>> waitVars=new ArrayList<Pair<Var,Ternary>>();
    Map<Var,Var> filenameMap=new HashMap<Var,Var>();
    OptUtil.buildWaitVars(block,it,req.in,req.out,filenameMap,waitVars);
    Block insideWaitBlock=enterWaits(fn,execCx,block,inst,req,waitVars);
    List<Statement> instBuffer=new ArrayList<Statement>();
    List<Arg> inVals=OptUtil.fetchMakeImmInputs(insideWaitBlock,req.in,instBuffer);
    List<Var> localOutputs=OptUtil.createMakeImmOutputs(insideWaitBlock,req.out,filenameMap,instBuffer);
    MakeImmChange change=inst.makeImmediate(new OptVarCreator(insideWaitBlock),Fetched.makeList(req.out,localOutputs,true),Fetched.makeList(req.in,inVals,false));
    OptUtil.fixupImmChange(fn.getName(),block,insideWaitBlock,inst,change,instBuffer,localOutputs,req.out);
    insideWaitBlock.addStatements(instBuffer);
    return true;
  }
  return false;
}",0.9960570215347284
132718,"private Function switchToValuePassing(Logger logger,Function fn,Set<String> usedFunctionNames){
  if (fn.blockingInputs().isEmpty())   return null;
  List<Var> switchVars=new ArrayList<Var>();
  for (  WaitVar input : fn.blockingInputs()) {
    if (Types.isPrimFuture(input.var.type())) {
      Type valueT=Types.retrievedType(input.var.type());
      if (Semantics.canPassToChildTask(valueT)) {
        switchVars.add(input.var);
      }
    }
  }
  if (switchVars.isEmpty())   return null;
  List<Pair<Var,Var>> futValPairs=createValueVars(fn,switchVars);
  Map<Var,Var> switched=new HashMap<Var,Var>();
  for (  Pair<Var,Var> fv : futValPairs) {
    switched.put(fv.val1,fv.val2);
    assert(fv.val2 != null);
  }
  List<Var> newIList=buildNewInputList(fn,switched);
  String newName=selectUniqueName(fn.getName(),usedFunctionNames);
  Block callNewFunction=callNewFunctionCode(fn,newName,switchVars);
  Block newBlock=fn.swapBlock(callNewFunction);
  for (  Pair<Var,Var> fv : futValPairs) {
    Var tmpfuture=new Var(fv.val1.type(),fv.val1.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.renamed(fv.val1));
    newBlock.renameVars(Collections.singletonMap(fv.val1,tmpfuture.asArg()),RenameMode.REPLACE_VAR,true);
    newBlock.addVariable(tmpfuture);
    Instruction store=TurbineOp.storePrim(tmpfuture,fv.val2.asArg());
    newBlock.addInstructionFront(store);
  }
  List<WaitVar> newBlocking=new ArrayList<WaitVar>();
  for (  WaitVar wv : fn.blockingInputs()) {
    if (!switchVars.contains(wv.var)) {
      newBlocking.add(wv);
    }
  }
  return new Function(newName,newIList,newBlocking,fn.getOutputList(),fn.mode(),newBlock);
}","private Function switchToValuePassing(Logger logger,Function fn,Set<String> usedFunctionNames){
  if (fn.blockingInputs().isEmpty())   return null;
  List<Var> switchVars=new ArrayList<Var>();
  for (  WaitVar input : fn.blockingInputs()) {
    if (Types.isPrimFuture(input.var.type())) {
      Type valueT=Types.retrievedType(input.var.type());
      if (Semantics.canPassToChildTask(valueT)) {
        switchVars.add(input.var);
      }
    }
  }
  if (switchVars.isEmpty())   return null;
  List<Pair<Var,Var>> futValPairs=createValueVars(fn,switchVars);
  Map<Var,Var> switched=new HashMap<Var,Var>();
  for (  Pair<Var,Var> fv : futValPairs) {
    switched.put(fv.val1,fv.val2);
    assert(fv.val2 != null);
  }
  List<Var> newIList=buildNewInputList(fn,switched);
  String newName=selectUniqueName(fn.getName(),usedFunctionNames);
  Block callNewFunction=callNewFunctionCode(fn,newName,switchVars);
  Block newBlock=fn.swapBlock(callNewFunction);
  for (  Pair<Var,Var> fv : futValPairs) {
    Var tmpfuture=new Var(fv.val1.type(),fv.val1.name(),Alloc.STACK,DefType.LOCAL_USER,VarProvenance.renamed(fv.val1));
    newBlock.renameVars(fn.getName(),Collections.singletonMap(fv.val1,tmpfuture.asArg()),RenameMode.REPLACE_VAR,true);
    newBlock.addVariable(tmpfuture);
    Instruction store=TurbineOp.storePrim(tmpfuture,fv.val2.asArg());
    newBlock.addInstructionFront(store);
  }
  List<WaitVar> newBlocking=new ArrayList<WaitVar>();
  for (  WaitVar wv : fn.blockingInputs()) {
    if (!switchVars.contains(wv.var)) {
      newBlocking.add(wv);
    }
  }
  return new Function(newName,newIList,newBlocking,fn.getOutputList(),fn.mode(),newBlock);
}",0.9960570215347284
132719,"private static boolean unrollLoops(Logger logger,Program prog,Function f,Block block){
  boolean unrolled=false;
  ListIterator<Continuation> it=block.continuationIterator();
  while (it.hasNext()) {
    Continuation c=it.next();
    for (    Block b : c.getBlocks()) {
      boolean res=unrollLoops(logger,prog,f,b);
      unrolled=unrolled || res;
    }
    Pair<Boolean,List<Continuation>> cRes=c.tryUnroll(logger,block);
    if (cRes.val1) {
      unrolled=true;
      for (      Continuation newC : cRes.val2) {
        it.add(newC);
      }
    }
  }
  return unrolled;
}","private static boolean unrollLoops(Logger logger,Program prog,Function f,Block block){
  boolean unrolled=false;
  ListIterator<Continuation> it=block.continuationIterator();
  while (it.hasNext()) {
    Continuation c=it.next();
    for (    Block b : c.getBlocks()) {
      boolean res=unrollLoops(logger,prog,f,b);
      unrolled=unrolled || res;
    }
    Pair<Boolean,List<Continuation>> cRes;
    cRes=c.tryUnroll(logger,f.getName(),block);
    if (cRes.val1) {
      unrolled=true;
      for (      Continuation newC : cRes.val2) {
        it.add(newC);
      }
    }
  }
  return unrolled;
}",0.9812925170068028
132720,"public static void fixupImmChange(Block srcBlock,Block targetBlock,Instruction oldInst,MakeImmChange change,List<Statement> instBuffer,List<Var> newOutVars,List<MakeImmVar> oldOutVars){
  instBuffer.addAll(Arrays.asList(change.newInsts));
  Logger logger=Logging.getSTCLogger();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + oldInst + ""String_Node_Str""+ Arrays.asList(change.newInsts));
  }
  if (!change.isOutVarSame()) {
    Var newOut=change.newOut;
    Var oldOut=change.oldOut;
    boolean initOutput=false;
    for (    Pair<Var,InitType> init : oldInst.getInitialized()) {
      if (init.val2 == InitType.FULL && init.val1.equals(oldOut)) {
        initOutput=true;
        break;
      }
    }
    replaceInstOutput(srcBlock,targetBlock,instBuffer,newOut,oldOut,initOutput);
  }
  if (change.storeOutputVals) {
    setLocalOutputs(targetBlock,oldOutVars,newOutVars,instBuffer);
  }
}","public static void fixupImmChange(String function,Block srcBlock,Block targetBlock,Instruction oldInst,MakeImmChange change,List<Statement> instBuffer,List<Var> newOutVars,List<MakeImmVar> oldOutVars){
  instBuffer.addAll(Arrays.asList(change.newInsts));
  Logger logger=Logging.getSTCLogger();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + oldInst + ""String_Node_Str""+ Arrays.asList(change.newInsts));
  }
  if (!change.isOutVarSame()) {
    Var newOut=change.newOut;
    Var oldOut=change.oldOut;
    boolean initOutput=false;
    for (    Pair<Var,InitType> init : oldInst.getInitialized()) {
      if (init.val2 == InitType.FULL && init.val1.equals(oldOut)) {
        initOutput=true;
        break;
      }
    }
    replaceInstOutput(function,srcBlock,targetBlock,instBuffer,newOut,oldOut,initOutput);
  }
  if (change.storeOutputVals) {
    setLocalOutputs(targetBlock,oldOutVars,newOutVars,instBuffer);
  }
}",0.9865229110512128
132721,"/** 
 * Do the manipulation necessary to allow an old instruction output variable to be replaced with a new one. Assume that newOut is a value type of oldOut
 * @param srcBlock source block for instruction 
 * @param targetBlock target block for instruction
 * @param instBuffer append any fixup instructions here
 * @param newOut
 * @param oldOut
 * @param recursive if it's to be fetched recursively
 */
public static void replaceInstOutput(Block srcBlock,Block targetBlock,List<Statement> instBuffer,Var newOut,Var oldOut,boolean initialisesOutput){
  boolean isDerefResult=Types.retrievedType(oldOut).assignableTo(newOut.type());
  if (isDerefResult) {
    Var oldOutReplacement;
    if (oldOut.storage() == Alloc.ALIAS && initialisesOutput) {
      oldOutReplacement=new Var(oldOut.type(),oldOut.name(),Alloc.TEMP,oldOut.defType(),oldOut.provenance(),oldOut.mappedDecl());
      replaceVarDeclaration(srcBlock,oldOut,oldOutReplacement);
      Map<Var,Arg> renames=Collections.singletonMap(oldOut,Arg.createVar(oldOutReplacement));
      for (      Statement inst : instBuffer) {
        inst.renameVars(renames,RenameMode.REPLACE_VAR);
      }
    }
 else {
      oldOutReplacement=oldOut;
    }
    WrapUtil.assignOutput(targetBlock,instBuffer,false,oldOutReplacement,newOut,false);
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + oldOut + ""String_Node_Str""+ newOut+ ""String_Node_Str""+ ""String_Node_Str"");
  }
}","/** 
 * Do the manipulation necessary to allow an old instruction output variable to be replaced with a new one. Assume that newOut is a value type of oldOut
 * @param function
 * @param srcBlock source block for instruction 
 * @param targetBlock target block for instruction
 * @param instBuffer append any fixup instructions here
 * @param newOut
 * @param oldOut
 * @param recursive if it's to be fetched recursively
 */
public static void replaceInstOutput(String function,Block srcBlock,Block targetBlock,List<Statement> instBuffer,Var newOut,Var oldOut,boolean initialisesOutput){
  boolean isDerefResult=Types.retrievedType(oldOut).assignableTo(newOut.type());
  if (isDerefResult) {
    Var oldOutReplacement;
    if (oldOut.storage() == Alloc.ALIAS && initialisesOutput) {
      oldOutReplacement=new Var(oldOut.type(),oldOut.name(),Alloc.TEMP,oldOut.defType(),oldOut.provenance(),oldOut.mappedDecl());
      replaceVarDeclaration(srcBlock,oldOut,oldOutReplacement);
      Map<Var,Arg> renames=Collections.singletonMap(oldOut,Arg.createVar(oldOutReplacement));
      for (      Statement inst : instBuffer) {
        inst.renameVars(function,renames,RenameMode.REPLACE_VAR);
      }
    }
 else {
      oldOutReplacement=oldOut;
    }
    WrapUtil.assignOutput(targetBlock,instBuffer,false,oldOutReplacement,newOut,false);
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + oldOut + ""String_Node_Str""+ newOut+ ""String_Node_Str""+ ""String_Node_Str"");
  }
}",0.9850644942294636
132722,"private static void replaceCongruentNonRec(Continuation cont,Congruences congruent,InitState init){
  for (  RenameMode mode : RENAME_MODES) {
    cont.renameVars(congruent.replacements(mode,init),mode,false);
  }
}","private static void replaceCongruentNonRec(String function,Continuation cont,Congruences congruent,InitState init){
  for (  RenameMode mode : RENAME_MODES) {
    cont.renameVars(function,congruent.replacements(mode,init),mode,false);
  }
}",0.945054945054945
132723,"private void runPass(Program prog,Function f){
  logger.trace(""String_Node_Str"" + f.getName());
  try {
    Map<Block,Congruences> congMap;
    congMap=findCongruences(prog,f,ExecContext.CONTROL);
    replaceVals(prog.constants(),f.mainBlock(),congMap,InitState.enterFunction(f));
    inlinePass(prog.constants(),f.mainBlock(),congMap);
  }
 catch (  OptUnsafeError e) {
    logger.debug(""String_Node_Str"" + f.getName());
  }
}","private void runPass(Program prog,Function f){
  logger.trace(""String_Node_Str"" + f.getName());
  try {
    Map<Block,Congruences> congMap;
    congMap=findCongruences(prog,f,ExecContext.CONTROL);
    replaceVals(prog.constants(),f.getName(),f.mainBlock(),congMap,InitState.enterFunction(f));
    inlinePass(prog.constants(),f.mainBlock(),congMap);
  }
 catch (  OptUnsafeError e) {
    logger.debug(""String_Node_Str"" + f.getName());
  }
}",0.9861431870669746
132724,"private void replaceVals(GlobalConstants consts,Block block,Map<Block,Congruences> congruences,InitState init){
  Congruences state=congruences.get(block);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + System.identityHashCode(block) + ""String_Node_Str""+ block.getType());
    state.printTraceInfo(logger,consts);
  }
  ListIterator<Statement> stmtIt=block.statementIterator();
  while (stmtIt.hasNext()) {
    Statement stmt=stmtIt.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      Instruction inst=stmt.instruction();
      replaceCongruent(inst,state,init);
      if (!inst.hasSideEffects() && inst.getOutputs().size() == 1) {
        Var output=inst.getOutput(0);
        if (!InitVariables.varMustBeInitialized(output,true) || init.isInitialized(output,true)) {
          if (Types.isScalarFuture(output)) {
            Arg val=state.findRetrieveResult(output,false);
            if (val != null && init.isInitialized(val,false)) {
              Instruction futureSet=TurbineOp.storePrim(output,val);
              stmtIt.set(futureSet);
              logger.trace(""String_Node_Str"" + futureSet);
            }
          }
 else           if (Types.isScalarValue(output)) {
            Arg val=state.findValue(output);
            if (val != null && val.isConstant()) {
              Instruction valueSet=ICInstructions.valueSet(output,val);
              stmtIt.set(valueSet);
              logger.trace(""String_Node_Str"" + valueSet);
            }
          }
        }
      }
      InitVariables.updateInitVars(logger,stmt,init,false);
    }
 else {
      assert(stmt.type() == StatementType.CONDITIONAL);
      replaceCongruentNonRec(stmt.conditional(),state,init);
      replaceValsRec(consts,stmt.conditional(),congruences,init);
    }
  }
  replaceCleanupCongruent(block,state,init);
  for (  Continuation cont : block.getContinuations()) {
    replaceCongruentNonRec(cont,state,init);
    replaceValsRec(consts,cont,congruences,init);
  }
}","private void replaceVals(GlobalConstants consts,String function,Block block,Map<Block,Congruences> congruences,InitState init){
  Congruences state=congruences.get(block);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + System.identityHashCode(block) + ""String_Node_Str""+ block.getType());
    state.printTraceInfo(logger,consts);
  }
  ListIterator<Statement> stmtIt=block.statementIterator();
  while (stmtIt.hasNext()) {
    Statement stmt=stmtIt.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      Instruction inst=stmt.instruction();
      replaceCongruent(function,inst,state,init);
      if (!inst.hasSideEffects() && inst.getOutputs().size() == 1) {
        Var output=inst.getOutput(0);
        if (!InitVariables.varMustBeInitialized(output,true) || init.isInitialized(output,true)) {
          if (Types.isScalarFuture(output)) {
            Arg val=state.findRetrieveResult(output,false);
            if (val != null && init.isInitialized(val,false)) {
              Instruction futureSet=TurbineOp.storePrim(output,val);
              stmtIt.set(futureSet);
              logger.trace(""String_Node_Str"" + futureSet);
            }
          }
 else           if (Types.isScalarValue(output)) {
            Arg val=state.findValue(output);
            if (val != null && val.isConstant()) {
              Instruction valueSet=ICInstructions.valueSet(output,val);
              stmtIt.set(valueSet);
              logger.trace(""String_Node_Str"" + valueSet);
            }
          }
        }
      }
      InitVariables.updateInitVars(logger,stmt,init,false);
    }
 else {
      assert(stmt.type() == StatementType.CONDITIONAL);
      replaceCongruentNonRec(function,stmt.conditional(),state,init);
      replaceValsRec(consts,function,stmt.conditional(),congruences,init);
    }
  }
  replaceCleanupCongruent(function,block,state,init);
  for (  Continuation cont : block.getContinuations()) {
    replaceCongruentNonRec(function,cont,state,init);
    replaceValsRec(consts,function,cont,congruences,init);
  }
}",0.9830508474576272
132725,"private void replaceCongruent(Instruction inst,Congruences congruent,InitState init){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst);
  }
  for (  RenameMode mode : RENAME_MODES) {
    inst.renameVars(congruent.replacements(mode,init),mode);
  }
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst);
  }
}","private void replaceCongruent(String function,Instruction inst,Congruences congruent,InitState init){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst);
  }
  for (  RenameMode mode : RENAME_MODES) {
    inst.renameVars(function,congruent.replacements(mode,init),mode);
  }
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst);
  }
}",0.96617050067659
132726,"/** 
 * @param logger
 * @param fn
 * @param block
 * @param cv
 * @param inst
 * @param insts if instructions inserted, leaves iterator pointing at previous instruction
 * @return
 */
private boolean switchToImmediate(Logger logger,Function fn,ExecContext execCx,Block block,Congruences state,Instruction inst,ListIterator<Statement> stmts,int stmtIndex){
  if (!finalizedVarEnabled) {
    return false;
  }
  MakeImmRequest req=inst.canMakeImmediate(state.getClosed(stmtIndex),state.getClosedLocs(stmtIndex),state.retrieveResultAvail(),false);
  if (req == null) {
    return false;
  }
  Block insertContext;
  ListIterator<Statement> insertPoint;
  boolean noWaitRequired=req.mode == TaskMode.LOCAL || req.mode == TaskMode.SYNC || (req.mode == TaskMode.LOCAL_CONTROL && execCx == ExecContext.CONTROL);
  stmts.remove();
  if (noWaitRequired) {
    insertContext=block;
    insertPoint=stmts;
  }
 else {
    WaitStatement wait=new WaitStatement(fn.getName() + ""String_Node_Str"" + inst.shortOpName(),WaitVar.NONE,PassedVar.NONE,Var.NONE,WaitMode.TASK_DISPATCH,false,req.mode,inst.getTaskProps());
    insertContext=wait.getBlock();
    block.addContinuation(wait);
    insertPoint=insertContext.statementIterator();
  }
  List<Statement> alt=new ArrayList<Statement>();
  List<Fetched<Arg>> inVals=fetchInputsForSwitch(state,req,insertContext,noWaitRequired,alt);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inVals + ""String_Node_Str""+ inst+ ""String_Node_Str""+ req.in);
  }
  Map<Var,Var> filenameVals=loadOutputFileNames(state,stmtIndex,req.out,insertContext,insertPoint);
  List<Var> outFetched=OptUtil.createLocalOpOutputVars(insertContext,insertPoint,req.out,filenameVals);
  MakeImmChange change;
  change=inst.makeImmediate(new OptVarCreator(block),Fetched.makeList(req.out,outFetched,true),inVals);
  OptUtil.fixupImmChange(block,insertContext,inst,change,alt,outFetched,req.out);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ alt.toString());
  }
  for (  Statement newStmt : alt) {
    insertPoint.add(newStmt);
  }
  if (stmts == insertPoint) {
    ICUtil.rewindIterator(stmts,alt.size());
  }
  return true;
}","/** 
 * @param logger
 * @param fn
 * @param block
 * @param cv
 * @param inst
 * @param insts if instructions inserted, leaves iterator pointing at previous instruction
 * @return
 */
private boolean switchToImmediate(Logger logger,Function fn,ExecContext execCx,Block block,Congruences state,Instruction inst,ListIterator<Statement> stmts,int stmtIndex){
  if (!finalizedVarEnabled) {
    return false;
  }
  MakeImmRequest req=inst.canMakeImmediate(state.getClosed(stmtIndex),state.getClosedLocs(stmtIndex),state.retrieveResultAvail(),false);
  if (req == null) {
    return false;
  }
  Block insertContext;
  ListIterator<Statement> insertPoint;
  boolean noWaitRequired=req.mode == TaskMode.LOCAL || req.mode == TaskMode.SYNC || (req.mode == TaskMode.LOCAL_CONTROL && execCx == ExecContext.CONTROL);
  stmts.remove();
  if (noWaitRequired) {
    insertContext=block;
    insertPoint=stmts;
  }
 else {
    WaitStatement wait=new WaitStatement(fn.getName() + ""String_Node_Str"" + inst.shortOpName(),WaitVar.NONE,PassedVar.NONE,Var.NONE,WaitMode.TASK_DISPATCH,false,req.mode,inst.getTaskProps());
    insertContext=wait.getBlock();
    block.addContinuation(wait);
    insertPoint=insertContext.statementIterator();
  }
  List<Statement> alt=new ArrayList<Statement>();
  List<Fetched<Arg>> inVals=fetchInputsForSwitch(state,req,insertContext,noWaitRequired,alt);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inVals + ""String_Node_Str""+ inst+ ""String_Node_Str""+ req.in);
  }
  Map<Var,Var> filenameVals=loadOutputFileNames(state,stmtIndex,req.out,insertContext,insertPoint);
  List<Var> outFetched=OptUtil.createLocalOpOutputVars(insertContext,insertPoint,req.out,filenameVals);
  MakeImmChange change;
  change=inst.makeImmediate(new OptVarCreator(block),Fetched.makeList(req.out,outFetched,true),inVals);
  OptUtil.fixupImmChange(fn.getName(),block,insertContext,inst,change,alt,outFetched,req.out);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ alt.toString());
  }
  for (  Statement newStmt : alt) {
    insertPoint.add(newStmt);
  }
  if (stmts == insertPoint) {
    ICUtil.rewindIterator(stmts,alt.size());
  }
  return true;
}",0.9970528224892314
132727,"private static void replaceCleanupCongruent(Block block,Congruences congruent,InitState init){
  for (  RenameMode mode : RENAME_MODES) {
    block.renameCleanupActions(congruent.replacements(mode,init),mode);
  }
}","private static void replaceCleanupCongruent(String function,Block block,Congruences congruent,InitState init){
  for (  RenameMode mode : RENAME_MODES) {
    block.renameCleanupActions(function,congruent.replacements(mode,init),mode);
  }
}",0.945054945054945
132728,"private void replaceValsRec(GlobalConstants consts,Continuation cont,Map<Block,Congruences> congruences,InitState init){
  InitState contInit=init.enterContinuation(cont);
  List<InitState> branchInits=new ArrayList<InitState>();
  for (  Block contBlock : cont.getBlocks()) {
    InitState branchInit=contInit.enterBlock(contBlock);
    replaceVals(consts,contBlock,congruences,branchInit);
    branchInits.add(branchInit);
  }
  if (InitState.canUnifyBranches(cont)) {
    init.unifyBranches(cont,branchInits);
  }
}","private void replaceValsRec(GlobalConstants consts,String function,Continuation cont,Map<Block,Congruences> congruences,InitState init){
  InitState contInit=init.enterContinuation(cont);
  List<InitState> branchInits=new ArrayList<InitState>();
  for (  Block contBlock : cont.getBlocks()) {
    InitState branchInit=contInit.enterBlock(contBlock);
    replaceVals(consts,function,contBlock,congruences,branchInit);
    branchInits.add(branchInit);
  }
  if (InitState.canUnifyBranches(cont)) {
    init.unifyBranches(cont,branchInits);
  }
}",0.9764373232799246
132729,"/** 
 * Fuse the other loop into this loop
 */
public void fuseInto(RangeLoop o,boolean insertAtTop){
  Map<Var,Arg> renames=new HashMap<Var,Arg>();
  renames.put(o.loopVar,Arg.createVar(this.loopVar));
  if (loopCounterVar != null)   renames.put(o.loopCounterVar,Arg.createVar(this.loopCounterVar));
  o.renameVars(renames,RenameMode.REPLACE_VAR,true);
  this.fuseIntoAbstract(o,insertAtTop);
}","/** 
 * Fuse the other loop into this loop
 */
public void fuseInto(String function,RangeLoop o,boolean insertAtTop){
  Map<Var,Arg> renames=new HashMap<Var,Arg>();
  renames.put(o.loopVar,Arg.createVar(this.loopVar));
  if (loopCounterVar != null)   renames.put(o.loopCounterVar,Arg.createVar(this.loopCounterVar));
  o.renameVars(function,renames,RenameMode.REPLACE_VAR,true);
  this.fuseIntoAbstract(o,insertAtTop);
}",0.9693251533742332
132730,"@Override public Pair<Boolean,List<Continuation>> tryUnroll(Logger logger,Block outerBlock){
  logger.trace(""String_Node_Str"" + loopName + ""String_Node_Str""+ desiredUnroll);
  boolean expandLoops=isExpandLoopsEnabled();
  boolean fullUnroll=isFullUnrollEnabled();
  if (!this.unrolled && this.desiredUnroll > 1) {
    if (this.loopCounterVar != null) {
      logger.warn(""String_Node_Str"" + ""String_Node_Str"");
      return NO_UNROLL;
    }
    return Pair.create(true,doUnroll(logger,outerBlock,desiredUnroll));
  }
 else   if (expandLoops || fullUnroll) {
    long instCount=loopBody.getInstructionCount();
    long iterCount=constIterCount();
    if (expandLoops && iterCount >= 0) {
      if (iterCount <= getUnrollMaxIters(true)) {
        long extraInstructions=instCount * (iterCount - 1);
        if (extraInstructions <= getUnrollMaxExtraInsts(true)) {
          return Pair.create(true,doUnroll(logger,outerBlock,(int)iterCount));
        }
      }
    }
    if (!fullUnroll) {
      logger.trace(""String_Node_Str"");
      return NO_UNROLL;
    }
    if (this.unrolled) {
      return NO_UNROLL;
    }
    long threshold=getUnrollMaxExtraInsts(false);
    long unrollFactor=Math.min(getUnrollMaxIters(false),(threshold / instCount) + 1);
    if (unrollFactor > 1) {
      return Pair.create(true,doUnroll(logger,outerBlock,(int)unrollFactor));
    }
  }
  return NO_UNROLL;
}","@Override public Pair<Boolean,List<Continuation>> tryUnroll(Logger logger,String function,Block outerBlock){
  logger.trace(""String_Node_Str"" + loopName + ""String_Node_Str""+ desiredUnroll);
  boolean expandLoops=isExpandLoopsEnabled();
  boolean fullUnroll=isFullUnrollEnabled();
  if (!this.unrolled && this.desiredUnroll > 1) {
    if (this.loopCounterVar != null) {
      logger.warn(""String_Node_Str"" + ""String_Node_Str"");
      return NO_UNROLL;
    }
    return Pair.create(true,doUnroll(logger,function,outerBlock,desiredUnroll));
  }
 else   if (expandLoops || fullUnroll) {
    long instCount=loopBody.getInstructionCount();
    long iterCount=constIterCount();
    if (expandLoops && iterCount >= 0) {
      if (iterCount <= getUnrollMaxIters(true)) {
        long extraInstructions=instCount * (iterCount - 1);
        if (extraInstructions <= getUnrollMaxExtraInsts(true)) {
          return Pair.create(true,doUnroll(logger,function,outerBlock,(int)iterCount));
        }
      }
    }
    if (!fullUnroll) {
      logger.trace(""String_Node_Str"");
      return NO_UNROLL;
    }
    if (this.unrolled) {
      return NO_UNROLL;
    }
    long threshold=getUnrollMaxExtraInsts(false);
    long unrollFactor=Math.min(getUnrollMaxIters(false),(threshold / instCount) + 1);
    if (unrollFactor > 1) {
      return Pair.create(true,doUnroll(logger,function,outerBlock,(int)unrollFactor));
    }
  }
  return NO_UNROLL;
}",0.984713828652684
132731,"/** 
 * Unroll a loop by splitting into two loops, one short one with original stride, and another with a long stride We transform: range_loop [start:end:step] =======> range_loop [start : unroll_end : big_step] range_loop [remainder_start  : end : step]
 */
private List<Continuation> doUnroll(Logger logger,Block outerBlock,int unrollFactor){
  logger.debug(""String_Node_Str"" + this.loopName + ""String_Node_Str""+ desiredUnroll+ ""String_Node_Str"");
  String vPrefix=Var.VALUEOF_VAR_PREFIX + loopName;
  String bigStepName=outerBlock.uniqueVarName(vPrefix + ""String_Node_Str"");
  VarProvenance prov=VarProvenance.optimizerTmp();
  Var bigIncr=new Var(Types.V_INT,bigStepName,Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var diff=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var diff2=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var extra=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var remainder=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var remainderStart=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var unrollEnd=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  outerBlock.addVariable(bigIncr);
  outerBlock.addVariable(diff);
  outerBlock.addVariable(diff2);
  outerBlock.addVariable(extra);
  outerBlock.addVariable(remainder);
  outerBlock.addVariable(remainderStart);
  outerBlock.addVariable(unrollEnd);
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MULT_INT,bigIncr,Arrays.asList(increment,Arg.createIntLit(unrollFactor))));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MINUS_INT,diff,Arrays.asList(end,start)));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,diff2,Arrays.asList(diff.asArg(),Arg.ONE)));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MOD_INT,remainder,Arrays.asList(diff2.asArg(),bigIncr.asArg())));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MINUS_INT,unrollEnd,Arrays.asList(end,remainder.asArg())));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,remainderStart,Arrays.asList(unrollEnd.asArg(),Arg.ONE)));
  RangeLoop unrolled=this.clone(false);
  this.start=remainderStart.asArg();
  this.unrolled=true;
  unrolled.end=unrollEnd.asArg();
  unrolled.increment=bigIncr.asArg();
  unrolled.unrolled=true;
  unrolled.leafDegree=Math.max(1,unrolled.leafDegree / unrollFactor);
  Block orig=this.loopBody;
  Arg oldIncr=this.increment;
  Var lastIterLoopVar=null;
  for (int i=0; i < unrollFactor; i++) {
    NestedBlock nb=new NestedBlock(orig.clone(BlockType.NESTED_BLOCK,null));
    Block unrolledBody=unrolled.getLoopBody();
    unrolledBody.addContinuation(nb);
    Var currIterLoopVar;
    if (i == 0) {
      currIterLoopVar=loopVar;
    }
 else {
      String newLoopVarName=outerBlock.uniqueVarName(unrolled.loopVar.name() + ""String_Node_Str"" + (i + 1));
      currIterLoopVar=new Var(Types.V_INT,newLoopVarName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.renamed(unrolled.loopVar));
      unrolledBody.addVariable(currIterLoopVar);
      unrolledBody.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,currIterLoopVar,Arrays.asList(Arg.createVar(lastIterLoopVar),oldIncr)));
      nb.renameVars(Collections.singletonMap(unrolled.loopVar,Arg.createVar(currIterLoopVar)),RenameMode.REPLACE_VAR,true);
    }
    lastIterLoopVar=currIterLoopVar;
  }
  return Collections.<Continuation>singletonList(unrolled);
}","/** 
 * Unroll a loop by splitting into two loops, one short one with original stride, and another with a long stride We transform: range_loop [start:end:step] =======> range_loop [start : unroll_end : big_step] range_loop [remainder_start  : end : step]
 */
private List<Continuation> doUnroll(Logger logger,String function,Block outerBlock,int unrollFactor){
  logger.debug(""String_Node_Str"" + this.loopName + ""String_Node_Str""+ desiredUnroll+ ""String_Node_Str"");
  String vPrefix=Var.VALUEOF_VAR_PREFIX + loopName;
  String bigStepName=outerBlock.uniqueVarName(vPrefix + ""String_Node_Str"");
  VarProvenance prov=VarProvenance.optimizerTmp();
  Var bigIncr=new Var(Types.V_INT,bigStepName,Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var diff=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var diff2=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var extra=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var remainder=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var remainderStart=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var unrollEnd=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  outerBlock.addVariable(bigIncr);
  outerBlock.addVariable(diff);
  outerBlock.addVariable(diff2);
  outerBlock.addVariable(extra);
  outerBlock.addVariable(remainder);
  outerBlock.addVariable(remainderStart);
  outerBlock.addVariable(unrollEnd);
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MULT_INT,bigIncr,Arrays.asList(increment,Arg.createIntLit(unrollFactor))));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MINUS_INT,diff,Arrays.asList(end,start)));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,diff2,Arrays.asList(diff.asArg(),Arg.ONE)));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MOD_INT,remainder,Arrays.asList(diff2.asArg(),bigIncr.asArg())));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MINUS_INT,unrollEnd,Arrays.asList(end,remainder.asArg())));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,remainderStart,Arrays.asList(unrollEnd.asArg(),Arg.ONE)));
  RangeLoop unrolled=this.clone(false);
  this.start=remainderStart.asArg();
  this.unrolled=true;
  unrolled.end=unrollEnd.asArg();
  unrolled.increment=bigIncr.asArg();
  unrolled.unrolled=true;
  unrolled.leafDegree=Math.max(1,unrolled.leafDegree / unrollFactor);
  Block orig=this.loopBody;
  Arg oldIncr=this.increment;
  Var lastIterLoopVar=null;
  for (int i=0; i < unrollFactor; i++) {
    NestedBlock nb=new NestedBlock(orig.clone(BlockType.NESTED_BLOCK,null));
    Block unrolledBody=unrolled.getLoopBody();
    unrolledBody.addContinuation(nb);
    Var currIterLoopVar;
    if (i == 0) {
      currIterLoopVar=loopVar;
    }
 else {
      String newLoopVarName=outerBlock.uniqueVarName(unrolled.loopVar.name() + ""String_Node_Str"" + (i + 1));
      currIterLoopVar=new Var(Types.V_INT,newLoopVarName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.renamed(unrolled.loopVar));
      unrolledBody.addVariable(currIterLoopVar);
      unrolledBody.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,currIterLoopVar,Arrays.asList(Arg.createVar(lastIterLoopVar),oldIncr)));
      nb.renameVars(function,Collections.singletonMap(unrolled.loopVar,Arg.createVar(currIterLoopVar)),RenameMode.REPLACE_VAR,true);
    }
    lastIterLoopVar=currIterLoopVar;
  }
  return Collections.<Continuation>singletonList(unrolled);
}",0.996727320329886
132732,"private List<Arg> canonicalizeAssignValue(GlobalConstants consts,ArgCV val){
  if (val.isArrayMemberVal() || val.isArrayMember()) {
    Arg arr=byAlias.findCanonical(val.getInput(0));
    Arg ix=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(arr,ix);
  }
 else   if (val.op == Opcode.GET_FILENAME_VAL) {
    return Arrays.asList(Arg.createStringLit(""String_Node_Str""),byAlias.findCanonical(val.getInput(0)));
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + val + ""String_Node_Str"");
  }
}","private List<Arg> canonicalizeAssignValue(GlobalConstants consts,ArgCV val){
  if (val.isArrayMemberVal() || val.isArrayMember()) {
    Arg arr=byAlias.findCanonical(val.getInput(0));
    Arg ix=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(arr,ix);
  }
 else   if (val.isStructFieldVal() || val.isStructFieldAlias() || val.isStructFieldCopy()|| val.isStructFieldValRef()) {
    Arg struct=byAlias.findCanonical(val.getInput(0));
    Arg fieldName=byValue.findCanonical(val.getInput(1));
    return Arrays.asList(struct,fieldName);
  }
 else   if (val.isFilenameValCV() || val.isFilenameAliasCV() || val.isLocalFilenameCV()) {
    return Arrays.asList(Arg.createStringLit(""String_Node_Str""),byAlias.findCanonical(val.getInput(0)));
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + val + ""String_Node_Str"");
  }
}",0.7302052785923754
132733,"public static ValLoc makeFilenameLocal(Arg outFilename,Var inFile,IsAssign isAssign){
  assert(isAssign != IsAssign.TO_VALUE);
  assert(Types.isFileVal(inFile));
  assert(outFilename.isImmediateString());
  return build(ComputedValue.localFilenameCV(inFile),outFilename,Closed.YES_NOT_RECURSIVE,isAssign);
}","public static ValLoc makeFilenameLocal(Arg outFilename,Var inFile,IsAssign isAssign){
  assert(Types.isFileVal(inFile));
  assert(outFilename.isImmediateString());
  return build(ComputedValue.localFilenameCV(inFile),outFilename,Closed.YES_NOT_RECURSIVE,isAssign);
}",0.9284467713787086
132734,"/** 
 * Add specific CVs for special operations
 * @param res
 */
private void addSpecialCVs(List<ValLoc> cvs){
  if (isImpl(SpecialFunction.INPUT_FILE) || isImpl(SpecialFunction.UNCACHED_INPUT_FILE) || isImpl(SpecialFunction.INPUT_URL)) {
    if (op == Opcode.CALL_FOREIGN) {
      cvs.add(ValLoc.makeFilename(getInput(0),getOutput(0)));
    }
 else     if (op == Opcode.CALL_FOREIGN_LOCAL) {
      cvs.add(ValLoc.makeFilenameLocal(getInput(0),getOutput(0),IsAssign.NO));
    }
  }
 else   if (op == Opcode.CALL_FOREIGN_LOCAL && (isImpl(SpecialFunction.RANGE) || isImpl(SpecialFunction.RANGE_STEP))) {
    addRangeCVs(cvs);
  }
 else   if (isImpl(SpecialFunction.SIZE)) {
    cvs.add(makeContainerSizeCV(IsAssign.NO));
  }
 else   if (isImpl(SpecialFunction.CONTAINS)) {
    cvs.add(makeArrayContainsCV(IsAssign.NO));
  }
}","/** 
 * Add specific CVs for special operations
 * @param res
 */
private void addSpecialCVs(List<ValLoc> cvs){
  if (isImpl(SpecialFunction.INPUT_FILE) || isImpl(SpecialFunction.UNCACHED_INPUT_FILE) || isImpl(SpecialFunction.INPUT_URL)) {
    if (op == Opcode.CALL_FOREIGN) {
      cvs.add(ValLoc.makeFilename(getInput(0),getOutput(0),IsAssign.TO_VALUE));
    }
 else     if (op == Opcode.CALL_FOREIGN_LOCAL) {
      cvs.add(ValLoc.makeFilenameLocal(getInput(0),getOutput(0),IsAssign.TO_VALUE));
    }
  }
 else   if (op == Opcode.CALL_FOREIGN_LOCAL && (isImpl(SpecialFunction.RANGE) || isImpl(SpecialFunction.RANGE_STEP))) {
    addRangeCVs(cvs);
  }
 else   if (isImpl(SpecialFunction.SIZE)) {
    cvs.add(makeContainerSizeCV(IsAssign.NO));
  }
 else   if (isImpl(SpecialFunction.CONTAINS)) {
    cvs.add(makeArrayContainsCV(IsAssign.NO));
  }
}",0.9832535885167464
132735,"/** 
 * Analyse the variables that are present in the block and add VariableUsageInfo object to the BLOCK AST nodes
 * @param function
 * @param oList
 * @param iList
 * @param block
 * @throws UserException
 */
public void analyzeVariableUsage(Context context,LineMapping lineMapping,String function,List<Var> iList,List<Var> oList,SwiftAST block) throws UserException {
  LogHelper.debug(context,""String_Node_Str"" + function);
  this.lineMapping=lineMapping;
  VariableUsageInfo globVui=new VariableUsageInfo();
  for (  Var global : context.getScopeVariables()) {
    globVui.declare(context,global.name(),global.type(),false);
    globVui.assign(context,global.name(),AssignOp.ASSIGN);
  }
  VariableUsageInfo argVui=globVui.createNested();
  Context fnContext=new LocalContext(context,function);
  for (  Var i : iList) {
    argVui.declare(context,i.name(),i.type(),false);
    argVui.assign(context,i.name(),AssignOp.ASSIGN);
    fnContext.declareVariable(i.type(),i.name(),i.storage(),i.defType(),VarProvenance.unknown(),i.mappedDecl());
  }
  for (  Var o : oList) {
    argVui.declare(context,o.name(),o.type(),false);
    argVui.read(context,o.name());
    fnContext.declareVariable(o.type(),o.name(),o.storage(),o.defType(),VarProvenance.unknown(),o.mappedDecl());
  }
  walkBlock(fnContext,block,argVui);
  boolean fatalError=false;
  for (  Violation v : argVui.getViolations()) {
    String msg=v.toException().getMessage();
    if (v.getType() == ViolationType.ERROR) {
      fatalError=true;
      LogHelper.log(0,Level.ERROR,msg);
    }
 else {
      LogHelper.log(0,Level.WARN,msg);
    }
  }
  if (fatalError) {
    throw new VariableUsageException(""String_Node_Str"" + function + ""String_Node_Str"");
  }
  LogHelper.debug(context,""String_Node_Str"" + function);
}","/** 
 * Analyse the variables that are present in the block and add VariableUsageInfo object to the BLOCK AST nodes
 * @param function
 * @param oList
 * @param iList
 * @param block
 * @throws UserException
 */
public void analyzeVariableUsage(Context context,LineMapping lineMapping,String function,List<Var> iList,List<Var> oList,SwiftAST block) throws UserException {
  LogHelper.debug(context,""String_Node_Str"" + function);
  this.lineMapping=lineMapping;
  VariableUsageInfo globVui=new VariableUsageInfo();
  for (  Var global : context.getScopeVariables()) {
    globVui.declare(context,global.name(),global.type(),false);
    globVui.assign(context,global.name(),AssignOp.ASSIGN);
  }
  VariableUsageInfo argVui=globVui.createNested();
  Context fnContext=new LocalContext(context,function);
  for (  Var i : iList) {
    argVui.declare(fnContext,i.name(),i.type(),false);
    argVui.assign(fnContext,i.name(),AssignOp.ASSIGN);
    fnContext.declareVariable(i.type(),i.name(),i.storage(),i.defType(),VarProvenance.unknown(),i.mappedDecl());
  }
  for (  Var o : oList) {
    argVui.declare(fnContext,o.name(),o.type(),false);
    argVui.read(fnContext,o.name());
    fnContext.declareVariable(o.type(),o.name(),o.storage(),o.defType(),VarProvenance.unknown(),o.mappedDecl());
  }
  walkBlock(fnContext,block,argVui);
  boolean fatalError=false;
  for (  Violation v : argVui.getViolations()) {
    String msg=v.toException().getMessage();
    if (v.getType() == ViolationType.ERROR) {
      fatalError=true;
      LogHelper.log(0,Level.ERROR,msg);
    }
 else {
      LogHelper.log(0,Level.WARN,msg);
    }
  }
  if (fatalError) {
    throw new VariableUsageException(""String_Node_Str"" + function + ""String_Node_Str"");
  }
  LogHelper.debug(context,""String_Node_Str"" + function);
}",0.9955207166853304
132736,"private List<Violation> arrayAssign(Context context,int arrayDepth,AssignOp op){
  if (assigned != Ternary.FALSE) {
    if (arrayAssignDepth != arrayDepth) {
      return Arrays.asList(makeArrDepthViolation(context,arrayDepth));
    }
    assigned=Ternary.TRUE;
  }
 else {
    assigned=Ternary.TRUE;
    arrayAssignDepth=arrayDepth;
  }
  return null;
}","private List<Violation> arrayAssign(Context context,int arrayDepth,AssignOp op){
  if (assigned != Ternary.FALSE) {
    if (arrayAssignDepth != arrayDepth) {
      return Arrays.asList(makeArrDepthViolation(context,arrayDepth));
    }
    this.assigned=Ternary.TRUE;
  }
 else {
    this.assigned=Ternary.TRUE;
    arrayAssignDepth=arrayDepth;
  }
  return Collections.emptyList();
}",0.949796472184532
132737,"private List<Violation> structAssign(Context context,List<String> fieldPath,int arrayDepth,AssignOp op){
  VInfo fieldVInfo;
  if (structFields != null) {
    if (this.assigned != Ternary.FALSE) {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + this.name + ""String_Node_Str"",context));
    }
    String field=fieldPath.get(0);
    if (structFields.containsKey(field)) {
      fieldVInfo=structFields.get(field);
    }
 else {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + field + ""String_Node_Str""+ this.name+ ""String_Node_Str""+ ((StructType)type).getStructTypeName(),context));
    }
  }
 else {
    return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + this.name + ""String_Node_Str"",context));
  }
  fieldPath.remove(0);
  return fieldVInfo.assign(context,fieldPath,arrayDepth,op);
}","private List<Violation> structAssign(Context context,List<String> fieldPath,int arrayDepth,AssignOp op){
  VInfo fieldVInfo;
  List<Violation> errs=new ArrayList<Violation>();
  if (structFields != null) {
    if (this.assigned == Ternary.TRUE) {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
 else     if (this.assigned == Ternary.MAYBE) {
      errs.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + fieldPath + ""String_Node_Str""+ this.name+ ""String_Node_Str"",context));
    }
    String field=fieldPath.get(0);
    if (structFields.containsKey(field)) {
      fieldVInfo=structFields.get(field);
    }
 else {
      return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + field + ""String_Node_Str""+ this.name+ ""String_Node_Str""+ ((StructType)type).getStructTypeName(),context));
    }
  }
 else {
    return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + this.name + ""String_Node_Str"",context));
  }
  fieldPath.remove(0);
  errs.addAll(fieldVInfo.assign(context,fieldPath,arrayDepth,op));
  mergeStructFieldWriteInfo(context);
  return errs;
}",0.7921267402784445
132738,"/** 
 * make a copy, except it might be in a different scope All of the usage tracking is reset
 */
public VInfo makeEmptyCopy(boolean locallyDeclared){
  HashMap<String,VInfo> structFieldsNew=null;
  if (structFields != null) {
    structFieldsNew=new HashMap<String,VInfo>();
    for (    Entry<String,VInfo> e : structFields.entrySet()) {
      structFieldsNew.put(e.getKey(),e.getValue().makeEmptyCopy(locallyDeclared));
    }
  }
  return new VInfo(type,hasMapping,structFields,locallyDeclared,name,Ternary.FALSE,Ternary.FALSE,Ternary.FALSE,0,-1);
}","/** 
 * make a copy, except it might be in a different scope All of the usage tracking is reset
 */
public VInfo makeEmptyCopy(boolean locallyDeclared){
  HashMap<String,VInfo> structFieldsNew=null;
  if (structFields != null) {
    structFieldsNew=new HashMap<String,VInfo>();
    for (    Entry<String,VInfo> e : structFields.entrySet()) {
      structFieldsNew.put(e.getKey(),e.getValue().makeEmptyCopy(locallyDeclared));
    }
  }
  return new VInfo(type,hasMapping,structFieldsNew,locallyDeclared,name,Ternary.FALSE,Ternary.FALSE,Ternary.FALSE,0,-1);
}",0.9972997299729972
132739,"/** 
 * Handles the situation where there are nested scopes
 * @param branches list of a set of mutually exclusive nested scopes
 * @param exhaustive are these branches exhaustive
 * @return
 */
public ArrayList<Violation> mergeBranchInfo(Context context,List<VInfo> branches,boolean exhaustive){
  if (branches.size() == 0) {
    throw new STCRuntimeError(""String_Node_Str"");
  }
  mergeBranchReadInfo(branches,exhaustive);
  ArrayList<Violation> errs=mergeBranchWriteInfo(context,branches,exhaustive);
  if (structFields != null) {
    for (    VInfo fieldVI : this.structFields.values()) {
      List<VInfo> vis=new ArrayList<VInfo>(branches.size());
      for (int i=0; i < branches.size(); i++) {
        vis.add(branches.get(i).getFieldVInfo(fieldVI.getName()));
      }
      errs.addAll(fieldVI.mergeBranchInfo(context,branches,exhaustive));
    }
  }
  return errs;
}","/** 
 * Handles the situation where there are nested scopes
 * @param branches list of a set of mutually exclusive nested scopes
 * @param exhaustive are these branches exhaustive
 * @return
 */
public ArrayList<Violation> mergeBranchInfo(Context context,List<VInfo> branches,boolean exhaustive){
  if (branches.size() == 0) {
    throw new STCRuntimeError(""String_Node_Str"");
  }
  mergeBranchReadInfo(branches,exhaustive);
  ArrayList<Violation> errs=mergeBranchWriteInfo(context,branches,exhaustive);
  if (structFields != null) {
    for (    VInfo fieldVI : this.structFields.values()) {
      List<VInfo> vis=new ArrayList<VInfo>(branches.size());
      for (int i=0; i < branches.size(); i++) {
        vis.add(branches.get(i).getFieldVInfo(fieldVI.getName()));
      }
      ArrayList<Violation> mergeErrs=fieldVI.mergeBranchInfo(context,branches,exhaustive);
      errs.addAll(mergeErrs);
    }
  }
  return errs;
}",0.9511111111111112
132740,"private List<Violation> plainAssign(Context context,AssignOp op){
  if (assigned == Ternary.TRUE || (op == AssignOp.ASSIGN && appended == Ternary.TRUE)) {
    return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str"",context));
  }
  ArrayList<Violation> res=new ArrayList<Violation>();
  if (assigned == Ternary.MAYBE || (op == AssignOp.ASSIGN && appended == Ternary.MAYBE)) {
    res.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + ""String_Node_Str"" + name,context));
  }
  if (op == AssignOp.ASSIGN) {
    if (Types.isStruct(type)) {
      for (      VInfo fieldVI : this.structFields.values()) {
        fieldVI.plainAssign(context,op);
      }
    }
    this.assigned=Ternary.TRUE;
  }
 else {
    assert(op == AssignOp.APPEND);
    this.appended=Ternary.TRUE;
  }
  return res;
}","private List<Violation> plainAssign(Context context,AssignOp op){
  if (assigned == Ternary.TRUE || (op == AssignOp.ASSIGN && appended == Ternary.TRUE)) {
    return Arrays.asList(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str"",context));
  }
  ArrayList<Violation> res=new ArrayList<Violation>();
  if (assigned == Ternary.MAYBE || (op == AssignOp.ASSIGN && appended == Ternary.MAYBE)) {
    res.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + ""String_Node_Str"" + name,context));
  }
  if (op == AssignOp.ASSIGN) {
    if (Types.isStruct(type)) {
      for (      VInfo fieldVI : this.structFields.values()) {
        fieldVI.plainAssign(context,op);
      }
    }
    this.assigned=Ternary.TRUE;
  }
 else {
    assert(op == AssignOp.APPEND);
    this.assigned=Ternary.TRUE;
  }
  return res;
}",0.9940405244338498
132741,"private VInfo(Type type,boolean mapped,boolean locallyDeclared,String name,Ternary assigned,Ternary read,Ternary appended,int arrayAssignDepth,int maxReadDepth){
  this.type=type;
  this.hasMapping=mapped;
  if (Types.isStruct(type)) {
    structFields=new HashMap<String,VInfo>();
    for (    StructField f : ((StructType)type).getFields()) {
      structFields.put(f.getName(),new VInfo(f.getType(),mapped,name + ""String_Node_Str"" + f.getName(),locallyDeclared));
    }
  }
 else {
    structFields=null;
  }
  this.declaredInCurrentScope=locallyDeclared;
  this.name=name;
  this.assigned=assigned;
  this.appended=appended;
  this.read=read;
  this.arrayAssignDepth=arrayAssignDepth;
  this.maxReadDepth=maxReadDepth;
}","private VInfo(Type type,boolean mapped,boolean locallyDeclared,String name,Ternary assigned,Ternary read,Ternary appended,int arrayAssignDepth,int maxReadDepth){
  this.type=type;
  this.hasMapping=mapped;
  if (Types.isStruct(type)) {
    structFields=new HashMap<String,VInfo>();
    for (    StructField f : ((StructType)type.getImplType()).getFields()) {
      VInfo fieldVInfo=new VInfo(f.getType(),mapped,name + ""String_Node_Str"" + f.getName(),locallyDeclared);
      structFields.put(f.getName(),fieldVInfo);
    }
  }
 else {
    structFields=null;
  }
  this.declaredInCurrentScope=locallyDeclared;
  this.name=name;
  this.assigned=assigned;
  this.appended=appended;
  this.read=read;
  this.arrayAssignDepth=arrayAssignDepth;
  this.maxReadDepth=maxReadDepth;
}",0.9231796927187708
132742,"/** 
 * Merge in information about writes to this variable that can occur in nested contexts
 * @param file
 * @param line
 * @param branches
 * @param exhaustive
 * @return
 */
private ArrayList<Violation> mergeBranchWriteInfo(Context context,List<VInfo> branches,boolean exhaustive){
  ArrayList<Violation> result=new ArrayList<Violation>();
  Ternary assignedInBranch;
  Ternary appendedInBranch;
  int expectAssignedDepth;
  if (exhaustive) {
    VInfo firstBranch=branches.get(0);
    assignedInBranch=firstBranch.assigned;
    appendedInBranch=firstBranch.appended;
    expectAssignedDepth=firstBranch.arrayAssignDepth;
  }
 else {
    assignedInBranch=Ternary.FALSE;
    appendedInBranch=Ternary.FALSE;
    expectAssignedDepth=0;
  }
  for (int i=0; i < branches.size(); i++) {
    VInfo currBr=branches.get(i);
    if (assignedInBranch == Ternary.FALSE) {
      expectAssignedDepth=currBr.arrayAssignDepth;
    }
    if (assignedInBranch != Ternary.FALSE && currBr.assigned != Ternary.FALSE) {
      if (currBr.arrayAssignDepth != expectAssignedDepth) {
        result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str""+ ""String_Node_Str"",context));
      }
    }
    assignedInBranch=Ternary.consensus(assignedInBranch,currBr.assigned);
    appendedInBranch=Ternary.consensus(appendedInBranch,currBr.appended);
  }
  this.appended=Ternary.or(appended,appendedInBranch);
  if (assignedInBranch == Ternary.FALSE) {
    return result;
  }
 else   if (this.assigned == Ternary.FALSE) {
    this.assigned=assignedInBranch;
    this.arrayAssignDepth=expectAssignedDepth;
    return result;
  }
 else   if (Types.isArray(this.type)) {
    this.assigned=Ternary.or(assigned,assignedInBranch);
    if (this.arrayAssignDepth != expectAssignedDepth) {
      result.add(makeArrDepthViolation(context,expectAssignedDepth));
    }
    return result;
  }
 else {
    if (Ternary.and(assignedInBranch,this.assigned) == Ternary.TRUE) {
      System.err.println(""String_Node_Str"" + name);
      new Exception().printStackTrace();
      result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str"",context));
      return result;
    }
    if (assignedInBranch == Ternary.MAYBE) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    if ((this.assigned != Ternary.FALSE && assignedInBranch == Ternary.MAYBE) || (assignedInBranch != Ternary.FALSE && this.assigned == Ternary.MAYBE)) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    this.assigned=Ternary.or(this.assigned,assignedInBranch);
    return result;
  }
}","/** 
 * Merge in information about writes to this variable that can occur in nested contexts
 * @param file
 * @param line
 * @param branches
 * @param exhaustive
 * @return
 */
private ArrayList<Violation> mergeBranchWriteInfo(Context context,List<VInfo> branches,boolean exhaustive){
  ArrayList<Violation> result=new ArrayList<Violation>();
  Ternary assignedInBranch;
  Ternary appendedInBranch;
  int expectAssignedDepth;
  if (exhaustive) {
    VInfo firstBranch=branches.get(0);
    assignedInBranch=firstBranch.assigned;
    appendedInBranch=firstBranch.appended;
    expectAssignedDepth=firstBranch.arrayAssignDepth;
  }
 else {
    assignedInBranch=Ternary.FALSE;
    appendedInBranch=Ternary.FALSE;
    expectAssignedDepth=0;
  }
  for (int i=0; i < branches.size(); i++) {
    VInfo currBr=branches.get(i);
    if (assignedInBranch == Ternary.FALSE) {
      expectAssignedDepth=currBr.arrayAssignDepth;
    }
    if (assignedInBranch != Ternary.FALSE && currBr.assigned != Ternary.FALSE) {
      if (currBr.arrayAssignDepth != expectAssignedDepth) {
        result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str""+ ""String_Node_Str"",context));
      }
    }
    assignedInBranch=Ternary.consensus(assignedInBranch,currBr.assigned);
    appendedInBranch=Ternary.consensus(appendedInBranch,currBr.appended);
  }
  this.appended=Ternary.or(appended,appendedInBranch);
  if (assignedInBranch == Ternary.FALSE) {
    return result;
  }
 else   if (this.assigned == Ternary.FALSE) {
    this.assigned=assignedInBranch;
    this.arrayAssignDepth=expectAssignedDepth;
    return result;
  }
 else   if (Types.isArray(this.type)) {
    this.assigned=Ternary.or(assigned,assignedInBranch);
    if (this.arrayAssignDepth != expectAssignedDepth) {
      result.add(makeArrDepthViolation(context,expectAssignedDepth));
    }
    return result;
  }
 else {
    if (Ternary.and(assignedInBranch,this.assigned) == Ternary.TRUE) {
      result.add(new Violation(ViolationType.ERROR,""String_Node_Str"" + name + ""String_Node_Str"",context));
      return result;
    }
    if (assignedInBranch == Ternary.MAYBE) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    if ((this.assigned != Ternary.FALSE && assignedInBranch == Ternary.MAYBE) || (assignedInBranch != Ternary.FALSE && this.assigned == Ternary.MAYBE)) {
      result.add(new Violation(ViolationType.WARNING,""String_Node_Str"" + name + ""String_Node_Str"",context));
    }
    this.assigned=Ternary.or(this.assigned,assignedInBranch);
    return result;
  }
}",0.9824891734136696
132743,"/** 
 * Called when any assignment occurs (i.e x = ...). These assignments can take the form: x(.<struct field>)*([<array_index>]) e.g. myVar = ... myVar.x.y = ... myVar[0] = ... myVar.x.y.z[0][1][2] = .. but not: myVar[0].x
 * @param fieldPath the path of struct fields (can be null for no path)
 * @param arrayDepth the number of array indices at end of assignment
 * @param op 
 */
public List<Violation> assign(Context context,List<String> fieldPath,int arrayDepth,AssignOp op){
  if (fieldPath != null && fieldPath.size() > 0) {
    return structAssign(context,fieldPath,arrayDepth,op);
  }
 else   if (arrayDepth > 0) {
    return arrayAssign(context,arrayDepth,op);
  }
 else {
    assert(arrayDepth == 0);
    assert(fieldPath == null || fieldPath.size() == 0);
    return plainAssign(context,op);
  }
}","/** 
 * Called when any assignment occurs (i.e x = ...). These assignments can take the form: x(.<struct field>)*([<array_index>]) e.g. myVar = ... myVar.x.y = ... myVar[0] = ... myVar.x.y.z[0][1][2] = .. but not: myVar[0].x
 * @param fieldPath the path of struct fields (can be null for no path)
 * @param arrayDepth the number of array indices at end of assignment
 * @param op 
 * @return list of violations, should not be null
 */
public List<Violation> assign(Context context,List<String> fieldPath,int arrayDepth,AssignOp op){
  Logger logger=Logging.getSTCLogger();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + this.name + ""String_Node_Str""+ fieldPath);
  }
  List<Violation> violations;
  if (fieldPath != null && fieldPath.size() > 0) {
    violations=structAssign(context,fieldPath,arrayDepth,op);
  }
 else   if (arrayDepth > 0) {
    violations=arrayAssign(context,arrayDepth,op);
  }
 else {
    assert(arrayDepth == 0);
    assert(fieldPath == null || fieldPath.size() == 0);
    violations=plainAssign(context,op);
  }
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + violations);
  }
  return violations;
}",0.7983830217281456
132744,"@Override public void retrieveRef(Var dst,Var src,Arg acquireRead,Arg acquireWrite,Arg decr){
  assert(Types.isRef(src.type()));
  assert(acquireRead.isIntVal());
  assert(acquireWrite.isIntVal());
  if (acquireWrite.isVar() || acquireWrite.getIntLit() > 0) {
    assert(Types.isAssignableRefTo(src.type(),dst.type(),true));
  }
 else {
    assert(Types.isAssignableRefTo(src.type(),dst.type()));
  }
  assert(decr.isImmediateInt());
  Expression acquireReadExpr=argToExpr(acquireRead);
  Expression acquireWriteExpr=argToExpr(acquireWrite);
  TypeName refType=refRepresentationType(dst.type());
  TclTree deref;
  if (acquireWriteExpr.equals(Arg.ZERO)) {
    deref=Turbine.readRefGet(prefixVar(dst),varToExpr(src),refType,acquireReadExpr,argToExpr(decr));
  }
 else {
    deref=Turbine.readWriteRefGet(prefixVar(dst),varToExpr(src),refType,acquireReadExpr,acquireWriteExpr,argToExpr(decr));
  }
  pointAdd(deref);
}","@Override public void retrieveRef(Var dst,Var src,Arg acquireRead,Arg acquireWrite,Arg decr){
  assert(Types.isRef(src.type()));
  assert(acquireRead.isIntVal());
  assert(acquireWrite.isIntVal());
  if (acquireWrite.isVar() || acquireWrite.getIntLit() > 0) {
    assert(Types.isAssignableRefTo(src.type(),dst.type(),true));
  }
 else {
    assert(Types.isAssignableRefTo(src.type(),dst.type()));
  }
  assert(decr.isImmediateInt());
  Expression acquireReadExpr=argToExpr(acquireRead);
  Expression acquireWriteExpr=argToExpr(acquireWrite);
  TypeName refType=refRepresentationType(dst.type());
  TclTree deref;
  if (acquireWrite.equals(Arg.ZERO)) {
    deref=Turbine.readRefGet(prefixVar(dst),varToExpr(src),refType,acquireReadExpr,argToExpr(decr));
  }
 else {
    deref=Turbine.readWriteRefGet(prefixVar(dst),varToExpr(src),refType,acquireReadExpr,acquireWriteExpr,argToExpr(decr));
  }
  pointAdd(deref);
}",0.9978118161925602
132745,"private void startForeachInner(Value arrayContents,Var memberVar,Var loopCountVar){
  Sequence curr=point();
  boolean haveKeys=loopCountVar != null;
  Sequence loopBody=new Sequence();
  String tclMemberVar=prefixVar(memberVar);
  String tclCountVar=haveKeys ? prefixVar(loopCountVar) : null;
  Sequence tclLoop;
  if (haveKeys) {
    tclLoop=new DictFor(new Token(tclCountVar),new Token(tclMemberVar),arrayContents,loopBody);
  }
 else {
    tclLoop=new ForEach(new Token(tclMemberVar),arrayContents,loopBody);
  }
  curr.add(tclLoop);
  pointPush(loopBody);
}","private void startForeachInner(Value arrayContents,Var memberVar,Var loopCountVar,boolean isDict){
  Sequence curr=point();
  Sequence loopBody=new Sequence();
  String tclMemberVar=prefixVar(memberVar);
  String tclCountVar=(loopCountVar != null) ? prefixVar(loopCountVar) : TCLTMP_IGNORE;
  Sequence tclLoop;
  if (isDict) {
    tclLoop=new DictFor(new Token(tclCountVar),new Token(tclMemberVar),arrayContents,loopBody);
  }
 else {
    tclLoop=new ForEach(new Token(tclMemberVar),arrayContents,loopBody);
  }
  curr.add(tclLoop);
  pointPush(loopBody);
}",0.2394995531724754
132746,"@Override public void startForeachLoop(String loopName,Var container,Var memberVar,Var loopCountVar,int splitDegree,int leafDegree,boolean arrayClosed,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  boolean haveKeys=loopCountVar != null;
  if (Types.isArray(container) || Types.isArrayLocal(container)) {
    assert(!haveKeys || Types.isArrayKeyVal(container,loopCountVar.asArg()));
  }
 else {
    assert(Types.isBag(container) || Types.isBagLocal(container));
    assert(!haveKeys);
  }
  boolean localContainer=Types.isContainerLocal(container);
  if (localContainer) {
    if (!arrayClosed || splitDegree >= 0) {
      throw new STCRuntimeError(""String_Node_Str"");
    }
    assert(Types.isElemType(container,memberVar));
  }
 else {
    assert(Types.isElemValType(container,memberVar));
  }
  if (!arrayClosed) {
    throw new STCRuntimeError(""String_Node_Str"");
  }
  Value tclContainer;
  if (splitDegree <= 0) {
    if (localContainer) {
      tclContainer=varToExpr(container);
    }
 else {
      tclContainer=new Value(TCLTMP_ARRAY_CONTENTS);
      pointAdd(Turbine.enumerateAll(tclContainer.variable(),varToExpr(container),haveKeys));
    }
    Expression containerSize;
    if (haveKeys) {
      containerSize=Turbine.dictSize(tclContainer);
    }
 else {
      containerSize=Turbine.listLength(tclContainer);
    }
    handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  }
 else {
    tclContainer=new Value(TCLTMP_ARRAY_CONTENTS);
    startForeachSplit(loopName,container,tclContainer.variable(),splitDegree,leafDegree,haveKeys,passedVars,perIterIncrs,constIncrs);
  }
  startForeachInner(tclContainer,memberVar,loopCountVar);
}","@Override public void startForeachLoop(String loopName,Var container,Var memberVar,Var loopCountVar,int splitDegree,int leafDegree,boolean arrayClosed,List<PassedVar> passedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  boolean haveKeys=loopCountVar != null;
  boolean isKVContainer;
  if (Types.isArray(container) || Types.isArrayLocal(container)) {
    assert(!haveKeys || Types.isArrayKeyVal(container,loopCountVar.asArg()));
    isKVContainer=true;
  }
 else {
    assert(Types.isBag(container) || Types.isBagLocal(container));
    assert(!haveKeys);
    isKVContainer=false;
  }
  boolean localContainer=Types.isContainerLocal(container);
  if (localContainer) {
    if (!arrayClosed || splitDegree >= 0) {
      throw new STCRuntimeError(""String_Node_Str"");
    }
    assert(Types.isElemType(container,memberVar));
  }
 else {
    assert(Types.isElemValType(container,memberVar));
  }
  if (!arrayClosed) {
    throw new STCRuntimeError(""String_Node_Str"");
  }
  boolean isDict;
  Value tclContainer;
  if (splitDegree <= 0) {
    if (localContainer) {
      tclContainer=varToExpr(container);
      isDict=isKVContainer;
    }
 else {
      tclContainer=new Value(TCLTMP_ARRAY_CONTENTS);
      pointAdd(Turbine.enumerateAll(tclContainer.variable(),varToExpr(container),haveKeys));
      isDict=haveKeys;
    }
    Expression containerSize;
    if (isDict) {
      containerSize=Turbine.dictSize(tclContainer);
    }
 else {
      containerSize=Turbine.listLength(tclContainer);
    }
    handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  }
 else {
    assert(!localContainer);
    tclContainer=new Value(TCLTMP_ARRAY_CONTENTS);
    startForeachSplit(loopName,container,tclContainer.variable(),splitDegree,leafDegree,haveKeys,passedVars,perIterIncrs,constIncrs);
    isDict=haveKeys;
  }
  startForeachInner(tclContainer,memberVar,loopCountVar,isDict);
}",0.9408512990602544
132747,"@Override public List<ValLoc> getResults(){
switch (op) {
case LOAD_SCALAR:
case LOAD_REF:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_STRUCT:
case LOAD_RECURSIVE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      if (op == Opcode.LOAD_REF) {
        return ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO,IsAssign.NO).asList();
      }
 else {
        boolean recursive=op == Opcode.LOAD_RECURSIVE;
        return ValLoc.retrieve(dst,src.getVar(),recursive,Closed.MAYBE_NOT,IsValCopy.NO,IsAssign.NO).asList();
      }
    }
case STORE_REF:
case STORE_SCALAR:
case STORE_FILE:
case STORE_ARRAY:
case STORE_BAG:
case STORE_STRUCT:
case STORE_RECURSIVE:
{
    Var dst=getOutput(0);
    Arg src=getInput(0);
    boolean recursive=(op == Opcode.STORE_RECURSIVE);
    ValLoc assign=ValLoc.assign(dst,src,recursive,recursive ? Closed.YES_RECURSIVE : Closed.YES_NOT_RECURSIVE,IsValCopy.NO,IsAssign.NO);
    if (op == Opcode.STORE_REF) {
      ValLoc retrieve=ValLoc.derefCompVal(src.getVar(),dst,IsValCopy.NO,IsAssign.NO);
      return Arrays.asList(retrieve,assign);
    }
 else {
      return assign.asList();
    }
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result,IsAssign.NO));
  }
}
case GET_FILENAME_ALIAS:
{
Arg filename=getOutput(0).asArg();
Var file=getInput(0).getVar();
return ValLoc.makeFilename(filename,file).asList();
}
case COPY_IN_FILENAME:
{
Arg filename=getInput(0);
Var file=getOutput(0);
return ValLoc.makeFilename(filename,file).asList();
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar(),IsAssign.TO_LOCATION).asList();
}
case SET_FILENAME_VAL:
{
Var file=getOutput(0);
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val,IsAssign.TO_VALUE).asList();
}
case GET_FILENAME_VAL:
{
Var file=getInput(0).getVar();
Var val=getOutput(0);
return ValLoc.makeFilenameVal(file,val.asArg(),IsAssign.NO).asList();
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES,IsAssign.NO).asList();
}
case STRUCT_CREATE_ALIAS:
case STRUCT_COPY_OUT:
case STRUCTREF_COPY_OUT:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
ValLoc copyV=ValLoc.makeStructFieldCopyResult(getOutput(0),struct,fields);
if (op == Opcode.STRUCT_CREATE_ALIAS) {
ValLoc aliasV=ValLoc.makeStructFieldAliasResult(getOutput(0),struct,fields);
return Arrays.asList(aliasV,copyV);
}
 else {
return copyV.asList();
}
}
case STRUCT_RETRIEVE_SUB:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(2);
return ValLoc.makeStructFieldValResult(getOutput(0).asArg(),struct,fields).asList();
}
case STRUCT_INIT_FIELDS:
{
List<ValLoc> results=new ArrayList<ValLoc>();
Out<List<List<Arg>>> fieldPaths=new Out<List<List<Arg>>>();
Out<List<Arg>> fieldVals=new Out<List<Arg>>();
unpackStructInitArgs(null,fieldPaths,fieldVals);
Var struct=getOutput(0);
assert(fieldPaths.val.size() == fieldVals.val.size());
for (int i=0; i < fieldPaths.val.size(); i++) {
results.add(ValLoc.makeStructFieldValResult(fieldVals.val.get(i),struct,fieldPaths.val.get(i)));
}
return results;
}
case STRUCT_STORE_SUB:
case STRUCTREF_STORE_SUB:
{
Var struct=getOutput(0);
Arg val=getInput(0);
List<Arg> fields;
if (op == Opcode.STRUCT_STORE_SUB) {
fields=getInputsTail(1);
}
 else {
assert(op == Opcode.STRUCTREF_STORE_SUB);
fields=getInputsTail(1);
}
return ValLoc.makeStructFieldValResult(val,struct,fields).asList();
}
case STRUCT_COPY_IN:
case STRUCTREF_COPY_IN:
{
Var struct=getOutput(0);
Var val=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldCopyResult(val,struct,fields).asList();
}
case ARR_STORE:
case ARR_STORE_FUTURE:
case AREF_STORE_IMM:
case AREF_STORE_FUTURE:
case ARR_COPY_IN_IMM:
case ARR_COPY_IN_FUTURE:
case AREF_COPY_IN_IMM:
case AREF_COPY_IN_FUTURE:
{
Var arr;
arr=getOutput(0);
Arg ix=getInput(0);
Arg member=getInput(1);
boolean insertingVal=isArrayValStore(op);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingVal,IsAssign.TO_VALUE));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE,IsAssign.NO));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Arg val=getInput(2 * i + 1);
res.add(ValLoc.makeArrayResult(arr,key,val,true,IsAssign.TO_VALUE));
}
res.add(ValLoc.makeContainerSizeCV(arr,Arg.createIntLit(elemCount),false,IsAssign.NO));
return res;
}
case ARR_CREATE_ALIAS:
case ARR_RETRIEVE:
case ARR_COPY_OUT_IMM:
case ARR_COPY_OUT_FUTURE:
case AREF_COPY_OUT_FUTURE:
case AREF_COPY_OUT_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARR_RETRIEVE) {
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),true,IsAssign.NO).asList();
}
 else if (op == Opcode.ARR_CREATE_ALIAS) {
ValLoc memVal=ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.NO);
ValLoc memAlias=ValLoc.makeArrayAlias(arr,ix,contents.asArg(),IsAssign.NO);
return Arrays.asList(memVal,memAlias);
}
 else {
assert(Types.isElemType(arr,contents));
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.TO_LOCATION).asList();
}
}
case ARR_CREATE_NESTED_FUTURE:
case ARR_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case ARR_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr=getOutput(1);
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsNonRef=op == Opcode.ARR_CREATE_NESTED_IMM || op == Opcode.ARR_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr.asArg(),returnsNonRef,IsAssign.NO));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsNonRef));
return res;
}
case SYNC_COPY:
case ASYNC_COPY:
{
return ValLoc.makeCopy(getOutput(0),getInput(0),IsAssign.TO_LOCATION).asList();
}
case COPY_REF:
{
Var srcRef=getInput(0).getVar();
return ValLoc.makeAlias(getOutput(0),srcRef).asList();
}
case LOOKUP_CHECKPOINT:
case UNPACK_VALUES:
{
List<ValLoc> res=new ArrayList<ValLoc>(outputs.size());
for (int i=0; i < outputs.size(); i++) {
Var out=outputs.get(i);
res.add(ValLoc.buildResult(op,(Object)i,getInput(0).asList(),out.asArg(),Closed.YES_RECURSIVE,IsValCopy.NO,IsAssign.NO));
}
return res;
}
case PACK_VALUES:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case CHECKPOINT_LOOKUP_ENABLED:
case CHECKPOINT_WRITE_ENABLED:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case UNPACK_ARRAY_TO_FLAT:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case ARR_CONTAINS:
case CONTAINER_SIZE:
case ARR_LOCAL_CONTAINS:
case CONTAINER_LOCAL_SIZE:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case STRUCT_LOCAL_BUILD:
return null;
default :
return null;
}
}","@Override public List<ValLoc> getResults(){
switch (op) {
case LOAD_SCALAR:
case LOAD_REF:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_STRUCT:
case LOAD_RECURSIVE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      if (op == Opcode.LOAD_REF) {
        return ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO,IsAssign.NO).asList();
      }
 else {
        boolean recursive=op == Opcode.LOAD_RECURSIVE;
        return ValLoc.retrieve(dst,src.getVar(),recursive,Closed.MAYBE_NOT,IsValCopy.NO,IsAssign.NO).asList();
      }
    }
case STORE_REF:
case STORE_SCALAR:
case STORE_FILE:
case STORE_ARRAY:
case STORE_BAG:
case STORE_STRUCT:
case STORE_RECURSIVE:
{
    Var dst=getOutput(0);
    Arg src=getInput(0);
    boolean recursive=(op == Opcode.STORE_RECURSIVE);
    ValLoc assign;
    if (op == Opcode.STORE_FILE) {
      assign=ValLoc.assignFile(dst,src,getInput(1),IsAssign.NO);
    }
 else {
      assign=ValLoc.assign(dst,src,recursive,recursive ? Closed.YES_RECURSIVE : Closed.YES_NOT_RECURSIVE,IsValCopy.NO,IsAssign.NO);
    }
    if (op == Opcode.STORE_REF) {
      ValLoc retrieve=ValLoc.derefCompVal(src.getVar(),dst,IsValCopy.NO,IsAssign.NO);
      return Arrays.asList(retrieve,assign);
    }
 else {
      return assign.asList();
    }
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result,IsAssign.NO));
  }
}
case GET_FILENAME_ALIAS:
{
Arg filename=getOutput(0).asArg();
Var file=getInput(0).getVar();
return ValLoc.makeFilename(filename,file).asList();
}
case COPY_IN_FILENAME:
{
Arg filename=getInput(0);
Var file=getOutput(0);
return ValLoc.makeFilename(filename,file).asList();
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar(),IsAssign.TO_LOCATION).asList();
}
case SET_FILENAME_VAL:
{
Var file=getOutput(0);
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val,IsAssign.TO_VALUE).asList();
}
case GET_FILENAME_VAL:
{
Var file=getInput(0).getVar();
Var val=getOutput(0);
return ValLoc.makeFilenameVal(file,val.asArg(),IsAssign.NO).asList();
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES,IsAssign.NO).asList();
}
case STRUCT_CREATE_ALIAS:
case STRUCT_COPY_OUT:
case STRUCTREF_COPY_OUT:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
ValLoc copyV=ValLoc.makeStructFieldCopyResult(getOutput(0),struct,fields);
if (op == Opcode.STRUCT_CREATE_ALIAS) {
ValLoc aliasV=ValLoc.makeStructFieldAliasResult(getOutput(0),struct,fields);
return Arrays.asList(aliasV,copyV);
}
 else {
return copyV.asList();
}
}
case STRUCT_RETRIEVE_SUB:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(2);
return ValLoc.makeStructFieldValResult(getOutput(0).asArg(),struct,fields).asList();
}
case STRUCT_INIT_FIELDS:
{
List<ValLoc> results=new ArrayList<ValLoc>();
Out<List<List<Arg>>> fieldPaths=new Out<List<List<Arg>>>();
Out<List<Arg>> fieldVals=new Out<List<Arg>>();
unpackStructInitArgs(null,fieldPaths,fieldVals);
Var struct=getOutput(0);
assert(fieldPaths.val.size() == fieldVals.val.size());
for (int i=0; i < fieldPaths.val.size(); i++) {
results.add(ValLoc.makeStructFieldValResult(fieldVals.val.get(i),struct,fieldPaths.val.get(i)));
}
return results;
}
case STRUCT_STORE_SUB:
case STRUCTREF_STORE_SUB:
{
Var struct=getOutput(0);
Arg val=getInput(0);
List<Arg> fields;
if (op == Opcode.STRUCT_STORE_SUB) {
fields=getInputsTail(1);
}
 else {
assert(op == Opcode.STRUCTREF_STORE_SUB);
fields=getInputsTail(1);
}
return ValLoc.makeStructFieldValResult(val,struct,fields).asList();
}
case STRUCT_COPY_IN:
case STRUCTREF_COPY_IN:
{
Var struct=getOutput(0);
Var val=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldCopyResult(val,struct,fields).asList();
}
case ARR_STORE:
case ARR_STORE_FUTURE:
case AREF_STORE_IMM:
case AREF_STORE_FUTURE:
case ARR_COPY_IN_IMM:
case ARR_COPY_IN_FUTURE:
case AREF_COPY_IN_IMM:
case AREF_COPY_IN_FUTURE:
{
Var arr;
arr=getOutput(0);
Arg ix=getInput(0);
Arg member=getInput(1);
boolean insertingVal=isArrayValStore(op);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingVal,IsAssign.TO_VALUE));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE,IsAssign.NO));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Arg val=getInput(2 * i + 1);
res.add(ValLoc.makeArrayResult(arr,key,val,true,IsAssign.TO_VALUE));
}
res.add(ValLoc.makeContainerSizeCV(arr,Arg.createIntLit(elemCount),false,IsAssign.NO));
return res;
}
case ARR_CREATE_ALIAS:
case ARR_RETRIEVE:
case ARR_COPY_OUT_IMM:
case ARR_COPY_OUT_FUTURE:
case AREF_COPY_OUT_FUTURE:
case AREF_COPY_OUT_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARR_RETRIEVE) {
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),true,IsAssign.NO).asList();
}
 else if (op == Opcode.ARR_CREATE_ALIAS) {
ValLoc memVal=ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.NO);
ValLoc memAlias=ValLoc.makeArrayAlias(arr,ix,contents.asArg(),IsAssign.NO);
return Arrays.asList(memVal,memAlias);
}
 else {
assert(Types.isElemType(arr,contents));
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.TO_LOCATION).asList();
}
}
case ARR_CREATE_NESTED_FUTURE:
case ARR_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case ARR_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr=getOutput(1);
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsNonRef=op == Opcode.ARR_CREATE_NESTED_IMM || op == Opcode.ARR_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr.asArg(),returnsNonRef,IsAssign.NO));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsNonRef));
return res;
}
case SYNC_COPY:
case ASYNC_COPY:
{
return ValLoc.makeCopy(getOutput(0),getInput(0),IsAssign.TO_LOCATION).asList();
}
case COPY_REF:
{
Var srcRef=getInput(0).getVar();
return ValLoc.makeAlias(getOutput(0),srcRef).asList();
}
case LOOKUP_CHECKPOINT:
case UNPACK_VALUES:
{
List<ValLoc> res=new ArrayList<ValLoc>(outputs.size());
for (int i=0; i < outputs.size(); i++) {
Var out=outputs.get(i);
res.add(ValLoc.buildResult(op,(Object)i,getInput(0).asList(),out.asArg(),Closed.YES_RECURSIVE,IsValCopy.NO,IsAssign.NO));
}
return res;
}
case PACK_VALUES:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case CHECKPOINT_LOOKUP_ENABLED:
case CHECKPOINT_WRITE_ENABLED:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case UNPACK_ARRAY_TO_FLAT:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case ARR_CONTAINS:
case CONTAINER_SIZE:
case ARR_LOCAL_CONTAINS:
case CONTAINER_LOCAL_SIZE:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case STRUCT_LOCAL_BUILD:
return null;
default :
return null;
}
}",0.9907777013076392
132748,"private void updateInfo(Logger logger,Block block,ArrayInfo info,Instruction inst,Set<Var> candidates){
  if (inst.op == Opcode.ARR_STORE) {
    Var arr=inst.getOutput(0);
    if (candidates.contains(arr)) {
      BlockVarInfo entry=info.getEntry(block,arr);
      entry.insertImmHere=true;
    }
  }
 else {
    for (    Var out : inst.getOutputs()) {
      if (isValidCandidate(out,false) && !initsAlias(inst,out)) {
        BlockVarInfo entry=info.getEntry(block,out);
        entry.otherModHere=true;
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ inst);
        }
      }
    }
  }
}","private void updateInfo(Logger logger,Map<String,Function> funcMap,Block block,ArrayInfo info,Instruction inst,Set<Var> candidates){
  if (inst.op == Opcode.ARR_STORE) {
    Var arr=inst.getOutput(0);
    if (candidates.contains(arr)) {
      BlockVarInfo entry=info.getEntry(block,arr);
      entry.insertImmHere=true;
    }
  }
 else {
    for (    Var out : inst.getOutputs()) {
      if (isValidCandidate(out,false) && !initsAlias(inst,out)) {
        BlockVarInfo entry=info.getEntry(block,out);
        entry.otherModHere=true;
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ inst);
        }
      }
    }
    Pair<List<VarCount>,List<VarCount>> inRC=inst.inRefCounts(funcMap);
    List<VarCount> inWriteRC=inRC.val2;
    for (    VarCount written : inWriteRC) {
      if (written.count != 0 && isValidCandidate(written.var,false)) {
        BlockVarInfo entry=info.getEntry(block,written.var);
        entry.otherModHere=true;
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + written + ""String_Node_Str""+ inst);
        }
      }
    }
  }
}",0.7244270542202348
132749,"/** 
 * Traverse and build up info about how array is modified in all blocks of function
 * @param logger
 * @param f
 * @return
 */
private ArrayInfo buildInfo(Logger logger,Function f){
  HierarchicalSet<Var> candidates=new HierarchicalSet<Var>();
  ArrayInfo info=new ArrayInfo();
  buildAliasInfoRec(logger,f,f.mainBlock(),info,new AliasTracker());
  buildInfoRec(logger,f,f.mainBlock(),info,candidates);
  return info;
}","/** 
 * Traverse and build up info about how array is modified in all blocks of function
 * @param logger
 * @param f
 * @param funcMap 
 * @return
 */
private ArrayInfo buildInfo(Logger logger,Map<String,Function> funcMap,Function f){
  HierarchicalSet<Var> candidates=new HierarchicalSet<Var>();
  ArrayInfo info=new ArrayInfo();
  buildAliasInfoRec(logger,f,f.mainBlock(),info,new AliasTracker());
  buildInfoRec(logger,funcMap,f,f.mainBlock(),info,candidates);
  return info;
}",0.9381898454746136
132750,"private void buildInfoRec(Logger logger,Function f,Block block,ArrayInfo info,HierarchicalSet<Var> candidates){
  addBlockCandidates(f,block,info,candidates);
  for (  Statement stmt : block.getStatements()) {
switch (stmt.type()) {
case INSTRUCTION:
      updateInfo(logger,block,info,stmt.instruction(),candidates);
    break;
default :
  break;
}
}
for (Continuation c : block.allComplexStatements()) {
for (Block inner : c.getBlocks()) {
buildInfoRec(logger,f,inner,info,candidates.makeChild());
}
}
updateInfoBottomUp(logger,block,info,candidates);
if (logger.isTraceEnabled()) {
logger.trace(""String_Node_Str"" + System.identityHashCode(block) + ""String_Node_Str""+ block.getType());
for (Var candidate : candidates) {
logger.trace(candidate + ""String_Node_Str"" + info.getEntry(block,candidate));
}
}
}","private void buildInfoRec(Logger logger,Map<String,Function> funcMap,Function f,Block block,ArrayInfo info,HierarchicalSet<Var> candidates){
  addBlockCandidates(f,block,info,candidates);
  for (  Statement stmt : block.getStatements()) {
switch (stmt.type()) {
case INSTRUCTION:
      updateInfo(logger,funcMap,block,info,stmt.instruction(),candidates);
    break;
default :
  break;
}
}
for (Continuation c : block.allComplexStatements()) {
for (Block inner : c.getBlocks()) {
buildInfoRec(logger,funcMap,f,inner,info,candidates.makeChild());
}
}
updateInfoBottomUp(logger,block,info,candidates);
if (logger.isTraceEnabled()) {
logger.trace(""String_Node_Str"" + System.identityHashCode(block) + ""String_Node_Str""+ block.getType());
for (Var candidate : candidates) {
logger.trace(candidate + ""String_Node_Str"" + info.getEntry(block,candidate));
}
}
}",0.9728424864212432
132751,"/** 
 * Attempt to explode individual instruction
 * @param logger
 * @param fn
 * @param execCx
 * @param block
 * @param it
 * @param inst
 * @param waitedFor any vars waited for, e.g. in outer exploded.  This preventsinfinite cycles of exploding if we didn't change instruction
 * @return true if exploded
 */
private static boolean tryExplode(Logger logger,Function fn,ExecContext execCx,Block block,ListIterator<Statement> it,Instruction inst,HierarchicalSet<Var> waitedFor){
  MakeImmRequest req=inst.canMakeImmediate(waitedFor,Collections.<ArgCV>emptySet(),Collections.<Var>emptySet(),true);
  if (req != null && req.in.size() > 0) {
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ fn.getName());
    }
    it.remove();
    List<Pair<Var,Ternary>> waitVars=new ArrayList<Pair<Var,Ternary>>();
    Map<Var,Var> filenameMap=new HashMap<Var,Var>();
    OptUtil.buildWaitVars(block,it,req.in,req.out,filenameMap,waitVars);
    Block insideWaitBlock=enterWaits(fn,execCx,block,inst,req,waitVars);
    List<Statement> instBuffer=new ArrayList<Statement>();
    List<Arg> inVals=OptUtil.fetchMakeImmInputs(insideWaitBlock,req.in,instBuffer);
    List<Var> localOutputs=OptUtil.createMakeImmOutputs(insideWaitBlock,req.out,filenameMap,instBuffer);
    MakeImmChange change=inst.makeImmediate(new OptVarCreator(insideWaitBlock),Fetched.makeList(req.out,localOutputs),Fetched.makeList(req.in,inVals));
    OptUtil.fixupImmChange(block,insideWaitBlock,inst,change,instBuffer,localOutputs,req.out);
    insideWaitBlock.addStatements(instBuffer);
    return true;
  }
  return false;
}","/** 
 * Attempt to explode individual instruction
 * @param logger
 * @param fn
 * @param execCx
 * @param block
 * @param it
 * @param inst
 * @param waitedFor any vars waited for, e.g. in outer exploded.  This preventsinfinite cycles of exploding if we didn't change instruction
 * @return true if exploded
 */
private static boolean tryExplode(Logger logger,Function fn,ExecContext execCx,Block block,ListIterator<Statement> it,Instruction inst,HierarchicalSet<Var> waitedFor){
  MakeImmRequest req=inst.canMakeImmediate(waitedFor,Collections.<ArgCV>emptySet(),Collections.<Var>emptySet(),true);
  if (req != null && req.in.size() > 0) {
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ fn.getName());
    }
    it.remove();
    List<Pair<Var,Ternary>> waitVars=new ArrayList<Pair<Var,Ternary>>();
    Map<Var,Var> filenameMap=new HashMap<Var,Var>();
    OptUtil.buildWaitVars(block,it,req.in,req.out,filenameMap,waitVars);
    Block insideWaitBlock=enterWaits(fn,execCx,block,inst,req,waitVars);
    List<Statement> instBuffer=new ArrayList<Statement>();
    List<Arg> inVals=OptUtil.fetchMakeImmInputs(insideWaitBlock,req.in,instBuffer);
    List<Var> localOutputs=OptUtil.createMakeImmOutputs(insideWaitBlock,req.out,filenameMap,instBuffer);
    MakeImmChange change=inst.makeImmediate(new OptVarCreator(insideWaitBlock),Fetched.makeList(req.out,localOutputs,true),Fetched.makeList(req.in,inVals,false));
    OptUtil.fixupImmChange(block,insideWaitBlock,inst,change,instBuffer,localOutputs,req.out);
    insideWaitBlock.addStatements(instBuffer);
    return true;
  }
  return false;
}",0.9966391689581424
132752,"/** 
 * @param logger
 * @param fn
 * @param block
 * @param cv
 * @param inst
 * @param insts if instructions inserted, leaves iterator pointing at previous instruction
 * @return
 */
private boolean switchToImmediate(Logger logger,Function fn,ExecContext execCx,Block block,Congruences state,Instruction inst,ListIterator<Statement> stmts,int stmtIndex){
  if (!finalizedVarEnabled) {
    return false;
  }
  MakeImmRequest req=inst.canMakeImmediate(state.getClosed(stmtIndex),state.getClosedLocs(stmtIndex),state.retrieveResultAvail(),false);
  if (req == null) {
    return false;
  }
  Block insertContext;
  ListIterator<Statement> insertPoint;
  boolean noWaitRequired=req.mode == TaskMode.LOCAL || req.mode == TaskMode.SYNC || (req.mode == TaskMode.LOCAL_CONTROL && execCx == ExecContext.CONTROL);
  stmts.remove();
  if (noWaitRequired) {
    insertContext=block;
    insertPoint=stmts;
  }
 else {
    WaitStatement wait=new WaitStatement(fn.getName() + ""String_Node_Str"" + inst.shortOpName(),WaitVar.NONE,PassedVar.NONE,Var.NONE,WaitMode.TASK_DISPATCH,false,req.mode,inst.getTaskProps());
    insertContext=wait.getBlock();
    block.addContinuation(wait);
    insertPoint=insertContext.statementIterator();
  }
  List<Statement> alt=new ArrayList<Statement>();
  List<Fetched<Arg>> inVals=fetchInputsForSwitch(state,req,insertContext,noWaitRequired,alt);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inVals + ""String_Node_Str""+ inst+ ""String_Node_Str""+ req.in);
  }
  Map<Var,Var> filenameVals=loadOutputFileNames(state,stmtIndex,req.out,insertContext,insertPoint);
  List<Var> outFetched=OptUtil.createLocalOpOutputVars(insertContext,insertPoint,req.out,filenameVals);
  MakeImmChange change;
  change=inst.makeImmediate(new OptVarCreator(block),Fetched.makeList(req.out,outFetched),inVals);
  OptUtil.fixupImmChange(block,insertContext,inst,change,alt,outFetched,req.out);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ alt.toString());
  }
  for (  Statement newStmt : alt) {
    insertPoint.add(newStmt);
  }
  if (stmts == insertPoint) {
    ICUtil.rewindIterator(stmts,alt.size());
  }
  return true;
}","/** 
 * @param logger
 * @param fn
 * @param block
 * @param cv
 * @param inst
 * @param insts if instructions inserted, leaves iterator pointing at previous instruction
 * @return
 */
private boolean switchToImmediate(Logger logger,Function fn,ExecContext execCx,Block block,Congruences state,Instruction inst,ListIterator<Statement> stmts,int stmtIndex){
  if (!finalizedVarEnabled) {
    return false;
  }
  MakeImmRequest req=inst.canMakeImmediate(state.getClosed(stmtIndex),state.getClosedLocs(stmtIndex),state.retrieveResultAvail(),false);
  if (req == null) {
    return false;
  }
  Block insertContext;
  ListIterator<Statement> insertPoint;
  boolean noWaitRequired=req.mode == TaskMode.LOCAL || req.mode == TaskMode.SYNC || (req.mode == TaskMode.LOCAL_CONTROL && execCx == ExecContext.CONTROL);
  stmts.remove();
  if (noWaitRequired) {
    insertContext=block;
    insertPoint=stmts;
  }
 else {
    WaitStatement wait=new WaitStatement(fn.getName() + ""String_Node_Str"" + inst.shortOpName(),WaitVar.NONE,PassedVar.NONE,Var.NONE,WaitMode.TASK_DISPATCH,false,req.mode,inst.getTaskProps());
    insertContext=wait.getBlock();
    block.addContinuation(wait);
    insertPoint=insertContext.statementIterator();
  }
  List<Statement> alt=new ArrayList<Statement>();
  List<Fetched<Arg>> inVals=fetchInputsForSwitch(state,req,insertContext,noWaitRequired,alt);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inVals + ""String_Node_Str""+ inst+ ""String_Node_Str""+ req.in);
  }
  Map<Var,Var> filenameVals=loadOutputFileNames(state,stmtIndex,req.out,insertContext,insertPoint);
  List<Var> outFetched=OptUtil.createLocalOpOutputVars(insertContext,insertPoint,req.out,filenameVals);
  MakeImmChange change;
  change=inst.makeImmediate(new OptVarCreator(block),Fetched.makeList(req.out,outFetched,true),inVals);
  OptUtil.fixupImmChange(block,insertContext,inst,change,alt,outFetched,req.out);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ alt.toString());
  }
  for (  Statement newStmt : alt) {
    insertPoint.add(newStmt);
  }
  if (stmts == insertPoint) {
    ICUtil.rewindIterator(stmts,alt.size());
  }
  return true;
}",0.9988618256316868
132753,"public static <T>List<Fetched<T>> makeList(List<MakeImmVar> original,List<T> fetched){
  if (original == null) {
    original=Collections.emptyList();
  }
  if (fetched == null) {
    fetched=Collections.emptyList();
  }
  assert(original.size() == fetched.size());
  List<Fetched<T>> result=new ArrayList<Fetched<T>>(fetched.size());
  for (int i=0; i < fetched.size(); i++) {
    result.add(new Fetched<T>(original.get(i).var,fetched.get(i)));
  }
  return result;
}","/** 
 * @param original
 * @param fetched
 * @param includeAll if true, all were fetched.  If false, check fetched field of original
 * @return
 */
public static <T>List<Fetched<T>> makeList(List<MakeImmVar> original,List<T> fetched,boolean includeAll){
}",0.243430152143845
132754,"public static void addOpEquiv(String builtinFunction,BuiltinOpcode op){
  equivalentOps.put(builtinFunction,op);
  equivalentOpsInv.put(op,builtinFunction);
}","public static void addOpEquiv(String functionName,BuiltinOpcode op){
  equivalentOps.put(functionName,op);
  equivalentOpsInv.put(op,functionName);
}",0.8729641693811075
132755,"private void defineBuiltinFunction(Context context,SwiftAST tree) throws UserException {
  final int REQUIRED_CHILDREN=5;
  assert(tree.getChildCount() >= REQUIRED_CHILDREN);
  String function=tree.child(0).getText();
  SwiftAST typeParamsT=tree.child(1);
  SwiftAST outputs=tree.child(2);
  SwiftAST inputs=tree.child(3);
  SwiftAST tclPackage=tree.child(4);
  assert(inputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(outputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(tclPackage.getType() == ExMParser.TCL_PACKAGE);
  assert(tclPackage.getChildCount() == 2);
  Set<String> typeParams=extractTypeParams(typeParamsT);
  FunctionDecl fdecl=FunctionDecl.fromAST(context,function,inputs,outputs,typeParams);
  FunctionType ft=fdecl.getFunctionType();
  LogHelper.debug(context,""String_Node_Str"" + function + ""String_Node_Str""+ ft);
  String pkg=Literals.extractLiteralString(context,tclPackage.child(0));
  String version=Literals.extractLiteralString(context,tclPackage.child(1));
  backend.requirePackage(pkg,version);
  int pos=REQUIRED_CHILDREN;
  TclFunRef impl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.TCL_FUN_REF) {
    SwiftAST tclImplRef=tree.child(pos);
    String symbol=Literals.extractLiteralString(context,tclImplRef.child(0));
    impl=new TclFunRef(pkg,symbol,version);
    pos++;
  }
  TclOpTemplate inlineTcl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.INLINE_TCL) {
    SwiftAST inlineTclTree=tree.child(pos);
    inlineTcl=wrapper.loadTclTemplate(context,function,fdecl,ft,inlineTclTree);
    pos++;
  }
  for (; pos < tree.getChildCount(); pos++) {
    handleBuiltinFunctionAnnotation(context,function,fdecl,tree.child(pos),inlineTcl != null);
  }
  TaskMode taskMode=ForeignFunctions.getTaskMode(function);
  boolean isTargetable=false;
  if (!taskMode.isLocal()) {
    isTargetable=true;
    context.setFunctionProperty(function,FnProp.TARGETABLE);
  }
  context.defineFunction(function,ft);
  FunctionType backendFT=VarRepr.backendFnType(ft);
  if (impl != null) {
    context.setFunctionProperty(function,FnProp.BUILTIN);
    backend.defineBuiltinFunction(function,backendFT,impl);
  }
 else {
    if (inlineTcl == null) {
      throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + function);
    }
    context.setFunctionProperty(function,FnProp.WRAPPED_BUILTIN);
    context.setFunctionProperty(function,FnProp.SYNC);
    boolean isParallel=context.hasFunctionProp(function,FnProp.PARALLEL);
    if (isParallel && taskMode != TaskMode.WORKER) {
      throw new UserException(context,""String_Node_Str"");
    }
    wrapper.saveWrapper(function,backendFT,fdecl,taskMode,isParallel,isTargetable);
  }
}","private void defineBuiltinFunction(Context context,SwiftAST tree) throws UserException {
  final int REQUIRED_CHILDREN=5;
  assert(tree.getChildCount() >= REQUIRED_CHILDREN);
  String function=tree.child(0).getText();
  SwiftAST typeParamsT=tree.child(1);
  SwiftAST outputs=tree.child(2);
  SwiftAST inputs=tree.child(3);
  SwiftAST tclPackage=tree.child(4);
  assert(inputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(outputs.getType() == ExMParser.FORMAL_ARGUMENT_LIST);
  assert(tclPackage.getType() == ExMParser.TCL_PACKAGE);
  assert(tclPackage.getChildCount() == 2);
  Set<String> typeParams=extractTypeParams(typeParamsT);
  FunctionDecl fdecl=FunctionDecl.fromAST(context,function,inputs,outputs,typeParams);
  FunctionType ft=fdecl.getFunctionType();
  LogHelper.debug(context,""String_Node_Str"" + function + ""String_Node_Str""+ ft);
  String pkg=Literals.extractLiteralString(context,tclPackage.child(0));
  String version=Literals.extractLiteralString(context,tclPackage.child(1));
  backend.requirePackage(pkg,version);
  int pos=REQUIRED_CHILDREN;
  TclFunRef impl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.TCL_FUN_REF) {
    SwiftAST tclImplRef=tree.child(pos);
    String symbol=Literals.extractLiteralString(context,tclImplRef.child(0));
    impl=new TclFunRef(pkg,symbol,version);
    pos++;
  }
  TclOpTemplate inlineTcl=null;
  if (pos < tree.getChildCount() && tree.child(pos).getType() == ExMParser.INLINE_TCL) {
    SwiftAST inlineTclTree=tree.child(pos);
    inlineTcl=wrapper.loadTclTemplate(context,function,fdecl,ft,inlineTclTree);
    pos++;
  }
  ForeignFunctions.addForeignFunction(function);
  for (; pos < tree.getChildCount(); pos++) {
    handleBuiltinFunctionAnnotation(context,function,fdecl,tree.child(pos),inlineTcl != null);
  }
  TaskMode taskMode=ForeignFunctions.getTaskMode(function);
  boolean isTargetable=false;
  if (!taskMode.isLocal()) {
    isTargetable=true;
    context.setFunctionProperty(function,FnProp.TARGETABLE);
  }
  context.defineFunction(function,ft);
  FunctionType backendFT=VarRepr.backendFnType(ft);
  if (impl != null) {
    context.setFunctionProperty(function,FnProp.BUILTIN);
    backend.defineBuiltinFunction(function,backendFT,impl);
  }
 else {
    if (inlineTcl == null) {
      throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + function);
    }
    context.setFunctionProperty(function,FnProp.WRAPPED_BUILTIN);
    context.setFunctionProperty(function,FnProp.SYNC);
    boolean isParallel=context.hasFunctionProp(function,FnProp.PARALLEL);
    if (isParallel && taskMode != TaskMode.WORKER) {
      throw new UserException(context,""String_Node_Str"");
    }
    wrapper.saveWrapper(function,backendFT,fdecl,taskMode,isParallel,isTargetable);
  }
}",0.9911440448219772
132756,"/** 
 * Check that old output type was swapped correctly for making immediate
 * @param oldOut
 * @param newOut
 */
private void checkSwappedOutput(Var oldOut,Var newOut){
  Type exp=Types.retrievedType(oldOut.type(),recursiveInOut(op));
  assert(exp.equals(newOut.type()));
}","/** 
 * Check that old output type was swapped correctly for making immediate
 * @param oldOut
 * @param newOut
 */
private void checkSwappedOutput(Var oldOut,Var newOut){
  Type exp=Types.retrievedType(oldOut.type(),recursiveInOut(op,functionName));
  assert(exp.equals(newOut.type()));
}",0.976991150442478
132757,"/** 
 * Returns true if we recursively pack and unpack inputs and outputs for this type of function call if converting to local version
 * @param op
 */
private static boolean recursiveInOut(Opcode op){
switch (op) {
case CALL_SYNC:
case CALL_LOCAL:
case CALL_LOCAL_CONTROL:
case CALL_CONTROL:
    return false;
case CALL_FOREIGN:
  return true;
default :
throw new STCRuntimeError(""String_Node_Str"" + op);
}
}","/** 
 * Returns true if we recursively pack and unpack inputs and outputs for this type of function call if converting to local version
 * @param op
 */
private static boolean recursiveInOut(Opcode op,String functionName){
switch (op) {
case CALL_SYNC:
case CALL_LOCAL:
case CALL_LOCAL_CONTROL:
case CALL_CONTROL:
case CALL_FOREIGN:
    if (ForeignFunctions.isForeignFunction(functionName)) {
      return ForeignFunctions.recursivelyUnpackedInOut(functionName);
    }
 else {
      return false;
    }
default :
  throw new STCRuntimeError(""String_Node_Str"" + op);
}
}",0.7783452502553626
132758,"/** 
 * Fix up any variables missing from the usedVariables passed through continuations. This is useful because it is easier to write other optimizations if they are allowed to mess up the usedVariables
 * @param updateLists modify tree and update pass and keep open lists
 */
public static void fixupProgram(Logger logger,Program prog,boolean updateLists){
  Set<Var> referencedGlobals=new HashSet<Var>();
  for (  Function fn : prog.getFunctions()) {
    fixupFunction(logger,prog.constants(),fn,referencedGlobals,updateLists);
  }
  if (updateLists)   removeUnusedGlobals(prog.constants(),referencedGlobals);
}","/** 
 * Fix up any variables missing from the usedVariables passed through continuations. This is useful because it is easier to write other optimizations if they are allowed to mess up the usedVariables
 * @param updateLists modify tree and update pass and keep open lists
 */
public static void fixupProgram(Logger logger,Program prog,boolean updateLists){
  Set<Var> referencedGlobals=new HashSet<Var>();
  for (  Function fn : prog.getFunctions()) {
    if (updateLists) {
      fixupFunction(logger,prog.constants(),fn,referencedGlobals,FixupVarMode.REBUILD);
      fixupFunction(logger,prog.constants(),fn,referencedGlobals,FixupVarMode.ADD);
    }
 else {
      fixupFunction(logger,prog.constants(),fn,referencedGlobals,FixupVarMode.NO_UPDATE);
    }
  }
  if (updateLists)   removeUnusedGlobals(prog.constants(),referencedGlobals);
}",0.8296703296703297
132759,"private static void rebuildContinuationKeepOpenVars(Function function,Continuation continuation,HierarchicalSet<Var> visible,Set<Var> outerBlockVars,AliasTracker outerAliases,Result outer,Result inner){
  List<Var> keepOpen=new ArrayList<Var>();
  for (  Var v : inner.written) {
    if (!outerBlockVars.contains(v)) {
      outer.addWritten(v,outerAliases);
    }
    if (!visible.contains(v)) {
      throw new STCRuntimeError(""String_Node_Str"" + v + ""String_Node_Str""+ ""String_Node_Str""+ function.getName());
    }
    if (RefCounting.trackWriteRefCount(v)) {
      keepOpen.add(v);
    }
  }
  continuation.setKeepOpenVars(keepOpen);
}","private static void rebuildContinuationKeepOpenVars(Function function,Continuation continuation,HierarchicalSet<Var> visible,Set<Var> outerBlockVars,AliasTracker outerAliases,Result outer,Result inner,FixupVarMode fixupMode){
  List<Var> keepOpen=new ArrayList<Var>();
  for (  Var v : inner.written) {
    if (!outerBlockVars.contains(v)) {
      outer.addWritten(v,outerAliases);
    }
    if (!visible.contains(v)) {
      throw new STCRuntimeError(""String_Node_Str"" + v + ""String_Node_Str""+ ""String_Node_Str""+ function.getName());
    }
    if (RefCounting.trackWriteRefCount(v)) {
      keepOpen.add(v);
    }
  }
  if (fixupMode == FixupVarMode.REBUILD) {
    continuation.setKeepOpenVars(keepOpen);
  }
 else {
    assert(fixupMode == FixupVarMode.ADD);
    keepOpen.addAll(continuation.getKeepOpenVars());
    ICUtil.removeDuplicates(keepOpen);
    continuation.setKeepOpenVars(keepOpen);
  }
}",0.8293316028552887
132760,"/** 
 * @param logger
 * @param block 
 * @param visible allvariables logically visible in this block. will be modified in fn
 * @param referencedGlobals updated with names of any globals used
 * @param aliases 
 * @param updateLists 
 * @return
 */
private static Result fixupBlockRec(Logger logger,Function function,Block block,ExecContext execCx,HierarchicalSet<Var> visible,Set<Var> referencedGlobals,AliasTracker aliases,boolean updateLists){
  if (updateLists)   removeGlobalImports(block);
  Set<Var> blockVars=new HashSet<Var>();
  for (  Var v : block.getVariables()) {
    blockVars.add(v);
    visible.add(v);
  }
  List<Pair<Var,Var>> createdAliases=new ArrayList<Pair<Var,Var>>();
  Result result=new Result();
  findBlockNeeded(block,result,aliases,createdAliases);
  for (  Continuation c : block.allComplexStatements()) {
    fixupContinuationRec(logger,function,execCx,c,visible,referencedGlobals,aliases,blockVars,result,updateLists);
  }
  for (  Pair<Var,Var> a : createdAliases) {
    if (result.written.contains(a.val2)) {
      result.addWritten(a.val1,aliases);
    }
  }
  result.removeReadWrite(blockVars);
  if (execCx == ExecContext.CONTROL) {
    Set<Var> globals=addGlobalImports(block,visible,updateLists,result.allSets());
    referencedGlobals.addAll(globals);
    result.removeReadWrite(globals);
  }
  return result;
}","/** 
 * @param logger
 * @param block 
 * @param visible allvariables logically visible in this block. will be modified in fn
 * @param referencedGlobals updated with names of any globals used
 * @param aliases 
 * @param fixupMode  
 * @return
 */
private static Result fixupBlockRec(Logger logger,Function function,Block block,ExecContext execCx,HierarchicalSet<Var> visible,Set<Var> referencedGlobals,AliasTracker aliases,FixupVarMode fixupMode){
  if (fixupMode == FixupVarMode.REBUILD)   removeGlobalImports(block);
  Set<Var> blockVars=new HashSet<Var>();
  for (  Var v : block.getVariables()) {
    blockVars.add(v);
    visible.add(v);
  }
  List<Pair<Var,Var>> createdAliases=new ArrayList<Pair<Var,Var>>();
  Result result=new Result();
  findBlockNeeded(block,result,aliases,createdAliases);
  for (  Continuation c : block.allComplexStatements()) {
    fixupContinuationRec(logger,function,execCx,c,visible,referencedGlobals,aliases,blockVars,result,fixupMode);
  }
  for (  Pair<Var,Var> a : createdAliases) {
    if (result.written.contains(a.val2)) {
      result.addWritten(a.val1,aliases);
    }
  }
  result.removeReadWrite(blockVars);
  if (canImportGlobals(execCx)) {
    Set<Var> globals=addGlobalImports(block,visible,fixupMode,result.allSets());
    referencedGlobals.addAll(globals);
    result.removeReadWrite(globals);
  }
  return result;
}",0.8099963248805586
132761,"/** 
 * Update variable passing for nested continuation
 * @param logger
 * @param function
 * @param outerCx exec context outside of continuation
 * @param continuation
 * @param visible
 * @param referencedGlobals
 * @param aliases 
 * @param outerBlockVars
 * @param neededVars
 * @param updateLists 
 */
private static void fixupContinuationRec(Logger logger,Function function,ExecContext outerCx,Continuation continuation,HierarchicalSet<Var> visible,Set<Var> referencedGlobals,AliasTracker outerAliases,Set<Var> outerBlockVars,Result result,boolean updateLists){
  List<Var> constructVars=continuation.constructDefinedVars(ContVarDefType.NEW_DEF);
  ExecContext innerCx=continuation.childContext(outerCx);
  AliasTracker contAliases=outerAliases.makeChild();
  for (  Block innerBlock : continuation.getBlocks()) {
    HierarchicalSet<Var> childVisible=visible.makeChild();
    for (    Var v : constructVars) {
      childVisible.add(v);
    }
    AliasTracker blockAliases=contAliases.makeChild();
    Result inner=fixupBlockRec(logger,function,innerBlock,innerCx,childVisible,referencedGlobals,blockAliases,updateLists);
    if (!constructVars.isEmpty()) {
      inner.removeReadWrite(constructVars);
    }
    if (continuation.inheritsParentVars()) {
      inner.removeReadWrite(outerBlockVars);
      result.add(inner);
    }
 else     if (updateLists) {
      rebuildContinuationPassedVars(function,continuation,innerCx,visible,outerBlockVars,outerAliases,result,inner);
      rebuildContinuationKeepOpenVars(function,continuation,visible,outerBlockVars,outerAliases,result,inner);
    }
  }
}","/** 
 * Update variable passing for nested continuation
 * @param logger
 * @param function
 * @param outerCx exec context outside of continuation
 * @param continuation
 * @param visible
 * @param referencedGlobals
 * @param aliases 
 * @param outerBlockVars
 * @param neededVars
 * @param fixupMode ! 
 */
private static void fixupContinuationRec(Logger logger,Function function,ExecContext outerCx,Continuation continuation,HierarchicalSet<Var> visible,Set<Var> referencedGlobals,AliasTracker outerAliases,Set<Var> outerBlockVars,Result result,FixupVarMode fixupMode){
  List<Var> constructVars=continuation.constructDefinedVars(ContVarDefType.NEW_DEF);
  ExecContext innerCx=continuation.childContext(outerCx);
  AliasTracker contAliases=outerAliases.makeChild();
  for (  Block innerBlock : continuation.getBlocks()) {
    HierarchicalSet<Var> childVisible=visible.makeChild();
    for (    Var v : constructVars) {
      childVisible.add(v);
    }
    AliasTracker blockAliases=contAliases.makeChild();
    Result inner=fixupBlockRec(logger,function,innerBlock,innerCx,childVisible,referencedGlobals,blockAliases,fixupMode);
    if (!constructVars.isEmpty()) {
      inner.removeReadWrite(constructVars);
    }
    if (continuation.inheritsParentVars()) {
      inner.removeReadWrite(outerBlockVars);
      result.add(inner);
    }
 else     if (fixupMode != FixupVarMode.NO_UPDATE) {
      rebuildContinuationPassedVars(function,continuation,innerCx,visible,outerBlockVars,outerAliases,result,inner,fixupMode);
      rebuildContinuationKeepOpenVars(function,continuation,visible,outerBlockVars,outerAliases,result,inner,fixupMode);
    }
  }
}",0.9542242703533026
132762,"/** 
 * @param block
 * @param visible
 * @param neededSets sets of vars needed from outside bock
 * @return set of global vars
 */
private static Set<Var> addGlobalImports(Block block,HierarchicalSet<Var> visible,boolean updateLists,List<Set<Var>> neededSets){
  Set<Var> addedGlobals=new HashSet<Var>();
  for (  Set<Var> neededSet : neededSets) {
    for (    Var var : neededSet) {
      if (visible.contains(var)) {
        if (var.storage() == Alloc.GLOBAL_CONST) {
          if (updateLists && !addedGlobals.contains(var))           block.addVariable(var,true);
          addedGlobals.add(var);
        }
      }
    }
  }
  return addedGlobals;
}","/** 
 * @param block
 * @param visible
 * @param neededSets sets of vars needed from outside bock
 * @return set of global vars needed from outside
 */
private static Set<Var> addGlobalImports(Block block,HierarchicalSet<Var> visible,FixupVarMode fixupMode,List<Set<Var>> neededSets){
  Set<Var> existingGlobals=new HashSet<Var>();
  if (fixupMode == FixupVarMode.ADD) {
    for (    Var v : block.getVariables()) {
      if (v.storage() == Alloc.GLOBAL_CONST) {
        existingGlobals.add(v);
      }
    }
  }
  for (  Set<Var> neededSet : neededSets) {
    for (    Var var : neededSet) {
      if (visible.contains(var)) {
        if (var.storage() == Alloc.GLOBAL_CONST) {
          if (fixupMode != FixupVarMode.NO_UPDATE && !existingGlobals.contains(var))           block.addVariable(var,true);
          existingGlobals.add(var);
        }
      }
    }
  }
  return existingGlobals;
}",0.6227390180878553
132763,"public static void fixupFunction(Logger logger,GlobalConstants constants,Function fn,Set<Var> referencedGlobals,boolean updateLists){
  HierarchicalSet<Var> fnargs=new HierarchicalSet<Var>();
  for (  Var v : fn.getInputList()) {
    fnargs.add(v);
  }
  for (  Var v : fn.getOutputList()) {
    fnargs.add(v);
  }
  fnargs.addAll(constants.vars());
  AliasTracker aliases=new AliasTracker();
  Result res=fixupBlockRec(logger,fn,fn.mainBlock(),ExecContext.CONTROL,fnargs,referencedGlobals,aliases,updateLists);
  if (updateLists) {
    for (int i=0; i < fn.getOutputList().size(); i++) {
      Var output=fn.getOutput(i);
      if (!res.read.contains(output) && !Types.hasReadableSideChannel(output.type())) {
        fn.makeOutputWriteOnly(i);
      }
    }
  }
  res.removeRead(fn.getInputList());
  res.removeReadWrite(fn.getOutputList());
  for (  Var v : fn.getInputList()) {
    if (Types.isPrimUpdateable(v.type())) {
      res.removeWritten(v);
    }
  }
  if (res.read.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ res.read.toString());
  }
  if (res.written.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ res.written.toString());
  }
  if (res.aliasWritten.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ res.aliasWritten.toString());
  }
}","public static void fixupFunction(Logger logger,GlobalConstants constants,Function fn,Set<Var> referencedGlobals,FixupVarMode fixupMode){
  HierarchicalSet<Var> fnargs=new HierarchicalSet<Var>();
  for (  Var v : fn.getInputList()) {
    fnargs.add(v);
  }
  for (  Var v : fn.getOutputList()) {
    fnargs.add(v);
  }
  fnargs.addAll(constants.vars());
  AliasTracker aliases=new AliasTracker();
  Result res=fixupBlockRec(logger,fn,fn.mainBlock(),ExecContext.CONTROL,fnargs,referencedGlobals,aliases,fixupMode);
  if (fixupMode != FixupVarMode.NO_UPDATE) {
    for (int i=0; i < fn.getOutputList().size(); i++) {
      Var output=fn.getOutput(i);
      if (!res.read.contains(output) && !Types.hasReadableSideChannel(output.type())) {
        fn.makeOutputWriteOnly(i);
      }
    }
  }
  res.removeRead(fn.getInputList());
  res.removeReadWrite(fn.getOutputList());
  for (  Var v : fn.getInputList()) {
    if (Types.isPrimUpdateable(v.type())) {
      res.removeWritten(v);
    }
  }
  if (res.read.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ res.read.toString());
  }
  if (res.written.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ res.written.toString());
  }
  if (res.aliasWritten.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ res.aliasWritten.toString());
  }
}",0.955877162019061
132764,"private static void rebuildContinuationPassedVars(Function function,Continuation continuation,ExecContext contCx,HierarchicalSet<Var> visibleVars,Set<Var> outerBlockVars,AliasTracker outerAliases,Result outer,Result inner){
  List<PassedVar> passedIn=new ArrayList<PassedVar>();
  for (  Var needed : inner.allNeeded()) {
    if (!visibleVars.contains(needed)) {
      throw new STCRuntimeError(""String_Node_Str"" + needed + ""String_Node_Str""+ ""String_Node_Str""+ function.getName());
    }
    boolean writeOnly=!inner.read.contains(needed);
    passedIn.add(new PassedVar(needed,writeOnly));
  }
  outer.addExcluding(inner,outerBlockVars);
  for (  PassedVar addtl : continuation.getMustPassVars()) {
    boolean mustAdd=true;
    ListIterator<PassedVar> it=passedIn.listIterator();
    while (it.hasNext()) {
      PassedVar existing=it.next();
      if (existing.var.equals(addtl.var)) {
        mustAdd=false;
        if (existing.writeOnly && !addtl.writeOnly) {
          it.set(addtl);
        }
      }
    }
    if (mustAdd) {
      if (addtl.var.storage() == Alloc.GLOBAL_CONST) {
        if (contCx == ExecContext.WORKER) {
          passedIn.add(addtl);
        }
 else {
          assert(contCx == ExecContext.CONTROL);
          for (          Block b : continuation.getBlocks()) {
            if (!b.getVariables().contains(addtl.var)) {
              b.addVariable(addtl.var);
            }
          }
        }
      }
 else {
        passedIn.add(addtl);
      }
    }
  }
  continuation.setPassedVars(passedIn);
}","private static void rebuildContinuationPassedVars(Function function,Continuation continuation,ExecContext contCx,HierarchicalSet<Var> visibleVars,Set<Var> outerBlockVars,AliasTracker outerAliases,Result outer,Result inner,FixupVarMode fixupMode){
  List<PassedVar> passedIn=new ArrayList<PassedVar>();
  for (  Var needed : inner.allNeeded()) {
    if (!visibleVars.contains(needed)) {
      throw new STCRuntimeError(""String_Node_Str"" + needed + ""String_Node_Str""+ ""String_Node_Str""+ function.getName());
    }
    boolean writeOnly=!inner.read.contains(needed);
    passedIn.add(new PassedVar(needed,writeOnly));
  }
  outer.addExcluding(inner,outerBlockVars);
  for (  PassedVar addtl : continuation.getMustPassVars()) {
    boolean mustAdd=true;
    ListIterator<PassedVar> it=passedIn.listIterator();
    while (it.hasNext()) {
      PassedVar existing=it.next();
      if (existing.var.equals(addtl.var)) {
        mustAdd=false;
        if (existing.writeOnly && !addtl.writeOnly) {
          it.set(addtl);
        }
      }
    }
    if (mustAdd) {
      if (addtl.var.storage() == Alloc.GLOBAL_CONST) {
        if (canImportGlobals(contCx)) {
          for (          Block b : continuation.getBlocks()) {
            if (!b.getVariables().contains(addtl.var)) {
              b.addVariable(addtl.var);
            }
          }
        }
 else {
          passedIn.add(addtl);
        }
      }
 else {
        passedIn.add(addtl);
      }
    }
  }
  if (fixupMode == FixupVarMode.REBUILD) {
    continuation.setPassedVars(passedIn);
  }
 else {
    assert(fixupMode == FixupVarMode.ADD);
    Collection<PassedVar> newPassedVars;
    Collection<PassedVar> oldPassedVars=continuation.getPassedVars();
    newPassedVars=PassedVar.mergeLists(passedIn,oldPassedVars);
    continuation.setPassedVars(newPassedVars);
  }
}",0.6779761904761905
132765,"private static void pipelineTasks(Logger logger,Function f,Block curr,ExecContext cx){
  for (  Continuation cont : curr.allComplexStatements()) {
    ExecContext childCx=cont.childContext(cx);
    for (    Block childBlock : cont.getBlocks()) {
      pipelineTasks(logger,f,childBlock,childCx);
    }
  }
  List<WaitStatement> candidates=new ArrayList<WaitStatement>();
  for (  Continuation cont : curr.getContinuations()) {
    if (cont.getType() == ContinuationType.WAIT_STATEMENT) {
      WaitStatement w=(WaitStatement)cont;
      boolean compatible=true;
      if (!w.getWaitVars().isEmpty()) {
        compatible=false;
      }
 else       if (!Settings.NO_TURBINE_ENGINE && w.childContext(cx) != cx) {
        compatible=false;
      }
 else       if (w.isParallel()) {
        compatible=false;
      }
      if (compatible) {
        candidates.add(w);
      }
    }
  }
  if (candidates.isEmpty()) {
    return;
  }
  logger.trace(""String_Node_Str"" + candidates.size() + ""String_Node_Str""+ ""String_Node_Str"");
  WaitStatement bestCand=candidates.get(0);
  if (candidates.size() > 1) {
    int bestCost=heuristicCost(logger,f,curr,bestCand);
    for (int i=1; i < candidates.size(); i++) {
      WaitStatement cand=candidates.get(i);
      int cost=heuristicCost(logger,f,curr,cand);
      if (cost < bestCost) {
        bestCost=cost;
        bestCand=cand;
      }
    }
  }
  if (candidates.size() == 1) {
    bestCand.inlineInto(curr);
  }
 else {
    NestedBlock nested=new NestedBlock();
    bestCand.inlineInto(nested.getBlock());
    nested.setRunLast(true);
    curr.addContinuation(nested);
  }
}","private static void pipelineTasks(Logger logger,Function f,Block curr,ExecContext cx){
  for (  Continuation cont : curr.allComplexStatements()) {
    ExecContext childCx=cont.childContext(cx);
    for (    Block childBlock : cont.getBlocks()) {
      pipelineTasks(logger,f,childBlock,childCx);
    }
  }
  List<WaitStatement> candidates=new ArrayList<WaitStatement>();
  for (  Continuation cont : curr.getContinuations()) {
    if (cont.getType() == ContinuationType.WAIT_STATEMENT) {
      WaitStatement w=(WaitStatement)cont;
      boolean compatible=true;
      if (!w.getWaitVars().isEmpty()) {
        compatible=false;
      }
 else       if (!Settings.NO_TURBINE_ENGINE && w.childContext(cx) != cx) {
        compatible=false;
      }
 else       if (w.isParallel()) {
        compatible=false;
      }
 else       if (w.targetLocation() != null && !w.targetLocation().equals(Location.ANY_LOCATION)) {
        compatible=false;
      }
      if (compatible) {
        candidates.add(w);
      }
    }
  }
  if (candidates.isEmpty()) {
    return;
  }
  logger.trace(""String_Node_Str"" + candidates.size() + ""String_Node_Str""+ ""String_Node_Str"");
  WaitStatement bestCand=candidates.get(0);
  if (candidates.size() > 1) {
    int bestCost=heuristicCost(logger,f,curr,bestCand);
    for (int i=1; i < candidates.size(); i++) {
      WaitStatement cand=candidates.get(i);
      int cost=heuristicCost(logger,f,curr,cand);
      if (cost < bestCost) {
        bestCost=cost;
        bestCand=cand;
      }
    }
  }
  if (candidates.size() == 1) {
    bestCand.inlineInto(curr);
  }
 else {
    NestedBlock nested=new NestedBlock();
    bestCand.inlineInto(nested.getBlock());
    nested.setRunLast(true);
    curr.addContinuation(nested);
  }
}",0.9604989604989606
132766,"/** 
 * Update variable passing for nested continuation
 * @param logger
 * @param function
 * @param outerCx exec context outside of continuation
 * @param continuation
 * @param visible
 * @param referencedGlobals
 * @param aliases 
 * @param outerBlockVars
 * @param neededVars
 * @param updateLists 
 */
private static void fixupContinuationRec(Logger logger,Function function,ExecContext outerCx,Continuation continuation,HierarchicalSet<Var> visible,Set<Var> referencedGlobals,AliasTracker outerAliases,Set<Var> outerBlockVars,Result result,boolean updateLists){
  List<Var> constructVars=continuation.constructDefinedVars(ContVarDefType.NEW_DEF);
  ExecContext innerCx=continuation.childContext(outerCx);
  AliasTracker contAliases=outerAliases.makeChild();
  for (  Block innerBlock : continuation.getBlocks()) {
    HierarchicalSet<Var> childVisible=visible.makeChild();
    for (    Var v : constructVars) {
      childVisible.add(v);
    }
    AliasTracker blockAliases=contAliases.makeChild();
    Result inner=fixupBlockRec(logger,function,innerBlock,innerCx,childVisible,referencedGlobals,blockAliases,updateLists);
    if (!constructVars.isEmpty()) {
      inner.removeReadWrite(constructVars);
    }
    if (continuation.inheritsParentVars()) {
      inner.removeReadWrite(outerBlockVars);
      result.add(inner);
    }
 else     if (updateLists) {
      rebuildContinuationPassedVars(function,continuation,visible,outerBlockVars,outerAliases,result,inner);
      rebuildContinuationKeepOpenVars(function,continuation,visible,outerBlockVars,outerAliases,result,inner);
    }
  }
}","/** 
 * Update variable passing for nested continuation
 * @param logger
 * @param function
 * @param outerCx exec context outside of continuation
 * @param continuation
 * @param visible
 * @param referencedGlobals
 * @param aliases 
 * @param outerBlockVars
 * @param neededVars
 * @param updateLists 
 */
private static void fixupContinuationRec(Logger logger,Function function,ExecContext outerCx,Continuation continuation,HierarchicalSet<Var> visible,Set<Var> referencedGlobals,AliasTracker outerAliases,Set<Var> outerBlockVars,Result result,boolean updateLists){
  List<Var> constructVars=continuation.constructDefinedVars(ContVarDefType.NEW_DEF);
  ExecContext innerCx=continuation.childContext(outerCx);
  AliasTracker contAliases=outerAliases.makeChild();
  for (  Block innerBlock : continuation.getBlocks()) {
    HierarchicalSet<Var> childVisible=visible.makeChild();
    for (    Var v : constructVars) {
      childVisible.add(v);
    }
    AliasTracker blockAliases=contAliases.makeChild();
    Result inner=fixupBlockRec(logger,function,innerBlock,innerCx,childVisible,referencedGlobals,blockAliases,updateLists);
    if (!constructVars.isEmpty()) {
      inner.removeReadWrite(constructVars);
    }
    if (continuation.inheritsParentVars()) {
      inner.removeReadWrite(outerBlockVars);
      result.add(inner);
    }
 else     if (updateLists) {
      rebuildContinuationPassedVars(function,continuation,innerCx,visible,outerBlockVars,outerAliases,result,inner);
      rebuildContinuationKeepOpenVars(function,continuation,visible,outerBlockVars,outerAliases,result,inner);
    }
  }
}",0.9975015615240476
132767,"private static void rebuildContinuationPassedVars(Function function,Continuation continuation,HierarchicalSet<Var> visibleVars,Set<Var> outerBlockVars,AliasTracker outerAliases,Result outer,Result inner){
  List<PassedVar> passedIn=new ArrayList<PassedVar>();
  for (  Var needed : inner.allNeeded()) {
    if (!visibleVars.contains(needed)) {
      throw new STCRuntimeError(""String_Node_Str"" + needed + ""String_Node_Str""+ ""String_Node_Str""+ function.getName());
    }
    boolean writeOnly=!inner.read.contains(needed);
    passedIn.add(new PassedVar(needed,writeOnly));
  }
  outer.addExcluding(inner,outerBlockVars);
  for (  PassedVar addtl : continuation.getMustPassVars()) {
    boolean mustAdd=true;
    ListIterator<PassedVar> it=passedIn.listIterator();
    while (it.hasNext()) {
      PassedVar existing=it.next();
      if (existing.var.equals(addtl.var)) {
        mustAdd=false;
        if (existing.writeOnly && !addtl.writeOnly) {
          it.set(addtl);
        }
      }
    }
    if (mustAdd) {
      passedIn.add(addtl);
    }
  }
  continuation.setPassedVars(passedIn);
}","private static void rebuildContinuationPassedVars(Function function,Continuation continuation,ExecContext contCx,HierarchicalSet<Var> visibleVars,Set<Var> outerBlockVars,AliasTracker outerAliases,Result outer,Result inner){
  List<PassedVar> passedIn=new ArrayList<PassedVar>();
  for (  Var needed : inner.allNeeded()) {
    if (!visibleVars.contains(needed)) {
      throw new STCRuntimeError(""String_Node_Str"" + needed + ""String_Node_Str""+ ""String_Node_Str""+ function.getName());
    }
    boolean writeOnly=!inner.read.contains(needed);
    passedIn.add(new PassedVar(needed,writeOnly));
  }
  outer.addExcluding(inner,outerBlockVars);
  for (  PassedVar addtl : continuation.getMustPassVars()) {
    boolean mustAdd=true;
    ListIterator<PassedVar> it=passedIn.listIterator();
    while (it.hasNext()) {
      PassedVar existing=it.next();
      if (existing.var.equals(addtl.var)) {
        mustAdd=false;
        if (existing.writeOnly && !addtl.writeOnly) {
          it.set(addtl);
        }
      }
    }
    if (mustAdd) {
      if (addtl.var.storage() == Alloc.GLOBAL_CONST) {
        if (contCx == ExecContext.WORKER) {
          passedIn.add(addtl);
        }
 else {
          assert(contCx == ExecContext.CONTROL);
          for (          Block b : continuation.getBlocks()) {
            if (!b.getVariables().contains(addtl.var)) {
              b.addVariable(addtl.var);
            }
          }
        }
      }
 else {
        passedIn.add(addtl);
      }
    }
  }
  continuation.setPassedVars(passedIn);
}",0.8332063975628332
132768,"/** 
 * Try to piggyback decrement operations on instructions in block TODO: this doesn't factor in alias info: if we're storing to an alias (e.g. struct field), we could piggyback a decrement for the root of he struct E.g. see test 367
 * @param logger
 * @param fn
 * @param block
 * @param tracker
 * @param rcType
 */
private void piggybackDecrementsOnInstructions(Logger logger,Function fn,Block block,RCTracker tracker,RefCountType rcType){
  if (!RCUtil.piggybackEnabled()) {
    return;
  }
  RefCountCandidates candidates=tracker.getVarCandidates(block,rcType,RCDir.DECR);
  UseFinder subblockWalker=new UseFinder(tracker,rcType,candidates.varKeySet(),null);
  ListIterator<Continuation> cit=block.continuationEndIterator();
  while (cit.hasPrevious()) {
    Continuation cont=cit.previous();
    if (RCUtil.isAsyncForeachLoop(cont)) {
      AbstractForeachLoop loop=(AbstractForeachLoop)cont;
      Var piggybacked;
      do {
        piggybacked=loop.tryPiggyBack(candidates,rcType,RCDir.DECR);
        if (piggybacked != null) {
          if (logger.isTraceEnabled()) {
            logger.trace(""String_Node_Str"" + piggybacked + ""String_Node_Str""+ rcType+ ""String_Node_Str""+ candidates.getCount(piggybacked));
          }
          candidates.reset(piggybacked);
          tracker.reset(rcType,piggybacked,RCDir.DECR);
        }
      }
 while (piggybacked != null);
    }
    subblockWalker.reset();
    TreeWalk.walkSyncChildren(logger,fn,cont,subblockWalker);
    removeCandidates(subblockWalker.getUsedVars(),tracker,candidates);
  }
  List<VarCount> successful=new ArrayList<VarCount>();
  ListIterator<Statement> it=block.statementEndIterator();
  while (it.hasPrevious()) {
    Statement stmt=it.previous();
switch (stmt.type()) {
case INSTRUCTION:
{
        Instruction inst=stmt.instruction();
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + inst);
        }
        VarCount piggybacked;
        do {
          piggybacked=inst.tryPiggyback(candidates,rcType);
          if (piggybacked != null && piggybacked.count != 0) {
            if (logger.isTraceEnabled()) {
              logger.trace(""String_Node_Str"" + piggybacked + ""String_Node_Str""+ inst);
            }
            candidates.add(piggybacked.var,-piggybacked.count);
            successful.add(piggybacked);
          }
        }
 while (piggybacked != null && piggybacked.count != 0);
        List<Var> used=findUses(inst,tracker,rcType,candidates.varKeySet());
        removeCandidates(used,tracker,candidates);
        break;
      }
case CONDITIONAL:
    subblockWalker.reset();
  TreeWalk.walkSyncChildren(logger,fn,stmt.conditional(),subblockWalker);
removeCandidates(subblockWalker.getUsedVars(),tracker,candidates);
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + stmt.type());
}
}
if (logger.isTraceEnabled()) {
logger.trace(successful);
}
for (VarCount vc : successful) {
assert(vc != null);
tracker.cancel(tracker.getRefCountVar(vc.var),rcType,-vc.count);
}
}","/** 
 * Try to piggyback decrement operations on instructions in block
 * @param logger
 * @param fn
 * @param block
 * @param tracker
 * @param rcType
 */
private void piggybackDecrementsOnInstructions(Logger logger,Function fn,Block block,RCTracker tracker,RefCountType rcType){
  if (!RCUtil.piggybackEnabled()) {
    return;
  }
  RefCountCandidates candidates=tracker.getVarCandidates(block,rcType,RCDir.DECR);
  UseFinder subblockWalker=new UseFinder(tracker,rcType,candidates.varKeySet(),null);
  ListIterator<Continuation> cit=block.continuationEndIterator();
  while (cit.hasPrevious()) {
    Continuation cont=cit.previous();
    if (RCUtil.isAsyncForeachLoop(cont)) {
      AbstractForeachLoop loop=(AbstractForeachLoop)cont;
      VarCount piggybacked;
      do {
        piggybacked=loop.tryPiggyBack(candidates,rcType,RCDir.DECR);
        if (piggybacked != null) {
          if (logger.isTraceEnabled()) {
            logger.trace(""String_Node_Str"" + piggybacked + ""String_Node_Str""+ rcType+ ""String_Node_Str""+ piggybacked.count);
          }
          candidates.add(piggybacked.var,-piggybacked.count);
          tracker.cancel(tracker.getRefCountVar(piggybacked.var),rcType,-piggybacked.count);
        }
      }
 while (piggybacked != null);
    }
    subblockWalker.reset();
    TreeWalk.walkSyncChildren(logger,fn,cont,subblockWalker);
    removeCandidates(subblockWalker.getUsedVars(),tracker,candidates);
  }
  List<VarCount> successful=new ArrayList<VarCount>();
  ListIterator<Statement> it=block.statementEndIterator();
  while (it.hasPrevious()) {
    Statement stmt=it.previous();
switch (stmt.type()) {
case INSTRUCTION:
{
        Instruction inst=stmt.instruction();
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + inst);
        }
        VarCount piggybacked;
        do {
          piggybacked=inst.tryPiggyback(candidates,rcType);
          if (piggybacked != null && piggybacked.count != 0) {
            if (logger.isTraceEnabled()) {
              logger.trace(""String_Node_Str"" + piggybacked + ""String_Node_Str""+ inst);
            }
            candidates.add(piggybacked.var,-piggybacked.count);
            successful.add(piggybacked);
          }
        }
 while (piggybacked != null && piggybacked.count != 0);
        List<Var> used=findUses(inst,tracker,rcType,candidates.varKeySet());
        removeCandidates(used,tracker,candidates);
        break;
      }
case CONDITIONAL:
    subblockWalker.reset();
  TreeWalk.walkSyncChildren(logger,fn,stmt.conditional(),subblockWalker);
removeCandidates(subblockWalker.getUsedVars(),tracker,candidates);
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + stmt.type());
}
}
if (logger.isTraceEnabled()) {
logger.trace(successful);
}
for (VarCount vc : successful) {
assert(vc != null);
tracker.cancel(tracker.getRefCountVar(vc.var),rcType,-vc.count);
}
}",0.9199185612487276
132769,"/** 
 * Try to piggyback constant incrs/decrs from outside continuation. Called repeatedly until it returns null.
 * @param increments
 * @param type
 * @param dir whether to piggyback decrements or increments
 * @return if piggybacked, the var for which increments were piggybackedotherwise null
 */
public Var tryPiggyBack(RefCountsToPlace increments,RefCountType type,RCDir dir){
  for (  RefCount startIncr : startIncrements) {
    if (startIncr.type == type) {
      long incr=increments.getCount(startIncr.var);
      if ((dir == RCDir.DECR && incr < 0) || (dir == RCDir.INCR && incr > 0)) {
        addConstantStartIncrement(startIncr.var,type,Arg.createIntLit(incr));
        return startIncr.var;
      }
    }
  }
  return null;
}","/** 
 * Try to piggyback constant incrs/decrs from outside continuation. Called repeatedly until it returns null.
 * @param increments
 * @param type
 * @param dir whether to piggyback decrements or increments
 * @return if piggybacked, the var for which increments were piggybackedand amount (e.g. -2 if 2 decrements were piggybacked), otherwise null
 */
public VarCount tryPiggyBack(RefCountsToPlace increments,RefCountType type,RCDir dir){
  for (  RefCount startIncr : startIncrements) {
    if (startIncr.type == type) {
      long incr=increments.getCount(startIncr.var);
      if ((dir == RCDir.DECR && incr < 0) || (dir == RCDir.INCR && incr > 0)) {
        addConstantStartIncrement(startIncr.var,type,Arg.createIntLit(incr));
        return new VarCount(startIncr.var,incr);
      }
    }
  }
  return null;
}",0.949326491340603
132770,"@Override public Pair<List<VarCount>,List<VarCount>> inRefCounts(Map<String,Function> functions){
switch (op) {
case STORE_REF:
    return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
case ARRAY_BUILD:
{
    List<VarCount> readIncr=new ArrayList<VarCount>();
    for (int i=0; i < getInputs().size() / 2; i++) {
      Arg elem=getInput(i * 2 + 1);
      if (elem.isVar() && RefCounting.trackReadRefCount(elem.getVar())) {
        readIncr.add(VarCount.one(elem.getVar()));
      }
    }
    Var arr=getOutput(0);
    return Pair.create(readIncr,VarCount.one(arr).asList());
  }
case ASYNC_COPY:
case SYNC_COPY:
{
  List<VarCount> writeRefs;
  if (RefCounting.trackWriteRefCount(getOutput(0))) {
    writeRefs=VarCount.one(getOutput(0)).asList();
  }
 else {
    writeRefs=VarCount.NONE;
  }
  return Pair.create(VarCount.one(getInput(0).getVar()).asList(),writeRefs);
}
case STORE_BAG:
case STORE_ARRAY:
case STORE_STRUCT:
case STORE_RECURSIVE:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.one(getOutput(0)).asList());
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
}
case AREF_COPY_OUT_FUTURE:
case ARR_COPY_OUT_FUTURE:
{
return Pair.create(Arrays.asList(VarCount.one(getInput(0).getVar()),VarCount.one(getInput(1).getVar())),VarCount.NONE);
}
case AREF_COPY_OUT_IMM:
case ARR_COPY_OUT_IMM:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
}
case ARR_CONTAINS:
case CONTAINER_SIZE:
{
return Pair.create(VarCount.NONE,VarCount.NONE);
}
case ARR_RETRIEVE:
{
VarCount readDecr=new VarCount(getInput(0).getVar(),getInput(2).getIntLit());
return Pair.create(readDecr.asList(),VarCount.NONE);
}
case ARR_STORE:
{
Arg mem=getInput(1);
List<VarCount> readIncr;
if (mem.isVar() && RefCounting.trackReadRefCount(mem.getVar())) {
readIncr=VarCount.one(mem.getVar()).asList();
}
 else {
readIncr=VarCount.NONE;
}
return Pair.create(readIncr,VarCount.NONE);
}
case ARR_COPY_IN_IMM:
{
Var mem=getInput(1).getVar();
return Pair.create(VarCount.one(mem).asList(),VarCount.one(getOutput(0)).asList());
}
case ARR_STORE_FUTURE:
case ARR_COPY_IN_FUTURE:
{
Var arr=getInput(0).getVar();
Arg mem=getInput(1);
List<VarCount> readIncr;
if (mem.isVar() && RefCounting.trackReadRefCount(mem.getVar())) {
readIncr=Arrays.asList(VarCount.one(arr),VarCount.one(mem.getVar()));
}
 else {
readIncr=VarCount.one(arr).asList();
}
return Pair.create(readIncr,VarCount.one((getOutput(0))).asList());
}
case AREF_STORE_IMM:
case AREF_COPY_IN_IMM:
case AREF_STORE_FUTURE:
case AREF_COPY_IN_FUTURE:
{
Arg ix=getInput(0);
Arg mem=getInput(1);
Var arrayRef=getOutput(0);
List<VarCount> readers=new ArrayList<VarCount>(3);
readers.add(VarCount.one(arrayRef));
if (mem.isVar() && RefCounting.trackReadRefCount(mem.getVar())) {
readers.add(VarCount.one(mem.getVar()));
}
if (op == Opcode.AREF_STORE_FUTURE || op == Opcode.AREF_COPY_IN_FUTURE) {
readers.add(VarCount.one(ix.getVar()));
}
 else {
assert(op == Opcode.AREF_STORE_IMM || op == Opcode.AREF_COPY_IN_IMM);
}
return Pair.create(readers,VarCount.NONE);
}
case ARR_CREATE_NESTED_IMM:
{
long readDecr=getInput(2).getIntLit();
long writeDecr=getInput(4).getIntLit();
Var resultArr=getOutput(0);
return Pair.create(new VarCount(resultArr,readDecr).asList(),new VarCount(resultArr,writeDecr).asList());
}
case ARR_CREATE_NESTED_FUTURE:
{
Var srcArray=getOutput(1);
Var ix=getInput(0).getVar();
return Pair.create(VarCount.one(ix).asList(),VarCount.one(srcArray).asList());
}
case AREF_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
{
Var arr=getOutput(1);
Arg ixArg=getInput(0);
List<VarCount> readVars;
if (op == Opcode.AREF_CREATE_NESTED_IMM) {
readVars=VarCount.one(arr).asList();
}
 else {
assert(op == Opcode.AREF_CREATE_NESTED_FUTURE);
readVars=Arrays.asList(VarCount.one(arr),VarCount.one(ixArg.getVar()));
}
return Pair.create(readVars,VarCount.NONE);
}
case BAG_INSERT:
{
Arg mem=getInput(0);
List<VarCount> readers=VarCount.NONE;
if (mem.isVar() && RefCounting.trackReadRefCount(mem.getVar())) {
readers=VarCount.one(mem.getVar()).asList();
}
return Pair.create(readers,VarCount.NONE);
}
case STRUCT_INIT_FIELDS:
{
Out<List<Arg>> fieldVals=new Out<List<Arg>>();
unpackStructInitArgs(null,null,fieldVals);
List<VarCount> readIncr=new ArrayList<VarCount>();
List<VarCount> writeIncr=new ArrayList<VarCount>();
for (Arg fieldVal : fieldVals.val) {
if (fieldVal.isVar()) {
Var fieldVar=fieldVal.getVar();
if (RefCounting.trackReadRefCount(fieldVar)) {
readIncr.add(VarCount.one(fieldVar));
}
if (RefCounting.trackWriteRefCount(fieldVar)) {
writeIncr.add(VarCount.one(fieldVar));
}
}
}
return Pair.create(readIncr,writeIncr);
}
case STRUCTREF_COPY_OUT:
case STRUCT_COPY_OUT:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
}
case STRUCT_STORE_SUB:
case STRUCT_COPY_IN:
case STRUCTREF_STORE_SUB:
case STRUCTREF_COPY_IN:
return Pair.create(VarCount.NONE,VarCount.NONE);
case COPY_REF:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.one(getInput(0).getVar()).asList());
}
case COPY_IN_FILENAME:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
}
case UPDATE_INCR:
case UPDATE_MIN:
case UPDATE_SCALE:
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.one(getOutput(0)).asList());
default :
return Pair.create(VarCount.NONE,VarCount.NONE);
}
}","@Override public Pair<List<VarCount>,List<VarCount>> inRefCounts(Map<String,Function> functions){
switch (op) {
case STORE_REF:
    return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
case ARRAY_BUILD:
{
    List<VarCount> readIncr=new ArrayList<VarCount>();
    for (int i=0; i < getInputs().size() / 2; i++) {
      Arg elem=getInput(i * 2 + 1);
      if (elem.isVar() && RefCounting.trackReadRefCount(elem.getVar())) {
        readIncr.add(VarCount.one(elem.getVar()));
      }
    }
    Var arr=getOutput(0);
    return Pair.create(readIncr,VarCount.one(arr).asList());
  }
case ASYNC_COPY:
case SYNC_COPY:
{
  List<VarCount> writeRefs;
  if (RefCounting.trackWriteRefCount(getOutput(0))) {
    writeRefs=VarCount.one(getOutput(0)).asList();
  }
 else {
    writeRefs=VarCount.NONE;
  }
  return Pair.create(VarCount.one(getInput(0).getVar()).asList(),writeRefs);
}
case STORE_BAG:
case STORE_ARRAY:
case STORE_STRUCT:
case STORE_RECURSIVE:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.one(getOutput(0)).asList());
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
}
case AREF_COPY_OUT_FUTURE:
case ARR_COPY_OUT_FUTURE:
{
return Pair.create(Arrays.asList(VarCount.one(getInput(0).getVar()),VarCount.one(getInput(1).getVar())),VarCount.NONE);
}
case AREF_COPY_OUT_IMM:
case ARR_COPY_OUT_IMM:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
}
case ARR_CONTAINS:
case CONTAINER_SIZE:
{
return Pair.create(VarCount.NONE,VarCount.NONE);
}
case ARR_RETRIEVE:
{
VarCount readDecr=new VarCount(getInput(0).getVar(),getInput(2).getIntLit());
return Pair.create(readDecr.asList(),VarCount.NONE);
}
case ARR_STORE:
{
Arg mem=getInput(1);
List<VarCount> readIncr;
if (mem.isVar() && RefCounting.trackReadRefCount(mem.getVar())) {
readIncr=VarCount.one(mem.getVar()).asList();
}
 else {
readIncr=VarCount.NONE;
}
return Pair.create(readIncr,VarCount.NONE);
}
case ARR_COPY_IN_IMM:
{
Var mem=getInput(1).getVar();
return Pair.create(VarCount.one(mem).asList(),VarCount.one(getOutput(0)).asList());
}
case ARR_STORE_FUTURE:
case ARR_COPY_IN_FUTURE:
{
Var arr=getInput(0).getVar();
Arg mem=getInput(1);
List<VarCount> readIncr;
if (mem.isVar() && RefCounting.trackReadRefCount(mem.getVar())) {
readIncr=Arrays.asList(VarCount.one(arr),VarCount.one(mem.getVar()));
}
 else {
readIncr=VarCount.one(arr).asList();
}
return Pair.create(readIncr,VarCount.one((getOutput(0))).asList());
}
case AREF_STORE_IMM:
case AREF_COPY_IN_IMM:
case AREF_STORE_FUTURE:
case AREF_COPY_IN_FUTURE:
{
Arg ix=getInput(0);
Arg mem=getInput(1);
Var arrayRef=getOutput(0);
List<VarCount> readers=new ArrayList<VarCount>(3);
readers.add(VarCount.one(arrayRef));
if (mem.isVar() && RefCounting.trackReadRefCount(mem.getVar())) {
readers.add(VarCount.one(mem.getVar()));
}
if (op == Opcode.AREF_STORE_FUTURE || op == Opcode.AREF_COPY_IN_FUTURE) {
readers.add(VarCount.one(ix.getVar()));
}
 else {
assert(op == Opcode.AREF_STORE_IMM || op == Opcode.AREF_COPY_IN_IMM);
}
return Pair.create(readers,VarCount.NONE);
}
case ARR_CREATE_NESTED_IMM:
{
long readDecr=getInput(3).getIntLit();
long writeDecr=getInput(4).getIntLit();
Var arr=getOutput(1);
return Pair.create(new VarCount(arr,readDecr).asList(),new VarCount(arr,writeDecr).asList());
}
case ARR_CREATE_NESTED_FUTURE:
{
Var srcArray=getOutput(1);
Var ix=getInput(0).getVar();
return Pair.create(VarCount.one(ix).asList(),VarCount.one(srcArray).asList());
}
case AREF_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
{
Var arr=getOutput(1);
Arg ixArg=getInput(0);
List<VarCount> readVars;
if (op == Opcode.AREF_CREATE_NESTED_IMM) {
readVars=VarCount.one(arr).asList();
}
 else {
assert(op == Opcode.AREF_CREATE_NESTED_FUTURE);
readVars=Arrays.asList(VarCount.one(arr),VarCount.one(ixArg.getVar()));
}
return Pair.create(readVars,VarCount.NONE);
}
case BAG_INSERT:
{
Arg mem=getInput(0);
List<VarCount> readers=VarCount.NONE;
if (mem.isVar() && RefCounting.trackReadRefCount(mem.getVar())) {
readers=VarCount.one(mem.getVar()).asList();
}
return Pair.create(readers,VarCount.NONE);
}
case STRUCT_INIT_FIELDS:
{
Out<List<Arg>> fieldVals=new Out<List<Arg>>();
unpackStructInitArgs(null,null,fieldVals);
List<VarCount> readIncr=new ArrayList<VarCount>();
List<VarCount> writeIncr=new ArrayList<VarCount>();
for (Arg fieldVal : fieldVals.val) {
if (fieldVal.isVar()) {
Var fieldVar=fieldVal.getVar();
if (RefCounting.trackReadRefCount(fieldVar)) {
readIncr.add(VarCount.one(fieldVar));
}
if (RefCounting.trackWriteRefCount(fieldVar)) {
writeIncr.add(VarCount.one(fieldVar));
}
}
}
return Pair.create(readIncr,writeIncr);
}
case STRUCTREF_COPY_OUT:
case STRUCT_COPY_OUT:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
}
case STRUCT_STORE_SUB:
case STRUCT_COPY_IN:
case STRUCTREF_STORE_SUB:
case STRUCTREF_COPY_IN:
return Pair.create(VarCount.NONE,VarCount.NONE);
case COPY_REF:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.one(getInput(0).getVar()).asList());
}
case COPY_IN_FILENAME:
{
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.NONE);
}
case UPDATE_INCR:
case UPDATE_MIN:
case UPDATE_SCALE:
return Pair.create(VarCount.one(getInput(0).getVar()).asList(),VarCount.one(getOutput(0)).asList());
default :
return Pair.create(VarCount.NONE,VarCount.NONE);
}
}",0.9974184030979164
132771,"/** 
 * Add any inverse operations that can be directly inferred from a value that was just added
 * @param errContext
 * @param canonLoc
 * @param canonVal
 * @throws OptUnsafeError 
 */
private void addInverses(GlobalConstants consts,String errContext,Arg canonLoc,ArgOrCV canonVal,int stmtIndex) throws OptUnsafeError {
  if (canonVal.isCV() && canonVal.cv().inputs.size() == 1) {
    ComputedValue<Arg> cv=canonVal.cv();
    Arg invOutput=cv.getInput(0);
    if (cv.op().isAssign(false)) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveAssign()) {
      ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar(),true);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRetrieve(false)) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveRetrieve()) {
      Opcode invOp=Opcode.assignOpcode(invOutput.getVar(),true);
      assert(invOp != null);
      ArgCV invVal=new ArgCV(invOp,canonLoc.asList());
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
  }
 else   if (canonVal.isArg() && canonVal.arg().isVar() && canonVal.arg().getVar().storage() == Alloc.GLOBAL_CONST) {
    Var globalConst=canonVal.arg().getVar();
    Arg constVal=consts.lookupByVar(globalConst);
    assert(constVal != null);
    ArgCV invVal=ComputedValue.retrieveCompVal(globalConst,false);
    updateInv(consts,errContext,constVal,invVal,stmtIndex);
  }
}","/** 
 * Add any inverse operations that can be directly inferred from a value that was just added
 * @param errContext
 * @param canonLoc
 * @param canonVal
 * @throws OptUnsafeError 
 */
private void addInverses(GlobalConstants consts,String errContext,Arg canonLoc,ArgOrCV canonVal,int stmtIndex) throws OptUnsafeError {
  if (canonVal.isCV() && canonVal.cv().inputs.size() == 1) {
    ComputedValue<Arg> cv=canonVal.cv();
    Arg invOutput=cv.getInput(0);
    if (cv.op().isAssign(false)) {
      ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveAssign()) {
      ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar(),true);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRetrieve(false)) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveRetrieve()) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),true);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
  }
 else   if (canonVal.isArg() && canonVal.arg().isVar() && canonVal.arg().getVar().storage() == Alloc.GLOBAL_CONST) {
    Var globalConst=canonVal.arg().getVar();
    Arg constVal=consts.lookupByVar(globalConst);
    assert(constVal != null);
    ArgCV invVal=ComputedValue.retrieveCompVal(globalConst,false);
    updateInv(consts,errContext,constVal,invVal,stmtIndex);
  }
}",0.7299757281553398
132772,"public static ValLoc assignValLoc(Var dst,Arg val,IsAssign isAssign,boolean recursive){
  ArgCV cv=assignCompVal(dst,recursive);
  if (cv == null)   return null;
  Closed closed=recursive ? Closed.YES_RECURSIVE : Closed.YES_NOT_RECURSIVE;
  return ValLoc.build(cv,dst.asArg(),closed,isAssign);
}","public static ValLoc assignValLoc(Var dst,Arg val,IsAssign isAssign,boolean recursive){
  ArgCV cv=assignCompVal(val,recursive);
  if (cv == null)   return null;
  Closed closed=recursive ? Closed.YES_RECURSIVE : Closed.YES_NOT_RECURSIVE;
  return ValLoc.build(cv,dst.asArg(),closed,isAssign);
}",0.9898305084745764
132773,"public static ArgCV assignCompVal(Var dst,boolean recursive){
  Type dstType=dst.type();
  if (Types.isPrimValue(dstType)) {
    BuiltinOpcode op;
switch (dstType.primType()) {
case BOOL:
      op=BuiltinOpcode.COPY_BOOL;
    break;
case INT:
  op=BuiltinOpcode.COPY_INT;
break;
case FLOAT:
op=BuiltinOpcode.COPY_FLOAT;
break;
case STRING:
op=BuiltinOpcode.COPY_STRING;
break;
case BLOB:
op=BuiltinOpcode.COPY_BLOB;
break;
case VOID:
op=BuiltinOpcode.COPY_VOID;
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + dstType);
}
return new ArgCV(Opcode.LOCAL_OP,op,dst.asArg().asList());
}
 else {
Opcode op=Opcode.assignOpcode(dstType,recursive);
if (op != null) {
return new ArgCV(op,dst.asArg().asList());
}
}
throw new STCRuntimeError(""String_Node_Str"" + dst + (recursive ? ""String_Node_Str"" : ""String_Node_Str""));
}","public static ArgCV assignCompVal(Arg dst,boolean recursive){
  Type dstType=dst.type();
  if (Types.isPrimValue(dstType)) {
    BuiltinOpcode op;
switch (dstType.primType()) {
case BOOL:
      op=BuiltinOpcode.COPY_BOOL;
    break;
case INT:
  op=BuiltinOpcode.COPY_INT;
break;
case FLOAT:
op=BuiltinOpcode.COPY_FLOAT;
break;
case STRING:
op=BuiltinOpcode.COPY_STRING;
break;
case BLOB:
op=BuiltinOpcode.COPY_BLOB;
break;
case VOID:
op=BuiltinOpcode.COPY_VOID;
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + dstType);
}
return new ArgCV(Opcode.LOCAL_OP,op,dst.asList());
}
 else {
Opcode op=Opcode.assignOpcode(dstType,recursive);
if (op != null) {
return new ArgCV(op,dst.asList());
}
}
throw new STCRuntimeError(""String_Node_Str"" + dst + (recursive ? ""String_Node_Str"" : ""String_Node_Str""));
}",0.9865853658536584
132774,"/** 
 * Add any inverse operations that can be directly inferred from a value that was just added
 * @param errContext
 * @param canonLoc
 * @param canonVal
 * @throws OptUnsafeError 
 */
private void addInverses(GlobalConstants consts,String errContext,Arg canonLoc,ArgOrCV canonVal,int stmtIndex) throws OptUnsafeError {
  if (canonVal.isCV() && canonVal.cv().inputs.size() == 1) {
    ComputedValue<Arg> cv=canonVal.cv();
    Arg invOutput=cv.getInput(0);
    if (cv.op().isAssign(false)) {
      ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveAssign()) {
      ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar(),true);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRetrieve(false)) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveRetrieve()) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),true);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
  }
 else   if (canonVal.isArg() && canonVal.arg().isVar() && canonVal.arg().getVar().storage() == Alloc.GLOBAL_CONST) {
    Var globalConst=canonVal.arg().getVar();
    Arg constVal=consts.lookupByVar(globalConst);
    assert(constVal != null);
    ArgCV invVal=ComputedValue.retrieveCompVal(globalConst,false);
    updateInv(consts,errContext,constVal,invVal,stmtIndex);
  }
}","/** 
 * Add any inverse operations that can be directly inferred from a value that was just added
 * @param errContext
 * @param canonLoc
 * @param canonVal
 * @throws OptUnsafeError 
 */
private void addInverses(GlobalConstants consts,String errContext,Arg canonLoc,ArgOrCV canonVal,int stmtIndex) throws OptUnsafeError {
}",0.3296032553407935
132775,"public static ArgCV assignCompVal(Var dst,boolean recursive){
  Type dstType=dst.type();
  if (Types.isPrimValue(dstType)) {
    BuiltinOpcode op;
switch (dstType.primType()) {
case BOOL:
      op=BuiltinOpcode.COPY_BOOL;
    break;
case INT:
  op=BuiltinOpcode.COPY_INT;
break;
case FLOAT:
op=BuiltinOpcode.COPY_FLOAT;
break;
case STRING:
op=BuiltinOpcode.COPY_STRING;
break;
case BLOB:
op=BuiltinOpcode.COPY_BLOB;
break;
case VOID:
op=BuiltinOpcode.COPY_VOID;
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + dstType);
}
return new ArgCV(Opcode.LOCAL_OP,op,dst.asArg().asList());
}
 else {
Opcode op=Opcode.assignOpcode(dstType,recursive);
if (op != null) {
return new ArgCV(op,dst.asArg().asList());
}
}
throw new STCRuntimeError(""String_Node_Str"" + dst);
}","public static ArgCV assignCompVal(Var dst,boolean recursive){
  Type dstType=dst.type();
  if (Types.isPrimValue(dstType)) {
    BuiltinOpcode op;
switch (dstType.primType()) {
case BOOL:
      op=BuiltinOpcode.COPY_BOOL;
    break;
case INT:
  op=BuiltinOpcode.COPY_INT;
break;
case FLOAT:
op=BuiltinOpcode.COPY_FLOAT;
break;
case STRING:
op=BuiltinOpcode.COPY_STRING;
break;
case BLOB:
op=BuiltinOpcode.COPY_BLOB;
break;
case VOID:
op=BuiltinOpcode.COPY_VOID;
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + dstType);
}
return new ArgCV(Opcode.LOCAL_OP,op,dst.asArg().asList());
}
 else {
Opcode op=Opcode.assignOpcode(dstType,recursive);
if (op != null) {
return new ArgCV(op,dst.asArg().asList());
}
}
throw new STCRuntimeError(""String_Node_Str"" + dst + (recursive ? ""String_Node_Str"" : ""String_Node_Str""));
}",0.9662921348314608
132776,"/** 
 * Add any inverse operations that can be directly inferred from a value that was just added
 * @param errContext
 * @param canonLoc
 * @param canonVal
 * @throws OptUnsafeError 
 */
private void addInverses(GlobalConstants consts,String errContext,Arg canonLoc,ArgOrCV canonVal,int stmtIndex) throws OptUnsafeError {
  if (canonVal.isCV() && canonVal.cv().inputs.size() == 1) {
    ComputedValue<Arg> cv=canonVal.cv();
    Arg invOutput=cv.getInput(0);
    if (cv.op().isAssign(false)) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),true);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveAssign()) {
      ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar(),true);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRetrieve(false)) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveRetrieve()) {
      Opcode invOp=Opcode.assignOpcode(invOutput.getVar(),true);
      assert(invOp != null);
      ArgCV invVal=new ArgCV(invOp,canonLoc.asList());
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
  }
 else   if (canonVal.isArg() && canonVal.arg().isVar() && canonVal.arg().getVar().storage() == Alloc.GLOBAL_CONST) {
    Var globalConst=canonVal.arg().getVar();
    Arg constVal=consts.lookupByVar(globalConst);
    assert(constVal != null);
    ArgCV invVal=ComputedValue.retrieveCompVal(globalConst,false);
    updateInv(consts,errContext,constVal,invVal,stmtIndex);
  }
}","/** 
 * Add any inverse operations that can be directly inferred from a value that was just added
 * @param errContext
 * @param canonLoc
 * @param canonVal
 * @throws OptUnsafeError 
 */
private void addInverses(GlobalConstants consts,String errContext,Arg canonLoc,ArgOrCV canonVal,int stmtIndex) throws OptUnsafeError {
  if (canonVal.isCV() && canonVal.cv().inputs.size() == 1) {
    ComputedValue<Arg> cv=canonVal.cv();
    Arg invOutput=cv.getInput(0);
    if (cv.op().isAssign(false)) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveAssign()) {
      ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar(),true);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRetrieve(false)) {
      ArgCV invVal=ComputedValue.assignCompVal(invOutput.getVar(),false);
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
 else     if (cv.op().isRecursiveRetrieve()) {
      Opcode invOp=Opcode.assignOpcode(invOutput.getVar(),true);
      assert(invOp != null);
      ArgCV invVal=new ArgCV(invOp,canonLoc.asList());
      updateInv(consts,errContext,invOutput,invVal,stmtIndex);
    }
  }
 else   if (canonVal.isArg() && canonVal.arg().isVar() && canonVal.arg().getVar().storage() == Alloc.GLOBAL_CONST) {
    Var globalConst=canonVal.arg().getVar();
    Arg constVal=consts.lookupByVar(globalConst);
    assert(constVal != null);
    ArgCV invVal=ComputedValue.retrieveCompVal(globalConst,false);
    updateInv(consts,errContext,constVal,invVal,stmtIndex);
  }
}",0.9978832778953736
132777,"/** 
 * Return list of ancestor datums
 * @param var
 * @return list of (ancestorKey, if part of separate structure) 
 */
public List<Pair<AliasKey,Boolean>> getAncestors(Var var){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + var + ""String_Node_Str"");
  }
  List<Pair<AliasKey,Boolean>> results=new ArrayList<Pair<AliasKey,Boolean>>();
  AliasKey canonKey=getCanonical(var);
  logger.trace(""String_Node_Str"" + canonKey);
  boolean traversedDeref=false;
  for (int i=canonKey.pathLength() - 1; i >= 0; i--) {
    AliasKey prefix=canonKey.prefix(i);
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + prefix + ""String_Node_Str""+ traversedDeref+ ""String_Node_Str"");
    }
    results.add(Pair.create(prefix,traversedDeref));
    if (canonKey.path[i] != null && canonKey.path[i].equals(Alias.DEREF_MARKER)) {
      traversedDeref=true;
    }
  }
  return results;
}","/** 
 * Return list of ancestor datums
 * @param var
 * @return list of (ancestorKey, if part of separate structure) 
 */
public List<Pair<AliasKey,Boolean>> getAncestors(Var var){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + var + ""String_Node_Str"");
  }
  List<Pair<AliasKey,Boolean>> results=new ArrayList<Pair<AliasKey,Boolean>>();
  AliasKey currKey=getCanonical(var);
  boolean traversedDeref=false;
  while (currKey != null) {
    logger.trace(""String_Node_Str"" + currKey);
    traversedDeref=addAncestors(results,currKey,traversedDeref);
    AliasKey next=getCanonical(currKey.var);
    if (next.pathLength() > 0) {
      currKey=next;
    }
 else {
      currKey=null;
    }
  }
  return results;
}",0.5947592931139549
132778,"public long getIntLit(){
  if (kind == ArgKind.INTVAL) {
    return intlit;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind);
  }
}","public long getIntLit(){
  if (kind == ArgKind.INTVAL) {
    return intlit;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind + ""String_Node_Str""+ this);
  }
}",0.9202453987730062
132779,"public String getStringLit(){
  if (kind == ArgKind.STRINGVAL) {
    return stringlit;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind);
  }
}","public String getStringLit(){
  if (kind == ArgKind.STRINGVAL) {
    return stringlit;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind + ""String_Node_Str""+ this);
  }
}",0.9252873563218392
132780,"public double getFloatLit(){
  if (kind == ArgKind.FLOATVAL) {
    return floatlit;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind);
  }
}","public double getFloatLit(){
  if (kind == ArgKind.FLOATVAL) {
    return floatlit;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind + ""String_Node_Str""+ this);
  }
}",0.9239766081871345
132781,"public Var getVar(){
  if (kind == ArgKind.VAR) {
    return var;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind);
  }
}","public Var getVar(){
  if (kind == ArgKind.VAR) {
    return var;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind + ""String_Node_Str""+ this);
  }
}",0.915032679738562
132782,"public boolean getBoolLit(){
  if (kind == ArgKind.BOOLVAL) {
    return boollit;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind);
  }
}","public boolean getBoolLit(){
  if (kind == ArgKind.BOOLVAL) {
    return boollit;
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + kind + ""String_Node_Str""+ this);
  }
}",0.9230769230769232
132783,"/** 
 * Helper to convert struct fields to input list
 * @param struct
 * @param fieldNames
 * @return
 */
private static List<Arg> structFieldInputs(Var struct,List<Arg> fieldNames){
  List<Arg> inputs=new ArrayList<Arg>(fieldNames.size() + 1);
  inputs.add(struct.asArg());
  for (  Arg fieldName : fieldNames) {
    assert(fieldName.isStringVal());
    inputs.add(fieldName);
  }
  return inputs;
}","/** 
 * Helper to convert struct fields to input list
 * @param struct
 * @param fieldNames
 * @return
 */
private static List<Arg> structFieldInputs(Var struct,List<Arg> fieldNames){
}",0.6313993174061433
132784,"@Override public List<Alias> getAliases(){
switch (this.op) {
case STRUCT_CREATE_ALIAS:
    return Alias.makeStructAliases2(getInput(0).getVar(),getInputsTail(1),getOutput(0),AliasTransform.IDENTITY);
case STRUCT_INIT_FIELDS:
{
    Out<List<List<String>>> fieldPaths=new Out<List<List<String>>>();
    Out<List<Arg>> fieldVals=new Out<List<Arg>>();
    List<Alias> aliases=new ArrayList<Alias>();
    unpackStructInitArgs(fieldPaths,null,fieldVals);
    assert(fieldPaths.val.size() == fieldVals.val.size());
    for (int i=0; i < fieldPaths.val.size(); i++) {
      List<String> fieldPath=fieldPaths.val.get(i);
      Arg fieldVal=fieldVals.val.get(i);
      if (fieldVal.isVar()) {
        aliases.addAll(Alias.makeStructAliases(getOutput(0),fieldPath,fieldVal.getVar(),AliasTransform.RETRIEVE));
      }
    }
    return aliases;
  }
case STRUCT_RETRIEVE_SUB:
return Alias.makeStructAliases2(getInput(0).getVar(),getInputsTail(2),getOutput(0),AliasTransform.RETRIEVE);
case STRUCT_STORE_SUB:
return Alias.makeStructAliases2(getOutput(0),getInputsTail(1),getInput(0).getVar(),AliasTransform.RETRIEVE);
case STRUCT_COPY_OUT:
return Alias.makeStructAliases2(getInput(0).getVar(),getInputsTail(1),getOutput(0),AliasTransform.COPY);
case STRUCT_COPY_IN:
return Alias.makeStructAliases2(getOutput(0),getInputsTail(1),getInput(0).getVar(),AliasTransform.COPY);
case STORE_REF:
{
Var ref=getOutput(0);
Var val=getInput(0).getVar();
return new Alias(ref,Collections.<String>emptyList(),AliasTransform.RETRIEVE,val).asList();
}
case LOAD_REF:
{
Var val=getOutput(0);
Var ref=getInput(0).getVar();
return new Alias(ref,Collections.<String>emptyList(),AliasTransform.RETRIEVE,val).asList();
}
case COPY_REF:
{
Var ref1=getOutput(0);
Var ref2=getInput(0).getVar();
return Arrays.asList(new Alias(ref1,Collections.<String>emptyList(),AliasTransform.COPY,ref2),new Alias(ref2,Collections.<String>emptyList(),AliasTransform.COPY,ref1));
}
case GET_FILENAME_ALIAS:
{
return new Alias(getInput(0).getVar(),Alias.FILENAME_PATH,AliasTransform.IDENTITY,getOutput(0)).asList();
}
default :
break;
}
return Alias.NONE;
}","@Override public List<Alias> getAliases(){
switch (this.op) {
case STRUCT_CREATE_ALIAS:
    return Alias.makeStructAliases2(getInput(0).getVar(),getInputsTail(1),getOutput(0),AliasTransform.IDENTITY);
case STRUCT_INIT_FIELDS:
{
    Out<List<List<String>>> fieldPaths=new Out<List<List<String>>>();
    Out<List<Arg>> fieldVals=new Out<List<Arg>>();
    List<Alias> aliases=new ArrayList<Alias>();
    unpackStructInitArgs(fieldPaths,null,fieldVals);
    assert(fieldPaths.val.size() == fieldVals.val.size());
    for (int i=0; i < fieldPaths.val.size(); i++) {
      List<String> fieldPath=fieldPaths.val.get(i);
      Arg fieldVal=fieldVals.val.get(i);
      if (fieldVal.isVar()) {
        aliases.addAll(Alias.makeStructAliases(getOutput(0),fieldPath,fieldVal.getVar(),AliasTransform.RETRIEVE));
      }
    }
    return aliases;
  }
case STRUCT_RETRIEVE_SUB:
return Alias.makeStructAliases2(getInput(0).getVar(),getInputsTail(2),getOutput(0),AliasTransform.RETRIEVE);
case STRUCT_STORE_SUB:
if (getInput(0).isVar()) {
return Alias.makeStructAliases2(getOutput(0),getInputsTail(1),getInput(0).getVar(),AliasTransform.RETRIEVE);
}
break;
case STRUCT_COPY_OUT:
return Alias.makeStructAliases2(getInput(0).getVar(),getInputsTail(1),getOutput(0),AliasTransform.COPY);
case STRUCT_COPY_IN:
return Alias.makeStructAliases2(getOutput(0),getInputsTail(1),getInput(0).getVar(),AliasTransform.COPY);
case STORE_REF:
{
Var ref=getOutput(0);
Var val=getInput(0).getVar();
return new Alias(ref,Collections.<String>emptyList(),AliasTransform.RETRIEVE,val).asList();
}
case LOAD_REF:
{
Var val=getOutput(0);
Var ref=getInput(0).getVar();
return new Alias(ref,Collections.<String>emptyList(),AliasTransform.RETRIEVE,val).asList();
}
case COPY_REF:
{
Var ref1=getOutput(0);
Var ref2=getInput(0).getVar();
return Arrays.asList(new Alias(ref1,Collections.<String>emptyList(),AliasTransform.COPY,ref2),new Alias(ref2,Collections.<String>emptyList(),AliasTransform.COPY,ref1));
}
case GET_FILENAME_ALIAS:
{
return new Alias(getInput(0).getVar(),Alias.FILENAME_PATH,AliasTransform.IDENTITY,getOutput(0)).asList();
}
default :
break;
}
return Alias.NONE;
}",0.9915014164305948
132785,"@Override public List<ValLoc> getResults(){
switch (op) {
case LOAD_SCALAR:
case LOAD_REF:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_STRUCT:
case LOAD_RECURSIVE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      Closed outIsClosed;
      if (op == Opcode.LOAD_REF) {
        outIsClosed=Closed.MAYBE_NOT;
      }
 else {
        outIsClosed=Closed.YES_NOT_RECURSIVE;
      }
      if (op == Opcode.LOAD_REF) {
        return ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO,IsAssign.NO).asList();
      }
 else {
        return vanillaResult(outIsClosed,IsAssign.NO).asList();
      }
    }
case STORE_REF:
case STORE_SCALAR:
case STORE_FILE:
case STORE_ARRAY:
case STORE_BAG:
case STORE_STRUCT:
case STORE_RECURSIVE:
{
    ValLoc assign=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
    Arg dst=getOutput(0).asArg();
    Arg src=getInput(0);
    if (op == Opcode.STORE_REF) {
      ValLoc retrieve=ValLoc.derefCompVal(src.getVar(),dst.getVar(),IsValCopy.NO,IsAssign.NO);
      return Arrays.asList(retrieve,assign);
    }
 else {
      return assign.asList();
    }
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result,IsAssign.NO));
  }
}
case GET_FILENAME_ALIAS:
{
Arg filename=getOutput(0).asArg();
Var file=getInput(0).getVar();
return ValLoc.makeFilename(filename,file).asList();
}
case COPY_IN_FILENAME:
{
Arg filename=getInput(0);
Var file=getOutput(0);
return ValLoc.makeFilename(filename,file).asList();
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar(),IsAssign.TO_LOCATION).asList();
}
case SET_FILENAME_VAL:
{
Var file=getOutput(0);
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val,IsAssign.TO_VALUE).asList();
}
case GET_FILENAME_VAL:
{
Var file=getInput(0).getVar();
Var val=getOutput(0);
return ValLoc.makeFilenameVal(file,val.asArg(),IsAssign.NO).asList();
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES,IsAssign.NO).asList();
}
case STRUCT_CREATE_ALIAS:
case STRUCT_COPY_OUT:
case STRUCTREF_COPY_OUT:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
ValLoc copyV=ValLoc.makeStructFieldCopyResult(getOutput(0),struct,fields);
if (op == Opcode.STRUCT_CREATE_ALIAS) {
ValLoc aliasV=ValLoc.makeStructFieldAliasResult(getOutput(0),struct,fields);
return Arrays.asList(aliasV,copyV);
}
 else {
return copyV.asList();
}
}
case STRUCT_RETRIEVE_SUB:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(2);
return ValLoc.makeStructFieldValResult(getOutput(0).asArg(),struct,fields).asList();
}
case STRUCT_INIT_FIELDS:
{
List<ValLoc> results=new ArrayList<ValLoc>();
Out<List<List<Arg>>> fieldPaths=new Out<List<List<Arg>>>();
Out<List<Arg>> fieldVals=new Out<List<Arg>>();
unpackStructInitArgs(null,fieldPaths,fieldVals);
Var struct=getOutput(0);
assert(fieldPaths.val.size() == fieldVals.val.size());
for (int i=0; i < fieldPaths.val.size(); i++) {
results.add(ValLoc.makeStructFieldValResult(fieldVals.val.get(i),struct,fieldPaths.val.get(i)));
}
return results;
}
case STRUCT_STORE_SUB:
case STRUCTREF_STORE_SUB:
{
Var struct=getOutput(0);
Arg val=getInput(0);
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldValResult(val,struct,fields).asList();
}
case STRUCT_COPY_IN:
case STRUCTREF_COPY_IN:
{
Var struct=getOutput(0);
Var val=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldCopyResult(val,struct,fields).asList();
}
case ARR_STORE:
case ARR_STORE_FUTURE:
case AREF_STORE_IMM:
case AREF_STORE_FUTURE:
case ARR_COPY_IN_IMM:
case ARR_COPY_IN_FUTURE:
case AREF_COPY_IN_IMM:
case AREF_COPY_IN_FUTURE:
{
Var arr;
arr=getOutput(0);
Arg ix=getInput(0);
Arg member=getInput(1);
boolean insertingVal=isArrayValStore(op);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingVal,IsAssign.TO_VALUE));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE,IsAssign.NO));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Arg val=getInput(2 * i + 1);
res.add(ValLoc.makeArrayResult(arr,key,val,true,IsAssign.TO_VALUE));
}
res.add(ValLoc.makeContainerSizeCV(arr,Arg.createIntLit(elemCount),false,IsAssign.NO));
return res;
}
case ARR_RETRIEVE:
case ARR_COPY_OUT_IMM:
case ARR_COPY_OUT_FUTURE:
case AREF_COPY_OUT_FUTURE:
case AREF_COPY_OUT_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARR_RETRIEVE) {
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),true,IsAssign.NO).asList();
}
 else {
assert(Types.isElemType(arr,contents));
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.TO_LOCATION).asList();
}
}
case ARR_CREATE_NESTED_FUTURE:
case ARR_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case ARR_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr=getOutput(1);
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsNonRef=op == Opcode.ARR_CREATE_NESTED_IMM || op == Opcode.ARR_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr.asArg(),returnsNonRef,IsAssign.NO));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsNonRef));
return res;
}
case SYNC_COPY_CONTAINER:
case ASYNC_COPY_CONTAINER:
case SYNC_COPY_STRUCT:
case ASYNC_COPY_STRUCT:
{
return ValLoc.makeCopy(getOutput(0),getInput(0),IsAssign.TO_LOCATION).asList();
}
case COPY_REF:
{
Var srcRef=getInput(0).getVar();
return ValLoc.makeAlias(getOutput(0),srcRef).asList();
}
case LOOKUP_CHECKPOINT:
case UNPACK_VALUES:
{
List<ValLoc> res=new ArrayList<ValLoc>(outputs.size());
for (int i=0; i < outputs.size(); i++) {
Var out=outputs.get(i);
res.add(ValLoc.buildResult(op,(Object)i,getInput(0).asList(),out.asArg(),Closed.YES_RECURSIVE,IsValCopy.NO,IsAssign.NO));
}
return res;
}
case PACK_VALUES:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case CHECKPOINT_LOOKUP_ENABLED:
case CHECKPOINT_WRITE_ENABLED:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case UNPACK_ARRAY_TO_FLAT:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case ARR_CONTAINS:
case CONTAINER_SIZE:
case ARR_LOCAL_CONTAINS:
case CONTAINER_LOCAL_SIZE:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
default :
return null;
}
}","@Override public List<ValLoc> getResults(){
switch (op) {
case LOAD_SCALAR:
case LOAD_REF:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_STRUCT:
case LOAD_RECURSIVE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      Closed outIsClosed;
      if (op == Opcode.LOAD_REF) {
        outIsClosed=Closed.MAYBE_NOT;
      }
 else {
        outIsClosed=Closed.YES_NOT_RECURSIVE;
      }
      if (op == Opcode.LOAD_REF) {
        return ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO,IsAssign.NO).asList();
      }
 else {
        return vanillaResult(outIsClosed,IsAssign.NO).asList();
      }
    }
case STORE_REF:
case STORE_SCALAR:
case STORE_FILE:
case STORE_ARRAY:
case STORE_BAG:
case STORE_STRUCT:
case STORE_RECURSIVE:
{
    ValLoc assign=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
    Arg dst=getOutput(0).asArg();
    Arg src=getInput(0);
    if (op == Opcode.STORE_REF) {
      ValLoc retrieve=ValLoc.derefCompVal(src.getVar(),dst.getVar(),IsValCopy.NO,IsAssign.NO);
      return Arrays.asList(retrieve,assign);
    }
 else {
      return assign.asList();
    }
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result,IsAssign.NO));
  }
}
case GET_FILENAME_ALIAS:
{
Arg filename=getOutput(0).asArg();
Var file=getInput(0).getVar();
return ValLoc.makeFilename(filename,file).asList();
}
case COPY_IN_FILENAME:
{
Arg filename=getInput(0);
Var file=getOutput(0);
return ValLoc.makeFilename(filename,file).asList();
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar(),IsAssign.TO_LOCATION).asList();
}
case SET_FILENAME_VAL:
{
Var file=getOutput(0);
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val,IsAssign.TO_VALUE).asList();
}
case GET_FILENAME_VAL:
{
Var file=getInput(0).getVar();
Var val=getOutput(0);
return ValLoc.makeFilenameVal(file,val.asArg(),IsAssign.NO).asList();
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES,IsAssign.NO).asList();
}
case STRUCT_CREATE_ALIAS:
case STRUCT_COPY_OUT:
case STRUCTREF_COPY_OUT:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
ValLoc copyV=ValLoc.makeStructFieldCopyResult(getOutput(0),struct,fields);
if (op == Opcode.STRUCT_CREATE_ALIAS) {
ValLoc aliasV=ValLoc.makeStructFieldAliasResult(getOutput(0),struct,fields);
return Arrays.asList(aliasV,copyV);
}
 else {
return copyV.asList();
}
}
case STRUCT_RETRIEVE_SUB:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(2);
return ValLoc.makeStructFieldValResult(getOutput(0).asArg(),struct,fields).asList();
}
case STRUCT_INIT_FIELDS:
{
List<ValLoc> results=new ArrayList<ValLoc>();
Out<List<List<Arg>>> fieldPaths=new Out<List<List<Arg>>>();
Out<List<Arg>> fieldVals=new Out<List<Arg>>();
unpackStructInitArgs(null,fieldPaths,fieldVals);
Var struct=getOutput(0);
assert(fieldPaths.val.size() == fieldVals.val.size());
for (int i=0; i < fieldPaths.val.size(); i++) {
results.add(ValLoc.makeStructFieldValResult(fieldVals.val.get(i),struct,fieldPaths.val.get(i)));
}
return results;
}
case STRUCT_STORE_SUB:
case STRUCTREF_STORE_SUB:
{
Var struct=getOutput(0);
Arg val=getInput(0);
List<Arg> fields;
if (op == Opcode.STRUCT_STORE_SUB) {
fields=getInputsTail(1);
}
 else {
assert(op == Opcode.STRUCTREF_STORE_SUB);
fields=getInputsTail(1);
}
return ValLoc.makeStructFieldValResult(val,struct,fields).asList();
}
case STRUCT_COPY_IN:
case STRUCTREF_COPY_IN:
{
Var struct=getOutput(0);
Var val=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldCopyResult(val,struct,fields).asList();
}
case ARR_STORE:
case ARR_STORE_FUTURE:
case AREF_STORE_IMM:
case AREF_STORE_FUTURE:
case ARR_COPY_IN_IMM:
case ARR_COPY_IN_FUTURE:
case AREF_COPY_IN_IMM:
case AREF_COPY_IN_FUTURE:
{
Var arr;
arr=getOutput(0);
Arg ix=getInput(0);
Arg member=getInput(1);
boolean insertingVal=isArrayValStore(op);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingVal,IsAssign.TO_VALUE));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE,IsAssign.NO));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Arg val=getInput(2 * i + 1);
res.add(ValLoc.makeArrayResult(arr,key,val,true,IsAssign.TO_VALUE));
}
res.add(ValLoc.makeContainerSizeCV(arr,Arg.createIntLit(elemCount),false,IsAssign.NO));
return res;
}
case ARR_RETRIEVE:
case ARR_COPY_OUT_IMM:
case ARR_COPY_OUT_FUTURE:
case AREF_COPY_OUT_FUTURE:
case AREF_COPY_OUT_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARR_RETRIEVE) {
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),true,IsAssign.NO).asList();
}
 else {
assert(Types.isElemType(arr,contents));
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.TO_LOCATION).asList();
}
}
case ARR_CREATE_NESTED_FUTURE:
case ARR_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case ARR_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr=getOutput(1);
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsNonRef=op == Opcode.ARR_CREATE_NESTED_IMM || op == Opcode.ARR_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr.asArg(),returnsNonRef,IsAssign.NO));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsNonRef));
return res;
}
case SYNC_COPY_CONTAINER:
case ASYNC_COPY_CONTAINER:
case SYNC_COPY_STRUCT:
case ASYNC_COPY_STRUCT:
{
return ValLoc.makeCopy(getOutput(0),getInput(0),IsAssign.TO_LOCATION).asList();
}
case COPY_REF:
{
Var srcRef=getInput(0).getVar();
return ValLoc.makeAlias(getOutput(0),srcRef).asList();
}
case LOOKUP_CHECKPOINT:
case UNPACK_VALUES:
{
List<ValLoc> res=new ArrayList<ValLoc>(outputs.size());
for (int i=0; i < outputs.size(); i++) {
Var out=outputs.get(i);
res.add(ValLoc.buildResult(op,(Object)i,getInput(0).asList(),out.asArg(),Closed.YES_RECURSIVE,IsValCopy.NO,IsAssign.NO));
}
return res;
}
case PACK_VALUES:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case CHECKPOINT_LOOKUP_ENABLED:
case CHECKPOINT_WRITE_ENABLED:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case UNPACK_ARRAY_TO_FLAT:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case ARR_CONTAINS:
case CONTAINER_SIZE:
case ARR_LOCAL_CONTAINS:
case CONTAINER_LOCAL_SIZE:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
default :
return null;
}
}",0.9909805062554552
132786,"/** 
 * try to reduce to a simpler form of wait
 * @param currContext
 * @param toInline
 * @param newContext
 * @param wait
 */
private boolean tryReduce(Logger logger,Function fn,ExecContext currContext,ExecContext newContext,WaitStatement wait){
  if ((currContext == newContext && ProgressOpcodes.isCheap(wait.getBlock())) || (currContext == ExecContext.WORKER && newContext == ExecContext.CONTROL && canSwitchControlToWorker(logger,fn,wait))) {
    if (wait.getWaitVars().isEmpty()) {
      return true;
    }
 else {
      if (wait.getTarget() == TaskMode.CONTROL) {
        if (currContext == ExecContext.CONTROL) {
          wait.setTarget(TaskMode.LOCAL_CONTROL);
        }
 else {
          wait.setTarget(TaskMode.LOCAL);
        }
      }
      if (wait.getMode() == WaitMode.TASK_DISPATCH) {
        wait.setMode(WaitMode.WAIT_ONLY);
      }
    }
  }
  return false;
}","/** 
 * try to reduce to a simpler form of wait
 * @param currContext
 * @param toInline
 * @param innerContext
 * @param wait
 * @return true if it should be inlined
 */
private boolean tryReduce(Logger logger,Function fn,ExecContext currContext,ExecContext innerContext,WaitStatement wait){
  if ((currContext == innerContext && ProgressOpcodes.isCheap(wait.getBlock())) || (currContext == ExecContext.WORKER && innerContext == ExecContext.CONTROL && canSwitchControlToWorker(logger,fn,wait))) {
    if (currContext == ExecContext.WORKER && innerContext == ExecContext.CONTROL) {
      replaceLocalControl(wait.getBlock());
    }
    if (wait.getWaitVars().isEmpty()) {
      return true;
    }
 else {
      if (wait.getTarget() == TaskMode.CONTROL) {
        if (currContext == ExecContext.CONTROL) {
          wait.setTarget(TaskMode.LOCAL_CONTROL);
        }
 else {
          wait.setTarget(TaskMode.LOCAL);
        }
      }
      if (wait.getMode() == WaitMode.TASK_DISPATCH) {
        wait.setMode(WaitMode.WAIT_ONLY);
      }
    }
  }
  return false;
}",0.894141829393628
132787,"public HoistTracking getAncestor(int links){
  HoistTracking curr=this;
  for (int i=0; i < links; i++) {
    curr=this.parent;
  }
  return curr;
}","public HoistTracking getAncestor(int links){
  Logger logger=Logging.getSTCLogger();
  HoistTracking curr=this;
  for (int i=0; i < links; i++) {
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + this.block.getType() + ""String_Node_Str""+ this.parent.block.getType());
    }
    curr=curr.parent;
  }
  return curr;
}",0.5901639344262295
132788,"/** 
 * Relocate output variables to target block
 * @param source
 * @param target
 * @param inst
 */
private static void relocateVarDefs(HoistTracking source,HoistTracking target,Instruction inst){
  assert(source != target);
  ListIterator<Var> varIt=source.block.variableIterator();
  while (varIt.hasNext()) {
    Var def=varIt.next();
    for (    Var out : inst.getOutputs()) {
      if (def.equals(out)) {
        varIt.remove();
        target.block.addVariable(def);
        moveVarCleanupAction(out,source.block,target.block);
        if (trackDeclares(out)) {
          source.declareMap.remove(out);
        }
        target.declare(out);
        break;
      }
    }
  }
}","/** 
 * Relocate var defs from this block and any in-between ancestors to target block
 * @param state
 * @param inst
 * @param targetAncestor
 */
private static void relocateVarDefs(HoistTracking state,Instruction inst,HoistTracking targetAncestor){
  HoistTracking ancestor=state;
  assert(targetAncestor != null && state != targetAncestor);
  while (ancestor != targetAncestor) {
    assert(ancestor != null);
    relocateVarDefsFromBlock(ancestor,targetAncestor,inst);
    ancestor=ancestor.parent;
  }
}",0.3316582914572864
132789,"private static void doHoist(Logger logger,Instruction inst,int hoistDepth,HoistTracking state){
  assert(hoistDepth > 0);
  logger.trace(""String_Node_Str"" + hoistDepth + ""String_Node_Str""+ inst.toString());
  state.getAncestor(hoistDepth).addInstruction(inst);
  relocateVarDefs(state,inst,hoistDepth);
  state.getAncestor(hoistDepth).updateState(inst);
}","private static void doHoist(Logger logger,Instruction inst,int hoistDepth,HoistTracking state){
  assert(hoistDepth > 0);
  logger.trace(""String_Node_Str"" + hoistDepth + ""String_Node_Str""+ inst.toString());
  HoistTracking ancestor=state.getAncestor(hoistDepth);
  logger.trace(""String_Node_Str"" + ancestor.block.getType());
  ancestor.addInstruction(inst);
  relocateVarDefs(state,inst,ancestor);
  ancestor.updateState(inst);
}",0.7933673469387755
132790,"public static void fixupImmChange(Block srcBlock,Block targetBlock,Instruction oldInst,MakeImmChange change,List<Statement> instBuffer,List<Var> newOutVars,List<Var> oldOutVars,boolean storeOutputMapping){
  instBuffer.addAll(Arrays.asList(change.newInsts));
  Logger logger=Logging.getSTCLogger();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + oldInst + ""String_Node_Str""+ Arrays.asList(change.newInsts));
  }
  if (!change.isOutVarSame()) {
    Var newOut=change.newOut;
    Var oldOut=change.oldOut;
    replaceInstOutput(srcBlock,targetBlock,instBuffer,newOut,oldOut,storeOutputMapping);
  }
  if (change.storeOutputVals) {
    WrapUtil.setLocalOpOutputs(targetBlock,oldOutVars,newOutVars,instBuffer,storeOutputMapping);
  }
}","public static void fixupImmChange(Block srcBlock,Block targetBlock,Instruction oldInst,MakeImmChange change,List<Statement> instBuffer,List<Var> newOutVars,List<Var> oldOutVars,boolean storeOutputMapping){
  instBuffer.addAll(Arrays.asList(change.newInsts));
  Logger logger=Logging.getSTCLogger();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + oldInst + ""String_Node_Str""+ Arrays.asList(change.newInsts));
  }
  if (!change.isOutVarSame()) {
    Var newOut=change.newOut;
    Var oldOut=change.oldOut;
    boolean initOutput=false;
    for (    Pair<Var,InitType> init : oldInst.getInitialized()) {
      if (init.val2 == InitType.FULL && init.val1.equals(oldOut)) {
        initOutput=true;
        break;
      }
    }
    replaceInstOutput(srcBlock,targetBlock,instBuffer,newOut,oldOut,storeOutputMapping,initOutput);
  }
  if (change.storeOutputVals) {
    WrapUtil.setLocalOpOutputs(targetBlock,oldOutVars,newOutVars,instBuffer,storeOutputMapping);
  }
}",0.8308400460299195
132791,"/** 
 * Do the manipulation necessary to allow an old instruction output variable to be replaced with a new one. Assume that newOut is a value type of oldOut
 * @param srcBlock source block for instruction 
 * @param targetBlock target block for instruction
 * @param instBuffer append any fixup instructions here
 * @param newOut
 * @param oldOut
 * @param storeOutputMapping if true, assign mapping
 */
public static void replaceInstOutput(Block srcBlock,Block targetBlock,List<Statement> instBuffer,Var newOut,Var oldOut,boolean storeOutputMapping){
  boolean isDerefResult=Types.retrievedType(oldOut).assignableTo(newOut.type());
  if (isDerefResult) {
    Var oldOutReplacement;
    if (oldOut.storage() == Alloc.ALIAS) {
      oldOutReplacement=new Var(oldOut.type(),oldOut.name(),Alloc.TEMP,oldOut.defType(),oldOut.provenance(),oldOut.mappedDecl());
      replaceVarDeclaration(srcBlock,oldOut,oldOutReplacement);
      Map<Var,Arg> renames=Collections.singletonMap(oldOut,Arg.createVar(oldOutReplacement));
      for (      Statement inst : instBuffer) {
        inst.renameVars(renames,RenameMode.REPLACE_VAR);
      }
    }
 else {
      oldOutReplacement=oldOut;
    }
    WrapUtil.assignOutput(targetBlock,instBuffer,storeOutputMapping,oldOutReplacement,newOut);
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + oldOut + ""String_Node_Str""+ newOut+ ""String_Node_Str""+ ""String_Node_Str"");
  }
}","/** 
 * Do the manipulation necessary to allow an old instruction output variable to be replaced with a new one. Assume that newOut is a value type of oldOut
 * @param srcBlock source block for instruction 
 * @param targetBlock target block for instruction
 * @param instBuffer append any fixup instructions here
 * @param newOut
 * @param oldOut
 * @param storeOutputMapping if true, assign mapping
 */
public static void replaceInstOutput(Block srcBlock,Block targetBlock,List<Statement> instBuffer,Var newOut,Var oldOut,boolean initialisesOutput,boolean storeOutputMapping){
  boolean isDerefResult=Types.retrievedType(oldOut).assignableTo(newOut.type());
  if (isDerefResult) {
    Var oldOutReplacement;
    if (oldOut.storage() == Alloc.ALIAS && initialisesOutput) {
      System.err.println(""String_Node_Str"" + oldOut);
      new Exception().printStackTrace();
      oldOutReplacement=new Var(oldOut.type(),oldOut.name(),Alloc.TEMP,oldOut.defType(),oldOut.provenance(),oldOut.mappedDecl());
      replaceVarDeclaration(srcBlock,oldOut,oldOutReplacement);
      Map<Var,Arg> renames=Collections.singletonMap(oldOut,Arg.createVar(oldOutReplacement));
      for (      Statement inst : instBuffer) {
        inst.renameVars(renames,RenameMode.REPLACE_VAR);
      }
    }
 else {
      oldOutReplacement=oldOut;
    }
    WrapUtil.assignOutput(targetBlock,instBuffer,storeOutputMapping,oldOutReplacement,newOut);
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + oldOut + ""String_Node_Str""+ newOut+ ""String_Node_Str""+ ""String_Node_Str"");
  }
}",0.9529177718832892
132792,"/** 
 * Create alias for a struct field
 * @param structVar
 * @param structField
 * @param alias
 */
@Override public void structCreateAlias(Var alias,Var struct,List<String> fields){
  assert(alias.storage() == Alloc.ALIAS);
  assert(Types.isStruct(struct));
  assert(Types.isStructField(struct,fields,alias));
  Expression aliasExpr=Turbine.structAlias(varToExpr(struct),structFieldIndices(struct.type(),fields));
  pointAdd(new SetVariable(prefixVar(alias),aliasExpr));
}","/** 
 * Create alias for a struct field
 * @param structVar
 * @param structField
 * @param alias
 */
@Override public void structCreateAlias(Var alias,Var struct,List<String> fields){
}",0.5627836611195158
132793,"public HoistTracking makeChild(Block childBlock,ExecContext newExecCx,int maxHoist,int maxLoopHoist){
  return new HoistTracking(this,childBlock,execCx,maxHoist,maxLoopHoist,writeMap.makeChildMap(),piecewiseWriteMap.makeChildMap(),declareMap.makeChildMap(),initializedMap.makeChildMap());
}","public HoistTracking makeChild(Block childBlock,boolean async,ExecContext newExecCx,int maxHoist,int maxLoopHoist){
  return new HoistTracking(this,childBlock,async,execCx,maxHoist,maxLoopHoist,writeMap.makeChildMap(),piecewiseWriteMap.makeChildMap(),declareMap.makeChildMap(),initializedMap.makeChildMap());
}",0.9666666666666668
132794,"@Override public void optimize(Logger logger,Program prog){
  for (  Function f : prog.getFunctions()) {
    HoistTracking global=new HoistTracking();
    for (    Var gv : prog.constants().vars()) {
      global.write(gv,false);
      global.declare(gv);
    }
    HoistTracking mainBlockState=global.makeChild(f.mainBlock(),ExecContext.CONTROL,0,0);
    for (    Var in : f.getInputList()) {
      mainBlockState.write(in,false);
      mainBlockState.declare(in);
    }
    for (    Var out : f.getOutputList()) {
      mainBlockState.declare(out);
    }
    hoistRec(logger,mainBlockState);
  }
}","@Override public void optimize(Logger logger,Program prog){
  for (  Function f : prog.getFunctions()) {
    HoistTracking global=new HoistTracking();
    for (    Var gv : prog.constants().vars()) {
      global.write(gv,false);
      global.declare(gv);
    }
    HoistTracking mainBlockState=global.makeChild(f.mainBlock(),true,ExecContext.CONTROL,0,0);
    for (    Var in : f.getInputList()) {
      mainBlockState.write(in,false);
      mainBlockState.declare(in);
    }
    for (    Var out : f.getOutputList()) {
      mainBlockState.declare(out);
    }
    hoistRec(logger,mainBlockState);
  }
}",0.9958437240232751
132795,"private HoistTracking(HoistTracking parent,Block block,ExecContext execCx,int maxHoist,int maxLoopHoist,HierarchicalMap<Var,Block> writeMap,HierarchicalMap<Var,Block> piecewiseWriteMap,HierarchicalMap<Var,Block> declareMap,HierarchicalMap<Var,Block> initializedMap){
  super();
  this.parent=parent;
  this.block=block;
  this.execCx=execCx;
  this.maxHoist=maxHoist;
  this.maxLoopHoist=maxLoopHoist;
  this.writeMap=writeMap;
  this.piecewiseWriteMap=piecewiseWriteMap;
  this.declareMap=declareMap;
  this.initializedMap=initializedMap;
}","private HoistTracking(HoistTracking parent,Block block,boolean async,ExecContext execCx,int maxHoist,int maxLoopHoist,HierarchicalMap<Var,Block> writeMap,HierarchicalMap<Var,Block> piecewiseWriteMap,HierarchicalMap<Var,Block> declareMap,HierarchicalMap<Var,Block> initializedMap){
  super();
  this.parent=parent;
  this.block=block;
  this.async=async;
  this.execCx=execCx;
  this.maxHoist=maxHoist;
  this.maxLoopHoist=maxLoopHoist;
  this.writeMap=writeMap;
  this.piecewiseWriteMap=piecewiseWriteMap;
  this.declareMap=declareMap;
  this.initializedMap=initializedMap;
}",0.9695340501792116
132796,"public void write(Var v,boolean piecewise){
  if (piecewise) {
    piecewiseWriteMap.put(v,this.block);
  }
 else {
    writeMap.put(v,this.block);
  }
}","public void write(Var v,boolean piecewise){
  HoistTracking taskRoot=this;
  while (!taskRoot.async && taskRoot.parent != null) {
    taskRoot=taskRoot.parent;
  }
  if (piecewise) {
    taskRoot.piecewiseWriteMap.put(v,this.block);
  }
 else {
    taskRoot.writeMap.put(v,this.block);
  }
}",0.6891891891891891
132797,"/** 
 * Retrieve a reference to a local handle 
 * @param dst alias variable to hold handle to referenced data
 * @param src Closed reference
 * @param acquireRead num of read refcounts to acquire
 * @param acquireWrite num of write refcounts to acquire
 * @return
 */
public static Instruction retrieveRef(Var dst,Var src,long acquireRead,long acquireWrite){
  assert(Types.isRef(src.type()));
  assert(acquireRead >= 0);
  assert(acquireWrite >= 0);
  if (acquireWrite > 0) {
    assert(Types.isAssignableRefTo(src.type(),dst.type(),true));
  }
 else {
    assert(Types.isAssignableRefTo(src.type(),dst.type()));
  }
  assert(dst.storage() == Alloc.ALIAS);
  return new TurbineOp(Opcode.LOAD_REF,dst,src.asArg());
}","/** 
 * Retrieve a reference to a local handle 
 * @param dst alias variable to hold handle to referenced data
 * @param src Closed reference
 * @param acquireRead num of read refcounts to acquire
 * @param acquireWrite num of write refcounts to acquire
 * @return
 */
public static Instruction retrieveRef(Var dst,Var src,long acquireRead,long acquireWrite){
  assert(Types.isRef(src.type()));
  assert(acquireRead >= 0);
  assert(acquireWrite >= 0);
  if (acquireWrite > 0) {
    assert(Types.isAssignableRefTo(src.type(),dst.type(),true));
  }
 else {
    assert(Types.isAssignableRefTo(src.type(),dst.type()));
  }
  assert(dst.storage() == Alloc.ALIAS);
  return new TurbineOp(Opcode.LOAD_REF,dst,src.asArg(),Arg.createIntLit(acquireRead),Arg.createIntLit(acquireWrite));
}",0.9591973244147156
132798,"public List<Var> getModifiedOutputs(){
switch (op) {
case ARR_CREATE_NESTED_IMM:
case ARR_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case ARRAY_CREATE_BAG:
    return Collections.singletonList(getOutput(0));
case AREF_STORE_FUTURE:
case AREF_STORE_IMM:
  return Collections.singletonList(getOutput(1));
default :
return this.getOutputs();
}
}","public List<Var> getModifiedOutputs(){
switch (op) {
case ARR_CREATE_NESTED_IMM:
case ARR_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case ARRAY_CREATE_BAG:
    return Collections.singletonList(getOutput(0));
default :
  return this.getOutputs();
}
}",0.7600596125186289
132799,"@Override public List<Var> tryPiggyback(Counters<Var> increments,RefCountType type){
switch (op) {
case LOAD_SCALAR:
case LOAD_FILE:
case LOAD_REF:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_STRUCT:
case LOAD_RECURSIVE:
{
      Var inVar=getInput(0).getVar();
      if (type == RefCountType.READERS) {
        long amt=increments.getCount(inVar);
        if (amt < 0) {
          assert(getInputs().size() == 1);
          this.inputs=Arrays.asList(getInput(0),Arg.createIntLit(amt * -1));
          return inVar.asList();
        }
      }
      break;
    }
case ARR_STORE:
case ARR_COPY_IN_IMM:
case ARR_STORE_FUTURE:
case ARR_COPY_IN_FUTURE:
{
    Var arr=getOutput(0);
    if (type == RefCountType.WRITERS) {
      long amt=increments.getCount(arr);
      if (amt < 0) {
        assert(getInputs().size() == 2);
        int defaultDecr=op == Opcode.ARR_STORE ? 0 : 1;
        Arg decrArg=Arg.createIntLit(amt * -1 + defaultDecr);
        this.inputs=Arrays.asList(getInput(0),getInput(1),decrArg);
        return arr.asList();
      }
    }
    break;
  }
case ARR_CREATE_NESTED_IMM:
case ARRAY_CREATE_BAG:
{
  Var nested=getOutput(0);
  assert(getInputs().size() == 3);
  return tryPiggyBackHelper(increments,type,nested,1,2);
}
case BAG_INSERT:
{
Var bag=getOutput(0);
return tryPiggyBackHelper(increments,type,bag,-1,1);
}
default :
}
return Var.NONE;
}","@Override public List<Var> tryPiggyback(Counters<Var> increments,RefCountType type){
switch (op) {
case LOAD_SCALAR:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_STRUCT:
case LOAD_RECURSIVE:
{
      Var inVar=getInput(0).getVar();
      if (type == RefCountType.READERS) {
        long amt=increments.getCount(inVar);
        if (amt < 0) {
          assert(getInputs().size() == 1);
          this.inputs=Arrays.asList(getInput(0),Arg.createIntLit(amt * -1));
          return inVar.asList();
        }
      }
      break;
    }
case LOAD_REF:
  Var inVar=getInput(0).getVar();
if (type == RefCountType.READERS) {
  long amt=increments.getCount(inVar);
  if (amt < 0) {
    assert(getInputs().size() == 3);
    this.inputs=Arrays.asList(getInput(0),getInput(1),getInput(2),Arg.createIntLit(amt * -1));
    return inVar.asList();
  }
}
break;
case ARR_STORE:
case ARR_COPY_IN_IMM:
case ARR_STORE_FUTURE:
case ARR_COPY_IN_FUTURE:
{
Var arr=getOutput(0);
if (type == RefCountType.WRITERS) {
long amt=increments.getCount(arr);
if (amt < 0) {
  assert(getInputs().size() == 2);
  int defaultDecr=op == Opcode.ARR_STORE ? 0 : 1;
  Arg decrArg=Arg.createIntLit(amt * -1 + defaultDecr);
  this.inputs=Arrays.asList(getInput(0),getInput(1),decrArg);
  return arr.asList();
}
}
break;
}
case ARR_CREATE_NESTED_IMM:
case ARRAY_CREATE_BAG:
{
Var nested=getOutput(0);
assert(getInputs().size() == 3);
return tryPiggyBackHelper(increments,type,nested,1,2);
}
case BAG_INSERT:
{
Var bag=getOutput(0);
return tryPiggyBackHelper(increments,type,bag,-1,1);
}
default :
}
return Var.NONE;
}",0.8499660556687033
132800,"/** 
 * Represent that these are aliases for one another
 * @param var1
 * @param var2
 * @return
 */
public static ComponentAlias directAlias(Var var1,Var var2){
  return new ComponentAlias(var1,var2,Arg.NONE);
}","/** 
 * Represent that these are aliases for one another
 * @param var1
 * @param var2
 * @return
 */
public static ComponentAlias directAlias(Var var1,Var var2){
  return new ComponentAlias(var2,Arg.NONE,var1);
}",0.976525821596244
132801,"/** 
 * Constructor for reference var being dereferenced
 * @param var
 * @param ref
 * @return
 */
public static ComponentAlias ref(Var var,Var ref){
  return new ComponentAlias(var,ref,Collections.<Arg>singletonList(null));
}","/** 
 * Constructor for reference var being dereferenced
 * @param var
 * @param ref
 * @return
 */
public static ComponentAlias ref(Var var,Var ref){
  return new ComponentAlias(ref,Collections.<Arg>singletonList(null),var);
}",0.9823788546255506
132802,"public ComponentAlias(Var part,Var whole,Arg key){
  this(part,whole,key.asList());
}","public ComponentAlias(Var whole,Arg key,Var part){
  this(whole,key.asList(),part);
}",0.8352941176470589
132803,"public Node(Var var){
  this.var=var;
}","private Node(Var var,int id){
  this.var=var;
  this.id=id;
}",0.7
132804,"/** 
 * Get node corresponding to variable
 * @param var
 * @return
 */
private Node getVarNode(Var var){
  Node node=varNodes.get(var);
  if (node == null) {
    node=new Node(var);
    varNodes.put(var,node);
  }
  return node;
}","/** 
 * Get node corresponding to variable
 * @param var
 * @return
 */
private Node getVarNode(Var var){
  Node node=varNodes.get(var);
  if (node == null) {
    node=new Node(var,nextNodeID++);
    varNodes.put(var,node);
  }
  return node;
}",0.9726315789473684
132805,"/** 
 * Add a (potential) component relationship
 * @param part
 * @param whole enclosing structure
 * @param key relation from whole to part
 */
public void addPotentialComponent(Var part,Var whole,List<Arg> key){
  assert(!key.isEmpty());
  Node wholeNode=getVarNode(whole);
  Node partNode=getVarNode(part);
  Node curr=partNode;
  for (int i=0; i < key.size(); i++) {
    Arg keyElem=key.get(i);
    if (keyElem != null && !keyElem.isConstant()) {
      keyElem=null;
    }
    assert(keyElem == null || keyElem.isConstant());
    Node parent;
    if (i == key.size() - 1) {
      parent=wholeNode;
    }
 else {
      parent=Node.anonymous();
    }
    parents.put(curr,new Edge(parent,keyElem));
    children.put(parent,new Edge(curr,keyElem));
  }
}","/** 
 * Add a (potential) component relationship
 * @param whole enclosing structure
 * @param key relation from whole to part
 * @param part
 */
public void addPotentialComponent(Var whole,List<Arg> key,Var part){
  assert(!key.isEmpty());
  Node wholeNode=getVarNode(whole);
  Node partNode=getVarNode(part);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + whole + ""String_Node_Str""+ keyToString(key)+ ""String_Node_Str""+ part);
  }
  Node curr=wholeNode;
  for (int i=0; i < key.size(); i++) {
    Arg keyElem=key.get(i);
    if (keyElem != null && !keyElem.isConstant()) {
      keyElem=null;
    }
    assert(keyElem == null || keyElem.isConstant());
    Node child;
    if (i == key.size() - 1) {
      child=partNode;
    }
 else {
      child=getAnonNode(curr,keyElem);
    }
    children.put(curr,new Edge(child,keyElem));
    parents.put(child,new Edge(curr,keyElem));
    logger.trace(child + ""String_Node_Str"" + keyToString(keyElem)+ ""String_Node_Str""+ curr);
    curr=child;
  }
}",0.4723163841807909
132806,"public ComponentGraph(){
  this.varNodes=new HashMap<Var,Node>();
  this.parents=new MultiMap<Node,Edge>();
  this.children=new MultiMap<Node,Edge>();
  this.aliases=new MultiMap<Node,Node>();
}","public ComponentGraph(){
  this.nextNodeID=0;
  this.varNodes=new HashMap<Var,Node>();
  this.anonNodes=new HashMap<Pair<Node,Arg>,Node>();
  this.parents=new MultiMap<Node,Edge>();
  this.children=new MultiMap<Node,Edge>();
  this.aliases=new MultiMap<Node,Node>();
}",0.8398268398268398
132807,"public static Node anonymous(){
  return new Node(null);
}","public static Node anonymous(int id){
  return new Node(null,id);
}",0.928
132808,"@Override public String toString(){
  return ""String_Node_Str"" + parents + ""String_Node_Str""+ children+ ""String_Node_Str""+ aliases+ ""String_Node_Str"";
}","@Override public String toString(){
  return ""String_Node_Str"" + keyToString(label) + ""String_Node_Str""+ dst+ ""String_Node_Str"";
}",0.8014184397163121
132809,"@Override public List<ValLoc> getResults(){
switch (op) {
case LOAD_SCALAR:
case LOAD_REF:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_STRUCT:
case LOAD_RECURSIVE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      Closed outIsClosed;
      if (op == Opcode.LOAD_REF) {
        outIsClosed=Closed.MAYBE_NOT;
      }
 else {
        outIsClosed=Closed.YES_NOT_RECURSIVE;
      }
      if (op == Opcode.LOAD_REF) {
        return ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO,IsAssign.NO).asList();
      }
 else {
        return vanillaResult(outIsClosed,IsAssign.NO).asList();
      }
    }
case STORE_REF:
case STORE_SCALAR:
case STORE_FILE:
case STORE_ARRAY:
case STORE_BAG:
case STORE_STRUCT:
case STORE_RECURSIVE:
{
    ValLoc assign=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
    Arg dst=getOutput(0).asArg();
    Arg src=getInput(0);
    if (op == Opcode.STORE_REF) {
      ValLoc retrieve=ValLoc.derefCompVal(src.getVar(),dst.getVar(),IsValCopy.NO,IsAssign.NO);
      return Arrays.asList(retrieve,assign);
    }
 else {
      return assign.asList();
    }
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result,IsAssign.NO));
  }
}
case GET_FILENAME_ALIAS:
{
Arg filename=getOutput(0).asArg();
Var file=getInput(0).getVar();
return ValLoc.makeFilename(filename,file).asList();
}
case COPY_IN_FILENAME:
{
Arg filename=getInput(0);
Var file=getOutput(0);
return ValLoc.makeFilename(filename,file).asList();
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar(),IsAssign.TO_LOCATION).asList();
}
case SET_FILENAME_VAL:
{
Var file=getOutput(0);
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val,IsAssign.TO_VALUE).asList();
}
case GET_FILENAME_VAL:
{
Var file=getInput(0).getVar();
Var val=getOutput(0);
return ValLoc.makeFilenameVal(file,val.asArg(),IsAssign.NO).asList();
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES,IsAssign.NO).asList();
}
case STRUCT_CREATE_ALIAS:
case STRUCT_COPY_OUT:
case STRUCTREF_COPY_OUT:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
ValLoc copyV=ValLoc.makeStructFieldCopyResult(getOutput(0),struct,fields);
if (op == Opcode.STRUCT_CREATE_ALIAS) {
ValLoc aliasV=ValLoc.makeStructFieldAliasResult(getOutput(0),struct,fields);
return Arrays.asList(aliasV,copyV);
}
 else {
return copyV.asList();
}
}
case STRUCT_RETRIEVE_SUB:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldValResult(getOutput(0).asArg(),struct,fields).asList();
}
case STRUCT_INIT_FIELDS:
{
List<ValLoc> results=new ArrayList<ValLoc>();
Out<List<List<Arg>>> fieldPaths=new Out<List<List<Arg>>>();
Out<List<Arg>> fieldVals=new Out<List<Arg>>();
unpackStructInitArgs(null,fieldPaths,fieldVals);
Var struct=getOutput(0);
assert(fieldPaths.val.size() == fieldVals.val.size());
for (int i=0; i < fieldPaths.val.size(); i++) {
ValLoc.makeStructFieldValResult(fieldVals.val.get(i),struct,fieldPaths.val.get(i)).asList();
}
return results;
}
case STRUCT_STORE_SUB:
case STRUCTREF_STORE_SUB:
{
Var struct=getOutput(0);
Arg val=getInput(0);
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldValResult(val,struct,fields).asList();
}
case STRUCT_COPY_IN:
case STRUCTREF_COPY_IN:
{
Var struct=getOutput(0);
Var val=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldCopyResult(val,struct,fields).asList();
}
case ARR_STORE:
case ARR_STORE_FUTURE:
case AREF_STORE_IMM:
case AREF_STORE_FUTURE:
case ARR_COPY_IN_IMM:
case ARR_COPY_IN_FUTURE:
case AREF_COPY_IN_IMM:
case AREF_COPY_IN_FUTURE:
{
Var arr;
arr=getOutput(0);
Arg ix=getInput(0);
Arg member=getInput(1);
boolean insertingVal=isArrayValStore(op);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingVal,IsAssign.TO_VALUE));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE,IsAssign.NO));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Arg val=getInput(2 * i + 1);
res.add(ValLoc.makeArrayResult(arr,key,val,true,IsAssign.TO_VALUE));
}
res.add(ValLoc.makeContainerSizeCV(arr,Arg.createIntLit(elemCount),false,IsAssign.NO));
return res;
}
case ARR_RETRIEVE:
case ARR_COPY_OUT_IMM:
case ARR_COPY_OUT_FUTURE:
case AREF_COPY_OUT_FUTURE:
case AREF_COPY_OUT_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARR_RETRIEVE) {
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),true,IsAssign.NO).asList();
}
 else {
assert(Types.isElemType(arr,contents));
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.TO_LOCATION).asList();
}
}
case ARR_CREATE_NESTED_FUTURE:
case ARR_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case ARRAY_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr=getOutput(1);
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsNonRef=op == Opcode.ARR_CREATE_NESTED_IMM || op == Opcode.ARRAY_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr.asArg(),returnsNonRef,IsAssign.NO));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsNonRef));
return res;
}
case SYNC_COPY_CONTAINER:
case ASYNC_COPY_CONTAINER:
case SYNC_COPY_STRUCT:
case ASYNC_COPY_STRUCT:
{
return ValLoc.makeCopy(getOutput(0),getInput(0),IsAssign.TO_LOCATION).asList();
}
case COPY_REF:
{
Var srcRef=getInput(0).getVar();
return ValLoc.makeAlias(getOutput(0),srcRef).asList();
}
case LOOKUP_CHECKPOINT:
case UNPACK_VALUES:
{
List<ValLoc> res=new ArrayList<ValLoc>(outputs.size());
for (int i=0; i < outputs.size(); i++) {
Var out=outputs.get(i);
res.add(ValLoc.buildResult(op,(Object)i,getInput(0).asList(),out.asArg(),Closed.YES_RECURSIVE,IsValCopy.NO,IsAssign.NO));
}
return res;
}
case PACK_VALUES:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case CHECKPOINT_LOOKUP_ENABLED:
case CHECKPOINT_WRITE_ENABLED:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case UNPACK_ARRAY_TO_FLAT:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case ARR_CONTAINS:
case CONTAINER_SIZE:
case ARR_LOCAL_CONTAINS:
case CONTAINER_LOCAL_SIZE:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
default :
return null;
}
}","@Override public List<ValLoc> getResults(){
switch (op) {
case LOAD_SCALAR:
case LOAD_REF:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_STRUCT:
case LOAD_RECURSIVE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      Closed outIsClosed;
      if (op == Opcode.LOAD_REF) {
        outIsClosed=Closed.MAYBE_NOT;
      }
 else {
        outIsClosed=Closed.YES_NOT_RECURSIVE;
      }
      if (op == Opcode.LOAD_REF) {
        return ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO,IsAssign.NO).asList();
      }
 else {
        return vanillaResult(outIsClosed,IsAssign.NO).asList();
      }
    }
case STORE_REF:
case STORE_SCALAR:
case STORE_FILE:
case STORE_ARRAY:
case STORE_BAG:
case STORE_STRUCT:
case STORE_RECURSIVE:
{
    ValLoc assign=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
    Arg dst=getOutput(0).asArg();
    Arg src=getInput(0);
    if (op == Opcode.STORE_REF) {
      ValLoc retrieve=ValLoc.derefCompVal(src.getVar(),dst.getVar(),IsValCopy.NO,IsAssign.NO);
      return Arrays.asList(retrieve,assign);
    }
 else {
      return assign.asList();
    }
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result,IsAssign.NO));
  }
}
case GET_FILENAME_ALIAS:
{
Arg filename=getOutput(0).asArg();
Var file=getInput(0).getVar();
return ValLoc.makeFilename(filename,file).asList();
}
case COPY_IN_FILENAME:
{
Arg filename=getInput(0);
Var file=getOutput(0);
return ValLoc.makeFilename(filename,file).asList();
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar(),IsAssign.TO_LOCATION).asList();
}
case SET_FILENAME_VAL:
{
Var file=getOutput(0);
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val,IsAssign.TO_VALUE).asList();
}
case GET_FILENAME_VAL:
{
Var file=getInput(0).getVar();
Var val=getOutput(0);
return ValLoc.makeFilenameVal(file,val.asArg(),IsAssign.NO).asList();
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES,IsAssign.NO).asList();
}
case STRUCT_CREATE_ALIAS:
case STRUCT_COPY_OUT:
case STRUCTREF_COPY_OUT:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
ValLoc copyV=ValLoc.makeStructFieldCopyResult(getOutput(0),struct,fields);
if (op == Opcode.STRUCT_CREATE_ALIAS) {
ValLoc aliasV=ValLoc.makeStructFieldAliasResult(getOutput(0),struct,fields);
return Arrays.asList(aliasV,copyV);
}
 else {
return copyV.asList();
}
}
case STRUCT_RETRIEVE_SUB:
{
Var struct=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldValResult(getOutput(0).asArg(),struct,fields).asList();
}
case STRUCT_INIT_FIELDS:
{
List<ValLoc> results=new ArrayList<ValLoc>();
Out<List<List<Arg>>> fieldPaths=new Out<List<List<Arg>>>();
Out<List<Arg>> fieldVals=new Out<List<Arg>>();
unpackStructInitArgs(null,fieldPaths,fieldVals);
Var struct=getOutput(0);
assert(fieldPaths.val.size() == fieldVals.val.size());
for (int i=0; i < fieldPaths.val.size(); i++) {
results.add(ValLoc.makeStructFieldValResult(fieldVals.val.get(i),struct,fieldPaths.val.get(i)));
}
return results;
}
case STRUCT_STORE_SUB:
case STRUCTREF_STORE_SUB:
{
Var struct=getOutput(0);
Arg val=getInput(0);
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldValResult(val,struct,fields).asList();
}
case STRUCT_COPY_IN:
case STRUCTREF_COPY_IN:
{
Var struct=getOutput(0);
Var val=getInput(0).getVar();
List<Arg> fields=getInputsTail(1);
return ValLoc.makeStructFieldCopyResult(val,struct,fields).asList();
}
case ARR_STORE:
case ARR_STORE_FUTURE:
case AREF_STORE_IMM:
case AREF_STORE_FUTURE:
case ARR_COPY_IN_IMM:
case ARR_COPY_IN_FUTURE:
case AREF_COPY_IN_IMM:
case AREF_COPY_IN_FUTURE:
{
Var arr;
arr=getOutput(0);
Arg ix=getInput(0);
Arg member=getInput(1);
boolean insertingVal=isArrayValStore(op);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingVal,IsAssign.TO_VALUE));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE,IsAssign.NO));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Arg val=getInput(2 * i + 1);
res.add(ValLoc.makeArrayResult(arr,key,val,true,IsAssign.TO_VALUE));
}
res.add(ValLoc.makeContainerSizeCV(arr,Arg.createIntLit(elemCount),false,IsAssign.NO));
return res;
}
case ARR_RETRIEVE:
case ARR_COPY_OUT_IMM:
case ARR_COPY_OUT_FUTURE:
case AREF_COPY_OUT_FUTURE:
case AREF_COPY_OUT_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARR_RETRIEVE) {
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),true,IsAssign.NO).asList();
}
 else {
assert(Types.isElemType(arr,contents));
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.TO_LOCATION).asList();
}
}
case ARR_CREATE_NESTED_FUTURE:
case ARR_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case ARRAY_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr=getOutput(1);
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsNonRef=op == Opcode.ARR_CREATE_NESTED_IMM || op == Opcode.ARRAY_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr.asArg(),returnsNonRef,IsAssign.NO));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsNonRef));
return res;
}
case SYNC_COPY_CONTAINER:
case ASYNC_COPY_CONTAINER:
case SYNC_COPY_STRUCT:
case ASYNC_COPY_STRUCT:
{
return ValLoc.makeCopy(getOutput(0),getInput(0),IsAssign.TO_LOCATION).asList();
}
case COPY_REF:
{
Var srcRef=getInput(0).getVar();
return ValLoc.makeAlias(getOutput(0),srcRef).asList();
}
case LOOKUP_CHECKPOINT:
case UNPACK_VALUES:
{
List<ValLoc> res=new ArrayList<ValLoc>(outputs.size());
for (int i=0; i < outputs.size(); i++) {
Var out=outputs.get(i);
res.add(ValLoc.buildResult(op,(Object)i,getInput(0).asList(),out.asArg(),Closed.YES_RECURSIVE,IsValCopy.NO,IsAssign.NO));
}
return res;
}
case PACK_VALUES:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case CHECKPOINT_LOOKUP_ENABLED:
case CHECKPOINT_WRITE_ENABLED:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case UNPACK_ARRAY_TO_FLAT:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case ARR_CONTAINS:
case CONTAINER_SIZE:
case ARR_LOCAL_CONTAINS:
case CONTAINER_LOCAL_SIZE:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
default :
return null;
}
}",0.9985324332257116
132810,"public static Instruction assignOutputFile(Block block,List<Statement> instBuffer,boolean storeOutputMapping,Var file,Var fileVal){
  Instruction store;
  Arg storeFilename;
  if (storeOutputMapping) {
    Var isMapped=block.declareUnmapped(Types.V_BOOL,block.uniqueVarName(Var.OPT_VAR_PREFIX),Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.unknown());
    Var storeFilenameV=block.declareUnmapped(Types.V_BOOL,block.uniqueVarName(Var.OPT_VAR_PREFIX),Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.unknown());
    instBuffer.add(TurbineOp.isMapped(isMapped,file));
    instBuffer.add(Builtin.createLocal(BuiltinOpcode.NOT,storeFilenameV,isMapped.asArg()));
    storeFilename=storeFilenameV.asArg();
  }
 else {
    storeFilename=Arg.FALSE;
  }
  store=TurbineOp.assignFile(file,fileVal.asArg(),storeFilename);
  return store;
}","public static void assignOutputFile(Block block,List<Statement> instBuffer,boolean storeOutputMapping,Var file,Var fileVal){
  Arg storeFilename;
  if (storeOutputMapping) {
    Var isMapped=block.declareUnmapped(Types.V_BOOL,block.uniqueVarName(Var.OPT_VAR_PREFIX),Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.unknown());
    Var storeFilenameV=block.declareUnmapped(Types.V_BOOL,block.uniqueVarName(Var.OPT_VAR_PREFIX),Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.unknown());
    instBuffer.add(TurbineOp.isMapped(isMapped,file));
    instBuffer.add(Builtin.createLocal(BuiltinOpcode.NOT,storeFilenameV,isMapped.asArg()));
    storeFilename=storeFilenameV.asArg();
  }
 else {
    storeFilename=Arg.FALSE;
  }
  instBuffer.add(TurbineOp.assignFile(file,fileVal.asArg(),storeFilename));
}",0.8869778869778869
132811,"private void initialiseStruct(Context context,Var struct) throws UndefinedTypeException, DoubleDefineException {
  StructType structType=(StructType)struct.type().getImplType();
  List<List<String>> fieldPaths=new ArrayList<List<String>>();
  List<Arg> fieldVals=new ArrayList<Arg>();
  Deque<String> currFieldPath=new ArrayDeque<String>();
  int initFieldCount=initialiseStructRec(context,struct,currFieldPath,structType,fieldPaths,fieldVals);
  if (initFieldCount > 0) {
    backend.structInitFields(struct,fieldPaths,fieldVals,Arg.createIntLit(initFieldCount));
  }
}","private void initialiseStruct(Context context,Var struct) throws UndefinedTypeException, DoubleDefineException {
  StructType structType=(StructType)struct.type().getImplType();
  List<List<String>> fieldPaths=new ArrayList<List<String>>();
  List<Arg> fieldVals=new ArrayList<Arg>();
  Deque<String> currFieldPath=new ArrayDeque<String>();
  int initFieldCount=initialiseStructRec(context,struct,currFieldPath,structType,fieldPaths,fieldVals);
  if (initFieldCount > 0) {
    backend.structInitFields(VarRepr.backendVar(struct),fieldPaths,VarRepr.backendArgs(fieldVals),Arg.createIntLit(initFieldCount));
  }
}",0.9415749364944962
132812,"/** 
 * @param fieldPaths if null, not filled
 * @param fieldPathsArgs if null, not filled
 * @param fieldVals if null, not filled
 * @return writeDecr
 */
private Arg unpackStructInitArgs(Out<List<List<String>>> fieldPaths,Out<List<List<Arg>>> fieldPathsArgs,Out<List<Arg>> fieldVals){
  if (fieldPaths != null) {
    fieldPaths.val=new ArrayList<List<String>>();
  }
  if (fieldPathsArgs != null) {
    fieldPathsArgs.val=new ArrayList<List<Arg>>();
  }
  if (fieldVals != null) {
    fieldVals.val=new ArrayList<Arg>();
  }
  int pos=0;
  while (pos < inputs.size() - 1) {
    long pathLength=inputs.get(pos).getIntLit();
    assert(pathLength > 0 && pathLength <= Integer.MAX_VALUE);
    pos++;
    List<String> fieldPath=(fieldPaths == null) ? null : new ArrayList<String>((int)pathLength);
    List<Arg> fieldPathArgs=(fieldPathsArgs == null) ? null : new ArrayList<Arg>((int)pathLength);
    for (int i=0; i < pathLength; i++) {
      if (fieldPath != null) {
        fieldPath.add(inputs.get(pos).getStringLit());
      }
      if (fieldPathArgs != null) {
        fieldPathArgs.add(inputs.get(pos));
      }
      pos++;
    }
    Arg fieldVal=inputs.get(pos);
    pos++;
    if (fieldPaths != null) {
      fieldPaths.val.add(fieldPath);
    }
    if (fieldPaths != null) {
      fieldPathsArgs.val.add(fieldPathArgs);
    }
    if (fieldVals != null) {
      fieldVals.val.add(fieldVal);
    }
  }
  Arg writeDecr=getInput(pos);
  return writeDecr;
}","/** 
 * @param fieldPaths if null, not filled
 * @param fieldPathsArgs if null, not filled
 * @param fieldVals if null, not filled
 * @return writeDecr
 */
private Arg unpackStructInitArgs(Out<List<List<String>>> fieldPaths,Out<List<List<Arg>>> fieldPathsArgs,Out<List<Arg>> fieldVals){
  if (fieldPaths != null) {
    fieldPaths.val=new ArrayList<List<String>>();
  }
  if (fieldPathsArgs != null) {
    fieldPathsArgs.val=new ArrayList<List<Arg>>();
  }
  if (fieldVals != null) {
    fieldVals.val=new ArrayList<Arg>();
  }
  int pos=0;
  while (pos < inputs.size() - 1) {
    long pathLength=inputs.get(pos).getIntLit();
    assert(pathLength > 0 && pathLength <= Integer.MAX_VALUE);
    pos++;
    List<String> fieldPath=(fieldPaths == null) ? null : new ArrayList<String>((int)pathLength);
    List<Arg> fieldPathArgs=(fieldPathsArgs == null) ? null : new ArrayList<Arg>((int)pathLength);
    for (int i=0; i < pathLength; i++) {
      if (fieldPath != null) {
        fieldPath.add(inputs.get(pos).getStringLit());
      }
      if (fieldPathArgs != null) {
        fieldPathArgs.add(inputs.get(pos));
      }
      pos++;
    }
    Arg fieldVal=inputs.get(pos);
    pos++;
    if (fieldPaths != null) {
      fieldPaths.val.add(fieldPath);
    }
    if (fieldPathsArgs != null) {
      fieldPathsArgs.val.add(fieldPathArgs);
    }
    if (fieldVals != null) {
      fieldVals.val.add(fieldVal);
    }
  }
  Arg writeDecr=getInput(pos);
  return writeDecr;
}",0.9986329460013672
132813,"/** 
 * Lookup subscript in a variable
 * @param var
 * @param subscript
 * @return
 */
public static Expression lookupStruct(Value var,Expression subscript,Expression decrRead){
  return Square.fnCall(STRUCT_LOOKUP,var,subscript,decrRead);
}","/** 
 * Lookup subscript in a variable
 * @param var
 * @param subscript
 * @return
 */
public static Expression lookupStruct(Value var,Expression subscript,Expression decrRead){
  return Square.fnCall(LOOKUP_STRUCT,var,subscript,decrRead);
}",0.9710743801652892
132814,"/** 
 * Return base reference count for type, i.e. what it's initialized to by default 
 * @param rcType 
 * @param blockVar
 * @param trackedCountOnly return only refcounts tracked by STC(not auto-decremented on assign)
 * @return
 */
private static long baseRefCount(Type type,DefType defType,RefCountType rcType,boolean trackedCountOnly){
}","/** 
 * Return base reference count for type, i.e. what it's initialized to by default 
 * @param rcType 
 * @param blockVar
 * @param trackedCountOnly return only refcounts tracked by STC(not auto-decremented on assign)
 * @return
 */
private static long baseRefCount(Type type,DefType defType,RefCountType rcType,boolean trackedCountOnly){
  if (Types.isStruct(type)) {
    StructType structT=(StructType)type.type().getImplType();
    long count=0;
    for (    StructField field : structT.getFields()) {
      count+=baseRefCount(field.getType(),defType,rcType,trackedCountOnly);
    }
    return count;
  }
 else   if (Types.isPrimValue(type) || Types.isContainerLocal(type) || Types.isStructLocal(type)) {
    return 0;
  }
 else {
    if (trackedCountOnly) {
      if (trackRefCount(type,defType,rcType)) {
        return 1;
      }
 else {
        return 0;
      }
    }
 else {
      return 1;
    }
  }
}",0.5453100158982512
132815,"/** 
 * Processes part of an assignTarget path: the prefix of struct lookups. E.g. if we're trying to assign to x.field.field.field[0], then this function handles the 3x field lookups, making sure that a handle to x.field.field.field is put into a temp variable, say called t0 This will then return a new assignment target for t0[0] for further processing
 * @param context
 * @param lval
 * @return
 * @throws UserException
 * @throws UndefinedTypeException
 * @throws TypeMismatchException
 */
private LValue reduceStructLVal(Context context,LValue lval) throws UserException, UndefinedTypeException, TypeMismatchException {
  Var rootVar=context.lookupVarUser(lval.varName);
  ArrayList<String> fieldPath=new ArrayList<String>();
  int structPathIndex=0;
  while (structPathIndex < lval.indices.size() && lval.indices.get(structPathIndex).getType() == ExMParser.STRUCT_PATH) {
    SwiftAST pathTree=lval.indices.get(structPathIndex);
    String fieldName=pathTree.child(0).getText();
    fieldPath.add(fieldName);
    structPathIndex++;
  }
  final int structPathLen=structPathIndex;
  Var fieldAlias=varCreator.createStructFieldAlias(context,rootVar,lval.getType(context,structPathLen - 1),fieldPath);
  backend.structCreateAlias(VarRepr.backendVar(fieldAlias),VarRepr.backendVar(rootVar),fieldPath);
  List<SwiftAST> indicesLeft=lval.indices.subList(structPathLen,lval.indices.size());
  LValue newTarget=new LValue(lval,lval.tree,fieldAlias,indicesLeft);
  LogHelper.trace(context,""String_Node_Str"" + lval.toString() + ""String_Node_Str""+ lval.getType(context).toString()+ ""String_Node_Str""+ newTarget.toString()+ ""String_Node_Str""+ newTarget.getType(context).toString()+ ""String_Node_Str""+ structPathLen+ ""String_Node_Str"");
  return newTarget;
}","/** 
 * Processes part of an assignTarget path: the prefix of struct lookups. E.g. if we're trying to assign to x.field.field.field[0], then this function handles the 3x field lookups, making sure that a handle to x.field.field.field is put into a temp variable, say called t0 This will then return a new assignment target for t0[0] for further processing
 * @param context
 * @param lval
 * @return
 * @throws UserException
 * @throws UndefinedTypeException
 * @throws TypeMismatchException
 */
private LValue reduceStructLVal(Context context,LValue lval) throws UserException, UndefinedTypeException, TypeMismatchException {
  Var rootVar=context.lookupVarUser(lval.varName);
  ArrayList<String> fieldPath=new ArrayList<String>();
  int structPathIndex=0;
  while (structPathIndex < lval.indices.size() && lval.indices.get(structPathIndex).getType() == ExMParser.STRUCT_PATH) {
    SwiftAST pathTree=lval.indices.get(structPathIndex);
    String fieldName=pathTree.child(0).getText();
    fieldPath.add(fieldName);
    structPathIndex++;
  }
  final int structPathLen=structPathIndex;
  Var fieldAlias=varCreator.createStructFieldAlias(context,rootVar,lval.getType(context,structPathLen),fieldPath);
  backend.structCreateAlias(VarRepr.backendVar(fieldAlias),VarRepr.backendVar(rootVar),fieldPath);
  List<SwiftAST> indicesLeft=lval.indices.subList(structPathLen,lval.indices.size());
  LValue newTarget=new LValue(lval,lval.tree,fieldAlias,indicesLeft);
  LogHelper.trace(context,""String_Node_Str"" + lval.toString() + ""String_Node_Str""+ lval.getType(context).toString()+ ""String_Node_Str""+ newTarget.toString()+ ""String_Node_Str""+ newTarget.getType(context).toString()+ ""String_Node_Str""+ structPathLen+ ""String_Node_Str"");
  return newTarget;
}",0.9988571428571428
132816,"public void structCreateAlias(Var fieldAlias,Var struct,List<String> fieldPath){
  assert(Types.isStruct(struct));
  assert(Types.isStructField(struct,fieldPath,fieldAlias));
  assert(fieldAlias.storage() == Alloc.ALIAS);
  currBlock().addInstruction(TurbineOp.structCreateAlias(fieldAlias,struct,fieldPath));
}","public void structCreateAlias(Var fieldAlias,Var struct,List<String> fieldPath){
}",0.4173027989821883
132817,"/** 
 * @param context local context for app function
 * @param appBody AST for app function body
 * @param inArgs input arguments for app function
 * @param outArgs output arguments for app function
 * @param hasSideEffects
 * @param deterministic
 * @param val 
 * @param props 
 * @param suppressions 
 * @throws UserException
 */
private void genAppFunctionBody(Context context,SwiftAST appBody,List<Var> inArgs,List<Var> outArgs,boolean hasSideEffects,boolean deterministic,AsyncExecutor asyncExec,TaskProps props,Set<Suppression> suppressions) throws UserException {
  assert(appBody.getType() == ExMParser.APP_BODY);
  assert(appBody.getChildCount() >= 1);
  SwiftAST cmd=appBody.child(0);
  assert(cmd.getType() == ExMParser.COMMAND);
  assert(cmd.getChildCount() >= 1);
  SwiftAST appNameT=cmd.child(0);
  assert(appNameT.getType() == ExMParser.STRING);
  String appName=Literals.extractLiteralString(context,appNameT);
  Pair<List<Var>,Boolean> evaledArgs=evalAppCmdArgs(context,cmd);
  List<Var> cmdArgs=evaledArgs.val1;
  boolean openedEvalWait=evaledArgs.val2;
  Redirects<Var> redirFutures=processAppRedirects(context,appBody.children(1));
  checkAppOutputs(context,appName,outArgs,cmdArgs,redirFutures,suppressions);
  Pair<Map<String,Var>,List<WaitVar>> wait=selectAppWaitVars(context,cmdArgs,inArgs,outArgs,redirFutures);
  Map<String,Var> fileNames=wait.val1;
  List<WaitVar> waitVars=wait.val2;
  String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
  backend.startWaitStatement(waitName,VarRepr.backendVars(waitVars),WaitMode.TASK_DISPATCH,true,TaskMode.WORKER,props);
  Pair<List<Arg>,Redirects<Arg>> retrieved=retrieveAppArgs(context,cmdArgs,redirFutures,fileNames);
  List<Arg> localArgs=retrieved.val1;
  Redirects<Arg> localRedirects=retrieved.val2;
  List<Arg> localInFiles=new ArrayList<Arg>();
  for (  Var inArg : inArgs) {
    if (Types.isFile(inArg.type())) {
      Var localInputFile=exprWalker.retrieveToVar(context,inArg);
      localInFiles.add(Arg.createVar(localInputFile));
    }
  }
  List<Var> localOutputs=new ArrayList<Var>(outArgs.size());
  for (  Var output : outArgs) {
    Var localOutput=varCreator.createValueOfVar(context,output);
    localOutputs.add(localOutput);
    Arg localOutputFileName=null;
    if (Types.isFile(output.type())) {
      localOutputFileName=Arg.createVar(exprWalker.retrieveToVar(context,fileNames.get(output.name())));
      backend.initLocalOutFile(VarRepr.backendVar(localOutput),VarRepr.backendArg(localOutputFileName),VarRepr.backendVar(output));
    }
  }
  List<Arg> beLocalArgs=VarRepr.backendArgs(localArgs);
  List<Var> beLocalOutputs=VarRepr.backendVars(localOutputs);
  List<Arg> beLocalInfiles=VarRepr.backendArgs(localInFiles);
  Redirects<Arg> beLocalRedirects=new Redirects<Arg>(VarRepr.backendArg(localRedirects.stdin,true),VarRepr.backendArg(localRedirects.stdout,true),VarRepr.backendArg(localRedirects.stderr,true));
  if (asyncExec == null) {
    backend.runExternal(appName,beLocalArgs,beLocalInfiles,beLocalOutputs,beLocalRedirects,hasSideEffects,deterministic);
  }
 else {
    String aeName=context.constructName(""String_Node_Str"");
    Map<String,Arg> taskProps=new HashMap<String,Arg>();
    beLocalRedirects.addProps(taskProps);
    backend.startAsyncExec(aeName,asyncExec,appName,beLocalOutputs,beLocalArgs,taskProps,!deterministic);
  }
  for (int i=0; i < outArgs.size(); i++) {
    Var output=outArgs.get(i);
    Var localOutput=localOutputs.get(i);
    if (Types.isFile(output.type())) {
      exprWalker.assign(output,Arg.createVar(localOutput));
      if (output.isMapped() != Ternary.TRUE && output.type().fileKind().supportsTmpImmediate()) {
        backend.decrLocalFileRef(VarRepr.backendVar(localOutput));
      }
    }
 else {
      assert(Types.isVoid(output.type()));
      exprWalker.assign(output,localOutput.asArg());
    }
  }
  if (asyncExec != null) {
    backend.endAsyncExec();
  }
  backend.endWaitStatement();
  if (openedEvalWait) {
    backend.endWaitStatement();
  }
}","/** 
 * @param context local context for app function
 * @param appBody AST for app function body
 * @param inArgs input arguments for app function
 * @param outArgs output arguments for app function
 * @param hasSideEffects
 * @param deterministic
 * @param val 
 * @param props 
 * @param suppressions 
 * @throws UserException
 */
private void genAppFunctionBody(Context context,SwiftAST appBody,List<Var> inArgs,List<Var> outArgs,boolean hasSideEffects,boolean deterministic,AsyncExecutor asyncExec,TaskProps props,Set<Suppression> suppressions) throws UserException {
  assert(appBody.getType() == ExMParser.APP_BODY);
  assert(appBody.getChildCount() >= 1);
  SwiftAST cmd=appBody.child(0);
  assert(cmd.getType() == ExMParser.COMMAND);
  assert(cmd.getChildCount() >= 1);
  SwiftAST appNameT=cmd.child(0);
  assert(appNameT.getType() == ExMParser.STRING);
  String appName=Literals.extractLiteralString(context,appNameT);
  Pair<List<Var>,Boolean> evaledArgs=evalAppCmdArgs(context,cmd);
  List<Var> cmdArgs=evaledArgs.val1;
  boolean openedEvalWait=evaledArgs.val2;
  Redirects<Var> redirFutures=processAppRedirects(context,appBody.children(1));
  checkAppOutputs(context,appName,outArgs,cmdArgs,redirFutures,suppressions);
  Pair<Map<String,Var>,List<WaitVar>> wait=selectAppWaitVars(context,cmdArgs,inArgs,outArgs,redirFutures);
  Map<String,Var> fileNames=wait.val1;
  List<WaitVar> waitVars=wait.val2;
  String waitName=context.getFunctionContext().constructName(""String_Node_Str"");
  backend.startWaitStatement(waitName,VarRepr.backendWaitVars(waitVars),WaitMode.TASK_DISPATCH,true,TaskMode.WORKER,props);
  Pair<List<Arg>,Redirects<Arg>> retrieved=retrieveAppArgs(context,cmdArgs,redirFutures,fileNames);
  List<Arg> localArgs=retrieved.val1;
  Redirects<Arg> localRedirects=retrieved.val2;
  List<Arg> localInFiles=new ArrayList<Arg>();
  for (  Var inArg : inArgs) {
    if (Types.isFile(inArg.type())) {
      Var localInputFile=exprWalker.retrieveToVar(context,inArg);
      localInFiles.add(Arg.createVar(localInputFile));
    }
  }
  List<Var> localOutputs=new ArrayList<Var>(outArgs.size());
  for (  Var output : outArgs) {
    Var localOutput=varCreator.createValueOfVar(context,output);
    localOutputs.add(localOutput);
    Arg localOutputFileName=null;
    if (Types.isFile(output.type())) {
      localOutputFileName=Arg.createVar(exprWalker.retrieveToVar(context,fileNames.get(output.name())));
      backend.initLocalOutFile(VarRepr.backendVar(localOutput),VarRepr.backendArg(localOutputFileName),VarRepr.backendVar(output));
    }
  }
  List<Arg> beLocalArgs=VarRepr.backendArgs(localArgs);
  List<Var> beLocalOutputs=VarRepr.backendVars(localOutputs);
  List<Arg> beLocalInfiles=VarRepr.backendArgs(localInFiles);
  Redirects<Arg> beLocalRedirects=new Redirects<Arg>(VarRepr.backendArg(localRedirects.stdin,true),VarRepr.backendArg(localRedirects.stdout,true),VarRepr.backendArg(localRedirects.stderr,true));
  if (asyncExec == null) {
    backend.runExternal(appName,beLocalArgs,beLocalInfiles,beLocalOutputs,beLocalRedirects,hasSideEffects,deterministic);
  }
 else {
    String aeName=context.constructName(""String_Node_Str"");
    Map<String,Arg> taskProps=new HashMap<String,Arg>();
    beLocalRedirects.addProps(taskProps);
    backend.startAsyncExec(aeName,asyncExec,appName,beLocalOutputs,beLocalArgs,taskProps,!deterministic);
  }
  for (int i=0; i < outArgs.size(); i++) {
    Var output=outArgs.get(i);
    Var localOutput=localOutputs.get(i);
    if (Types.isFile(output.type())) {
      exprWalker.assign(output,Arg.createVar(localOutput));
      if (output.isMapped() != Ternary.TRUE && output.type().fileKind().supportsTmpImmediate()) {
        backend.decrLocalFileRef(VarRepr.backendVar(localOutput));
      }
    }
 else {
      assert(Types.isVoid(output.type()));
      exprWalker.assign(output,localOutput.asArg());
    }
  }
  if (asyncExec != null) {
    backend.endAsyncExec();
  }
  backend.endWaitStatement();
  if (openedEvalWait) {
    backend.endWaitStatement();
  }
}",0.9995032290114256
132818,"private void addInferred(GlobalConstants consts,String errContext,CongruentSets congruent,int stmtIndex,Arg canonLoc,ArgOrCV canonVal) throws OptUnsafeError {
  if (canonVal.isCV()) {
    for (    ArgCV extra : Algebra.tryAlgebra(this,canonVal.cv())) {
      update(consts,errContext,canonLoc,extra,IsAssign.NO,congruent,false,stmtIndex);
    }
  }
}","private void addInferred(GlobalConstants consts,String errContext,CongruentSets congruent,int stmtIndex,Arg canonLoc,ArgOrCV canonVal) throws OptUnsafeError {
  if (canonVal.isCV()) {
    ArgCV cv=canonVal.cv();
    for (    ArgCV extra : Algebra.tryAlgebra(this,cv)) {
      update(consts,errContext,canonLoc,extra,IsAssign.NO,congruent,false,stmtIndex);
    }
    if (cv.isArrayMemberVal() || cv.isArrayMemberRef()) {
      Var arr=cv.getInput(0).getVar();
      Arg ix=cv.getInput(1);
      update(consts,errContext,Arg.TRUE,ComputedValue.arrayContainsCV(arr,ix),IsAssign.NO,congruent,false,stmtIndex);
    }
  }
}",0.6990692864529473
132819,"public static Type derefResultType(Typed t){
  if (isScalarFuture(t) || isScalarUpdateable(t)) {
    return new ScalarValueType(t.type().primType());
  }
 else   if (isFile(t)) {
    return new FileValueType(t.type().fileKind());
  }
 else   if (isRef(t)) {
    return t.type().baseType().memberType();
  }
 else   if (isArray(t)) {
    ArrayType at=(ArrayType)t.type().getImplType();
    return new ArrayType(true,at.keyType(),at.memberType());
  }
 else   if (isBag(t)) {
    BagType bt=(BagType)t.type().getImplType();
    return new BagType(true,bt.memberType());
  }
 else {
    throw new STCRuntimeError(t.type() + ""String_Node_Str"");
  }
}","public static Type derefResultType(Typed t){
  if (isScalarFuture(t) || isScalarUpdateable(t)) {
    return new ScalarValueType(t.type().primType());
  }
 else   if (isFile(t)) {
    return new FileValueType(t.type().fileKind());
  }
 else   if (isRef(t)) {
    return t.type().baseType().memberType();
  }
 else   if (isArray(t)) {
    ArrayType at=(ArrayType)t.type().getImplType();
    return ArrayType.localArray(at.keyType(),at.memberType());
  }
 else   if (isBag(t)) {
    BagType bt=(BagType)t.type().getImplType();
    return BagType.localBag(bt.memberType());
  }
 else {
    throw new STCRuntimeError(t.type() + ""String_Node_Str"");
  }
}",0.9489953632148376
132820,"/** 
 * Handle the general foreach loop where we are looping over array
 * @param context
 * @param loop
 * @throws UserException
 * @throws UndefinedTypeException
 */
private void foreachArray(Context context,ForeachLoop loop) throws UserException, UndefinedTypeException {
  Var arrayVar=exprWalker.eval(context,loop.getArrayVarTree(),loop.findArrayType(context),false,null);
  VariableUsageInfo bodyVU=loop.getBody().checkedGetVariableUsage();
  List<Var> writtenVars=new ArrayList<Var>();
  summariseBranchVariableUsage(context,Arrays.asList(bodyVU),writtenVars);
  for (  Var v : writtenVars) {
    if (v.equals(arrayVar)) {
      throw new STCRuntimeError(""String_Node_Str"" + v + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    }
  }
  FunctionContext fc=context.getFunctionContext();
  int loopNum=fc.getCounterVal(""String_Node_Str"");
  Var realArray;
  Context outsideLoopContext;
  if (Types.isContainerRef(arrayVar)) {
    backend.startWaitStatement(fc.getFunctionName() + ""String_Node_Str"" + loopNum,VarRepr.backendVar(arrayVar).asList(),WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL_CONTROL);
    outsideLoopContext=new LocalContext(context);
    realArray=varCreator.createTmp(outsideLoopContext,arrayVar.type().memberType(),false,true);
    exprWalker.retrieveRef(realArray,arrayVar);
  }
 else {
    assert(Types.isContainer(arrayVar));
    realArray=arrayVar;
    outsideLoopContext=context;
  }
  backend.startWaitStatement(fc.getFunctionName() + ""String_Node_Str"" + loopNum,VarRepr.backendVar(realArray).asList(),WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL_CONTROL);
  loop.setupLoopBodyContext(outsideLoopContext,false,false);
  Context loopBodyContext=loop.getBodyContext();
  Var loopCountVal=loop.getLoopCountVal();
  Var backendMemberVal=VarRepr.backendVar(loop.getMemberVal());
  backend.startForeachLoop(fc.getFunctionName() + ""String_Node_Str"" + loopNum,VarRepr.backendVar(realArray),backendMemberVal,loopCountVal == null ? null : VarRepr.backendVar(loopCountVal),loop.getSplitDegree(),loop.getLeafDegree(),true);
  varCreator.initialiseVariable(loopBodyContext,loop.getMemberVar());
  backend.assignScalar(VarRepr.backendVar(loop.getMemberVar()),backendMemberVal.asArg());
  if (!loop.isSyncLoop()) {
    backend.startWaitStatement(fc.getFunctionName() + ""String_Node_Str"" + loopNum,Var.NONE,WaitMode.TASK_DISPATCH,false,false,TaskMode.CONTROL);
  }
  if (loop.getCountVarName() != null) {
    Var loopCountVar=varCreator.createVariable(loop.getBodyContext(),loop.createCountVar(context));
    exprWalker.assign(loopCountVar,loop.getLoopCountVal().asArg());
  }
  block(loopBodyContext,loop.getBody());
  if (!loop.isSyncLoop()) {
    backend.endWaitStatement();
  }
  backend.endForeachLoop();
  backend.endWaitStatement();
  if (Types.isContainerRef(arrayVar.type())) {
    backend.endWaitStatement();
  }
}","/** 
 * Handle the general foreach loop where we are looping over array
 * @param context
 * @param loop
 * @throws UserException
 * @throws UndefinedTypeException
 */
private void foreachArray(Context context,ForeachLoop loop) throws UserException, UndefinedTypeException {
  Var arrayVar=exprWalker.eval(context,loop.getArrayVarTree(),loop.findArrayType(context),false,null);
  VariableUsageInfo bodyVU=loop.getBody().checkedGetVariableUsage();
  List<Var> writtenVars=new ArrayList<Var>();
  summariseBranchVariableUsage(context,Arrays.asList(bodyVU),writtenVars);
  for (  Var v : writtenVars) {
    if (v.equals(arrayVar)) {
      throw new STCRuntimeError(""String_Node_Str"" + v + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    }
  }
  FunctionContext fc=context.getFunctionContext();
  int loopNum=fc.getCounterVal(""String_Node_Str"");
  Var realArray;
  Context outsideLoopContext;
  if (Types.isContainerRef(arrayVar)) {
    backend.startWaitStatement(fc.getFunctionName() + ""String_Node_Str"" + loopNum,VarRepr.backendVar(arrayVar).asList(),WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL_CONTROL);
    outsideLoopContext=new LocalContext(context);
    realArray=varCreator.createTmp(outsideLoopContext,arrayVar.type().memberType(),false,true);
    exprWalker.retrieveRef(realArray,arrayVar);
  }
 else {
    assert(Types.isContainer(arrayVar));
    realArray=arrayVar;
    outsideLoopContext=context;
  }
  backend.startWaitStatement(fc.getFunctionName() + ""String_Node_Str"" + loopNum,VarRepr.backendVar(realArray).asList(),WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL_CONTROL);
  loop.setupLoopBodyContext(outsideLoopContext,false,false);
  Context loopBodyContext=loop.getBodyContext();
  Var loopCountVal=loop.getLoopCountVal();
  boolean memberIsVal=(loop.getMemberVal() != null);
  Var backendIterVar=null;
  if (memberIsVal) {
    backendIterVar=VarRepr.backendVar(loop.getMemberVal());
  }
 else {
    backendIterVar=VarRepr.backendVar(loop.getMemberVar());
  }
  backend.startForeachLoop(fc.getFunctionName() + ""String_Node_Str"" + loopNum,VarRepr.backendVar(realArray),backendIterVar,loopCountVal == null ? null : VarRepr.backendVar(loopCountVal),loop.getSplitDegree(),loop.getLeafDegree(),true);
  if (memberIsVal) {
    varCreator.initialiseVariable(loopBodyContext,loop.getMemberVar());
    exprWalker.assign(VarRepr.backendVar(loop.getMemberVar()),backendIterVar.asArg());
  }
  if (!loop.isSyncLoop()) {
    backend.startWaitStatement(fc.getFunctionName() + ""String_Node_Str"" + loopNum,Var.NONE,WaitMode.TASK_DISPATCH,false,false,TaskMode.CONTROL);
  }
  if (loop.getCountVarName() != null) {
    Var loopCountVar=varCreator.createVariable(loop.getBodyContext(),loop.createCountVar(context));
    exprWalker.assign(loopCountVar,loop.getLoopCountVal().asArg());
  }
  block(loopBodyContext,loop.getBody());
  if (!loop.isSyncLoop()) {
    backend.endWaitStatement();
  }
  backend.endForeachLoop();
  backend.endWaitStatement();
  if (Types.isContainerRef(arrayVar.type())) {
    backend.endWaitStatement();
  }
}",0.9530428886251908
132821,"/** 
 * @param type
 * @param var the name of the variable this is the value of:try and work this into the generated name. Can be left as null
 * @return
 * @throws UserException
 */
@Override public Var createLocalValueVariable(Type type,Var var) throws UserException {
  String varName;
  VarProvenance prov;
  if (var != null) {
    prov=VarProvenance.valueOf(var,getSourceLoc());
    varName=var.name();
  }
 else {
    prov=VarProvenance.exprTmp(getSourceLoc());
    varName=null;
  }
  Alloc storage;
  if (Types.isPrimValue(type)) {
    storage=Alloc.LOCAL;
  }
 else {
    storage=Alloc.ALIAS;
  }
  String name=chooseVariableName(Var.LOCAL_VALUE_VAR_PREFIX,varName,""String_Node_Str"");
  return declareVariable(type,name,storage,DefType.LOCAL_COMPILER,prov,null);
}","/** 
 * @param type
 * @param var the name of the variable this is the value of:try and work this into the generated name. Can be left as null
 * @return
 * @throws UserException
 */
@Override public Var createLocalValueVariable(Type type,Var var) throws UserException {
  String varName;
  VarProvenance prov;
  if (var != null) {
    prov=VarProvenance.valueOf(var,getSourceLoc());
    varName=var.name();
  }
 else {
    prov=VarProvenance.exprTmp(getSourceLoc());
    varName=null;
  }
  Alloc storage;
  if (Types.isPrimValue(type) || Types.isContainerLocal(type)) {
    storage=Alloc.LOCAL;
  }
 else {
    storage=Alloc.ALIAS;
  }
  String name=chooseVariableName(Var.LOCAL_VALUE_VAR_PREFIX,varName,""String_Node_Str"");
  return declareVariable(type,name,storage,DefType.LOCAL_COMPILER,prov,null);
}",0.9797211660329532
132822,"/** 
 * Initialize the context and define the two variables: for array member and (optionally) for the loop count
 * @param context
 * @param rangeLoop where a range loop or container
 * @param includeIndexFuture if true, initialize the count var(getCountVarName()), rather than just loopCountval
 * @return
 * @throws UserException
 */
public Context setupLoopBodyContext(Context context,boolean rangeLoop,boolean includeIndexFuture) throws UserException {
  Type arrayType=findArrayType(context);
  loopBodyContext=new LocalContext(context);
  if (countVarName != null) {
    if (!Types.isArray(arrayType) && !Types.isArrayRef(arrayType)) {
      throw new UserException(""String_Node_Str"" + ""String_Node_Str"" + arrayType.typeName());
    }
    keyType=Types.arrayKeyType(arrayType);
    Type keyValType=Types.derefResultType(keyType);
    Var countVar=createCountVar(context);
    loopCountVal=context.createLocalValueVariable(keyValType,countVar);
    if (includeIndexFuture) {
      loopBodyContext.declareVariable(countVar);
    }
  }
 else {
    loopCountVal=null;
  }
  memberVar=loopBodyContext.declareVariable(findElemType(arrayType),getMemberVarName(),Alloc.TEMP,DefType.LOCAL_USER,VarProvenance.userVar(context.getSourceLoc()),null);
  Type memberValType=Types.derefResultType(VarRepr.fieldRepr(memberVar.type()));
  memberVal=loopBodyContext.createLocalValueVariable(memberValType,memberVar);
  return loopBodyContext;
}","/** 
 * Initialize the context and define the two variables: for array member and (optionally) for the loop count
 * @param context
 * @param rangeLoop where a range loop or container
 * @param includeIndexFuture if true, initialize the count var(getCountVarName()), rather than just loopCountval
 * @return
 * @throws UserException
 */
public Context setupLoopBodyContext(Context context,boolean rangeLoop,boolean includeIndexFuture) throws UserException {
  Type arrayType=findArrayType(context);
  loopBodyContext=new LocalContext(context);
  if (countVarName != null) {
    if (!Types.isArray(arrayType) && !Types.isArrayRef(arrayType)) {
      throw new UserException(""String_Node_Str"" + ""String_Node_Str"" + arrayType.typeName());
    }
    keyType=Types.arrayKeyType(arrayType);
    Type keyValType=Types.derefResultType(keyType);
    Var countVar=createCountVar(context);
    loopCountVal=context.createLocalValueVariable(keyValType,countVar);
    if (includeIndexFuture) {
      loopBodyContext.declareVariable(countVar);
    }
  }
 else {
    loopCountVal=null;
  }
  memberVar=loopBodyContext.declareVariable(findElemType(arrayType),getMemberVarName(),Alloc.TEMP,DefType.LOCAL_USER,VarProvenance.userVar(context.getSourceLoc()),null);
  Type memberValRepr=VarRepr.fieldRepr(memberVar.type());
  if (Types.isRef(memberValRepr)) {
    memberVal=null;
  }
 else {
    Type memberValType=Types.derefResultType(memberValRepr);
    memberVal=loopBodyContext.createLocalValueVariable(memberValType,memberVar);
  }
  return loopBodyContext;
}",0.941532258064516
132823,"public void assignScalar(Var dst,Arg src){
  assert(Types.isScalarFuture(dst));
  assert(Types.isScalarValue(src));
  assert(src.type().assignableTo(Types.derefResultType(dst)));
  currBlock().addInstruction(TurbineOp.assignScalar(dst,src));
}","public void assignScalar(Var dst,Arg src){
}",0.3066202090592334
132824,"public void assignArray(Var target,Arg src){
  assert(Types.isArray(target.type()));
  assert(Types.isArrayLocal(src.type()));
  assert(Types.containerElemType(src.type()).assignableTo(Types.containerElemType(target)));
  currBlock().addInstruction(TurbineOp.assignArray(target,src));
}","public void assignArray(Var target,Arg src){
}",0.2771084337349397
132825,"/** 
 * @param logger
 * @param fn
 * @param block
 * @param cv
 * @param inst
 * @param insts if instructions inserted, leaves iterator pointing at previous instruction
 * @return
 */
private boolean switchToImmediate(Logger logger,Function fn,ExecContext execCx,Block block,Congruences state,Instruction inst,ListIterator<Statement> stmts,int stmtIndex){
  if (!finalizedVarEnabled) {
    return false;
  }
  MakeImmRequest req=inst.canMakeImmediate(state.getClosed(stmtIndex),false);
  if (req == null) {
    return false;
  }
  Block insertContext;
  ListIterator<Statement> insertPoint;
  boolean noWaitRequired=req.mode == TaskMode.LOCAL || req.mode == TaskMode.SYNC || (req.mode == TaskMode.LOCAL_CONTROL && execCx == ExecContext.CONTROL);
  stmts.remove();
  if (noWaitRequired) {
    insertContext=block;
    insertPoint=stmts;
  }
 else {
    WaitStatement wait=new WaitStatement(fn.getName() + ""String_Node_Str"" + inst.shortOpName(),WaitVar.NONE,PassedVar.NONE,Var.NONE,WaitMode.TASK_DISPATCH,false,req.mode,inst.getTaskProps());
    insertContext=wait.getBlock();
    block.addContinuation(wait);
    insertPoint=insertContext.statementIterator();
  }
  List<Instruction> alt=new ArrayList<Instruction>();
  List<Fetched<Arg>> inVals=fetchInputsForSwitch(state,req,insertContext,noWaitRequired,alt);
  Map<Var,Var> filenameVals=loadOutputFileNames(state,stmtIndex,req.out,insertContext,insertPoint,req.mapOutVars);
  List<Var> outFetched=OptUtil.createLocalOpOutputVars(insertContext,insertPoint,req.out,filenameVals,req.mapOutVars);
  MakeImmChange change;
  change=inst.makeImmediate(new OptVarCreator(block),Fetched.makeList(req.out,outFetched),inVals);
  OptUtil.fixupImmChange(block,insertContext,inst,change,alt,outFetched,req.out,req.mapOutVars);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ alt.toString());
  }
  for (  Instruction newInst : alt) {
    insertPoint.add(newInst);
  }
  if (stmts == insertPoint) {
    ICUtil.rewindIterator(stmts,alt.size());
  }
  return true;
}","/** 
 * @param logger
 * @param fn
 * @param block
 * @param cv
 * @param inst
 * @param insts if instructions inserted, leaves iterator pointing at previous instruction
 * @return
 */
private boolean switchToImmediate(Logger logger,Function fn,ExecContext execCx,Block block,Congruences state,Instruction inst,ListIterator<Statement> stmts,int stmtIndex){
  if (!finalizedVarEnabled) {
    return false;
  }
  MakeImmRequest req=inst.canMakeImmediate(state.getClosed(stmtIndex),false);
  if (req == null) {
    return false;
  }
  Block insertContext;
  ListIterator<Statement> insertPoint;
  boolean noWaitRequired=req.mode == TaskMode.LOCAL || req.mode == TaskMode.SYNC || (req.mode == TaskMode.LOCAL_CONTROL && execCx == ExecContext.CONTROL);
  stmts.remove();
  if (noWaitRequired) {
    insertContext=block;
    insertPoint=stmts;
  }
 else {
    WaitStatement wait=new WaitStatement(fn.getName() + ""String_Node_Str"" + inst.shortOpName(),WaitVar.NONE,PassedVar.NONE,Var.NONE,WaitMode.TASK_DISPATCH,false,req.mode,inst.getTaskProps());
    insertContext=wait.getBlock();
    block.addContinuation(wait);
    insertPoint=insertContext.statementIterator();
  }
  List<Instruction> alt=new ArrayList<Instruction>();
  List<Fetched<Arg>> inVals=fetchInputsForSwitch(state,req,insertContext,noWaitRequired,alt);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inVals + ""String_Node_Str""+ inst+ ""String_Node_Str""+ req.in);
  }
  Map<Var,Var> filenameVals=loadOutputFileNames(state,stmtIndex,req.out,insertContext,insertPoint,req.mapOutVars);
  List<Var> outFetched=OptUtil.createLocalOpOutputVars(insertContext,insertPoint,req.out,filenameVals,req.mapOutVars);
  MakeImmChange change;
  change=inst.makeImmediate(new OptVarCreator(block),Fetched.makeList(req.out,outFetched),inVals);
  OptUtil.fixupImmChange(block,insertContext,inst,change,alt,outFetched,req.out,req.mapOutVars);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ alt.toString());
  }
  for (  Instruction newInst : alt) {
    insertPoint.add(newInst);
  }
  if (stmts == insertPoint) {
    ICUtil.rewindIterator(stmts,alt.size());
  }
  return true;
}",0.967894239848914
132826,"@Override public void arrayRefCopyOutFuture(Var oVar,Var arrayVar,Var indexVar){
  assert(Types.isArrayRef(arrayVar));
  assert(Types.isElemType(arrayVar,oVar));
  assert(Types.isArrayKeyFuture(arrayVar,indexVar));
  Command getRef=Turbine.arrayLookupComputed(varToExpr(oVar),refRepresentationType(oVar.type().memberType(),false),varToExpr(arrayVar),varToExpr(indexVar),true);
  pointAdd(getRef);
}","@Override public void arrayRefCopyOutFuture(Var oVar,Var arrayVar,Var indexVar){
  assert(Types.isArrayRef(arrayVar));
  assert(Types.isElemType(arrayVar,oVar));
  assert(Types.isArrayKeyFuture(arrayVar,indexVar));
  Command getRef=Turbine.arrayLookupComputed(varToExpr(oVar),representationType(oVar.type(),false),varToExpr(arrayVar),varToExpr(indexVar),true);
  pointAdd(getRef);
}",0.7307692307692307
132827,"/** 
 * Select the location to insert the array build instruction
 * @param block
 * @param array
 * @param keys
 * @param vals
 * @return
 */
private ListIterator<Statement> findArrayBuildPos(Logger logger,Block block,InitState outerInit,Var array,List<Arg> keys,List<Arg> vals){
  Set<Var> needsInit=new HashSet<Var>();
  if (InitVariables.varMustBeInitialized(array,true)) {
    if (!outerInit.initVars.contains(array)) {
      needsInit.add(array);
    }
  }
  for (  Arg key : keys) {
    if (key.isVar()) {
      assert(InitVariables.assignBeforeRead(key.getVar()));
      if (!outerInit.assignedVals.contains(key.getVar())) {
        needsInit.add(key.getVar());
      }
    }
  }
  for (  Arg val : vals) {
    assert(val.isVar());
    if (InitVariables.varMustBeInitialized(val.getVar(),false)) {
      if (!outerInit.initVars.contains(val.getVar())) {
        needsInit.add(val.getVar());
      }
    }
  }
  InitState blockInit=outerInit.enterBlock(block);
  ListIterator<Statement> insertPos=block.statementIterator();
  while (insertPos.hasNext() && !needsInit.isEmpty()) {
    Statement stmt=insertPos.next();
    InitVariables.updateInitVars(logger,stmt,blockInit,false);
    Iterator<Var> it=needsInit.iterator();
    while (it.hasNext()) {
      Var v=it.next();
      if (InitVariables.assignBeforeRead(v)) {
        if (blockInit.assignedVals.contains(v)) {
          it.remove();
        }
      }
 else       if (InitVariables.varMustBeInitialized(v,false)) {
        if (blockInit.initVars.contains(v)) {
          it.remove();
        }
      }
    }
  }
  if (!needsInit.isEmpty()) {
    logger.warn(""String_Node_Str"" + needsInit + ""String_Node_Str"");
  }
  return insertPos;
}","/** 
 * Select the location to insert the array build instruction
 * @param block
 * @param array
 * @param keys
 * @param vals
 * @return
 */
private ListIterator<Statement> findArrayBuildPos(Logger logger,Block block,InitState outerInit,Var array,List<Arg> keys,List<Arg> vals){
  Set<Var> needsInit=new HashSet<Var>();
  if (InitVariables.varMustBeInitialized(array,true)) {
    if (!outerInit.initVars.contains(array)) {
      needsInit.add(array);
    }
  }
  for (  Arg key : keys) {
    if (key.isVar()) {
      assert(InitVariables.assignBeforeRead(key.getVar()));
      if (!outerInit.assignedVals.contains(key.getVar())) {
        needsInit.add(key.getVar());
      }
    }
  }
  for (  Arg val : vals) {
    if (val.isVar() && InitVariables.varMustBeInitialized(val.getVar(),false)) {
      if (!outerInit.initVars.contains(val.getVar())) {
        needsInit.add(val.getVar());
      }
    }
  }
  InitState blockInit=outerInit.enterBlock(block);
  ListIterator<Statement> insertPos=block.statementIterator();
  while (insertPos.hasNext() && !needsInit.isEmpty()) {
    Statement stmt=insertPos.next();
    InitVariables.updateInitVars(logger,stmt,blockInit,false);
    Iterator<Var> it=needsInit.iterator();
    while (it.hasNext()) {
      Var v=it.next();
      if (InitVariables.assignBeforeRead(v)) {
        if (blockInit.assignedVals.contains(v)) {
          it.remove();
        }
      }
 else       if (InitVariables.varMustBeInitialized(v,false)) {
        if (blockInit.initVars.contains(v)) {
          it.remove();
        }
      }
    }
  }
  if (!needsInit.isEmpty()) {
    logger.warn(""String_Node_Str"" + needsInit + ""String_Node_Str"");
  }
  return insertPos;
}",0.9858490566037736
132828,"private Pair<List<Arg>,List<Arg>> removeOldInserts(Block block,Var arr){
  List<Arg> keys=new ArrayList<Arg>();
  List<Arg> vals=new ArrayList<Arg>();
  ListIterator<Statement> it=block.statementIterator();
  while (it.hasNext()) {
    Statement stmt=it.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      Instruction inst=stmt.instruction();
      if (inst.op == Opcode.ARR_STORE) {
        if (inst.getOutput(0).equals(arr)) {
          it.remove();
          Arg key=inst.getInput(0);
          Arg val=inst.getInput(1);
          assert(Types.isArrayKeyVal(arr,key));
          assert(Types.isElemType(arr,val.getVar()));
          keys.add(key);
          vals.add(val);
        }
      }
    }
  }
  return Pair.create(keys,vals);
}","private Pair<List<Arg>,List<Arg>> removeOldInserts(Block block,Var arr){
  List<Arg> keys=new ArrayList<Arg>();
  List<Arg> vals=new ArrayList<Arg>();
  ListIterator<Statement> it=block.statementIterator();
  while (it.hasNext()) {
    Statement stmt=it.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      Instruction inst=stmt.instruction();
      if (inst.op == Opcode.ARR_STORE) {
        if (inst.getOutput(0).equals(arr)) {
          it.remove();
          Arg key=inst.getInput(0);
          Arg val=inst.getInput(1);
          assert(Types.isArrayKeyVal(arr,key));
          assert(Types.isElemValType(arr,val));
          keys.add(key);
          vals.add(val);
        }
      }
    }
  }
  return Pair.create(keys,vals);
}",0.9919893190921228
132829,"/** 
 * Generate backend instruction for function call
 * @param context
 * @param function name of function
 * @param oList list of output variables
 * @param iList list of input variables (with correct types)
 * @param priorityVal optional priority value (can be null)
 * @throws UserException 
 */
private void backendFunctionCall(Context context,String function,FunctionType concrete,List<Var> oList,List<Var> iList,TaskProps props) throws UserException {
  props.assertInternalTypesValid();
  FunctionType def=context.lookupFunction(function);
  if (def == null) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + function);
  }
  if (context.hasFunctionProp(function,FnProp.DEPRECATED)) {
    LogHelper.warn(context,""String_Node_Str"" + function);
  }
  List<Var> backendIList=VarRepr.backendVars(iList);
  List<Var> backendOList=VarRepr.backendVars(oList);
  if (context.isIntrinsic(function)) {
    IntrinsicFunction intF=context.lookupIntrinsic(function);
    backend.intrinsicCall(intF,backendIList,backendOList,props);
  }
 else   if (context.hasFunctionProp(function,FnProp.BUILTIN)) {
    if (ForeignFunctions.hasOpEquiv(function)) {
      assert(oList.size() <= 1);
      Var backendOut=(backendOList.size() == 0 ? null : backendOList.get(0));
      backend.asyncOp(ForeignFunctions.getOpEquiv(function),backendOut,Arg.fromVarList(backendIList),props);
    }
 else {
      backend.builtinFunctionCall(function,backendOList,oList,props);
    }
  }
 else   if (context.hasFunctionProp(function,FnProp.COMPOSITE)) {
    TaskMode mode;
    if (context.hasFunctionProp(function,FnProp.SYNC)) {
      mode=TaskMode.SYNC;
    }
 else {
      mode=TaskMode.CONTROL;
    }
    backend.functionCall(function,Var.asArgList(backendIList),backendOList,mode,props);
  }
 else {
    backendCallWrapped(context,function,concrete,backendOList,backendIList,props);
  }
}","/** 
 * Generate backend instruction for function call
 * @param context
 * @param function name of function
 * @param oList list of output variables
 * @param iList list of input variables (with correct types)
 * @param priorityVal optional priority value (can be null)
 * @throws UserException 
 */
private void backendFunctionCall(Context context,String function,FunctionType concrete,List<Var> oList,List<Var> iList,TaskProps props) throws UserException {
  props.assertInternalTypesValid();
  FunctionType def=context.lookupFunction(function);
  if (def == null) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + function);
  }
  if (context.hasFunctionProp(function,FnProp.DEPRECATED)) {
    LogHelper.warn(context,""String_Node_Str"" + function);
  }
  List<Var> backendIList=VarRepr.backendVars(iList);
  List<Var> backendOList=VarRepr.backendVars(oList);
  if (context.isIntrinsic(function)) {
    IntrinsicFunction intF=context.lookupIntrinsic(function);
    backend.intrinsicCall(intF,backendIList,backendOList,props);
  }
 else   if (context.hasFunctionProp(function,FnProp.BUILTIN)) {
    if (ForeignFunctions.hasOpEquiv(function)) {
      assert(oList.size() <= 1);
      Var backendOut=(backendOList.size() == 0 ? null : backendOList.get(0));
      backend.asyncOp(ForeignFunctions.getOpEquiv(function),backendOut,Arg.fromVarList(backendIList),props);
    }
 else {
      backend.builtinFunctionCall(function,backendIList,backendOList,props);
    }
  }
 else   if (context.hasFunctionProp(function,FnProp.COMPOSITE)) {
    TaskMode mode;
    if (context.hasFunctionProp(function,FnProp.SYNC)) {
      mode=TaskMode.SYNC;
    }
 else {
      mode=TaskMode.CONTROL;
    }
    backend.functionCall(function,Var.asArgList(backendIList),backendOList,mode,props);
  }
 else {
    backendCallWrapped(context,function,concrete,backendOList,backendIList,props);
  }
}",0.9949722148716592
132830,"/** 
 * Make a standard computed value for array contents
 * @param arr
 * @param ix
 * @param contents
 * @param refResult if contents is ref
 * @return
 */
public static ValLoc makeArrayResult(Var arr,Arg ix,Var contents,boolean refResult,IsAssign isAssign){
}","/** 
 * Make a standard computed value for array contents
 * @param arr
 * @param ix
 * @param contents
 * @param refResult if contents is ref
 * @return
 */
public static ValLoc makeArrayResult(Var arr,Arg ix,Arg contents,boolean refResult,IsAssign isAssign){
}",0.9885496183206108
132831,"public static Instruction arrayStore(Var array,Arg ix,Arg member){
  assert(Types.isArray(array));
  assert(Types.isArrayKeyVal(array,ix));
  assert(Types.isElemType(array,member));
  return new TurbineOp(Opcode.ARR_STORE,array,ix,member);
}","public static Instruction arrayStore(Var array,Arg ix,Arg member){
}",0.4401294498381877
132832,"public static Instruction arrayStoreFuture(Var array,Var ix,Arg member){
  return new TurbineOp(Opcode.ARR_STORE_FUTURE,array,ix.asArg(),member);
}","public static Instruction arrayStoreFuture(Var array,Var ix,Arg member){
  assert(Types.isArray(array));
  assert(Types.isArrayKeyFuture(array,ix));
  assert(Types.isElemValType(array,member));
  return new TurbineOp(Opcode.ARR_STORE_FUTURE,array,ix.asArg(),member);
}",0.708433734939759
132833,"public static Instruction arrayRefStoreImm(Var outerArray,Var array,Arg ix,Arg member){
  return new TurbineOp(Opcode.AREF_STORE_IMM,Arrays.asList(outerArray,array),ix,member);
}","public static Instruction arrayRefStoreImm(Var outerArray,Var array,Arg ix,Arg member){
  assert(Types.isArrayRef(array));
  assert(Types.isArrayKeyVal(array,ix));
  assert(Types.isElemValType(array,member));
  return new TurbineOp(Opcode.AREF_STORE_IMM,Arrays.asList(outerArray,array),ix,member);
}",0.7463312368972747
132834,"public static Instruction arrayRefStoreFuture(Var outerArray,Var array,Var ix,Arg member){
  return new TurbineOp(Opcode.AREF_STORE_FUTURE,Arrays.asList(outerArray,array),ix.asArg(),member);
}","public static Instruction arrayRefStoreFuture(Var outerArray,Var array,Var ix,Arg member){
  assert(Types.isArrayRef(array));
  assert(Types.isArrayKeyFuture(array,ix));
  assert(Types.isElemValType(array,member));
  return new TurbineOp(Opcode.AREF_STORE_FUTURE,Arrays.asList(outerArray,array),ix.asArg(),member);
}",0.7559055118110236
132835,"@Override public List<ValLoc> getResults(){
switch (op) {
case LOAD_SCALAR:
case LOAD_REF:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_RECURSIVE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      Closed outIsClosed;
      if (op == Opcode.LOAD_REF) {
        outIsClosed=Closed.MAYBE_NOT;
      }
 else {
        outIsClosed=Closed.YES_NOT_RECURSIVE;
      }
      if (op == Opcode.LOAD_REF) {
        return ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO,IsAssign.NO).asList();
      }
 else {
        return vanillaResult(outIsClosed,IsAssign.NO).asList();
      }
    }
case STORE_REF:
case STORE_SCALAR:
case STORE_FILE:
case STORE_ARRAY:
case STORE_BAG:
case STORE_RECURSIVE:
{
    ValLoc assign=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
    Arg dst=getOutput(0).asArg();
    Arg src=getInput(0);
    if (op == Opcode.STORE_REF) {
      ValLoc retrieve=ValLoc.derefCompVal(src.getVar(),dst.getVar(),IsValCopy.NO,IsAssign.NO);
      return Arrays.asList(retrieve,assign);
    }
 else {
      return assign.asList();
    }
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result,IsAssign.NO));
  }
}
case GET_FILENAME:
{
Arg filename=getOutput(0).asArg();
Var file=getInput(0).getVar();
return ValLoc.makeFilename(filename,file).asList();
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar(),IsAssign.TO_LOCATION).asList();
}
case SET_FILENAME_VAL:
{
Var file=getOutput(0);
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val,IsAssign.TO_VALUE).asList();
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES,IsAssign.NO).asList();
}
case STRUCT_INIT_FIELD:
{
ValLoc lookup=ValLoc.makeStructLookupResult(getInput(1).getVar(),getOutput(0),getInput(0).getStringLit());
return lookup.asList();
}
case STRUCT_LOOKUP:
{
ValLoc lookup=ValLoc.makeStructLookupResult(getOutput(0),getInput(0).getVar(),getInput(1).getStringLit());
return lookup.asList();
}
case ARR_STORE:
case ARR_COPY_IN_IMM:
case ARR_STORE_FUTURE:
case ARR_COPY_IN_FUTURE:
case AREF_STORE_IMM:
case AREF_COPY_IN_IMM:
case AREF_STORE_FUTURE:
case AREF_COPY_IN_FUTURE:
{
Var arr;
if (op == Opcode.AREF_STORE_FUTURE || op == Opcode.AREF_STORE_IMM || op == Opcode.AREF_COPY_IN_FUTURE || op == Opcode.AREF_COPY_IN_IMM) {
arr=getOutput(1);
}
 else {
arr=getOutput(0);
}
Arg ix=getInput(0);
Var member=getInput(1).getVar();
boolean insertingRef=(op == Opcode.AREF_COPY_IN_FUTURE || op == Opcode.AREF_COPY_IN_IMM || op == Opcode.ARR_COPY_IN_FUTURE || op == Opcode.ARR_COPY_IN_IMM);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingRef,IsAssign.TO_VALUE));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE,IsAssign.NO));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Var val=getInput(2 * i + 1).getVar();
res.add(ValLoc.makeArrayResult(arr,key,val,false,IsAssign.TO_VALUE));
}
res.add(CommonFunctionCall.makeContainerSizeCV(arr,Arg.createIntLit(elemCount),false,IsAssign.NO));
return res;
}
case ARR_RETRIEVE:
case ARR_COPY_OUT_IMM:
case ARR_COPY_OUT_FUTURE:
case AREF_COPY_OUT_FUTURE:
case AREF_COPY_OUT_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARR_RETRIEVE) {
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,contents,false,IsAssign.NO));
}
 else {
assert(Types.isMemberReference(contents,arr));
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.makeArrayResult(arr,ix,contents,true,IsAssign.TO_LOCATION));
return res;
}
}
case ARR_CREATE_NESTED_FUTURE:
case ARR_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case ARRAY_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr;
if (op == Opcode.AREF_CREATE_NESTED_FUTURE || op == Opcode.AREF_CREATE_NESTED_IMM) {
arr=getOutput(2);
}
 else {
arr=getOutput(1);
}
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsRef=op != Opcode.ARR_CREATE_NESTED_IMM && op != Opcode.ARRAY_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr,returnsRef,IsAssign.NO));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsRef));
return res;
}
case COPY_REF:
{
List<ValLoc> res=new ArrayList<ValLoc>();
Var srcRef=getInput(0).getVar();
res.add(ValLoc.makeAlias(getOutput(0),srcRef));
return res;
}
case LOOKUP_CHECKPOINT:
case UNPACK_VALUES:
{
List<ValLoc> res=new ArrayList<ValLoc>(outputs.size());
for (int i=0; i < outputs.size(); i++) {
Var out=outputs.get(i);
res.add(ValLoc.buildResult(op,(Object)i,getInput(0).asList(),out.asArg(),Closed.YES_RECURSIVE,IsValCopy.NO,IsAssign.NO));
}
return res;
}
case PACK_VALUES:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case CHECKPOINT_LOOKUP_ENABLED:
case CHECKPOINT_WRITE_ENABLED:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case UNPACK_ARRAY_TO_FLAT:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
default :
return null;
}
}","@Override public List<ValLoc> getResults(){
switch (op) {
case LOAD_SCALAR:
case LOAD_REF:
case LOAD_FILE:
case LOAD_ARRAY:
case LOAD_BAG:
case LOAD_RECURSIVE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      Closed outIsClosed;
      if (op == Opcode.LOAD_REF) {
        outIsClosed=Closed.MAYBE_NOT;
      }
 else {
        outIsClosed=Closed.YES_NOT_RECURSIVE;
      }
      if (op == Opcode.LOAD_REF) {
        return ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO,IsAssign.NO).asList();
      }
 else {
        return vanillaResult(outIsClosed,IsAssign.NO).asList();
      }
    }
case STORE_REF:
case STORE_SCALAR:
case STORE_FILE:
case STORE_ARRAY:
case STORE_BAG:
case STORE_RECURSIVE:
{
    ValLoc assign=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
    Arg dst=getOutput(0).asArg();
    Arg src=getInput(0);
    if (op == Opcode.STORE_REF) {
      ValLoc retrieve=ValLoc.derefCompVal(src.getVar(),dst.getVar(),IsValCopy.NO,IsAssign.NO);
      return Arrays.asList(retrieve,assign);
    }
 else {
      return assign.asList();
    }
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.TO_LOCATION);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result,IsAssign.NO));
  }
}
case GET_FILENAME:
{
Arg filename=getOutput(0).asArg();
Var file=getInput(0).getVar();
return ValLoc.makeFilename(filename,file).asList();
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar(),IsAssign.TO_LOCATION).asList();
}
case SET_FILENAME_VAL:
{
Var file=getOutput(0);
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val,IsAssign.TO_VALUE).asList();
}
case DEREF_SCALAR:
case DEREF_FILE:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES,IsAssign.NO).asList();
}
case STRUCT_INIT_FIELD:
{
ValLoc lookup=ValLoc.makeStructLookupResult(getInput(1).getVar(),getOutput(0),getInput(0).getStringLit());
return lookup.asList();
}
case STRUCT_LOOKUP:
{
ValLoc lookup=ValLoc.makeStructLookupResult(getOutput(0),getInput(0).getVar(),getInput(1).getStringLit());
return lookup.asList();
}
case ARR_STORE:
case ARR_COPY_IN_IMM:
case ARR_STORE_FUTURE:
case ARR_COPY_IN_FUTURE:
case AREF_STORE_IMM:
case AREF_COPY_IN_IMM:
case AREF_STORE_FUTURE:
case AREF_COPY_IN_FUTURE:
{
Var arr;
if (op == Opcode.AREF_STORE_FUTURE || op == Opcode.AREF_STORE_IMM || op == Opcode.AREF_COPY_IN_FUTURE || op == Opcode.AREF_COPY_IN_IMM) {
arr=getOutput(1);
}
 else {
arr=getOutput(0);
}
Arg ix=getInput(0);
Arg member=getInput(1);
boolean insertingRef=(op == Opcode.AREF_COPY_IN_FUTURE || op == Opcode.AREF_COPY_IN_IMM || op == Opcode.ARR_COPY_IN_FUTURE || op == Opcode.ARR_COPY_IN_IMM);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingRef,IsAssign.TO_VALUE));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE,IsAssign.NO));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Arg val=getInput(2 * i + 1);
res.add(ValLoc.makeArrayResult(arr,key,val,false,IsAssign.TO_VALUE));
}
res.add(CommonFunctionCall.makeContainerSizeCV(arr,Arg.createIntLit(elemCount),false,IsAssign.NO));
return res;
}
case ARR_RETRIEVE:
case ARR_COPY_OUT_IMM:
case ARR_COPY_OUT_FUTURE:
case AREF_COPY_OUT_FUTURE:
case AREF_COPY_OUT_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARR_RETRIEVE) {
return ValLoc.makeArrayResult(arr,ix,contents.asArg(),false,IsAssign.NO).asList();
}
 else {
assert(Types.isMemberReference(contents,arr));
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.makeArrayResult(arr,ix,contents.asArg(),true,IsAssign.TO_LOCATION));
return res;
}
}
case ARR_CREATE_NESTED_FUTURE:
case ARR_CREATE_NESTED_IMM:
case AREF_CREATE_NESTED_FUTURE:
case AREF_CREATE_NESTED_IMM:
case ARRAY_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr;
if (op == Opcode.AREF_CREATE_NESTED_FUTURE || op == Opcode.AREF_CREATE_NESTED_IMM) {
arr=getOutput(2);
}
 else {
arr=getOutput(1);
}
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsRef=op != Opcode.ARR_CREATE_NESTED_IMM && op != Opcode.ARRAY_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr.asArg(),returnsRef,IsAssign.NO));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsRef));
return res;
}
case COPY_REF:
{
List<ValLoc> res=new ArrayList<ValLoc>();
Var srcRef=getInput(0).getVar();
res.add(ValLoc.makeAlias(getOutput(0),srcRef));
return res;
}
case LOOKUP_CHECKPOINT:
case UNPACK_VALUES:
{
List<ValLoc> res=new ArrayList<ValLoc>(outputs.size());
for (int i=0; i < outputs.size(); i++) {
Var out=outputs.get(i);
res.add(ValLoc.buildResult(op,(Object)i,getInput(0).asList(),out.asArg(),Closed.YES_RECURSIVE,IsValCopy.NO,IsAssign.NO));
}
return res;
}
case PACK_VALUES:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case CHECKPOINT_LOOKUP_ENABLED:
case CHECKPOINT_WRITE_ENABLED:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
case UNPACK_ARRAY_TO_FLAT:
return vanillaResult(Closed.YES_NOT_RECURSIVE,IsAssign.NO).asList();
default :
return null;
}
}",0.9910860469346916
132836,"/** 
 * In cases where variable is closed upon starting loop iterations, switch to passing as value
 * @param logger
 * @param loop
 */
private void optimizeLoop(Logger logger,Loop loop){
  List<BlockingVar> closedVars=loop.closedLoopVars();
  if (closedVars.isEmpty()) {
    return;
  }
  Block outerBlock=loop.parent();
  ListIterator<Statement> outerInsertPoint=outerBlock.statementIterator();
  List<Statement> outerFetches=new ArrayList<Statement>();
  List<Var> outerFetched=new ArrayList<Var>();
  LoopInstructions insts=loop.findInstructions();
  Block innerBlock=insts.continueInstBlock;
  ListIterator<Statement> innerInsertPoint=findLoopContinueInsertPoint(insts.continueInst,innerBlock);
  List<Statement> innerFetches=new ArrayList<Statement>();
  List<Var> innerFetched=new ArrayList<Var>();
  for (  BlockingVar closedVar : closedVars) {
    if (replaceLoopVarWithVal(closedVar.var)) {
      Var init=loop.getInitVal(closedVar.var).getVar();
      Var subsequent=loop.getUpdateVal(closedVar.var).getVar();
      Var outerFetchedV=fetchLoopVar(init,closedVar.recursive,outerBlock,outerFetches);
      Var innerFetchedV=fetchLoopVar(subsequent,closedVar.recursive,innerBlock,innerFetches);
      outerFetched.add(outerFetchedV);
      innerFetched.add(innerFetchedV);
      placeAndClear(outerInsertPoint,outerFetches);
      placeAndClear(innerInsertPoint,innerFetches);
      Var oldLoopVar=closedVar.var;
      Var newLoopVar=new Var(outerFetchedV.type(),OptUtil.optVPrefix(outerBlock,oldLoopVar),outerFetchedV.storage(),DefType.LOCAL_COMPILER,VarProvenance.valueOf(oldLoopVar));
      boolean blockOn=Types.canWaitForFinalize(newLoopVar) && loop.isBlocking(oldLoopVar);
      loop.replaceLoopVar(oldLoopVar,newLoopVar,outerFetchedV.asArg(),innerFetchedV.asArg(),blockOn);
      loop.getLoopBody().addVariable(oldLoopVar);
      loop.getLoopBody().addInstructionFront(ICInstructions.futureSet(oldLoopVar,newLoopVar.asArg()));
    }
  }
}","/** 
 * In cases where variable is closed upon starting loop iterations, switch to passing as value
 * @param logger
 * @param loop
 */
private void optimizeLoop(Logger logger,Loop loop){
  List<BlockingVar> closedVars=loop.closedLoopVars();
  if (closedVars.isEmpty()) {
    return;
  }
  Block outerBlock=loop.parent();
  ListIterator<Statement> outerInsertPoint=outerBlock.statementEndIterator();
  List<Statement> outerFetches=new ArrayList<Statement>();
  List<Var> outerFetched=new ArrayList<Var>();
  LoopInstructions insts=loop.findInstructions();
  Block innerBlock=insts.continueInstBlock;
  ListIterator<Statement> innerInsertPoint=findLoopContinueInsertPoint(insts.continueInst,innerBlock);
  List<Statement> innerFetches=new ArrayList<Statement>();
  List<Var> innerFetched=new ArrayList<Var>();
  for (  BlockingVar closedVar : closedVars) {
    if (replaceLoopVarWithVal(closedVar.var)) {
      Var init=loop.getInitVal(closedVar.var).getVar();
      Var subsequent=loop.getUpdateVal(closedVar.var).getVar();
      Var outerFetchedV=fetchLoopVar(init,closedVar.recursive,outerBlock,outerFetches);
      Var innerFetchedV=fetchLoopVar(subsequent,closedVar.recursive,innerBlock,innerFetches);
      outerFetched.add(outerFetchedV);
      innerFetched.add(innerFetchedV);
      placeAndClear(outerInsertPoint,outerFetches);
      placeAndClear(innerInsertPoint,innerFetches);
      Var oldLoopVar=closedVar.var;
      Var newLoopVar=new Var(outerFetchedV.type(),OptUtil.optVPrefix(outerBlock,oldLoopVar),outerFetchedV.storage(),DefType.LOCAL_COMPILER,VarProvenance.valueOf(oldLoopVar));
      boolean blockOn=Types.canWaitForFinalize(newLoopVar) && loop.isBlocking(oldLoopVar);
      loop.replaceLoopVar(oldLoopVar,newLoopVar,outerFetchedV.asArg(),innerFetchedV.asArg(),blockOn);
      loop.getLoopBody().addVariable(oldLoopVar);
      loop.getLoopBody().addInstructionFront(ICInstructions.futureSet(oldLoopVar,newLoopVar.asArg()));
    }
  }
}",0.9992325402916348
132837,"/** 
 * @param context
 * @param tree
 * @param expectedResult
 * @param outType if not null, only return operators with this output type
 * @param outArgTypes Put the argument types of the operator in this list if not null
 * @param expectUnambig at this stage, expect operator not to be ambiguous - logdebug message if it is
 * @return at least one builtin operator  
 * @throws TypeMismatchException if no matches possible
 */
private static List<Op> getOpsFromTree(Context context,SwiftAST tree,Type outType,List<Type> outArgTypes,boolean expectUnambig) throws UserException {
  assert(tree.getType() == ExMParser.OPERATOR);
  assert(tree.getChildCount() >= 1);
  int opTok=tree.child(0).getType();
  String opName=extractOpName(tree);
  List<Type> argTypes=findOperatorArgTypes(context,tree,opName);
  if (outArgTypes != null) {
    outArgTypes.clear();
    outArgTypes.addAll(argTypes);
  }
  List<Op> matched=new ArrayList<Op>();
  for (  Op candidate : Operators.getOps(opTok)) {
    OpType opType=candidate.type;
    if (opOutputMatches(outType,opType) && opInputsMatch(argTypes,opType)) {
      matched.add(candidate);
    }
  }
  if (matched.size() > 1) {
    List<String> typeNames=new ArrayList<String>();
    for (    Type argType : argTypes) {
      typeNames.add(argType.typeName());
    }
    if (matched.size() == 0) {
      String msg=""String_Node_Str"" + opName + ""String_Node_Str""+ ""String_Node_Str""+ typeNames.toString();
      if (outType != null) {
        msg+=""String_Node_Str"" + outType;
      }
      throw new UndefinedOperatorException(context,msg);
    }
 else     if (expectUnambig) {
      assert(matched.size() > 1);
      Logging.getSTCLogger().debug(""String_Node_Str"" + opName + ""String_Node_Str""+ typeNames+ ""String_Node_Str""+ matched.toString());
    }
  }
  return matched;
}","/** 
 * @param context
 * @param tree
 * @param expectedResult
 * @param outType if not null, only return operators with this output type
 * @param outArgTypes Put the argument types of the operator in this list if not null
 * @param expectUnambig at this stage, expect operator not to be ambiguous - logdebug message if it is
 * @return at least one builtin operator  
 * @throws TypeMismatchException if no matches possible
 */
private static List<Op> getOpsFromTree(Context context,SwiftAST tree,Type outType,List<Type> outArgTypes,boolean expectUnambig) throws UserException {
  assert(tree.getType() == ExMParser.OPERATOR);
  assert(tree.getChildCount() >= 1);
  int opTok=tree.child(0).getType();
  String opName=extractOpName(tree);
  List<Type> argTypes=findOperatorArgTypes(context,tree,opName);
  if (outArgTypes != null) {
    outArgTypes.clear();
    outArgTypes.addAll(argTypes);
  }
  List<Op> matched=new ArrayList<Op>();
  for (  Op candidate : Operators.getOps(opTok)) {
    OpType opType=candidate.type;
    if (opOutputMatches(outType,opType) && opInputsMatch(argTypes,opType)) {
      matched.add(candidate);
    }
  }
  if (matched.size() != 1) {
    List<String> typeNames=new ArrayList<String>();
    for (    Type argType : argTypes) {
      typeNames.add(argType.typeName());
    }
    if (matched.size() == 0) {
      String msg=""String_Node_Str"" + opName + ""String_Node_Str""+ ""String_Node_Str""+ typeNames.toString();
      if (outType != null) {
        msg+=""String_Node_Str"" + outType;
      }
      throw new UndefinedOperatorException(context,msg);
    }
 else     if (expectUnambig) {
      assert(matched.size() > 1);
      Logging.getSTCLogger().debug(""String_Node_Str"" + opName + ""String_Node_Str""+ typeNames+ ""String_Node_Str""+ matched.toString());
    }
  }
  return matched;
}",0.999172870140612
132838,"public Set<Var> getRecursivelyClosed(int stmtIndex){
  return new ClosedSet(stmtIndex,false);
}","public Set<Var> getRecursivelyClosed(int stmtIndex){
  return new ClosedSet(stmtIndex,true);
}",0.9629629629629628
132839,"public static SetVariable structRefGet(String target,Value variable){
  return fileRefDecrGet(target,variable,LiteralInt.ZERO);
}","public static SetVariable structRefGet(String target,Value variable){
  return structRefDecrGet(target,variable,LiteralInt.ZERO);
}",0.9615384615384616
132840,"private void piggybackDecrementsOnInstructions(Logger logger,Function fn,Block block,final RCTracker tracker,final RefCountType rcType){
  final Counters<Var> candidates=tracker.getVarCandidates(block,rcType,RCDir.DECR);
  UseFinder subblockWalker=new UseFinder(tracker,rcType,candidates.keySet(),null);
  ListIterator<Continuation> cit=block.continuationEndIterator();
  while (cit.hasPrevious()) {
    Continuation cont=cit.previous();
    if (RCUtil.isAsyncForeachLoop(cont)) {
      AbstractForeachLoop loop=(AbstractForeachLoop)cont;
      List<Var> piggybacked=loop.tryPiggyBack(candidates,rcType,true);
      for (      Var pv : piggybacked) {
        logger.trace(""String_Node_Str"" + pv + ""String_Node_Str""+ rcType+ ""String_Node_Str""+ candidates.getCount(pv));
      }
      candidates.resetAll(piggybacked);
      tracker.resetAll(rcType,piggybacked,RCDir.DECR);
    }
    subblockWalker.reset();
    TreeWalk.walkSyncChildren(logger,fn,cont,subblockWalker);
    removeCandidates(subblockWalker.getUsedVars(),tracker,candidates.keySet());
  }
  List<Var> successful=new ArrayList<Var>();
  ListIterator<Statement> it=block.statementEndIterator();
  while (it.hasPrevious()) {
    Statement stmt=it.previous();
switch (stmt.type()) {
case INSTRUCTION:
{
        Instruction inst=stmt.instruction();
        List<Var> piggybacked=inst.tryPiggyback(candidates,rcType);
        candidates.resetAll(piggybacked);
        successful.addAll(piggybacked);
        List<Var> used=findUses(inst,rcType,candidates.keySet());
        removeCandidates(used,tracker,candidates.keySet());
        break;
      }
case CONDITIONAL:
    subblockWalker.reset();
  TreeWalk.walkSyncChildren(logger,fn,stmt.conditional(),subblockWalker);
removeCandidates(subblockWalker.getUsedVars(),tracker,candidates.keySet());
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + stmt.type());
}
}
for (Var v : successful) {
assert(v != null);
tracker.reset(rcType,v);
}
}","private void piggybackDecrementsOnInstructions(Logger logger,Function fn,Block block,final RCTracker tracker,final RefCountType rcType){
  final Counters<Var> candidates=tracker.getVarCandidates(block,rcType,RCDir.DECR);
  UseFinder subblockWalker=new UseFinder(tracker,rcType,candidates.keySet(),null);
  ListIterator<Continuation> cit=block.continuationEndIterator();
  while (cit.hasPrevious()) {
    Continuation cont=cit.previous();
    if (RCUtil.isAsyncForeachLoop(cont)) {
      AbstractForeachLoop loop=(AbstractForeachLoop)cont;
      List<Var> piggybacked=loop.tryPiggyBack(candidates,rcType,RCDir.DECR);
      for (      Var pv : piggybacked) {
        logger.trace(""String_Node_Str"" + pv + ""String_Node_Str""+ rcType+ ""String_Node_Str""+ candidates.getCount(pv));
      }
      candidates.resetAll(piggybacked);
      tracker.resetAll(rcType,piggybacked,RCDir.DECR);
    }
    subblockWalker.reset();
    TreeWalk.walkSyncChildren(logger,fn,cont,subblockWalker);
    removeCandidates(subblockWalker.getUsedVars(),tracker,candidates.keySet());
  }
  List<Var> successful=new ArrayList<Var>();
  ListIterator<Statement> it=block.statementEndIterator();
  while (it.hasPrevious()) {
    Statement stmt=it.previous();
switch (stmt.type()) {
case INSTRUCTION:
{
        Instruction inst=stmt.instruction();
        List<Var> piggybacked=inst.tryPiggyback(candidates,rcType);
        if (piggybacked.size() > 0 && logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + piggybacked + ""String_Node_Str""+ inst);
        }
        candidates.resetAll(piggybacked);
        successful.addAll(piggybacked);
        List<Var> used=findUses(inst,rcType,candidates.keySet());
        removeCandidates(used,tracker,candidates.keySet());
        break;
      }
case CONDITIONAL:
    subblockWalker.reset();
  TreeWalk.walkSyncChildren(logger,fn,stmt.conditional(),subblockWalker);
removeCandidates(subblockWalker.getUsedVars(),tracker,candidates.keySet());
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + stmt.type());
}
}
for (Var v : successful) {
assert(v != null);
tracker.reset(rcType,v);
}
}",0.9578224619911722
132841,"private void removeCandidate(Var var,RCTracker tracker,Set<AliasKey> keySet,Set<Var> varSet){
  if (keySet != null) {
    keySet.remove(tracker.getCountKey(var));
  }
  if (varSet != null) {
    varSet.remove(var);
  }
}","private void removeCandidate(Var var,RCTracker tracker,Set<AliasKey> keySet,Set<Var> varSet){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + var.name());
  }
  if (keySet != null) {
    keySet.remove(tracker.getCountKey(var));
  }
  if (varSet != null) {
    varSet.remove(var);
  }
}",0.8349146110056926
132842,"/** 
 * Update increments for variables passed into continuation
 * @param cont
 * @param increments
 */
private void updateIncrementsPassIntoCont(Continuation cont,RCTracker increments){
  if (cont.spawnsSingleTask()) {
    long incr=1;
    for (    Var keepOpen : cont.getKeepOpenVars()) {
      assert(RefCounting.hasWriteRefCount(keepOpen));
      increments.writeIncr(keepOpen,incr);
    }
    Set<Var> readIncrTmp=new HashSet<Var>();
    for (    PassedVar passedIn : cont.getAllPassedVars()) {
      if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
        readIncrTmp.add(passedIn.var);
      }
    }
    for (    BlockingVar blockingVar : cont.blockingVars(false)) {
      if (RefCounting.hasReadRefCount(blockingVar.var)) {
        readIncrTmp.add(blockingVar.var);
      }
    }
    for (    Var v : readIncrTmp) {
      increments.readIncr(v,incr);
    }
  }
 else   if (RCUtil.isForeachLoop(cont)) {
    AbstractForeachLoop foreach=(AbstractForeachLoop)cont;
    updateIncrementsPassIntoForeach(increments,foreach);
  }
}","/** 
 * Update increments for variables passed into continuation
 * @param cont
 * @param increments
 */
private void updateIncrementsPassIntoCont(Continuation cont,RCTracker increments){
  if (cont.spawnsSingleTask()) {
    long incr=1;
    Set<Var> alreadyAdded=new HashSet<Var>();
    for (    Var keepOpen : cont.getKeepOpenVars()) {
      assert(RefCounting.hasWriteRefCount(keepOpen));
      alreadyAdded.add(keepOpen);
      increments.writeIncr(keepOpen,incr);
    }
    for (    PassedVar passedIn : cont.getAllPassedVars()) {
      logger.trace(cont.getType() + ""String_Node_Str"" + passedIn.var.name());
      if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
        increments.readIncr(passedIn.var,incr);
        alreadyAdded.add(passedIn.var);
      }
    }
    for (    BlockingVar blockingVar : cont.blockingVars(false)) {
      if (RefCounting.hasReadRefCount(blockingVar.var) && !alreadyAdded.contains(blockingVar.var)) {
        increments.readIncr(blockingVar.var,incr);
      }
    }
  }
 else   if (RCUtil.isForeachLoop(cont)) {
    AbstractForeachLoop foreach=(AbstractForeachLoop)cont;
    updateIncrementsPassIntoForeach(increments,foreach);
  }
}",0.6965792980897378
132843,"private void removePullupIncrements(Block block,RCTracker toRemove){
  ListIterator<Statement> it=block.statementIterator();
  while (it.hasNext()) {
    Statement stmt=it.next();
switch (stmt.type()) {
case INSTRUCTION:
{
        Instruction inst=stmt.instruction();
        if (RefCountOp.isIncrement(inst.op)) {
          Var v=RefCountOp.getRCTarget(inst);
          Arg amountArg=RefCountOp.getRCAmount(inst);
          RefCountType rcType=RefCountOp.getRCType(inst.op);
          if (amountArg.isIntVal()) {
            long amount=amountArg.getIntLit();
            long toRemoveAmount=toRemove.getCount(rcType,v,RCDir.INCR);
            assert(toRemoveAmount <= amount && toRemoveAmount >= 0);
            if (toRemoveAmount > 0) {
              it.remove();
              if (toRemoveAmount < amount) {
                Arg newAmount=Arg.createIntLit(amount - toRemoveAmount);
                it.add(RefCountOp.incrRef(rcType,v,newAmount));
              }
            }
          }
        }
        break;
      }
case CONDITIONAL:
    break;
default :
  throw new STCRuntimeError(""String_Node_Str"" + stmt.type());
}
}
}","private void removePullupIncrements(Block block,RCTracker toRemove){
  ListIterator<Statement> it=block.statementIterator();
  while (it.hasNext()) {
    Statement stmt=it.next();
switch (stmt.type()) {
case INSTRUCTION:
{
        Instruction inst=stmt.instruction();
        if (RefCountOp.isIncrement(inst.op)) {
          Var v=RefCountOp.getRCTarget(inst);
          Arg amountArg=RefCountOp.getRCAmount(inst);
          RefCountType rcType=RefCountOp.getRCType(inst.op);
          if (amountArg.isIntVal()) {
            long amount=amountArg.getIntLit();
            long toRemoveAmount=toRemove.getCount(rcType,v,RCDir.INCR);
            assert(toRemoveAmount <= amount && toRemoveAmount >= 0);
            if (toRemoveAmount > 0) {
              logger.trace(""String_Node_Str"" + v.name() + ""String_Node_Str""+ rcType+ ""String_Node_Str""+ toRemoveAmount);
              it.remove();
              if (toRemoveAmount < amount) {
                Arg newAmount=Arg.createIntLit(amount - toRemoveAmount);
                it.add(RefCountOp.incrRef(rcType,v,newAmount));
              }
            }
          }
        }
        break;
      }
case CONDITIONAL:
    break;
default :
  throw new STCRuntimeError(""String_Node_Str"" + stmt.type());
}
}
}",0.9491810163796724
132844,"private void addDecrementsAsyncCont(Continuation cont,RCTracker increments){
  long amount=1;
  for (  Var keepOpen : cont.getKeepOpenVars()) {
    increments.writeDecr(keepOpen,amount);
  }
  Set<Var> readIncrTmp=new HashSet<Var>();
  for (  PassedVar passedIn : cont.getAllPassedVars()) {
    if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
      readIncrTmp.add(passedIn.var);
    }
  }
  for (  BlockingVar blockingVar : cont.blockingVars(false)) {
    if (RefCounting.hasReadRefCount(blockingVar.var)) {
      readIncrTmp.add(blockingVar.var);
    }
  }
  for (  Var v : readIncrTmp) {
    increments.readDecr(v,amount);
  }
}","private void addDecrementsAsyncCont(Continuation cont,RCTracker increments){
  long amount=1;
  Set<Var> alreadyAdded=new HashSet<Var>();
  for (  Var keepOpen : cont.getKeepOpenVars()) {
    increments.writeDecr(keepOpen,amount);
    alreadyAdded.add(keepOpen);
  }
  for (  PassedVar passedIn : cont.getAllPassedVars()) {
    logger.trace(cont.getType() + ""String_Node_Str"" + passedIn.var.name());
    if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
      increments.readDecr(passedIn.var,amount);
      alreadyAdded.add(passedIn.var);
    }
  }
  for (  BlockingVar blockingVar : cont.blockingVars(false)) {
    if (RefCounting.hasReadRefCount(blockingVar.var) && !alreadyAdded.contains(blockingVar.var)) {
      increments.readDecr(blockingVar.var,amount);
    }
  }
}",0.6243969676085458
132845,"private void removePullupDecrements(Block block,RCTracker toRemove){
  ListIterator<CleanupAction> it=block.cleanupIterator();
  while (it.hasNext()) {
    CleanupAction ca=it.next();
    Instruction inst=ca.action();
    if (RefCountOp.isDecrement(inst.op)) {
      Var v=RefCountOp.getRCTarget(inst);
      Arg amountArg=RefCountOp.getRCAmount(inst);
      RefCountType rcType=RefCountOp.getRCType(inst.op);
      if (amountArg.isIntVal()) {
        long amount=amountArg.getIntLit();
        long toRemoveAmount=-1 * toRemove.getCount(rcType,v,RCDir.DECR);
        assert(toRemoveAmount >= 0 && toRemoveAmount <= amount);
        if (toRemoveAmount > 0) {
          it.remove();
          if (toRemoveAmount < amount) {
            Arg newAmount=Arg.createIntLit(amount - toRemoveAmount);
            Instruction newDecr=RefCountOp.decrRef(rcType,v,newAmount);
            it.add(new CleanupAction(ca.var(),newDecr));
          }
        }
      }
    }
  }
}","private void removePullupDecrements(Block block,RCTracker toRemove){
  ListIterator<CleanupAction> it=block.cleanupIterator();
  while (it.hasNext()) {
    CleanupAction ca=it.next();
    Instruction inst=ca.action();
    if (RefCountOp.isDecrement(inst.op)) {
      Var v=RefCountOp.getRCTarget(inst);
      Arg amountArg=RefCountOp.getRCAmount(inst);
      RefCountType rcType=RefCountOp.getRCType(inst.op);
      if (amountArg.isIntVal()) {
        long amount=amountArg.getIntLit();
        long toRemoveAmount=-1 * toRemove.getCount(rcType,v,RCDir.DECR);
        logger.trace(""String_Node_Str"" + v.name() + ""String_Node_Str""+ rcType+ ""String_Node_Str""+ toRemoveAmount);
        assert(toRemoveAmount >= 0 && toRemoveAmount <= amount);
        if (toRemoveAmount > 0) {
          it.remove();
          if (toRemoveAmount < amount) {
            Arg newAmount=Arg.createIntLit(amount - toRemoveAmount);
            Instruction newDecr=RefCountOp.decrRef(rcType,v,newAmount);
            it.add(new CleanupAction(ca.var(),newDecr));
          }
        }
      }
    }
  }
}",0.9435998038254046
132846,"/** 
 * Try to piggyback constant incrs/decrs from outside continuation.
 * @param increments
 * @param type
 * @param decrement if true, try to piggyback decrements, if false, increments
 * @return list of vars for which increments were piggybacked
 */
public List<Var> tryPiggyBack(Counters<Var> increments,RefCountType type,boolean decrement){
  List<Var> result=new ArrayList<Var>();
  for (  RefCount startIncr : startIncrements) {
    if (startIncr.type == type) {
      long incr=increments.getCount(startIncr.var);
      if ((decrement && incr < 0) || (!decrement && incr > 0)) {
        addConstantStartIncrement(startIncr.var,type,Arg.createIntLit(incr));
        result.add(startIncr.var);
      }
    }
  }
  return result;
}","/** 
 * Try to piggyback constant incrs/decrs from outside continuation.
 * @param increments
 * @param type
 * @param dir whether to piggyback decrements or increments
 * @return list of vars for which increments were piggybacked
 */
public List<Var> tryPiggyBack(Counters<Var> increments,RefCountType type,RCDir dir){
  List<Var> result=new ArrayList<Var>();
  for (  RefCount startIncr : startIncrements) {
    if (startIncr.type == type) {
      long incr=increments.getCount(startIncr.var);
      if ((dir == RCDir.DECR && incr < 0) || (dir == RCDir.INCR && incr > 0)) {
        addConstantStartIncrement(startIncr.var,type,Arg.createIntLit(incr));
        result.add(startIncr.var);
      }
    }
  }
  return result;
}",0.8317373461012312
132847,"/** 
 * Insert all reference increments and decrements in place
 * @param stmtIt
 * @param increments
 */
public void dumpIncrements(Instruction inst,Block block,ListIterator<Statement> stmtIt,RCTracker increments){
  for (  RefCountType rcType : RefcountPass.RC_TYPES) {
    for (    Entry<AliasKey,Long> e : increments.rcIter(rcType,RCDir.INCR)) {
      Var var=increments.getRefCountVar(block,e.getKey(),true);
      Long incr=e.getValue();
      assert(incr >= 0);
      if (incr > 0) {
        boolean varInit=inst.isInitialized(var);
        if (inst != null && !(var.storage() == Alloc.ALIAS && varInit)) {
          insertIncrBefore(block,stmtIt,var,incr,rcType);
        }
 else {
          insertIncrAfter(block,stmtIt,var,incr,rcType);
        }
      }
 else       if (incr < 0) {
        insertDecrAfter(block,stmtIt,var,incr * -1,rcType);
      }
    }
  }
  increments.resetAll();
}","/** 
 * Insert all reference increments and decrements in place
 * @param stmt the statement to insert before or afternull indicates end of the block
 * @param stmtIt
 * @param increments
 */
public void dumpIncrements(Statement stmt,Block block,ListIterator<Statement> stmtIt,RCTracker increments){
  for (  RefCountType rcType : RefcountPass.RC_TYPES) {
    for (    Entry<AliasKey,Long> e : increments.rcIter(rcType,RCDir.INCR)) {
      Var var=increments.getRefCountVar(block,e.getKey(),true);
      assert(var != null);
      Long incr=e.getValue();
      assert(incr >= 0);
      if (incr > 0) {
        boolean varInit=stmt != null && stmt.type() == StatementType.INSTRUCTION && stmt.instruction().isInitialized(var);
        if (stmt != null && (stmt.type() == StatementType.INSTRUCTION && !(var.storage() == Alloc.ALIAS && varInit))) {
          insertIncrBefore(block,stmtIt,var,incr,rcType);
        }
 else {
          insertIncrAfter(block,stmtIt,var,incr,rcType);
        }
      }
 else       if (incr < 0) {
        insertDecrAfter(block,stmtIt,var,incr * -1,rcType);
      }
    }
  }
  increments.resetAll(RCDir.INCR);
}",0.8275184275184275
132848,"public void dumpDecrements(Block block,RCTracker increments){
  for (  RefCountType rcType : RefcountPass.RC_TYPES) {
    for (    Entry<AliasKey,Long> e : increments.rcIter(rcType,RCDir.DECR)) {
      assert(e.getValue() <= 0);
      Var var=increments.getRefCountVar(block,e.getKey(),true);
      Arg amount=Arg.createIntLit(e.getValue() * -1);
      block.addCleanup(var,RefCountOp.decrRef(rcType,var,amount));
      e.setValue(0L);
    }
  }
}","public void dumpDecrements(Block block,RCTracker increments){
  for (  RefCountType rcType : RefcountPass.RC_TYPES) {
    for (    Entry<AliasKey,Long> e : increments.rcIter(rcType,RCDir.DECR)) {
      assert(e.getValue() <= 0);
      Var var=increments.getRefCountVar(block,e.getKey(),true);
      Arg amount=Arg.createIntLit(e.getValue() * -1);
      block.addCleanup(var,RefCountOp.decrRef(rcType,var,amount));
    }
  }
  increments.resetAll(RCDir.DECR);
}",0.9305402425578831
132849,"public void resetAll(){
  for (  RefCountType rcType : RefcountPass.RC_TYPES) {
    for (    RCDir dir : RCDir.values()) {
      getCounters(rcType,dir).resetAll();
    }
  }
}","public void resetAll(RCDir dir){
  for (  RefCountType rcType : RefcountPass.RC_TYPES) {
    getCounters(rcType,dir).resetAll();
  }
}",0.8064516129032258
132850,"/** 
 * Work out how much each variable needs to be decrementent. In some cases this will place the decrement instructions right away
 * @param block
 * @param increments
 */
private void countBlockDecrements(Function fn,Block block,RCTracker increments){
  if (block.getType() == BlockType.MAIN_BLOCK) {
    assert(block == fn.mainBlock());
    if (fn.isAsync()) {
      for (      Var i : fn.getInputList()) {
        increments.readDecr(i);
        if (Types.isScalarUpdateable(i) && RefCounting.WRITABLE_UPDATEABLE_INARGS) {
          increments.writeDecr(i);
        }
      }
      for (      PassedVar o : fn.getPassedOutputList()) {
        increments.writeDecr(o.var);
        if (!o.writeOnly) {
          increments.readDecr(o.var);
        }
      }
    }
  }
  for (  Var var : block.getVariables()) {
    if (var.storage() != Alloc.ALIAS && !Types.isStruct(var)) {
      increments.readDecr(var);
      increments.writeDecr(var);
    }
  }
  if (RCUtil.cancelEnabled()) {
    ListIterator<CleanupAction> caIt=block.cleanupIterator();
    while (caIt.hasNext()) {
      CleanupAction ca=caIt.next();
      Instruction action=ca.action();
      if (RefCountOp.isRefcountOp(action.op) && RefCountOp.isDecrement(action.op)) {
        Var decrVar=RefCountOp.getRCTarget(action);
        Arg amount=RefCountOp.getRCAmount(action);
        if (amount.isIntVal()) {
          RefCountType rcType=RefCountOp.getRCType(action.op);
          increments.decr(decrVar,rcType,amount.getIntLit());
          caIt.remove();
        }
      }
    }
  }
  if (!RCUtil.cancelEnabled()) {
    placer.dumpDecrements(block,increments);
  }
}","/** 
 * Work out how much each variable needs to be decrementent. In some cases this will place the decrement instructions right away
 * @param block
 * @param increments
 */
private void countBlockDecrements(Function fn,Block block,RCTracker increments){
  if (block.getType() == BlockType.MAIN_BLOCK) {
    assert(block == fn.mainBlock());
    if (fn.isAsync()) {
      for (      Var i : fn.getInputList()) {
        increments.readDecr(i);
        if (Types.isScalarUpdateable(i) && RefCounting.WRITABLE_UPDATEABLE_INARGS) {
          increments.writeDecr(i);
        }
      }
      for (      PassedVar o : fn.getPassedOutputList()) {
        increments.writeDecr(o.var);
        if (!o.writeOnly) {
          increments.readDecr(o.var);
        }
      }
    }
  }
  for (  Var var : block.getVariables()) {
    if (var.storage() != Alloc.ALIAS && !Types.isStruct(var)) {
      increments.readDecr(var);
      increments.writeDecr(var);
    }
  }
  ListIterator<CleanupAction> caIt=block.cleanupIterator();
  while (caIt.hasNext()) {
    CleanupAction ca=caIt.next();
    Instruction action=ca.action();
    if (RefCountOp.isRefcountOp(action.op) && RefCountOp.isDecrement(action.op)) {
      Var decrVar=RefCountOp.getRCTarget(action);
      Arg amount=RefCountOp.getRCAmount(action);
      if (amount.isIntVal()) {
        RefCountType rcType=RefCountOp.getRCType(action.op);
        increments.decr(decrVar,rcType,amount.getIntLit());
        caIt.remove();
      }
    }
  }
  if (!RCUtil.cancelEnabled()) {
    placer.dumpDecrements(block,increments);
  }
}",0.9800124921923796
132851,"private void addDecrementsAsyncCont(Continuation cont,RCTracker increments){
  long amount=1;
  Set<Var> alreadyAdded=new HashSet<Var>();
  for (  Var keepOpen : cont.getKeepOpenVars()) {
    increments.writeDecr(keepOpen,amount);
    alreadyAdded.add(keepOpen);
  }
  for (  PassedVar passedIn : cont.getAllPassedVars()) {
    logger.trace(cont.getType() + ""String_Node_Str"" + passedIn.var.name());
    if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
      increments.readDecr(passedIn.var,amount);
      alreadyAdded.add(passedIn.var);
    }
  }
  for (  BlockingVar blockingVar : cont.blockingVars(false)) {
    if (RefCounting.hasReadRefCount(blockingVar.var) && !alreadyAdded.contains(blockingVar.var)) {
      increments.readDecr(blockingVar.var,amount);
    }
  }
}","private void addDecrementsAsyncCont(Continuation cont,RCTracker increments){
  long amount=1;
  Set<Var> alreadyAdded=new HashSet<Var>();
  for (  Var keepOpen : cont.getKeepOpenVars()) {
    increments.writeDecr(keepOpen,amount);
    alreadyAdded.add(keepOpen);
  }
  for (  PassedVar passedIn : cont.getAllPassedVars()) {
    if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
      increments.readDecr(passedIn.var,amount);
      alreadyAdded.add(passedIn.var);
    }
  }
  for (  BlockingVar blockingVar : cont.blockingVars(false)) {
    if (RefCounting.hasReadRefCount(blockingVar.var) && !alreadyAdded.contains(blockingVar.var)) {
      increments.readDecr(blockingVar.var,amount);
    }
  }
}",0.949868073878628
132852,"/** 
 * Work out how much each variable needs to be incremented. In some cases this will place the increment instructions right away
 * @param block
 * @param increments
 */
private void countBlockIncrements(Block block,RCTracker increments){
  ListIterator<Statement> stmtIt=block.statementIterator();
  while (stmtIt.hasNext()) {
    Statement stmt=stmtIt.next();
    Instruction inst;
switch (stmt.type()) {
case INSTRUCTION:
      inst=stmt.instruction();
    updateCountsInstruction(inst,increments);
  increments.updateForInstruction(inst);
break;
case CONDITIONAL:
inst=null;
updateIncrementsPassIntoCont(stmt.conditional(),increments);
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + stmt.type());
}
if (!RCUtil.cancelEnabled()) {
placer.dumpIncrements(inst,block,stmtIt,increments);
}
}
for (Continuation cont : block.getContinuations()) {
updateIncrementsPassIntoCont(cont,increments);
if (!RCUtil.cancelEnabled()) {
placer.dumpIncrements(null,block,block.statementEndIterator(),increments);
}
}
}","/** 
 * Work out how much each variable needs to be incremented. In some cases this will place the increment instructions right away
 * @param block
 * @param increments
 */
private void countBlockIncrements(Block block,RCTracker increments){
  ListIterator<Statement> stmtIt=block.statementIterator();
  while (stmtIt.hasNext()) {
    Statement stmt=stmtIt.next();
switch (stmt.type()) {
case INSTRUCTION:
      Instruction inst=stmt.instruction();
    updateCountsInstruction(inst,increments);
  increments.updateForInstruction(inst);
break;
case CONDITIONAL:
updateIncrementsPassIntoCont(stmt.conditional(),increments);
break;
default :
throw new STCRuntimeError(""String_Node_Str"" + stmt.type());
}
if (!RCUtil.cancelEnabled()) {
placer.dumpIncrements(stmt,block,stmtIt,increments);
}
}
for (Continuation cont : block.getContinuations()) {
updateIncrementsPassIntoCont(cont,increments);
if (!RCUtil.cancelEnabled()) {
placer.dumpIncrements(null,block,block.statementEndIterator(),increments);
}
}
}",0.9747899159663864
132853,"public ClosedEntry getClosedEntry(Var var,boolean recursive,int stmtIndex){
  ClosedEntry ce=getDirectClosedEntry(var,recursive,stmtIndex);
  if (ce != null && ce.matches(recursive,stmtIndex)) {
    return ce;
  }
  if (recursive || useTransitiveDeps) {
    return ce;
  }
  Deque<Var> depStack=new ArrayDeque<Var>();
  depStack.addAll(directDeps(var));
  Set<Var> visited=new HashSet<Var>();
  visited.add(var);
  while (!depStack.isEmpty()) {
    Var predecessor=depStack.pop();
    if (!visited.contains(predecessor)) {
      ClosedEntry predCE=getDirectClosedEntry(predecessor,false,stmtIndex);
      if (ce != null) {
        assert(predCE.matches(recursive,stmtIndex));
        closed.put(var,predCE);
        return predCE;
      }
      depStack.addAll(directDeps(predecessor));
    }
  }
  return null;
}","public ClosedEntry getClosedEntry(Var var,boolean recursive,int stmtIndex){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + var.name() + ""String_Node_Str""+ stmtIndex+ ""String_Node_Str""+ recursive);
  }
  ClosedEntry ce=getDirectClosedEntry(var,recursive,stmtIndex);
  if (ce != null && ce.matches(recursive,stmtIndex)) {
    return ce;
  }
  if (recursive || useTransitiveDeps) {
    return ce;
  }
  Deque<Var> depStack=new ArrayDeque<Var>();
  depStack.addAll(directDeps(var));
  Set<Var> visited=new HashSet<Var>();
  visited.add(var);
  while (!depStack.isEmpty()) {
    Var predecessor=depStack.pop();
    if (!visited.contains(predecessor)) {
      ClosedEntry predCE=getDirectClosedEntry(predecessor,false,stmtIndex);
      if (predCE != null) {
        assert(predCE.matches(recursive,stmtIndex));
        closed.put(var,predCE);
        return predCE;
      }
      depStack.addAll(directDeps(predecessor));
    }
  }
  return null;
}",0.9122609673790776
132854,"/** 
 * Check if variable is directly closed. Returns a matching entry 
 * @param var
 * @param recursive
 * @param stmtIndex
 * @return
 */
private ClosedEntry getDirectClosedEntry(Var var,boolean recursive,int stmtIndex){
  ClosedVarTracker curr=this;
  int currStmtIndex=stmtIndex;
  while (curr != null) {
    for (    ClosedEntry ce : curr.closed.get(var)) {
      logger.trace(var + ""String_Node_Str"" + ce+ ""String_Node_Str""+ recursive+ ""String_Node_Str""+ currStmtIndex);
      if (ce.matches(recursive,currStmtIndex)) {
        if (curr == this) {
          return ce;
        }
 else {
          ClosedEntry origScopeEntry=new ClosedEntry(0,ce.recursive);
          closed.put(var,origScopeEntry);
          return origScopeEntry;
        }
      }
    }
    currStmtIndex=curr.parentStmtIndex;
    curr=curr.parent;
  }
  return null;
}","/** 
 * Check if variable is directly closed. Returns a matching entry 
 * @param var
 * @param recursive
 * @param stmtIndex
 * @return
 */
private ClosedEntry getDirectClosedEntry(Var var,boolean recursive,int stmtIndex){
  ClosedVarTracker curr=this;
  int currStmtIndex=stmtIndex;
  while (curr != null) {
    for (    ClosedEntry ce : curr.closed.get(var)) {
      logger.trace(var + ""String_Node_Str"" + ce+ ""String_Node_Str""+ recursive+ ""String_Node_Str""+ currStmtIndex);
      if (ce.matches(recursive,currStmtIndex)) {
        logger.trace(""String_Node_Str"");
        if (curr == this) {
          return ce;
        }
 else {
          ClosedEntry origScopeEntry=new ClosedEntry(0,ce.recursive);
          closed.put(var,origScopeEntry);
          return origScopeEntry;
        }
      }
    }
    currStmtIndex=curr.parentStmtIndex;
    curr=curr.parent;
  }
  return null;
}",0.97631426920855
132855,"public boolean matches(boolean recursive,int stmtIndex){
  return this.stmtIndex <= stmtIndex && (!recursive || this.recursive);
}","public boolean matches(boolean recursive,int stmtIndex){
  return this.stmtIndex < stmtIndex && (!recursive || this.recursive);
}",0.996138996138996
132856,"private ClosedVarTracker(Logger logger,boolean useTransitiveDeps,ClosedVarTracker parent,int parentStmtIndex){
  this.logger=logger;
  this.useTransitiveDeps=useTransitiveDeps;
  this.parent=parent;
  this.parentStmtIndex=parentStmtIndex;
  this.closed=new MultiMap<Var,ClosedEntry>();
  this.dependsOn=new MultiMap<Var,Var>();
}","private ClosedVarTracker(Logger logger,boolean useTransitiveDeps,ClosedVarTracker parent,int parentStmtIndex){
  assert(parentStmtIndex >= 0);
  this.logger=logger;
  this.useTransitiveDeps=useTransitiveDeps;
  this.parent=parent;
  this.parentStmtIndex=parentStmtIndex;
  this.closed=new MultiMap<Var,ClosedEntry>();
  this.dependsOn=new MultiMap<Var,Var>();
}",0.9536231884057972
132857,"/** 
 * Inline from bottom-up
 * @param mainBlock
 * @param cong
 */
private void inlinePass(GlobalConstants consts,Block block,Map<Block,Congruences> cong){
  Congruences blockState=cong.get(block);
  assert(blockState != null);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + System.identityHashCode(block) + ""String_Node_Str""+ block.getType());
    blockState.printTraceInfo(logger,consts);
  }
  int origStmtCount=block.getStatements().size();
  ListIterator<Statement> stmtIt=block.statementIterator();
  while (stmtIt.hasNext()) {
    Statement stmt=stmtIt.next();
    if (stmt.type() == StatementType.CONDITIONAL) {
      tryInlineConditional(consts,block,stmtIt,stmt.conditional(),cong);
    }
 else {
      assert(stmt.type() == StatementType.INSTRUCTION);
    }
  }
  Set<Var> closedVars=blockState.getClosed(origStmtCount);
  Set<Var> recClosedVars=blockState.getRecursivelyClosed(origStmtCount);
  ListIterator<Continuation> contIt=block.continuationIterator();
  while (contIt.hasNext()) {
    Continuation cont=contIt.next();
    inlinePassRecurse(consts,cont,cong);
    if (cont.isNoop()) {
      contIt.remove();
    }
 else     if (tryInlineContinuation(block,cont,contIt,closedVars,recClosedVars)) {
    }
  }
}","/** 
 * Inline from bottom-up
 * @param mainBlock
 * @param cong
 */
private void inlinePass(GlobalConstants consts,Block block,Map<Block,Congruences> cong){
  Congruences blockState=cong.get(block);
  assert(blockState != null);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + System.identityHashCode(block) + ""String_Node_Str""+ block.getType());
    blockState.printTraceInfo(logger,consts);
  }
  int origStmtCount=block.getStatements().size();
  ListIterator<Statement> stmtIt=block.statementIterator();
  while (stmtIt.hasNext()) {
    Statement stmt=stmtIt.next();
    if (stmt.type() == StatementType.CONDITIONAL) {
      tryInlineConditional(consts,block,stmtIt,stmt.conditional(),cong);
    }
 else {
      assert(stmt.type() == StatementType.INSTRUCTION);
    }
  }
  Set<Var> closedVars=blockState.getClosed(origStmtCount);
  Set<Var> recClosedVars=blockState.getRecursivelyClosed(origStmtCount);
  ListIterator<Continuation> contIt=block.continuationIterator();
  while (contIt.hasNext()) {
    Continuation cont=contIt.next();
    inlinePassRecurse(consts,cont,cong);
    if (cont.isNoop()) {
      logger.trace(""String_Node_Str"" + cont.getType());
      contIt.remove();
    }
 else     if (tryInlineContinuation(block,cont,contIt,closedVars,recClosedVars)) {
      logger.trace(""String_Node_Str"" + cont.getType());
    }
  }
}",0.9583333333333334
132858,"private void findCongruencesRec(Program program,Function f,Block block,ExecContext execCx,Congruences state,Map<Block,Congruences> result) throws OptUnsafeError {
  result.put(block,state);
  for (  Var v : block.getVariables()) {
    if (v.mapping() != null && Types.isFile(v.type())) {
      ValLoc filenameVal=ValLoc.makeFilename(v.mapping().asArg(),v);
      state.update(program.constants(),f.getName(),filenameVal,0);
    }
  }
  ListIterator<Statement> stmts=block.statementIterator();
  while (stmts.hasNext()) {
    int stmtIndex=stmts.nextIndex();
    Statement stmt=stmts.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      Instruction inst=stmt.instruction();
      if (logger.isTraceEnabled() && inst.op != Opcode.COMMENT) {
        state.printTraceInfo(logger,program.constants());
        logger.trace(""String_Node_Str"");
        logger.trace(""String_Node_Str"" + inst);
      }
      if (switchToImmediate(logger,f,execCx,block,state,inst,stmts,stmtIndex)) {
        continue;
      }
      findCongruencesInst(program,f,execCx,block,stmts,inst,stmtIndex,state);
    }
 else {
      assert(stmt.type() == StatementType.CONDITIONAL);
      UnifiedValues unified=findCongruencesContRec(program,f,execCx,stmt.conditional(),stmtIndex,state,result);
      state.addUnifiedValues(program.constants(),f.getName(),stmtIndex,unified);
    }
  }
  for (  Continuation cont : block.getContinuations()) {
    findCongruencesContRec(program,f,execCx,cont,stmts.previousIndex(),state,result);
  }
  validateState(program.constants(),state);
}","private void findCongruencesRec(Program program,Function f,Block block,ExecContext execCx,Congruences state,Map<Block,Congruences> result) throws OptUnsafeError {
  result.put(block,state);
  for (  Var v : block.getVariables()) {
    if (v.mapping() != null && Types.isFile(v.type())) {
      ValLoc filenameVal=ValLoc.makeFilename(v.mapping().asArg(),v);
      state.update(program.constants(),f.getName(),filenameVal,0);
    }
  }
  ListIterator<Statement> stmts=block.statementIterator();
  while (stmts.hasNext()) {
    int stmtIndex=stmts.nextIndex();
    Statement stmt=stmts.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      Instruction inst=stmt.instruction();
      if (logger.isTraceEnabled() && inst.op != Opcode.COMMENT) {
        state.printTraceInfo(logger,program.constants());
        logger.trace(""String_Node_Str"");
        logger.trace(""String_Node_Str"" + inst);
      }
      if (switchToImmediate(logger,f,execCx,block,state,inst,stmts,stmtIndex)) {
        continue;
      }
      findCongruencesInst(program,f,execCx,block,stmts,inst,stmtIndex,state);
    }
 else {
      assert(stmt.type() == StatementType.CONDITIONAL);
      UnifiedValues unified=findCongruencesContRec(program,f,execCx,stmt.conditional(),stmtIndex,state,result);
      state.addUnifiedValues(program.constants(),f.getName(),stmtIndex,unified);
    }
  }
  int stmtCount=block.getStatements().size();
  for (  Continuation cont : block.getContinuations()) {
    findCongruencesContRec(program,f,execCx,cont,stmtCount,state,result);
  }
  validateState(program.constants(),state);
}",0.9383735705209656
132859,"/** 
 * Internal helper to implement constructs that need to wait for a number of variables, and then run some code
 * @param procName
 * @param waitVars
 * @param passIn
 * @param keepOpenVars
 * @param priority 
 * @param recursive
 */
private void startAsync(String procName,List<Var> waitVars,List<Var> passIn,boolean recursive,TaskMode mode,TaskProps props){
  props.assertInternalTypesValid();
  mode.checkSpawn(execContextStack.peek());
  for (  Var v : passIn) {
    if (Types.isBlobVal(v)) {
      throw new STCRuntimeError(""String_Node_Str"");
    }
  }
  List<String> args=new ArrayList<String>();
  args.add(Turbine.LOCAL_STACK_NAME);
  for (  Var v : passIn) {
    args.add(prefixVar(v));
  }
  Sequence constructProc=new Sequence();
  String uniqueName=uniqueTCLFunctionName(procName);
  Proc proc=new Proc(uniqueName,usedTclFunctionNames,args,constructProc);
  tree.add(proc);
  boolean useDeepWait=false;
  List<Expression> waitFor=new ArrayList<Expression>();
  for (  Var w : waitVars) {
    if (recursive) {
      Type baseType=w.type();
      if (Types.isArray(w.type()) || Types.isBag(w.type())) {
        baseType=new NestedContainerInfo(w.type()).baseType;
        useDeepWait=true;
      }
      if (Types.isPrimFuture(baseType)) {
      }
 else       if (Types.isRef(baseType)) {
      }
 else {
        throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + w.type().typeName());
      }
    }
    Expression waitExpr=getTurbineWaitId(w);
    waitFor.add(waitExpr);
  }
  List<Expression> action=buildActionFromVars(uniqueName,passIn);
  RuleProps ruleProps=buildRuleProps(props);
  if (useDeepWait) {
    int depths[]=new int[waitVars.size()];
    boolean isFile[]=new boolean[waitVars.size()];
    for (int i=0; i < waitVars.size(); i++) {
      Type waitVarType=waitVars.get(i).type();
      Type baseType;
      if (Types.isArray(waitVarType)) {
        NestedContainerInfo ai=new NestedContainerInfo(waitVarType);
        depths[i]=ai.nesting;
        baseType=ai.baseType;
      }
 else {
        depths[i]=0;
        baseType=waitVarType;
      }
      isFile[i]=Types.isFile(baseType);
    }
    Sequence rule=Turbine.deepRule(uniqueName,waitFor,depths,isFile,action,mode,execContextStack.peek(),ruleProps);
    point().append(rule);
  }
 else {
    point().append(Turbine.rule(uniqueName,waitFor,action,mode,execContextStack.peek(),ruleProps));
  }
  pointStack.push(constructProc);
  ExecContext newExecContext;
  if (mode == TaskMode.WORKER) {
    newExecContext=ExecContext.WORKER;
  }
 else   if (mode == TaskMode.CONTROL) {
    newExecContext=ExecContext.CONTROL;
  }
 else {
    newExecContext=execContextStack.peek();
  }
  execContextStack.push(newExecContext);
}","/** 
 * Internal helper to implement constructs that need to wait for a number of variables, and then run some code
 * @param procName
 * @param waitVars
 * @param passIn
 * @param keepOpenVars
 * @param priority 
 * @param recursive
 */
private void startAsync(String procName,List<Var> waitVars,List<Var> passIn,boolean recursive,TaskMode mode,TaskProps props){
  props.assertInternalTypesValid();
  mode.checkSpawn(execContextStack.peek());
  for (  Var v : passIn) {
    if (Types.isBlobVal(v)) {
      throw new STCRuntimeError(""String_Node_Str"");
    }
  }
  List<String> args=new ArrayList<String>();
  args.add(Turbine.LOCAL_STACK_NAME);
  for (  Var v : passIn) {
    args.add(prefixVar(v));
  }
  Sequence constructProc=new Sequence();
  String uniqueName=uniqueTCLFunctionName(procName);
  Proc proc=new Proc(uniqueName,usedTclFunctionNames,args,constructProc);
  tree.add(proc);
  boolean useDeepWait=false;
  List<Expression> waitFor=new ArrayList<Expression>();
  for (  Var w : waitVars) {
    if (recursive) {
      Type baseType=w.type();
      if (Types.isArray(w.type()) || Types.isBag(w.type())) {
        baseType=new NestedContainerInfo(w.type()).baseType;
        useDeepWait=true;
      }
      if (Types.isPrimFuture(baseType)) {
      }
 else       if (Types.isRef(baseType)) {
      }
 else {
        throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + w.type().typeName());
      }
    }
    Expression waitExpr=getTurbineWaitId(w);
    waitFor.add(waitExpr);
  }
  List<Expression> action=buildActionFromVars(uniqueName,passIn);
  RuleProps ruleProps=buildRuleProps(props);
  if (useDeepWait) {
    int depths[]=new int[waitVars.size()];
    boolean isFile[]=new boolean[waitVars.size()];
    for (int i=0; i < waitVars.size(); i++) {
      Type waitVarType=waitVars.get(i).type();
      Type baseType;
      if (Types.isContainer(waitVarType)) {
        NestedContainerInfo ai=new NestedContainerInfo(waitVarType);
        depths[i]=ai.nesting;
        baseType=ai.baseType;
      }
 else {
        depths[i]=0;
        baseType=waitVarType;
      }
      isFile[i]=Types.isFile(baseType);
    }
    Sequence rule=Turbine.deepRule(uniqueName,waitFor,depths,isFile,action,mode,execContextStack.peek(),ruleProps);
    point().append(rule);
  }
 else {
    point().append(Turbine.rule(uniqueName,waitFor,action,mode,execContextStack.peek(),ruleProps));
  }
  pointStack.push(constructProc);
  ExecContext newExecContext;
  if (mode == TaskMode.WORKER) {
    newExecContext=ExecContext.WORKER;
  }
 else   if (mode == TaskMode.CONTROL) {
    newExecContext=ExecContext.CONTROL;
  }
 else {
    newExecContext=execContextStack.peek();
  }
  execContextStack.push(newExecContext);
}",0.9974245768947756
132860,"private List<TypeName> nestedTypeList(Type type,boolean includeKeyTypes,boolean valueType,boolean includeBaseType){
  List<TypeName> typeList=new ArrayList<TypeName>();
  Type curr=type;
  do {
    TypeName reprType;
    if (valueType) {
      reprType=valRepresentationType(curr);
    }
 else {
      reprType=representationType(curr,false);
    }
    typeList.add(reprType);
    if (includeKeyTypes && Types.isArray(curr)) {
      typeList.add(representationType(Types.arrayKeyType(curr),false));
    }
    curr=Types.containerElemType(curr);
  }
 while (Types.isContainer(curr));
  if (includeBaseType) {
    TypeName reprType;
    if (valueType) {
      reprType=valRepresentationType(curr);
    }
 else {
      reprType=representationType(curr,false);
    }
    typeList.add(reprType);
  }
  return typeList;
}","private List<TypeName> nestedTypeList(Type type,boolean includeKeyTypes,boolean valueType,boolean includeBaseType){
  List<TypeName> typeList=new ArrayList<TypeName>();
  Type curr=type;
  do {
    TypeName reprType;
    if (valueType) {
      reprType=valRepresentationType(curr);
    }
 else {
      reprType=representationType(curr,false);
    }
    typeList.add(reprType);
    if (includeKeyTypes && (Types.isArray(curr) || Types.isArrayLocal(curr))) {
      typeList.add(representationType(Types.arrayKeyType(curr),false));
    }
    curr=Types.containerElemType(curr);
  }
 while (Types.isContainer(curr) || Types.isContainerLocal(curr));
  if (includeBaseType) {
    TypeName reprType;
    if (valueType) {
      reprType=valRepresentationType(curr);
    }
 else {
      reprType=representationType(curr,false);
    }
    typeList.add(reprType);
  }
  return typeList;
}",0.963356973995272
132861,"private void startForeachSplit(String procName,Var arrayVar,String contentsVar,int splitDegree,int leafDegree,boolean haveKeys,List<PassedVar> usedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  pointAdd(Turbine.containerSize(TCLTMP_CONTAINER_SIZE,varToExpr(arrayVar)));
  Value containerSize=Value.numericValue(TCLTMP_CONTAINER_SIZE);
  Expression lastIndex=TclExpr.minus(containerSize,LiteralInt.ONE);
  handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  ArrayList<PassedVar> splitUsedVars=new ArrayList<PassedVar>(usedVars);
  if (!PassedVar.contains(splitUsedVars,arrayVar)) {
    splitUsedVars.add(new PassedVar(arrayVar,false));
  }
  startRangeSplit(procName,splitUsedVars,perIterIncrs,splitDegree,leafDegree,LiteralInt.ZERO,lastIndex,LiteralInt.ONE);
  pointAdd(new SetVariable(TCLTMP_SPLITLEN,new TclExpr(Value.numericValue(TCLTMP_RANGE_HI),TclExpr.MINUS,Value.numericValue(TCLTMP_RANGE_LO),TclExpr.PLUS,LiteralInt.ONE)));
  pointAdd(Turbine.enumerate(contentsVar,varToExpr(arrayVar),haveKeys,Value.numericValue(TCLTMP_SPLITLEN),TCLTMP_RANGE_LO_V));
}","private void startForeachSplit(String procName,Var arrayVar,String contentsVar,int splitDegree,int leafDegree,boolean haveKeys,List<PassedVar> usedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  pointAdd(Turbine.containerSize(TCLTMP_CONTAINER_SIZE,varToExpr(arrayVar)));
  Value containerSize=Value.numericValue(TCLTMP_CONTAINER_SIZE);
  Expression lastIndex=TclExpr.minus(containerSize,LiteralInt.ONE);
  handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  ArrayList<PassedVar> splitUsedVars=new ArrayList<PassedVar>(usedVars);
  if (!PassedVar.contains(splitUsedVars,arrayVar)) {
    splitUsedVars.add(new PassedVar(arrayVar,false));
  }
  startRangeSplit(procName,splitUsedVars,perIterIncrs,splitDegree,leafDegree,LiteralInt.ZERO,lastIndex,LiteralInt.ONE);
  pointAdd(new SetVariable(TCLTMP_SPLITLEN,new TclExpr(Value.numericValue(TCLTMP_RANGE_HI),TclExpr.MINUS,Value.numericValue(TCLTMP_RANGE_LO),TclExpr.PLUS,LiteralInt.ONE)));
  pointAdd(Turbine.enumerate(contentsVar,varToExpr(arrayVar),haveKeys,TCLTMP_RANGE_LO_V,Value.numericValue(TCLTMP_SPLITLEN)));
}",0.9674502712477396
132862,"/** 
 * Check variable initialization recursively on continuation. This is the workhorse of this module that traverses the intermediate representation and finds out which variables are initialized 
 * @param logger
 * @param fn
 * @param state Initialized vars.  Updated if we discover that more varsare initialized after continuation
 * @param validate if true, validate correct usage of initialized variables.For validation to work, initVars and assignedVals must be updated correctly to reflect initializations that occurred in outer continuations
 * @param c
 */
public static void checkInitCont(Logger logger,InitState state,Continuation c,boolean validate){
  if (validate) {
    for (    Var v : c.requiredVars(false)) {
      state.assertInitialized(c.getType(),v,false);
      state.assertAssigned(c.getType(),v);
    }
  }
  if (validate && c.isAsync()) {
    for (    PassedVar pv : c.getPassedVars()) {
      state.assertInitialized(c.getType(),pv.var,false);
      state.assertAssigned(c.getType(),pv.var);
    }
  }
  if (validate || InitState.canUnifyBranches(c)) {
    recurseOnContinuation(logger,state,c,validate);
  }
}","/** 
 * Check variable initialization recursively on continuation. This is the workhorse of this module that traverses the intermediate representation and finds out which variables are initialized 
 * @param logger
 * @param state Initialized vars.  Updated if we discover that more varsare initialized after continuation
 * @param validate if true, validate correct usage of initialized variables.For validation to work, initVars and assignedVals must be updated correctly to reflect initializations that occurred in outer continuations
 * @param c
 */
public static void checkInitCont(Logger logger,InitState state,Continuation c,boolean validate){
  if (validate) {
    for (    Var v : c.requiredVars(false)) {
      state.assertInitialized(c.getType(),v,false);
      state.assertAssigned(c.getType(),v);
    }
  }
  if (validate && c.isAsync()) {
    for (    PassedVar pv : c.getPassedVars()) {
      state.assertInitialized(c.getType(),pv.var,false);
      state.assertAssigned(c.getType(),pv.var);
    }
  }
  if (validate || InitState.canUnifyBranches(c)) {
    recurseOnContinuation(logger,state,c,validate);
  }
}",0.9942554131683606
132863,"/** 
 * Analysis that performs validation of variable initialization within a function. Throws a runtime error if a problem is found.
 * @param logger
 * @param fn
 * @param block
 */
public static void checkVarInit(Logger logger,Function fn){
  recurseOnBlock(logger,fn.mainBlock(),InitState.enterFunction(fn),true);
}","/** 
 * Analysis that performs validation of variable initialization within a function. Throws a runtime error if a problem is found.
 * @param logger
 * @param fn
 */
public static void checkVarInit(Logger logger,Function fn){
  recurseOnBlock(logger,fn.mainBlock(),InitState.enterFunction(fn),true);
}",0.9742765273311896
132864,"private void replaceVals(GlobalConstants consts,Block block,Map<Block,Congruences> congruences,InitState init){
  Congruences state=congruences.get(block);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + System.identityHashCode(block) + ""String_Node_Str""+ block.getType());
    state.printTraceInfo(logger,consts);
  }
  ListIterator<Statement> stmtIt=block.statementIterator();
  while (stmtIt.hasNext()) {
    Statement stmt=stmtIt.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      Instruction inst=stmt.instruction();
      replaceCongruent(inst,state,init);
      if (!inst.hasSideEffects() && inst.getOutputs().size() == 1) {
        Var output=inst.getOutput(0);
        if (!InitVariables.varMustBeInitialized(output,true) || init.isInitialized(output,true)) {
          if (Types.isScalarFuture(output)) {
            Arg val=state.findRetrieveResult(output);
            if (val != null) {
              Instruction futureSet=ICInstructions.futureSet(output,val);
              stmtIt.set(futureSet);
              logger.trace(""String_Node_Str"" + futureSet);
            }
          }
 else           if (Types.isScalarValue(output)) {
            Arg val=state.findValue(output);
            if (val != null && val.isConstant()) {
              Instruction valueSet=ICInstructions.valueSet(output,val);
              stmtIt.set(valueSet);
              logger.trace(""String_Node_Str"" + valueSet);
            }
          }
        }
      }
      InitVariables.updateInitVars(logger,stmt,init,false);
    }
 else {
      assert(stmt.type() == StatementType.CONDITIONAL);
      replaceCongruentNonRec(stmt.conditional(),state,init);
      replaceValsRec(consts,stmt.conditional(),congruences,init);
    }
  }
  replaceCleanupCongruent(block,state,init);
  for (  Continuation cont : block.getContinuations()) {
    replaceCongruentNonRec(cont,state,init);
    replaceValsRec(consts,cont,congruences,init);
  }
}","private void replaceVals(GlobalConstants consts,Block block,Map<Block,Congruences> congruences,InitState init){
  Congruences state=congruences.get(block);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + System.identityHashCode(block) + ""String_Node_Str""+ block.getType());
    state.printTraceInfo(logger,consts);
  }
  ListIterator<Statement> stmtIt=block.statementIterator();
  while (stmtIt.hasNext()) {
    Statement stmt=stmtIt.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      Instruction inst=stmt.instruction();
      replaceCongruent(inst,state,init);
      if (!inst.hasSideEffects() && inst.getOutputs().size() == 1) {
        Var output=inst.getOutput(0);
        if (!InitVariables.varMustBeInitialized(output,true) || init.isInitialized(output,true)) {
          if (Types.isScalarFuture(output)) {
            Arg val=state.findRetrieveResult(output);
            if (val != null && init.isInitialized(val,false)) {
              Instruction futureSet=ICInstructions.futureSet(output,val);
              stmtIt.set(futureSet);
              logger.trace(""String_Node_Str"" + futureSet);
            }
          }
 else           if (Types.isScalarValue(output)) {
            Arg val=state.findValue(output);
            if (val != null && val.isConstant()) {
              Instruction valueSet=ICInstructions.valueSet(output,val);
              stmtIt.set(valueSet);
              logger.trace(""String_Node_Str"" + valueSet);
            }
          }
        }
      }
      InitVariables.updateInitVars(logger,stmt,init,false);
    }
 else {
      assert(stmt.type() == StatementType.CONDITIONAL);
      replaceCongruentNonRec(stmt.conditional(),state,init);
      replaceValsRec(consts,stmt.conditional(),congruences,init);
    }
  }
  replaceCleanupCongruent(block,state,init);
  for (  Continuation cont : block.getContinuations()) {
    replaceCongruentNonRec(cont,state,init);
    replaceValsRec(consts,cont,congruences,init);
  }
}",0.9918012422360248
132865,"/** 
 * Unroll a loop by splitting into two loops, one short one with original stride, and another with a long stride We transform: range_loop [start:end:step] =======> range_loop [start : unroll_end : big_step] range_loop [remainder_start  : end : step]
 */
private List<Continuation> doUnroll(Logger logger,Block outerBlock,int unrollFactor){
  logger.debug(""String_Node_Str"" + this.loopName + ""String_Node_Str""+ desiredUnroll+ ""String_Node_Str"");
  String vPrefix=Var.OPT_VALUE_VAR_PREFIX + loopName;
  String bigStepName=outerBlock.uniqueVarName(vPrefix + ""String_Node_Str"");
  VarProvenance prov=VarProvenance.optimizerTmp();
  Var bigIncr=new Var(Types.V_INT,bigStepName,Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var diff=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var diff2=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var extra=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var remainder=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var remainderStart=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var unrollEnd=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  outerBlock.addVariable(bigIncr);
  outerBlock.addVariable(diff);
  outerBlock.addVariable(diff2);
  outerBlock.addVariable(extra);
  outerBlock.addVariable(remainder);
  outerBlock.addVariable(remainderStart);
  outerBlock.addVariable(unrollEnd);
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MULT_INT,bigIncr,Arrays.asList(increment,Arg.createIntLit(unrollFactor))));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MINUS_INT,diff,Arrays.asList(end,start)));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,diff2,Arrays.asList(diff.asArg(),Arg.ONE)));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MOD_INT,remainder,Arrays.asList(diff2.asArg(),bigIncr.asArg())));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MINUS_INT,unrollEnd,Arrays.asList(end,remainder.asArg())));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,remainderStart,Arrays.asList(unrollEnd.asArg(),Arg.ONE)));
  RangeLoop unrolled=this.clone(false);
  this.start=remainderStart.asArg();
  this.unrolled=true;
  unrolled.end=unrollEnd.asArg();
  unrolled.increment=bigIncr.asArg();
  unrolled.unrolled=true;
  unrolled.leafDegree=Math.max(1,unrolled.leafDegree / unrollFactor);
  Block orig=this.loopBody;
  Arg oldIncr=this.increment;
  Var lastIterLoopVar=null;
  for (int i=0; i < unrollFactor; i++) {
    NestedBlock nb=new NestedBlock(orig.clone(BlockType.NESTED_BLOCK,null));
    Block unrolledBody=unrolled.getLoopBody();
    unrolledBody.addContinuation(nb);
    Var currIterLoopVar;
    if (i == 0) {
      currIterLoopVar=loopVar;
    }
 else {
      String newLoopVarName=outerBlock.uniqueVarName(unrolled.loopVar.name() + ""String_Node_Str"" + (i + 1));
      currIterLoopVar=new Var(Types.V_INT,newLoopVarName,Alloc.LOCAL,DefType.LOCAL_COMPILER,null);
      unrolledBody.addVariable(currIterLoopVar);
      unrolledBody.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,currIterLoopVar,Arrays.asList(Arg.createVar(lastIterLoopVar),oldIncr)));
      nb.renameVars(Collections.singletonMap(unrolled.loopVar,Arg.createVar(currIterLoopVar)),RenameMode.REPLACE_VAR,true);
    }
    lastIterLoopVar=currIterLoopVar;
  }
  return Collections.<Continuation>singletonList(unrolled);
}","/** 
 * Unroll a loop by splitting into two loops, one short one with original stride, and another with a long stride We transform: range_loop [start:end:step] =======> range_loop [start : unroll_end : big_step] range_loop [remainder_start  : end : step]
 */
private List<Continuation> doUnroll(Logger logger,Block outerBlock,int unrollFactor){
  logger.debug(""String_Node_Str"" + this.loopName + ""String_Node_Str""+ desiredUnroll+ ""String_Node_Str"");
  String vPrefix=Var.OPT_VALUE_VAR_PREFIX + loopName;
  String bigStepName=outerBlock.uniqueVarName(vPrefix + ""String_Node_Str"");
  VarProvenance prov=VarProvenance.optimizerTmp();
  Var bigIncr=new Var(Types.V_INT,bigStepName,Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var diff=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var diff2=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var extra=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var remainder=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var remainderStart=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  Var unrollEnd=new Var(Types.V_INT,outerBlock.uniqueVarName(vPrefix + ""String_Node_Str""),Alloc.LOCAL,DefType.LOCAL_COMPILER,prov);
  outerBlock.addVariable(bigIncr);
  outerBlock.addVariable(diff);
  outerBlock.addVariable(diff2);
  outerBlock.addVariable(extra);
  outerBlock.addVariable(remainder);
  outerBlock.addVariable(remainderStart);
  outerBlock.addVariable(unrollEnd);
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MULT_INT,bigIncr,Arrays.asList(increment,Arg.createIntLit(unrollFactor))));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MINUS_INT,diff,Arrays.asList(end,start)));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,diff2,Arrays.asList(diff.asArg(),Arg.ONE)));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MOD_INT,remainder,Arrays.asList(diff2.asArg(),bigIncr.asArg())));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.MINUS_INT,unrollEnd,Arrays.asList(end,remainder.asArg())));
  outerBlock.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,remainderStart,Arrays.asList(unrollEnd.asArg(),Arg.ONE)));
  RangeLoop unrolled=this.clone(false);
  this.start=remainderStart.asArg();
  this.unrolled=true;
  unrolled.end=unrollEnd.asArg();
  unrolled.increment=bigIncr.asArg();
  unrolled.unrolled=true;
  unrolled.leafDegree=Math.max(1,unrolled.leafDegree / unrollFactor);
  Block orig=this.loopBody;
  Arg oldIncr=this.increment;
  Var lastIterLoopVar=null;
  for (int i=0; i < unrollFactor; i++) {
    NestedBlock nb=new NestedBlock(orig.clone(BlockType.NESTED_BLOCK,null));
    Block unrolledBody=unrolled.getLoopBody();
    unrolledBody.addContinuation(nb);
    Var currIterLoopVar;
    if (i == 0) {
      currIterLoopVar=loopVar;
    }
 else {
      String newLoopVarName=outerBlock.uniqueVarName(unrolled.loopVar.name() + ""String_Node_Str"" + (i + 1));
      currIterLoopVar=new Var(Types.V_INT,newLoopVarName,Alloc.LOCAL,DefType.LOCAL_COMPILER,VarProvenance.renamed(unrolled.loopVar));
      unrolledBody.addVariable(currIterLoopVar);
      unrolledBody.addInstruction(Builtin.createLocal(BuiltinOpcode.PLUS_INT,currIterLoopVar,Arrays.asList(Arg.createVar(lastIterLoopVar),oldIncr)));
      nb.renameVars(Collections.singletonMap(unrolled.loopVar,Arg.createVar(currIterLoopVar)),RenameMode.REPLACE_VAR,true);
    }
    lastIterLoopVar=currIterLoopVar;
  }
  return Collections.<Continuation>singletonList(unrolled);
}",0.9943294210734538
132866,"/** 
 * Update increments for variables passed into continuation
 * @param cont
 * @param increments
 */
private void updateIncrementsPassIntoCont(Continuation cont,RCTracker increments){
  if (cont.spawnsSingleTask()) {
    long incr=1;
    for (    Var keepOpen : cont.getKeepOpenVars()) {
      assert(RefCounting.hasWriteRefCount(keepOpen));
      increments.writeIncr(keepOpen,incr);
    }
    Set<Var> readIncrTmp=new HashSet<Var>();
    for (    PassedVar passedIn : cont.getAllPassedVars()) {
      if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
        readIncrTmp.add(passedIn.var);
      }
    }
    for (    BlockingVar blockingVar : cont.blockingVars(false)) {
      if (RefCounting.hasReadRefCount(blockingVar.var)) {
        readIncrTmp.add(blockingVar.var);
      }
    }
    for (    Var v : readIncrTmp) {
      increments.readIncr(v,incr);
    }
  }
  if (RCUtil.isForeachLoop(cont)) {
    AbstractForeachLoop foreach=(AbstractForeachLoop)cont;
    updateIncrementsPassIntoForeach(increments,foreach);
  }
}","/** 
 * Update increments for variables passed into continuation
 * @param cont
 * @param increments
 */
private void updateIncrementsPassIntoCont(Continuation cont,RCTracker increments){
  if (cont.spawnsSingleTask()) {
    long incr=1;
    for (    Var keepOpen : cont.getKeepOpenVars()) {
      assert(RefCounting.hasWriteRefCount(keepOpen));
      increments.writeIncr(keepOpen,incr);
    }
    Set<Var> readIncrTmp=new HashSet<Var>();
    for (    PassedVar passedIn : cont.getAllPassedVars()) {
      if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
        readIncrTmp.add(passedIn.var);
        if (cont.getType() == ContinuationType.FOREACH_LOOP) {
          System.err.println(passedIn.var);
        }
      }
    }
    for (    BlockingVar blockingVar : cont.blockingVars(false)) {
      if (RefCounting.hasReadRefCount(blockingVar.var)) {
        readIncrTmp.add(blockingVar.var);
      }
    }
    for (    Var v : readIncrTmp) {
      increments.readIncr(v,incr);
    }
  }
 else   if (RCUtil.isForeachLoop(cont)) {
    AbstractForeachLoop foreach=(AbstractForeachLoop)cont;
    updateIncrementsPassIntoForeach(increments,foreach);
  }
}",0.9447191011235956
132867,"public void updateIncrementsPassIntoForeach(RCTracker increments,AbstractForeachLoop foreach){
  for (  RefCount rc : foreach.getStartIncrements()) {
    increments.incr(rc.var,rc.type,1);
    foreach.addConstantStartIncrement(rc.var,rc.type,Arg.createIntLit(-1));
  }
}","private void updateIncrementsPassIntoForeach(RCTracker increments,AbstractForeachLoop foreach){
  if (foreach.isAsync()) {
    for (    RefCount rc : foreach.getStartIncrements()) {
      increments.incr(rc.var,rc.type,1);
      foreach.addConstantStartIncrement(rc.var,rc.type,Arg.createIntLit(-1));
    }
  }
}",0.8934707903780069
132868,"/** 
 * Handle foreach loop as special case where we want to increment <# of iterations> * <needed increments inside loop> before loop. This function will work out if it can pull out increments from foreach loop body
 */
private void postProcessForeachLoop(Continuation c){
  AbstractForeachLoop loop=(AbstractForeachLoop)c;
  Counters<Var> readIncrs=new Counters<Var>();
  Counters<Var> writeIncrs=new Counters<Var>();
  if (loop.isAsync()) {
    for (    PassedVar v : loop.getPassedVars()) {
      if (!v.writeOnly && RefCounting.hasReadRefCount(v.var)) {
        readIncrs.increment(v.var);
      }
    }
    for (    Var v : loop.getKeepOpenVars()) {
      if (RefCounting.hasWriteRefCount(v)) {
        writeIncrs.increment(v);
      }
    }
  }
  Block loopBody=loop.getLoopBody();
  ListIterator<Statement> it=loopBody.statementIterator();
  while (it.hasNext()) {
    Statement stmt=it.next();
    if (stmt.type() != StatementType.INSTRUCTION || !RefCountOp.isIncrement(stmt.instruction().op)) {
      break;
    }
    Instruction inst=stmt.instruction();
    Arg amount=RefCountOp.getRCAmount(inst);
    Var var=RefCountOp.getRCTarget(inst);
    RefCountType rcType=RefCountOp.getRCType(inst.op);
    if (amount.isIntVal() && RCUtil.definedOutsideCont(loop,loopBody,var)) {
      if (rcType == RefCountType.READERS) {
        readIncrs.add(var,amount.getIntLit());
      }
 else {
        assert(rcType == RefCountType.WRITERS);
        writeIncrs.add(var,amount.getIntLit());
      }
      it.remove();
    }
  }
  for (  Entry<Var,Long> read : readIncrs.entries()) {
    loop.addStartIncrement(new RefCount(read.getKey(),RefCountType.READERS,Arg.createIntLit(read.getValue())));
  }
  for (  Entry<Var,Long> write : writeIncrs.entries()) {
    loop.addStartIncrement(new RefCount(write.getKey(),RefCountType.WRITERS,Arg.createIntLit(write.getValue())));
  }
}","/** 
 * Handle foreach loop as special case where we want to increment <# of iterations> * <needed increments inside loop> before loop. This function will work out if it can pull out increments from foreach loop body
 */
private void postProcessForeachLoop(Continuation c){
  AbstractForeachLoop loop=(AbstractForeachLoop)c;
  Counters<Var> readIncrs=new Counters<Var>();
  Counters<Var> writeIncrs=new Counters<Var>();
  if (loop.isAsync()) {
    for (    PassedVar v : loop.getAllPassedVars()) {
      if (!v.writeOnly && RefCounting.hasReadRefCount(v.var)) {
        readIncrs.increment(v.var);
      }
    }
    for (    Var v : loop.getKeepOpenVars()) {
      if (RefCounting.hasWriteRefCount(v)) {
        writeIncrs.increment(v);
      }
    }
  }
  Block loopBody=loop.getLoopBody();
  ListIterator<Statement> it=loopBody.statementIterator();
  while (it.hasNext()) {
    Statement stmt=it.next();
    if (stmt.type() != StatementType.INSTRUCTION || !RefCountOp.isIncrement(stmt.instruction().op)) {
      break;
    }
    Instruction inst=stmt.instruction();
    Arg amount=RefCountOp.getRCAmount(inst);
    Var var=RefCountOp.getRCTarget(inst);
    RefCountType rcType=RefCountOp.getRCType(inst.op);
    if (amount.isIntVal() && RCUtil.definedOutsideCont(loop,loopBody,var)) {
      if (rcType == RefCountType.READERS) {
        readIncrs.add(var,amount.getIntLit());
      }
 else {
        assert(rcType == RefCountType.WRITERS);
        writeIncrs.add(var,amount.getIntLit());
      }
      it.remove();
    }
  }
  for (  Entry<Var,Long> read : readIncrs.entries()) {
    loop.addStartIncrement(new RefCount(read.getKey(),RefCountType.READERS,Arg.createIntLit(read.getValue())));
  }
  for (  Entry<Var,Long> write : writeIncrs.entries()) {
    loop.addStartIncrement(new RefCount(write.getKey(),RefCountType.WRITERS,Arg.createIntLit(write.getValue())));
  }
}",0.9991989319092124
132869,"private void addDecrementsAsyncCont(Continuation cont,RCTracker increments){
  long amount=1;
  for (  Var keepOpen : cont.getKeepOpenVars()) {
    increments.writeDecr(keepOpen,amount);
  }
  Set<Var> readIncrTmp=new HashSet<Var>();
  for (  PassedVar passedIn : cont.getPassedVars()) {
    if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
      readIncrTmp.add(passedIn.var);
    }
  }
  for (  BlockingVar blockingVar : cont.blockingVars(false)) {
    if (RefCounting.hasReadRefCount(blockingVar.var)) {
      readIncrTmp.add(blockingVar.var);
    }
  }
  for (  Var v : readIncrTmp) {
    increments.readDecr(v,amount);
  }
}","private void addDecrementsAsyncCont(Continuation cont,RCTracker increments){
  long amount=1;
  for (  Var keepOpen : cont.getKeepOpenVars()) {
    increments.writeDecr(keepOpen,amount);
  }
  Set<Var> readIncrTmp=new HashSet<Var>();
  for (  PassedVar passedIn : cont.getAllPassedVars()) {
    if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
      readIncrTmp.add(passedIn.var);
    }
  }
  for (  BlockingVar blockingVar : cont.blockingVars(false)) {
    if (RefCounting.hasReadRefCount(blockingVar.var)) {
      readIncrTmp.add(blockingVar.var);
    }
  }
  for (  Var v : readIncrTmp) {
    increments.readDecr(v,amount);
  }
}",0.9977046671767408
132870,"private void placeRefcounts(Logger logger,Function fn,Block block,RCTracker increments,Set<Var> parentAssignedAliasVars){
  increments.canonicalize();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + block.getType() + ""String_Node_Str""+ fn.getName());
    logger.trace(""String_Node_Str"");
    logger.trace(increments);
  }
  if (RCUtil.cancelEnabled()) {
    pullUpRefIncrements(block,increments);
  }
  for (  RefCountType rcType : RC_TYPES) {
    placer.placeDecrements(logger,fn,block,increments,rcType);
    placer.placeIncrements(block,increments,rcType,parentAssignedAliasVars);
    RCUtil.checkRCZero(block,increments,rcType,true,true);
  }
}","private void placeRefcounts(Logger logger,Function fn,Block block,RCTracker increments,Set<Var> parentAssignedAliasVars){
  increments.canonicalize();
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + block.getType() + ""String_Node_Str""+ fn.getName());
    logger.trace(""String_Node_Str"");
    logger.trace(increments);
  }
  reorderContinuations(logger,block);
  if (RCUtil.cancelEnabled()) {
    pullUpRefIncrements(block,increments);
  }
  for (  RefCountType rcType : RC_TYPES) {
    placer.placeDecrements(logger,fn,block,increments,rcType);
    placer.placeIncrements(block,increments,rcType,parentAssignedAliasVars);
    RCUtil.checkRCZero(block,increments,rcType,true,true);
  }
}",0.9738292011019284
132871,"public void markContradiction(Arg arg){
  arg=findCanonical(arg);
  boolean newContradiction=contradictions.add(arg);
  if (!newContradiction) {
    return;
  }
  CongruentSets curr=this;
  do {
    for (    RecCV setMember : curr.canonicalInv.get(arg)) {
      if (setMember.isArg() && !setMember.arg().equals(arg)) {
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ setMember);
        }
        contradictions.add(setMember.arg());
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
  curr=this;
  do {
    for (    RecCV cv : curr.componentIndex.get(arg)) {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ cv);
      }
      Arg set=findCanonical(cv);
      if (set != null) {
        markContradiction(set);
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
}","public void markContradiction(Arg arg){
  assert(arg != null);
  arg=findCanonical(arg);
  boolean newContradiction=contradictions.add(arg);
  if (!newContradiction) {
    return;
  }
  CongruentSets curr=this;
  do {
    for (    RecCV setMember : curr.canonicalInv.get(arg)) {
      if (setMember.isArg() && !setMember.arg().equals(arg)) {
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ setMember);
        }
        contradictions.add(setMember.arg());
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
  curr=this;
  do {
    for (    RecCV cv : curr.componentIndex.get(arg)) {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ cv);
      }
      Arg set=findCanonical(cv);
      if (set != null) {
        markContradiction(set);
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
}",0.9874110563765736
132872,"/** 
 * Check if we can actually access a var here
 * @param var
 * @return
 */
public boolean isAccessible(Var var){
  Arg varArg=var.asArg();
  if (!isUnpassable(varArg)) {
    return true;
  }
  CongruentSets curr=this;
  boolean allPassed=true;
  while (curr != null) {
    if (canonicalInv.containsKey(varArg)) {
      return allPassed;
    }
    allPassed=allPassed && curr.varsFromParent;
    curr=curr.parent;
  }
  throw new STCRuntimeError(""String_Node_Str"" + var);
}","/** 
 * Check if we can actually access a var here
 * @param var
 * @return
 */
public boolean isAccessible(Var var){
  Arg varArg=var.asArg();
  if (!isUnpassable(varArg)) {
    return true;
  }
  CongruentSets curr=this;
  boolean allPassed=true;
  while (curr != null) {
    if (curr.canonicalInv.containsKey(varArg)) {
      return allPassed;
    }
    allPassed=allPassed && curr.varsFromParent;
    curr=curr.parent;
  }
  throw new STCRuntimeError(""String_Node_Str"" + var);
}",0.994786235662148
132873,"public Arg arg(){
  return arg;
}","public Arg arg(){
}",0.7307692307692307
132874,"public ArgCV cv(){
  return cv;
}","public ArgCV cv(){
}",0.7547169811320755
132875,"/** 
 * User friendly string for location.
 * @param assigned
 * @return
 */
private String printableAssignValue(List<Arg> assigned){
  assert(assigned.size() > 0);
  StringBuilder sb=new StringBuilder();
  sb.append(assigned.toString());
  for (  Arg arg : assigned.subList(1,assigned.size())) {
    sb.append(""String_Node_Str"" + arg + ""String_Node_Str"");
  }
  return assigned.toString();
}","/** 
 * User friendly string for location.
 * @param assigned
 * @return
 */
private String printableAssignValue(List<Arg> assigned){
  assert(assigned.size() > 0);
  StringBuilder sb=new StringBuilder();
  sb.append(assigned.get(0).toString());
  for (  Arg arg : assigned.subList(1,assigned.size())) {
    sb.append(""String_Node_Str"" + arg + ""String_Node_Str"");
  }
  return sb.toString();
}",0.978343949044586
132876,"private CongruentSets(CongruenceType congType,CongruentSets parent,Set<Arg> contradictions){
  this.congType=congType;
  this.parent=parent;
  this.canonical=new HashMap<ArgOrCV,Arg>();
  this.canonicalInv=new MultiMap<Arg,ArgOrCV>();
  this.mergedInto=new MultiMap<Arg,Arg>();
  this.componentIndex=new MultiMap<Arg,ArgOrCV>();
  this.inaccessible=new HashSet<Var>();
  this.contradictions=contradictions;
  this.mergeQueue=new LinkedList<ToMerge>();
  this.recanonicalizeQueue=new LinkedList<Arg>();
  if (parent != null) {
    this.constShareEnabled=parent.constShareEnabled;
    this.constFoldEnabled=parent.constFoldEnabled;
  }
 else {
    try {
      this.constShareEnabled=Settings.getBoolean(Settings.OPT_SHARED_CONSTANTS);
      this.constFoldEnabled=Settings.getBoolean(Settings.OPT_CONSTANT_FOLD);
    }
 catch (    InvalidOptionException e) {
      e.printStackTrace();
      throw new STCRuntimeError(e.getMessage());
    }
  }
}","private CongruentSets(CongruenceType congType,CongruentSets parent,Set<Arg> contradictions){
  this.congType=congType;
  this.parent=parent;
  this.canonical=new HashMap<ArgOrCV,Arg>();
  this.canonicalInv=new MultiMap<Arg,ArgOrCV>();
  this.mergedInto=new MultiMap<Arg,Arg>();
  this.componentIndex=new MultiMap<Arg,ArgCV>();
  this.inaccessible=new HashSet<Var>();
  this.contradictions=contradictions;
  this.mergeQueue=new LinkedList<ToMerge>();
  this.recanonicalizeQueue=new LinkedList<Arg>();
  if (parent != null) {
    this.constShareEnabled=parent.constShareEnabled;
    this.constFoldEnabled=parent.constFoldEnabled;
  }
 else {
    try {
      this.constShareEnabled=Settings.getBoolean(Settings.OPT_SHARED_CONSTANTS);
      this.constFoldEnabled=Settings.getBoolean(Settings.OPT_CONSTANT_FOLD);
    }
 catch (    InvalidOptionException e) {
      e.printStackTrace();
      throw new STCRuntimeError(e.getMessage());
    }
  }
}",0.9989384288747346
132877,"private ArgOrCV canonicalize(GlobalConstants consts,ArgOrCV result){
  if (result.isCV()) {
    ComputedValue<Arg> resultValue=result.cv();
    if (resultValue.isCopy() || resultValue.isAlias()) {
      result=new ArgOrCV(resultValue.getInput(0));
    }
 else     if (resultValue.isDerefCompVal()) {
      ArgOrCV resolvedRef=tryResolveRef(resultValue);
      if (resolvedRef != null) {
        result=resolvedRef;
      }
    }
 else     if (constFoldEnabled && this.congType == CongruenceType.VALUE) {
      ArgOrCV constantFolded=tryConstantFold(resultValue);
      if (constantFolded != null) {
        result=constantFolded;
      }
    }
  }
  if (constShareEnabled && result.isCV()) {
    result=tryReplaceGlobalConstant(consts,result);
  }
  return result;
}","/** 
 * Do any canonicalization of result value here, e.g. to implement constant folding, etc.
 * @param consts 
 * @param resVal
 * @return
 */
public ArgOrCV canonicalize(GlobalConstants consts,ArgCV origVal){
  return canonicalizeInternal(consts,canonicalizeInputs(origVal));
}",0.124282982791587
132878,"/** 
 * Convert ComputedValue parameterized with <Arg> to ComputedValue parameterized with ComputedValue
 * @param cv
 * @return
 */
private ArgOrCV canonicalizeInputs(ArgCV cv){
  List<Arg> inputs=cv.getInputs();
  List<Arg> newInputs=new ArrayList<Arg>(inputs.size());
  for (  Arg input : inputs) {
    if (cv.canSubstituteInputs(congType)) {
      Arg canonicalInput=findCanonical(new ArgOrCV(input));
      assert(canonicalInput != null);
      newInputs.add(canonicalInput);
    }
 else {
      newInputs.add(input);
    }
  }
  ArgOrCV newCV=new ArgOrCV(cv.op,cv.subop,newInputs);
  if (findCanonical(newCV) == null) {
    for (    Arg newInput : newInputs) {
      addInputIndex(newInput,newCV);
    }
  }
  return newCV;
}","/** 
 * Convert ComputedValue parameterized with <Arg> to ComputedValue parameterized with ComputedValue
 * @param cv
 * @return
 */
private ArgCV canonicalizeInputs(ArgCV cv){
  List<Arg> inputs=cv.getInputs();
  List<Arg> newInputs=new ArrayList<Arg>(inputs.size());
  for (  Arg input : inputs) {
    if (cv.canSubstituteInputs(congType)) {
      Arg canonicalInput=findCanonical(input);
      assert(canonicalInput != null);
      newInputs.add(canonicalInput);
    }
 else {
      newInputs.add(input);
    }
  }
  ArgCV newCV=new ArgCV(cv.op,cv.subop,newInputs);
  if (findCanonicalInternal(newCV) == null) {
    for (    Arg newInput : newInputs) {
      addInputIndex(newInput,newCV);
    }
  }
  return newCV;
}",0.974500344589938
132879,"public void markContradiction(Arg arg){
  assert(arg != null);
  arg=findCanonical(arg);
  boolean newContradiction=contradictions.add(arg);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ newContradiction);
  }
  if (!newContradiction) {
    return;
  }
  CongruentSets curr=this;
  do {
    for (    ArgOrCV setMember : curr.canonicalInv.get(arg)) {
      if (setMember.isArg() && !setMember.arg().equals(arg)) {
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ setMember);
        }
        markContradiction(setMember.arg());
      }
 else       if (setMember.isCV()) {
        addCVContradictions(setMember.cv());
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
  curr=this;
  do {
    for (    ArgOrCV cv : curr.componentIndex.get(arg)) {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ cv);
      }
      Arg set=findCanonical(cv);
      if (set != null) {
        markContradiction(set);
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
}","public void markContradiction(Arg arg){
  assert(arg != null);
  arg=findCanonical(arg);
  boolean newContradiction=contradictions.add(arg);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ newContradiction);
  }
  if (!newContradiction) {
    return;
  }
  CongruentSets curr=this;
  do {
    for (    ArgOrCV setMember : curr.canonicalInv.get(arg)) {
      if (setMember.isArg() && !setMember.arg().equals(arg)) {
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ setMember);
        }
        markContradiction(setMember.arg());
      }
 else       if (setMember.isCV()) {
        addCVContradictions(setMember.cv());
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
  curr=this;
  do {
    for (    ArgCV cv : curr.componentIndex.get(arg)) {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ cv);
      }
      Arg set=findCanonical(new ArgOrCV(cv));
      if (set != null) {
        markContradiction(set);
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
}",0.993423936869794
132880,"/** 
 * Try to resolve a reference lookup to the original thing dereferenced
 * @param val
 * @return
 */
private ArgOrCV tryResolveRef(ComputedValue<Arg> val){
  assert(val.isDerefCompVal());
  Arg ref=val.getInput(0);
  for (  ArgOrCV v : findCongruentValues(ref)) {
    if (v.isCV()) {
      ArgCV v2=v.cv();
      if (v2.isArrayMemberRef()) {
        return new ArgOrCV(ComputedValue.derefArrayMemberRef(v2));
      }
    }
  }
  return null;
}","/** 
 * Try to resolve a reference lookup to the original thing dereferenced
 * @param val
 * @return
 */
private ArgCV tryResolveRef(ComputedValue<Arg> val){
  assert(val.isDerefCompVal());
  Arg ref=val.getInput(0);
  for (  ArgOrCV v : findCongruentValues(ref)) {
    if (v.isCV()) {
      ArgCV v2=v.cv();
      if (v2.isArrayMemberRef()) {
        return ComputedValue.derefArrayMemberRef(v2);
      }
    }
  }
  return null;
}",0.9829738933030648
132881,"/** 
 * Try to constant-fold the expression
 * @param val
 * @return
 */
private ArgOrCV tryConstantFold(ComputedValue<Arg> val){
  assert(constFoldEnabled);
  assert(this.congType == CongruenceType.VALUE);
  return ConstantFolder.constantFold(logger,this,val);
}","/** 
 * Try to constant-fold the expression
 * @param val
 * @return
 */
private ArgOrCV tryConstantFold(ArgCV val){
  assert(constFoldEnabled);
  assert(this.congType == CongruenceType.VALUE);
  return ConstantFolder.constantFold(logger,this,val);
}",0.95906432748538
132882,"/** 
 * Keep index of where Arg appears.  When merge happens, need to go through and replace old Arg with new canonical Arg.  We also use this to propagate contradiction info among different sets.  We need to track constants as these are often the canonical member of a set.
 */
private void addInputIndex(Arg newInput,ArgOrCV newCV){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + newInput + ""String_Node_Str""+ newCV);
  }
  componentIndex.put(newInput,newCV);
}","/** 
 * Keep index of where Arg appears.  When merge happens, need to go through and replace old Arg with new canonical Arg.  We also use this to propagate contradiction info among different sets.  We need to track constants as these are often the canonical member of a set.
 */
private void addInputIndex(Arg newInput,ArgCV newCV){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + newInput + ""String_Node_Str""+ newCV);
  }
  componentIndex.put(newInput,newCV);
}",0.9979381443298968
132883,"/** 
 * See if the result of a value retrieval is already in scope
 * @param v
 * @return
 */
public Arg findRetrieveResult(Var v){
  ArgCV cvRetrieve=ComputedValue.retrieveCompVal(v);
  if (cvRetrieve == null) {
    return null;
  }
  return byValue.findCanonical(cvRetrieve);
}","/** 
 * See if the result of a value retrieval is already in scope
 * @param v
 * @return
 */
public Arg findRetrieveResult(Var v){
  ArgCV cvRetrieve=ComputedValue.retrieveCompVal(v);
  if (cvRetrieve == null) {
    return null;
  }
  Arg val=byValue.findCanonical(cvRetrieve);
  if (val != null && !byValue.contradictions.contains(val)) {
    return val;
  }
 else {
    return null;
  }
}",0.8119402985074626
132884,"public void markContradiction(Arg arg){
  arg=findCanonical(arg);
  boolean newContradiction=contradictions.add(arg);
  if (!newContradiction) {
    return;
  }
  CongruentSet curr=this;
  do {
    for (    RecCV setMember : curr.canonicalInv.get(arg)) {
      if (setMember.isArg() && !setMember.arg().equals(arg)) {
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ setMember);
        }
        contradictions.add(setMember.arg());
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
  curr=this;
  do {
    for (    RecCV cv : curr.componentIndex.get(arg)) {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ cv);
      }
      markContradiction(findCanonical(cv));
    }
    curr=curr.parent;
  }
 while (curr != null);
}","public void markContradiction(Arg arg){
  arg=findCanonical(arg);
  boolean newContradiction=contradictions.add(arg);
  if (!newContradiction) {
    return;
  }
  CongruentSet curr=this;
  do {
    for (    RecCV setMember : curr.canonicalInv.get(arg)) {
      if (setMember.isArg() && !setMember.arg().equals(arg)) {
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ setMember);
        }
        contradictions.add(setMember.arg());
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
  curr=this;
  do {
    for (    RecCV cv : curr.componentIndex.get(arg)) {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + arg + ""String_Node_Str""+ cv);
      }
      Arg set=findCanonical(cv);
      if (set != null) {
        markContradiction(set);
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
}",0.9473684210526316
132885,"public boolean isAvailable(RecCV val,CongruenceType congType){
  return findCanonical(val,congType) != null;
}","public boolean isAvailable(ArgCV val,CongruenceType congType){
  return findCanonical(val,congType) != null;
}",0.9727272727272728
132886,"/** 
 * Update a congruentSet with the information that value is stored in location
 * @param errContext
 * @param location
 * @param value
 * @param congruent
 * @param addInverses
 * @return
 */
public boolean update(String errContext,Arg location,ArgCV value,CongruentSet congruent,boolean addInverses){
  Arg canonLoc=congruent.findCanonical(new RecCV(location));
  RecCV canonVal=canonicalize(congruent,value);
  updateCanonical(errContext,canonLoc,canonVal,congruent);
  if (addInverses) {
    addInverses(errContext,canonLoc,canonVal);
  }
  return true;
}","/** 
 * Update a congruentSet with the information that value is stored in location
 * @param errContext
 * @param location
 * @param value
 * @param congruent
 * @param addInverses
 * @return
 */
public boolean update(String errContext,Arg location,ArgCV value,CongruentSet congruent,boolean addInverses){
  Arg canonLoc=congruent.findCanonical(new RecCV(location));
  RecCV canonVal=canonicalize(congruent,value);
  Arg canonLocFromVal=congruent.findCanonical(canonVal);
  if (canonLocFromVal == null) {
    congruent.addToSet(canonVal,canonLoc);
  }
 else {
    mergeSets(errContext,canonVal,congruent,canonLoc,canonLocFromVal);
  }
  if (addInverses) {
    addInverses(errContext,canonLoc,canonVal);
  }
  return true;
}",0.8313908313908314
132887,"private void addInverses(String errContext,Arg canonLoc,RecCV canonVal){
  if (canonVal.isCV() && canonVal.cv().inputs.size() == 1) {
    ComputedValue<RecCV> cv=canonVal.cv();
    RecCV input=cv.getInput(0);
    if (input.isArg()) {
      Arg invOutput=input.arg();
      CongruentSet valueSet=getCongruentSet(CongruenceType.VALUE);
      if (cv.op().isAssign()) {
        ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar());
        update(errContext,invOutput,invVal,valueSet,false);
      }
 else       if (cv.op().isRetrieve()) {
        ArgCV invVal=new ArgCV(Opcode.assignOpcode(invOutput.getVar()),canonLoc.asList());
        update(errContext,invOutput,invVal,valueSet,false);
      }
    }
  }
}","/** 
 * Add any inverse operations that can be directly inferred from a value that was just added
 * @param errContext
 * @param canonLoc
 * @param canonVal
 */
private void addInverses(String errContext,Arg canonLoc,RecCV canonVal){
  if (canonVal.isCV() && canonVal.cv().inputs.size() == 1) {
    ComputedValue<RecCV> cv=canonVal.cv();
    RecCV input=cv.getInput(0);
    if (input.isArg()) {
      Arg invOutput=input.arg();
      CongruentSet valueSet=getCongruentSet(CongruenceType.VALUE);
      if (cv.op().isAssign()) {
        ArgCV invVal=ComputedValue.retrieveCompVal(canonLoc.getVar());
        update(errContext,invOutput,invVal,valueSet,false);
      }
 else       if (cv.op().isRetrieve()) {
        ArgCV invVal=new ArgCV(Opcode.assignOpcode(invOutput.getVar()),canonLoc.asList());
        update(errContext,invOutput,invVal,valueSet,false);
      }
    }
  }
}",0.8988057825267127
132888,"@Override public boolean equals(Object obj){
  if (this == obj)   return true;
  if (obj == null)   return false;
  if (getClass() != obj.getClass())   return false;
  RecCV other=(RecCV)obj;
  if (arg == null) {
    if (other.arg != null)     return false;
  }
 else   if (!arg.equals(other.arg))   return false;
  if (cv == null) {
    if (other.cv != null)     return false;
  }
 else   if (!cv.equals(other.cv))   return false;
  return true;
}","@Override public boolean equals(Object obj){
  if (this == obj)   return true;
  if (obj == null)   return false;
  if (!(obj instanceof RecCV))   throw new STCRuntimeError(""String_Node_Str"" + this.getClass().getName() + ""String_Node_Str""+ obj.getClass().getName());
  RecCV other=(RecCV)obj;
  if (arg == null) {
    if (other.arg != null)     return false;
  }
 else   if (!arg.equals(other.arg))   return false;
  if (cv == null) {
    if (other.cv != null)     return false;
  }
 else   if (!cv.equals(other.cv))   return false;
  return true;
}",0.7903711133400201
132889,"@Override public Arg get(Object key){
  assert(key instanceof Var);
  Var v=(Var)key;
  Arg replace=null;
  CongruentSet curr=CongruentSet.this;
  List<CongruentSet> visited=new ArrayList<CongruentSet>();
  do {
    replace=curr.canonical.get(v.asArg());
    if (replace != null) {
      break;
    }
    visited.add(curr);
    curr=curr.parent;
  }
 while (curr != null);
  if (replace != null && replace.isVar()) {
    for (    CongruentSet s : visited) {
      if (s.inaccessible.contains(replace.getVar())) {
        return null;
      }
    }
  }
  if (replace != null && contradictions.contains(replace)) {
    return null;
  }
  return replace;
}","@Override public Arg get(Object key){
  assert(key instanceof Var);
  Var v=(Var)key;
  RecCV cv=new RecCV(v.asArg());
  Arg replace=null;
  CongruentSet curr=CongruentSet.this;
  List<CongruentSet> visited=new ArrayList<CongruentSet>();
  do {
    replace=curr.canonical.get(cv);
    if (replace != null) {
      break;
    }
    visited.add(curr);
    curr=curr.parent;
  }
 while (curr != null);
  if (replace != null && replace.isVar()) {
    for (    CongruentSet s : visited) {
      if (s.inaccessible.contains(replace.getVar())) {
        if (logger.isTraceEnabled()) {
          logger.trace(v + ""String_Node_Str"" + replace+ ""String_Node_Str""+ congType+ ""String_Node_Str""+ ""String_Node_Str"");
        }
        return null;
      }
    }
  }
  if (replace != null && contradictions.contains(replace)) {
    if (logger.isTraceEnabled()) {
      logger.trace(v + ""String_Node_Str"" + replace+ ""String_Node_Str""+ congType+ ""String_Node_Str""+ ""String_Node_Str"");
    }
    return null;
  }
  if (logger.isTraceEnabled()) {
    logger.trace(v + ""String_Node_Str"" + replace+ ""String_Node_Str""+ congType+ ""String_Node_Str"");
  }
  return replace;
}",0.683684794672586
132890,"/** 
 * Find computed values that appear in all branches but not parent
 * @param parentState
 * @param congType 
 * @param branchStates
 * @param alreadyAdded ignore these
 * @return
 */
private static List<ArgCV> findAllBranchCVs(CongruentVars parentState,CongruenceType congType,List<CongruentVars> branchStates,List<ArgCV> alreadyAdded){
  List<ArgCV> allBranchCVs=new ArrayList<ArgCV>();
  CongruentVars firstState=branchStates.get(0);
  for (  RecCV val : firstState.availableThisScope(congType)) {
    ArgCV convertedVal=parentState.convertToArgs(val,congType);
    if (convertedVal != null) {
      if (!alreadyAdded.contains(val) && !parentState.isAvailable(val,congType)) {
        int nBranches=branchStates.size();
        boolean presentInAll=true;
        for (        CongruentVars otherState : branchStates.subList(1,nBranches)) {
          if (!otherState.isAvailable(val,congType)) {
            presentInAll=false;
            break;
          }
        }
        if (presentInAll) {
          allBranchCVs.add(convertedVal);
        }
      }
    }
  }
  return allBranchCVs;
}","/** 
 * Find computed values that appear in all branches but not parent
 * @param parentState
 * @param congType 
 * @param branchStates
 * @param alreadyAdded ignore these
 * @return
 */
private static List<ArgCV> findAllBranchCVs(CongruentVars parentState,CongruenceType congType,List<CongruentVars> branchStates,List<ArgCV> alreadyAdded){
  List<ArgCV> allBranchCVs=new ArrayList<ArgCV>();
  CongruentVars firstState=branchStates.get(0);
  for (  RecCV val : firstState.availableThisScope(congType)) {
    ArgCV convertedVal=parentState.convertToArgs(val,congType);
    if (convertedVal != null) {
      if (!alreadyAdded.contains(convertedVal) && !parentState.isAvailable(convertedVal,congType)) {
        int nBranches=branchStates.size();
        boolean presentInAll=true;
        for (        CongruentVars otherState : branchStates.subList(1,nBranches)) {
          if (!otherState.isAvailable(convertedVal,congType)) {
            presentInAll=false;
            break;
          }
        }
        if (presentInAll) {
          allBranchCVs.add(convertedVal);
        }
      }
    }
  }
  return allBranchCVs;
}",0.985141828005403
132891,"/** 
 * ValLoc representing result of dereference ref
 * @param contents of ref
 * @param ref
 * @param copied if it is a copy of the original
 */
public static ValLoc derefCompVal(Var v,Var ref,IsValCopy copied){
  assert(Types.isRefTo(ref,v));
  return new ValLoc(ComputedValue.derefCompVal(ref),v.asArg(),Closed.MAYBE_NOT,copied);
}","/** 
 * ValLoc representing result of dereference ref
 * @param contents of ref
 * @param ref
 * @param copied if it is a copy of the original
 */
public static ValLoc derefCompVal(Var v,Var ref,IsValCopy copied){
}",0.7818181818181819
132892,"@Override public List<ValLoc> getResults(ValueState existing){
switch (op) {
case LOAD_BOOL:
case LOAD_FLOAT:
case LOAD_INT:
case LOAD_REF:
case LOAD_STRING:
case LOAD_BLOB:
case LOAD_VOID:
case LOAD_FILE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      if (Types.isPrimUpdateable(src.getVar().type())) {
        return null;
      }
      Closed outIsClosed;
      if (op == Opcode.LOAD_REF) {
        outIsClosed=Closed.MAYBE_NOT;
      }
 else {
        outIsClosed=Closed.YES_NOT_RECURSIVE;
      }
      List<ValLoc> result=new ArrayList<ValLoc>();
      ValLoc retrieve;
      if (op == Opcode.LOAD_REF) {
        retrieve=ValLoc.derefCompVal(src.getVar(),dst,IsValCopy.NO);
      }
 else {
        retrieve=vanillaResult(outIsClosed);
      }
      result.add(retrieve);
      Opcode cvop=Opcode.assignOpcode(src.futureType());
      if (cvop == null) {
        throw new STCRuntimeError(""String_Node_Str"" + src.getVar());
      }
      ValLoc assign=ValLoc.buildResult(cvop,Arrays.asList(dst.asArg()),src,outIsClosed);
      result.add(assign);
      Opcode derefOp=Opcode.derefOpCode(src.futureType());
      if (derefOp != null) {
        ValLoc deref=ValLoc.buildResult(derefOp,Arrays.asList(src),dst.asArg(),Closed.MAYBE_NOT);
        result.add(deref);
        for (        RecCV val : existing.findCongruent(src.getVar().asArg(),CongruenceType.VALUE)) {
          if (val.isCV()) {
            result.addAll(ValLoc.createLoadRefCVs(val.cv(),dst));
          }
        }
      }
      return result;
    }
case STORE_REF:
case STORE_BOOL:
case STORE_FLOAT:
case STORE_INT:
case STORE_STRING:
case STORE_BLOB:
case STORE_VOID:
case STORE_FILE:
{
    ValLoc assign=vanillaResult(Closed.YES_NOT_RECURSIVE);
    Arg dst=getOutput(0).asArg();
    Arg src=getInput(0);
    ValLoc retrieve;
    if (op == Opcode.STORE_REF) {
      retrieve=ValLoc.derefCompVal(src.getVar(),dst.getVar(),IsValCopy.NO);
    }
 else {
      retrieve=ValLoc.buildResult(Opcode.retrieveOpcode(dst.futureType()),Arrays.asList(dst),src,Closed.MAYBE_NOT);
    }
    return Arrays.asList(retrieve,assign);
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result));
  }
}
case GET_FILENAME:
{
List<ValLoc> res=new ArrayList<ValLoc>();
Arg filename=getOutput(0).asArg();
Arg file=getInput(0);
res.add(ValLoc.makeFilename(filename,file.getVar()));
ArgCV filenameCV=ValLoc.makeFilenameVal(file,null).value();
Arg filenameVal=existing.findCanonical(filenameCV,CongruenceType.VALUE);
if (filenameVal != null) {
  res.add(ValLoc.buildResult(Opcode.LOAD_STRING,filename,filenameVal,Closed.YES_NOT_RECURSIVE));
}
return res;
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar()).asList();
}
case SET_FILENAME_VAL:
{
Arg file=getOutput(0).asArg();
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val).asList();
}
case DEREF_BLOB:
case DEREF_BOOL:
case DEREF_FLOAT:
case DEREF_INT:
case DEREF_STRING:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES).asList();
}
case DEREF_FILE:
{
if (getOutput(0).isMapped() == Ternary.FALSE) {
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES).asList();
}
 else {
return null;
}
}
case STRUCT_INIT_FIELD:
{
ValLoc lookup=ValLoc.makeStructLookupResult(getInput(1).getVar(),getOutput(0),getInput(0).getStringLit());
return lookup.asList();
}
case STRUCT_LOOKUP:
{
ValLoc lookup=ValLoc.makeStructLookupResult(getOutput(0),getInput(0).getVar(),getInput(1).getStringLit());
return lookup.asList();
}
case ARRAY_INSERT_IMM:
case ARRAY_DEREF_INSERT_IMM:
case ARRAY_INSERT_FUTURE:
case ARRAY_DEREF_INSERT_FUTURE:
case ARRAYREF_INSERT_IMM:
case ARRAYREF_DEREF_INSERT_IMM:
case ARRAYREF_INSERT_FUTURE:
case ARRAYREF_DEREF_INSERT_FUTURE:
{
Var arr;
if (op == Opcode.ARRAYREF_INSERT_FUTURE || op == Opcode.ARRAYREF_INSERT_IMM || op == Opcode.ARRAYREF_DEREF_INSERT_FUTURE || op == Opcode.ARRAYREF_DEREF_INSERT_IMM) {
arr=getOutput(1);
}
 else {
arr=getOutput(0);
}
Arg ix=getInput(0);
Var member=getInput(1).getVar();
boolean insertingRef=(op == Opcode.ARRAYREF_DEREF_INSERT_FUTURE || op == Opcode.ARRAYREF_DEREF_INSERT_IMM || op == Opcode.ARRAY_DEREF_INSERT_FUTURE || op == Opcode.ARRAY_DEREF_INSERT_IMM);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingRef));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Var val=getInput(2 * i + 1).getVar();
res.add(ValLoc.makeArrayResult(arr,key,val,false));
}
res.add(CommonFunctionCall.makeArraySizeCV(arr,Arg.createIntLit(elemCount),false));
return res;
}
case ARRAY_LOOKUP_IMM:
case ARRAY_LOOKUP_REF_IMM:
case ARRAY_LOOKUP_FUTURE:
case ARRAYREF_LOOKUP_FUTURE:
case ARRAYREF_LOOKUP_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARRAY_LOOKUP_IMM) {
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,contents,false));
}
 else {
assert(Types.isMemberReference(contents,arr));
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.makeArrayResult(arr,ix,contents,true));
addDerefMemberVals(existing,arr,contents,ix,res);
return res;
}
}
case ARRAY_CREATE_NESTED_FUTURE:
case ARRAY_CREATE_NESTED_IMM:
case ARRAYREF_CREATE_NESTED_FUTURE:
case ARRAYREF_CREATE_NESTED_IMM:
case ARRAY_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr;
if (op == Opcode.ARRAYREF_CREATE_NESTED_FUTURE || op == Opcode.ARRAYREF_CREATE_NESTED_IMM) {
arr=getOutput(2);
}
 else {
arr=getOutput(1);
}
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsRef=op != Opcode.ARRAY_CREATE_NESTED_IMM && op != Opcode.ARRAY_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr,returnsRef));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsRef));
if (op == Opcode.ARRAY_CREATE_NESTED_IMM || op == Opcode.ARRAY_CREATE_BAG) {
}
 else {
addDerefMemberVals(existing,arr,nestedArr,ix,res);
}
return res;
}
case COPY_REF:
{
List<ValLoc> res=new ArrayList<ValLoc>();
Var srcRef=getInput(0).getVar();
res.add(ValLoc.makeAlias(getOutput(0),srcRef));
return res;
}
default :
return null;
}
}","@Override public List<ValLoc> getResults(ValueState existing){
switch (op) {
case LOAD_BOOL:
case LOAD_FLOAT:
case LOAD_INT:
case LOAD_REF:
case LOAD_STRING:
case LOAD_BLOB:
case LOAD_VOID:
case LOAD_FILE:
{
      Arg src=getInput(0);
      Var dst=getOutput(0);
      if (Types.isPrimUpdateable(src.getVar().type())) {
        return null;
      }
      Closed outIsClosed;
      if (op == Opcode.LOAD_REF) {
        outIsClosed=Closed.MAYBE_NOT;
      }
 else {
        outIsClosed=Closed.YES_NOT_RECURSIVE;
      }
      List<ValLoc> result=new ArrayList<ValLoc>();
      ValLoc retrieve;
      if (op == Opcode.LOAD_REF) {
        retrieve=ValLoc.derefCompVal(dst,src.getVar(),IsValCopy.NO);
      }
 else {
        retrieve=vanillaResult(outIsClosed);
      }
      result.add(retrieve);
      Opcode cvop=Opcode.assignOpcode(src.futureType());
      if (cvop == null) {
        throw new STCRuntimeError(""String_Node_Str"" + src.getVar());
      }
      ValLoc assign=ValLoc.buildResult(cvop,Arrays.asList(dst.asArg()),src,outIsClosed);
      result.add(assign);
      return result;
    }
case STORE_REF:
case STORE_BOOL:
case STORE_FLOAT:
case STORE_INT:
case STORE_STRING:
case STORE_BLOB:
case STORE_VOID:
case STORE_FILE:
{
    ValLoc assign=vanillaResult(Closed.YES_NOT_RECURSIVE);
    Arg dst=getOutput(0).asArg();
    Arg src=getInput(0);
    ValLoc retrieve;
    if (op == Opcode.STORE_REF) {
      retrieve=ValLoc.derefCompVal(src.getVar(),dst.getVar(),IsValCopy.NO);
    }
 else {
      retrieve=ValLoc.buildResult(Opcode.retrieveOpcode(dst.futureType()),Arrays.asList(dst),src,Closed.MAYBE_NOT);
    }
    return Arrays.asList(retrieve,assign);
  }
case IS_MAPPED:
{
  ValLoc vanilla=vanillaResult(Closed.YES_NOT_RECURSIVE);
  assert(vanilla != null);
  Var fileVar=getInput(0).getVar();
  if (fileVar.isMapped() == Ternary.MAYBE) {
    return vanilla.asList();
  }
 else {
    Arg result=Arg.createBoolLit(fileVar.isMapped() == Ternary.TRUE);
    return Arrays.asList(vanilla,ValLoc.makeCopy(getOutput(0),result));
  }
}
case GET_FILENAME:
{
List<ValLoc> res=new ArrayList<ValLoc>();
Arg filename=getOutput(0).asArg();
Arg file=getInput(0);
res.add(ValLoc.makeFilename(filename,file.getVar()));
ArgCV filenameCV=ValLoc.makeFilenameVal(file,null).value();
Arg filenameVal=existing.findCanonical(filenameCV,CongruenceType.VALUE);
if (filenameVal != null) {
  res.add(ValLoc.buildResult(Opcode.LOAD_STRING,filename,filenameVal,Closed.YES_NOT_RECURSIVE));
}
return res;
}
case GET_LOCAL_FILENAME:
{
return ValLoc.makeFilenameLocal(getOutput(0).asArg(),getInput(0).getVar()).asList();
}
case SET_FILENAME_VAL:
{
Arg file=getOutput(0).asArg();
Arg val=getInput(0);
return ValLoc.makeFilenameVal(file,val).asList();
}
case DEREF_BLOB:
case DEREF_BOOL:
case DEREF_FLOAT:
case DEREF_INT:
case DEREF_STRING:
{
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES).asList();
}
case DEREF_FILE:
{
if (getOutput(0).isMapped() == Ternary.FALSE) {
return ValLoc.derefCompVal(getOutput(0),getInput(0).getVar(),IsValCopy.YES).asList();
}
 else {
return null;
}
}
case STRUCT_INIT_FIELD:
{
ValLoc lookup=ValLoc.makeStructLookupResult(getInput(1).getVar(),getOutput(0),getInput(0).getStringLit());
return lookup.asList();
}
case STRUCT_LOOKUP:
{
ValLoc lookup=ValLoc.makeStructLookupResult(getOutput(0),getInput(0).getVar(),getInput(1).getStringLit());
return lookup.asList();
}
case ARRAY_INSERT_IMM:
case ARRAY_DEREF_INSERT_IMM:
case ARRAY_INSERT_FUTURE:
case ARRAY_DEREF_INSERT_FUTURE:
case ARRAYREF_INSERT_IMM:
case ARRAYREF_DEREF_INSERT_IMM:
case ARRAYREF_INSERT_FUTURE:
case ARRAYREF_DEREF_INSERT_FUTURE:
{
Var arr;
if (op == Opcode.ARRAYREF_INSERT_FUTURE || op == Opcode.ARRAYREF_INSERT_IMM || op == Opcode.ARRAYREF_DEREF_INSERT_FUTURE || op == Opcode.ARRAYREF_DEREF_INSERT_IMM) {
arr=getOutput(1);
}
 else {
arr=getOutput(0);
}
Arg ix=getInput(0);
Var member=getInput(1).getVar();
boolean insertingRef=(op == Opcode.ARRAYREF_DEREF_INSERT_FUTURE || op == Opcode.ARRAYREF_DEREF_INSERT_IMM || op == Opcode.ARRAY_DEREF_INSERT_FUTURE || op == Opcode.ARRAY_DEREF_INSERT_IMM);
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,member,insertingRef));
}
case ARRAY_BUILD:
{
Var arr=getOutput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.buildResult(op,getInputs(),arr.asArg(),Closed.YES_NOT_RECURSIVE));
assert(getInputs().size() % 2 == 0);
int elemCount=getInputs().size() / 2;
for (int i=0; i < elemCount; i++) {
Arg key=getInput(2 * i);
Var val=getInput(2 * i + 1).getVar();
res.add(ValLoc.makeArrayResult(arr,key,val,false));
}
res.add(CommonFunctionCall.makeArraySizeCV(arr,Arg.createIntLit(elemCount),false));
return res;
}
case ARRAY_LOOKUP_IMM:
case ARRAY_LOOKUP_REF_IMM:
case ARRAY_LOOKUP_FUTURE:
case ARRAYREF_LOOKUP_FUTURE:
case ARRAYREF_LOOKUP_IMM:
{
Var arr=getInput(0).getVar();
Arg ix=getInput(1);
Var contents=getOutput(0);
if (op == Opcode.ARRAY_LOOKUP_IMM) {
return Arrays.asList(ValLoc.makeArrayResult(arr,ix,contents,false));
}
 else {
assert(Types.isMemberReference(contents,arr));
List<ValLoc> res=new ArrayList<ValLoc>();
res.add(ValLoc.makeArrayResult(arr,ix,contents,true));
addDerefMemberVals(existing,arr,contents,ix,res);
return res;
}
}
case ARRAY_CREATE_NESTED_FUTURE:
case ARRAY_CREATE_NESTED_IMM:
case ARRAYREF_CREATE_NESTED_FUTURE:
case ARRAYREF_CREATE_NESTED_IMM:
case ARRAY_CREATE_BAG:
{
Var nestedArr=getOutput(0);
Var arr;
if (op == Opcode.ARRAYREF_CREATE_NESTED_FUTURE || op == Opcode.ARRAYREF_CREATE_NESTED_IMM) {
arr=getOutput(2);
}
 else {
arr=getOutput(1);
}
Arg ix=getInput(0);
List<ValLoc> res=new ArrayList<ValLoc>();
boolean returnsRef=op != Opcode.ARRAY_CREATE_NESTED_IMM && op != Opcode.ARRAY_CREATE_BAG;
res.add(ValLoc.makeArrayResult(arr,ix,nestedArr,returnsRef));
res.add(ValLoc.makeCreateNestedResult(arr,ix,nestedArr,returnsRef));
if (op == Opcode.ARRAY_CREATE_NESTED_IMM || op == Opcode.ARRAY_CREATE_BAG) {
}
 else {
addDerefMemberVals(existing,arr,nestedArr,ix,res);
}
return res;
}
case COPY_REF:
{
List<ValLoc> res=new ArrayList<ValLoc>();
Var srcRef=getInput(0).getVar();
res.add(ValLoc.makeAlias(getOutput(0),srcRef));
return res;
}
default :
return null;
}
}",0.9632128010040004
132893,"@Override public boolean equals(Object obj){
  if (this == obj)   return true;
  if (obj == null)   return false;
  if (!(obj instanceof RecCV))   throw new STCRuntimeError(""String_Node_Str"" + this.getClass().getName() + ""String_Node_Str""+ obj.getClass().getName());
  RecCV other=(RecCV)obj;
  if (arg == null) {
    if (other.arg != null)     return false;
  }
 else   if (!arg.equals(other.arg))   return false;
  if (cv == null) {
    if (other.cv != null)     return false;
  }
 else   if (!cv.equals(other.cv))   return false;
  return true;
}","@Override public boolean equals(Object obj){
  if (this == obj)   return true;
  if (obj == null)   return false;
  if (!(obj instanceof RecCV))   throw new STCRuntimeError(""String_Node_Str"" + this.getClass().getName() + ""String_Node_Str""+ obj.getClass().getName());
  RecCV other=(RecCV)obj;
  if (arg != null) {
    if (other.arg == null) {
      return false;
    }
    return arg.equals(other.arg);
  }
 else {
    assert(cv != null);
    if (other.cv == null) {
      return false;
    }
    return cv.equals(other.cv);
  }
}",0.680259499536608
132894,"/** 
 * Recanonicalize components.  If an old computed value had a reference to oldCanonical in it, then it's no longer canonical, so need to replace. This will add to the merge queue, so callers need to process it before returning to other modules
 * @param oldComponent
 * @param newComponent if null, just recanonicalize
 */
private void updateCanonicalComponents(GlobalConstants consts,Arg oldComponent,Arg newComponent){
  CongruentSets curr=this;
  do {
    for (    RecCV outerCV : curr.componentIndex.get(oldComponent)) {
      assert(outerCV.isCV());
      RecCV newOuterCV=outerCV;
      if (newComponent != null) {
        List<RecCV> newInputs=replaceInput(outerCV.cv().getInputs(),oldComponent,newComponent);
        newOuterCV=new RecCV(outerCV.cv().substituteInputs(newInputs));
      }
      newOuterCV=canonicalize(consts,newOuterCV);
      if (newOuterCV != outerCV) {
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ newComponent+ ""String_Node_Str""+ outerCV);
        }
        this.componentIndex.put(newComponent,newOuterCV);
        Arg canonical=findCanonical(outerCV);
        if (canonical != null) {
          Arg newCanonical=findCanonical(newOuterCV);
          if (newCanonical != null && !newCanonical.equals(canonical)) {
            mergeQueue.add(new ToMerge(canonical,newCanonical));
            if (logger.isTraceEnabled()) {
              if (newComponent != null) {
                logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ newComponent+ ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
              }
 else {
                logger.trace(""String_Node_Str"" + oldComponent + ""String_Node_Str""+ canonical+ ""String_Node_Str""+ newCanonical);
              }
            }
          }
 else {
            addToSet(consts,newOuterCV,canonical);
          }
          if (contradictions.contains(newComponent) && !contradictions.contains(oldComponent)) {
            markContradiction(canonical);
          }
        }
      }
    }
    curr=curr.parent;
  }
 while (curr != null);
  this.componentIndex.remove(oldComponent);
}","/** 
 * Recanonicalize components.  If an old computed value had a reference to oldCanonical in it, then it's no longer canonical, so need to replace. This will add to the merge queue, so callers need to process it before returning to other modules
 * @param oldComponent
 * @param newComponent if null, just recanonicalize
 */
private void updateCanonicalComponents(GlobalConstants consts,Arg oldComponent,Arg newComponent){
}",0.3294753086419753
132895,"/** 
 * Keep index of where Arg appears.  When merge happens, need to go through and replace old Arg with new canonical Arg.
 */
private void addInputIndex(Arg newInput,RecCV newCV){
  componentIndex.put(newInput,newCV);
}","/** 
 * Keep index of where Arg appears.  When merge happens, need to go through and replace old Arg with new canonical Arg.
 */
private void addInputIndex(Arg newInput,RecCV newCV){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + newInput + ""String_Node_Str""+ newCV);
  }
  componentIndex.put(newInput,newCV);
}",0.697841726618705
132896,"private void processQueues(GlobalConstants consts){
  Arg oldCanon;
  Arg newCanon;
  do {
    while (!mergeQueue.isEmpty()) {
      ToMerge merge=mergeQueue.removeFirst();
      oldCanon=findCanonical(merge.oldSet);
      newCanon=findCanonical(merge.newSet);
      if (!oldCanon.equals(newCanon)) {
        changeCanonicalOnce(consts,oldCanon,newCanon);
      }
    }
    while (!futureValQueue.isEmpty()) {
      Var futureWithVal=futureValQueue.removeFirst();
      updateCanonicalComponents(consts,futureWithVal.asArg(),null);
    }
  }
 while (!mergeQueue.isEmpty() || !futureValQueue.isEmpty());
}","private void processQueues(GlobalConstants consts){
  if (logger.isTraceEnabled() && !mergeQueue.isEmpty() && !futureValQueue.isEmpty()) {
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + mergeQueue);
    logger.trace(""String_Node_Str"" + futureValQueue);
  }
  Arg oldCanon;
  Arg newCanon;
  do {
    while (!mergeQueue.isEmpty()) {
      ToMerge merge=mergeQueue.removeFirst();
      oldCanon=findCanonical(merge.oldSet);
      newCanon=findCanonical(merge.newSet);
      if (!oldCanon.equals(newCanon)) {
        changeCanonicalOnce(consts,oldCanon,newCanon);
      }
    }
    while (!futureValQueue.isEmpty()) {
      Var futureWithVal=futureValQueue.removeFirst();
      updateCanonicalComponents(consts,futureWithVal.asArg(),null);
    }
  }
 while (!mergeQueue.isEmpty() || !futureValQueue.isEmpty());
}",0.8388888888888889
132897,"/** 
 * Try to do compile-time evaluation of operator
 * @param op
 * @param inputs inputs to operator.  Null or a variable if not constant
 * @return output value of op if it could be evaluated at compile-time, nullotherwise.  Also returns null if the operator is simply a copy, since no reduction can occur then 
 */
public static Arg eval(BuiltinOpcode op,List<Arg> inputs){
  if (Operators.isShortCircuitable(op)) {
    return evalShortCircuit(op,inputs);
  }
 else   if (Operators.isCopy(op)) {
    return null;
  }
 else {
    boolean allInt=true;
    boolean allFloat=true;
    boolean allString=true;
    boolean allBool=true;
    for (    Arg in : inputs) {
      if (in == null) {
        return null;
      }
      allInt=allInt && in.isIntVal();
      allFloat=allFloat && in.isFloatVal();
      allString=allString && in.isStringVal();
      allBool=allBool && in.isBoolVal();
    }
    if (allInt) {
      return evalIntOp(op,inputs);
    }
 else     if (allFloat) {
      return evalFloatOp(op,inputs);
    }
 else     if (allString) {
      return evalStringOp(op,inputs);
    }
 else     if (allBool) {
      return evalBoolOp(op,inputs);
    }
 else {
      return evalOtherOp(op,inputs);
    }
  }
}","/** 
 * Try to do compile-time evaluation of operator
 * @param op
 * @param inputs inputs to operator.  Null or a variable if not constant
 * @return output value of op if it could be evaluated at compile-time, nullotherwise.  Also returns null if the operator is simply a copy, since no reduction can occur then 
 */
public static Arg eval(BuiltinOpcode op,List<Arg> inputs){
  if (Operators.isShortCircuitable(op)) {
    return evalShortCircuit(op,inputs);
  }
 else   if (Operators.isCopy(op)) {
    return null;
  }
 else {
    boolean allInt=true;
    boolean allFloat=true;
    boolean allString=true;
    boolean allBool=true;
    boolean allConst=true;
    for (    Arg in : inputs) {
      if (in == null) {
        return null;
      }
      allInt=allInt && in.isImmediateInt();
      allFloat=allFloat && in.isImmediateFloat();
      allString=allString && in.isImmediateString();
      allBool=allBool && in.isImmediateBool();
      allConst=allConst && in.isConstant();
    }
    if (allConst) {
      if (allInt) {
        return evalIntOp(op,inputs);
      }
 else       if (allFloat) {
        return evalFloatOp(op,inputs);
      }
 else       if (allString) {
        return evalStringOp(op,inputs);
      }
 else       if (allBool) {
        return evalBoolOp(op,inputs);
      }
 else {
        return evalOtherOp(op,inputs);
      }
    }
    return null;
  }
}",0.9254419677171408
132898,"/** 
 * Constant folding for short-circuitable operations where we don't always need to know both arguments to evaluate
 * @param constArgs unknown args are null
 * @return
 */
private static Arg evalShortCircuit(BuiltinOpcode op,List<Arg> constArgs){
  List<Arg> constInputs=new ArrayList<Arg>(2);
  for (  Arg in : constArgs) {
    if (in != null) {
      assert(in.isBoolVal());
      constInputs.add(in);
    }
  }
  if (constInputs.size() >= 1) {
    boolean arg1=constInputs.get(0).getBoolLit();
    if (constInputs.size() == 2) {
      boolean arg2=constInputs.get(1).getBoolLit();
switch (op) {
case OR:
        return Arg.createBoolLit(arg1 || arg2);
case AND:
      return Arg.createBoolLit(arg1 && arg2);
default :
  }
}
 else if (constInputs.size() == 1) {
  if (op == BuiltinOpcode.AND && arg1) {
    return Arg.createBoolLit(true);
  }
 else   if (op == BuiltinOpcode.OR && !arg1) {
    return Arg.createBoolLit(false);
  }
}
}
return null;
}","/** 
 * Constant folding for short-circuitable operations where we don't always need to know both arguments to evaluate
 * @param constArgs unknown args are null.  Currently assume 2 args
 * @return
 */
private static Arg evalShortCircuit(BuiltinOpcode op,List<Arg> constArgs){
}",0.4097165991902834
132899,"private RecCV tryConstantFold(CongruentSet congruent,ComputedValue<RecCV> val){
  if (val.op == Opcode.ASYNC_OP || val.op == Opcode.LOCAL_OP) {
    List<Arg> inputs;
    if (val.op == Opcode.LOCAL_OP) {
      inputs=convertToArgs(val);
    }
 else {
      assert(val.op == Opcode.ASYNC_OP);
      inputs=findFutureValues(val,congruent);
    }
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + val + ""String_Node_Str""+ inputs);
    }
    if (inputs != null) {
      Arg res=OpEvaluator.eval((BuiltinOpcode)val.subop,inputs);
      if (res != null) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + val + ""String_Node_Str""+ res);
        }
        boolean futureResult=val.op != Opcode.LOCAL_OP;
        return canonicalConstantVal(futureResult,res);
      }
    }
  }
 else   if (val.op == Opcode.IS_MAPPED) {
  }
  return null;
}","private RecCV tryConstantFold(CongruentSet congruent,ComputedValue<RecCV> val){
  if (val.op == Opcode.ASYNC_OP || val.op == Opcode.LOCAL_OP) {
    List<Arg> inputs;
    if (val.op == Opcode.LOCAL_OP) {
      inputs=convertToArgs(val);
    }
 else {
      assert(val.op == Opcode.ASYNC_OP);
      inputs=findFutureValues(val,congruent);
    }
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + val + ""String_Node_Str""+ inputs);
    }
    if (inputs != null) {
      Arg res=OpEvaluator.eval((BuiltinOpcode)val.subop,inputs);
      if (res != null) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + val + ""String_Node_Str""+ res);
        }
        boolean futureResult=val.op != Opcode.LOCAL_OP;
        return valFromArg(futureResult,res);
      }
    }
  }
 else   if (val.op == Opcode.IS_MAPPED) {
  }
  return null;
}",0.9829931972789115
132900,"public static Map<Var,Arg> constantFold(BuiltinOpcode op,Var outVar,List<Arg> inputs){
  Arg out=OpEvaluator.eval(op,inputs);
  return (out == null) ? null : Collections.singletonMap(outVar,out);
}","public static Map<Var,Arg> constantFold(BuiltinOpcode op,Var outVar,List<Arg> inputs){
  Arg out=OpEvaluator.eval(op,inputs);
  if (out != null && out.isConstant()) {
    return Collections.singletonMap(outVar,out);
  }
 else {
    return null;
  }
}",0.7516778523489933
132901,"private UnifiedValues findCongruencesContRec(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result){
  logger.trace(""String_Node_Str"" + cont.getType());
  Congruences contState=state.enterContinuation(cont.inheritsParentVars(),stmtIndex);
  List<BlockingVar> contClosedVars=cont.blockingVars(true);
  if (contClosedVars != null) {
    for (    BlockingVar bv : contClosedVars) {
      contState.markClosed(bv.var,stmtIndex,bv.recursive);
    }
  }
  boolean unifyBranches=cont.isExhaustiveSyncConditional();
  List<Congruences> branchStates=unifyBranches ? new ArrayList<Congruences>() : null;
  for (  Block contBlock : cont.getBlocks()) {
    Congruences blockState=contState.enterBlock();
    findCongruencesRec(prog,fn,contBlock,cont.childContext(execCx),blockState,result);
    if (unifyBranches) {
      branchStates.add(blockState);
    }
  }
  if (unifyBranches) {
    return UnifiedValues.unify(logger,prog.constants(),fn,reorderingAllowed,stmtIndex,state,cont,branchStates,cont.getBlocks());
  }
 else {
    return UnifiedValues.EMPTY;
  }
}","private UnifiedValues findCongruencesContRec(Program prog,Function fn,ExecContext execCx,Continuation cont,int stmtIndex,Congruences state,Map<Block,Congruences> result){
  logger.trace(""String_Node_Str"" + cont.getType());
  Congruences contState=state.enterContinuation(cont.inheritsParentVars(),stmtIndex);
  List<BlockingVar> contClosedVars=cont.blockingVars(true);
  if (contClosedVars != null) {
    for (    BlockingVar bv : contClosedVars) {
      int contStmtIndex=0;
      contState.markClosed(bv.var,contStmtIndex,bv.recursive);
    }
  }
  boolean unifyBranches=cont.isExhaustiveSyncConditional();
  List<Congruences> branchStates=unifyBranches ? new ArrayList<Congruences>() : null;
  for (  Block contBlock : cont.getBlocks()) {
    Congruences blockState=contState.enterBlock();
    findCongruencesRec(prog,fn,contBlock,cont.childContext(execCx),blockState,result);
    if (unifyBranches) {
      branchStates.add(blockState);
    }
  }
  if (unifyBranches) {
    return UnifiedValues.unify(logger,prog.constants(),fn,reorderingAllowed,stmtIndex,state,cont,branchStates,cont.getBlocks());
  }
 else {
    return UnifiedValues.EMPTY;
  }
}",0.9854817421909372
132902,"/** 
 * Check if variable is directly closed. Returns a matching entry 
 * @param var
 * @param recursive
 * @param stmtIndex
 * @return
 */
private ClosedEntry getDirectClosedEntry(Var var,boolean recursive,int stmtIndex){
  ClosedVarTracker curr=this;
  int currStmtIndex=stmtIndex;
  while (curr != null) {
    for (    ClosedEntry ce : curr.closed.get(var)) {
      if (ce.matches(recursive,currStmtIndex)) {
        if (curr == this) {
          return ce;
        }
 else {
          ClosedEntry origScopeEntry=new ClosedEntry(0,ce.recursive);
          closed.put(var,origScopeEntry);
          return origScopeEntry;
        }
      }
    }
    currStmtIndex=curr.parentStmtIndex;
    curr=curr.parent;
  }
  return null;
}","/** 
 * Check if variable is directly closed. Returns a matching entry 
 * @param var
 * @param recursive
 * @param stmtIndex
 * @return
 */
private ClosedEntry getDirectClosedEntry(Var var,boolean recursive,int stmtIndex){
  ClosedVarTracker curr=this;
  int currStmtIndex=stmtIndex;
  while (curr != null) {
    for (    ClosedEntry ce : curr.closed.get(var)) {
      logger.trace(var + ""String_Node_Str"" + ce+ ""String_Node_Str""+ recursive+ ""String_Node_Str""+ currStmtIndex);
      if (ce.matches(recursive,currStmtIndex)) {
        if (curr == this) {
          return ce;
        }
 else {
          ClosedEntry origScopeEntry=new ClosedEntry(0,ce.recursive);
          closed.put(var,origScopeEntry);
          return origScopeEntry;
        }
      }
    }
    currStmtIndex=curr.parentStmtIndex;
    curr=curr.parent;
  }
  return null;
}",0.9276649746192892
132903,"/** 
 * Merge two congruence sets that are newly connected via value
 * @param errContext
 * @param resVal
 * @param congruent
 * @param newLoc representative of set with location just assigned
 * @param oldLoc representative of existing set
 */
private void mergeSets(String errContext,RecCV value,GlobalConstants consts,CongruentSets congruent,Arg newLoc,Arg oldLoc,int stmtIndex){
  if (newLoc.equals(oldLoc)) {
    return;
  }
  if (!checkNoContradiction(errContext,congruent.congType,value,newLoc,oldLoc)) {
    congruent.markContradiction(newLoc);
    congruent.markContradiction(oldLoc);
  }
  Arg winner=preferred(congruent,oldLoc,newLoc,stmtIndex);
  Arg loser=(winner == oldLoc ? newLoc : oldLoc);
  changeCanonical(consts,congruent,loser,winner);
}","/** 
 * Merge two congruence sets that are newly connected via value
 * @param errContext
 * @param resVal
 * @param congruent
 * @param newLoc representative of set with location just assigned
 * @param oldLoc representative of existing set
 */
private void mergeSets(String errContext,RecCV value,GlobalConstants consts,CongruentSets congruent,Arg newLoc,Arg oldLoc,int stmtIndex){
  if (newLoc.equals(oldLoc)) {
    return;
  }
  if (!checkNoContradiction(errContext,congruent.congType,value,newLoc,oldLoc)) {
    congruent.markContradiction(newLoc);
    congruent.markContradiction(oldLoc);
  }
  Arg winner=preferred(congruent,oldLoc,newLoc,stmtIndex);
  Arg loser=(winner == oldLoc ? newLoc : oldLoc);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + oldLoc + ""String_Node_Str""+ newLoc+ ""String_Node_Str""+ winner);
  }
  changeCanonical(consts,congruent,loser,winner);
}",0.9166666666666666
132904,"private boolean isClosed(Arg varArg,int stmtIndex,boolean recursive){
  if (varArg.isConstant() || !trackClosed(varArg.getVar())) {
    logger.trace(varArg + ""String_Node_Str"");
    return true;
  }
  Var canonicalAlias=getCanonicalAlias(varArg);
  ClosedEntry canonicalClosed;
  canonicalClosed=isClosedNonAlias(canonicalAlias,stmtIndex,recursive);
  if (canonicalClosed != null && canonicalClosed.matches(recursive,stmtIndex)) {
    logger.trace(varArg + ""String_Node_Str"" + canonicalClosed.stmtIndex);
    return true;
  }
  for (  Arg mergedSet : byAlias.allMergedCanonicals(canonicalAlias.asArg())) {
    assert(mergedSet.isVar());
    Var merged=mergedSet.getVar();
    ClosedEntry ce=isClosedNonAlias(merged,stmtIndex,recursive);
    if (ce != null) {
      track.close(canonicalAlias,ce);
      if (ce.matches(recursive,stmtIndex)) {
        logger.trace(varArg + ""String_Node_Str"" + canonicalClosed.stmtIndex+ ""String_Node_Str""+ merged);
        return true;
      }
    }
  }
  return false;
}","private boolean isClosed(Arg varArg,int stmtIndex,boolean recursive){
  if (varArg.isConstant() || !trackClosed(varArg.getVar())) {
    logger.trace(varArg + ""String_Node_Str"");
    return true;
  }
  Var canonicalAlias=getCanonicalAlias(varArg);
  ClosedEntry ce;
  ce=isClosedNonAlias(canonicalAlias,stmtIndex,recursive);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + varArg + ""String_Node_Str""+ canonicalAlias+ ""String_Node_Str""+ ce);
  }
  if (ce != null && ce.matches(recursive,stmtIndex)) {
    logger.trace(varArg + ""String_Node_Str"" + ce.stmtIndex);
    return true;
  }
  for (  Arg mergedSet : byAlias.allMergedCanonicals(canonicalAlias.asArg())) {
    assert(mergedSet.isVar());
    Var merged=mergedSet.getVar();
    isClosedNonAlias(merged,stmtIndex,recursive);
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + varArg + ""String_Node_Str""+ mergedSet+ ""String_Node_Str""+ ce);
    }
    if (ce != null) {
      track.close(canonicalAlias,ce);
      if (ce.matches(recursive,stmtIndex)) {
        logger.trace(varArg + ""String_Node_Str"" + ce.stmtIndex+ ""String_Node_Str""+ merged);
        return true;
      }
    }
  }
  return false;
}",0.8171064604185623
132905,"@Override public Map<String,Type> matchTypeVars(Type concrete){
  return baseType.matchTypeVars(concrete);
}","@Override public Map<String,Type> matchTypeVars(Type concrete){
  while (concrete instanceof SubType) {
    if (this.equals(concrete)) {
      break;
    }
 else {
      concrete=((SubType)concrete).baseType();
    }
  }
  return baseType.matchTypeVars(concrete);
}",0.579088471849866
132906,"/** 
 * Initialise a local file with a filename
 * @param localFile
 * @param filenameVal
 */
public void initLocalOutputFile(Var localFile,Arg filenameVal);","/** 
 * Initialise a local file with a filename
 * @param localFile an uninitialized local file var
 * @param filenameVal an immediate string containing the filename
 * @param isMapped an immediate bool saying whether the file is mapped- i.e. whether it must be retained in all cases
 */
public void initLocalOutputFile(Var localFile,Arg filenameVal,Arg isMapped);",0.6026871401151631
132907,"public static SetVariable createLocalFile(String varName,Expression fileName){
  return new SetVariable(varName,new Square(CREATE_LOCAL_FILE_REF,fileName));
}","public static SetVariable createLocalFile(String varName,Expression fileName,Expression initRefcount){
  return new SetVariable(varName,new Square(CREATE_LOCAL_FILE_REF,fileName,initRefcount));
}",0.8951841359773371
132908,"@Override public void initLocalOutputFile(Var localFile,Arg filenameVal){
  assert(localFile.type().assignableTo(Types.V_FILE));
  assert(filenameVal.type().assignableTo(Types.V_STRING));
  pointStack.peek().add(Turbine.createLocalFile(prefixVar(localFile),argToExpr(filenameVal)));
}","@Override public void initLocalOutputFile(Var localFile,Arg filenameVal,Arg isMapped){
  assert(localFile.type().assignableTo(Types.V_FILE));
  assert(filenameVal.isImmediateString());
  assert(isMapped.isImmediateBool());
  Sequence ifMapped=new Sequence(), ifUnmapped=new Sequence();
  ifMapped.add(new SetVariable(TCLTMP_INIT_REFCOUNT,LiteralInt.TWO));
  ifUnmapped.add(new SetVariable(TCLTMP_INIT_REFCOUNT,LiteralInt.ONE));
  pointStack.peek().add(new If(argToExpr(isMapped),ifMapped,ifUnmapped));
  pointStack.peek().add(Turbine.createLocalFile(prefixVar(localFile),argToExpr(filenameVal),new Value(TCLTMP_INIT_REFCOUNT)));
}",0.4420131291028446
132909,"/** 
 * Initialize the context and define the two variables: for array member and (optionally) for the loop count
 * @param context
 * @param rangeLoop where a range loop or container
 * @param includeIndexFuture if true, initialize the count var(getCountVarName()), rather than just loopCountval
 * @return
 * @throws UserException
 */
public Context setupLoopBodyContext(Context context,boolean rangeLoop,boolean includeIndexFuture) throws UserException {
  Type arrayType=findArrayType(context);
  loopBodyContext=new LocalContext(context);
  if (countVarName != null) {
    keyType=Types.arrayKeyType(arrayType);
    Type keyValType=Types.derefResultType(keyType);
    loopCountVal=context.createLocalValueVariable(keyValType,countVarName);
    if (includeIndexFuture) {
      Var loopCountVar=createCountVar();
      context.declareVariable(loopCountVar.type(),loopCountVar.name(),loopCountVar.storage(),loopCountVar.defType(),loopCountVar.mapping());
    }
  }
 else {
    loopCountVal=null;
  }
  VarStorage memberVarStorage=rangeLoop ? VarStorage.TEMP : VarStorage.ALIAS;
  memberVar=loopBodyContext.declareVariable(Types.arrayMemberType(arrayType),getMemberVarName(),memberVarStorage,DefType.LOCAL_USER,null);
  return loopBodyContext;
}","/** 
 * Initialize the context and define the two variables: for array member and (optionally) for the loop count
 * @param context
 * @param rangeLoop where a range loop or container
 * @param includeIndexFuture if true, initialize the count var(getCountVarName()), rather than just loopCountval
 * @return
 * @throws UserException
 */
public Context setupLoopBodyContext(Context context,boolean rangeLoop,boolean includeIndexFuture) throws UserException {
  Type arrayType=findArrayType(context);
  loopBodyContext=new LocalContext(context);
  if (countVarName != null) {
    keyType=Types.arrayKeyType(arrayType);
    Type keyValType=Types.derefResultType(keyType);
    loopCountVal=context.createLocalValueVariable(keyValType,countVarName);
    if (includeIndexFuture) {
      Var countVar=createCountVar();
      loopBodyContext.declareVariable(countVar.type(),countVar.name(),countVar.storage(),countVar.defType(),countVar.mapping());
    }
  }
 else {
    loopCountVal=null;
  }
  VarStorage memberVarStorage=rangeLoop ? VarStorage.TEMP : VarStorage.ALIAS;
  memberVar=loopBodyContext.declareVariable(Types.arrayMemberType(arrayType),getMemberVarName(),memberVarStorage,DefType.LOCAL_USER,null);
  return loopBodyContext;
}",0.981421647819063
132910,"/** 
 * Handle a prefix of array lookups for the assign target
 * @param afterActions 
 * @throws UserException
 * @throws UndefinedTypeException
 * @throws TypeMismatchException 
 */
private LValue reduceArrayLVal(Context context,LValue origLval,LValue lval,SwiftAST rValExpr,Type rValType,Deque<Runnable> afterActions) throws TypeMismatchException, UndefinedTypeException, UserException {
  SwiftAST indexExpr=lval.indices.get(0);
  assert(indexExpr.getType() == ExMParser.ARRAY_PATH);
  assert(indexExpr.getChildCount() == 1);
  Type indexType=TypeChecker.findSingleExprType(context,indexExpr.child(0));
  if (!Types.isArrayKeyFuture(lval.var,indexType)) {
    throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + indexType.typeName());
  }
  if (lval.indices.size() == 1) {
    Var lookedup=assignTo1DArray(context,origLval,lval,rValExpr,rValType,afterActions);
    return new LValue(lval.tree,lookedup,new ArrayList<SwiftAST>());
  }
 else {
    Var lvalArr=context.lookupVarUser(lval.varName);
    Type memberType=lval.getType(context,1);
    Var mVar;
    if (Types.isArray(memberType)) {
      Long literal=Literals.extractIntLit(context,indexExpr.child(0));
      if (literal != null) {
        long arrIx=literal;
        if (Types.isArray(lvalArr.type())) {
          mVar=varCreator.createTmpAlias(context,memberType);
          backend.arrayCreateNestedImm(mVar,lvalArr,Arg.createIntLit(arrIx));
        }
 else {
          assert(Types.isArrayRef(lvalArr.type()));
          mVar=varCreator.createTmp(context,new RefType(memberType));
          backend.arrayRefCreateNestedImm(mVar,origLval.var,lvalArr,Arg.createIntLit(arrIx));
        }
      }
 else {
        mVar=varCreator.createTmp(context,new RefType(memberType));
        Var indexVar=exprWalker.eval(context,indexExpr.child(0),Types.F_INT,false,null);
        if (Types.isArray(lvalArr.type())) {
          backend.arrayCreateNestedFuture(mVar,lvalArr,indexVar);
        }
 else {
          assert(Types.isArrayRef(lvalArr.type()));
          backend.arrayRefCreateNestedFuture(mVar,origLval.var,lvalArr,indexVar);
        }
      }
    }
 else {
      mVar=varCreator.createTmp(context,new RefType(memberType));
    }
    return new LValue(lval.tree,mVar,lval.indices.subList(1,lval.indices.size()));
  }
}","/** 
 * Handle a prefix of array lookups for the assign target
 * @param afterActions 
 * @throws UserException
 * @throws UndefinedTypeException
 * @throws TypeMismatchException 
 */
private LValue reduceArrayLVal(Context context,LValue origLval,LValue lval,SwiftAST rValExpr,Type rValType,Deque<Runnable> afterActions) throws TypeMismatchException, UndefinedTypeException, UserException {
  SwiftAST indexExpr=lval.indices.get(0);
  assert(indexExpr.getType() == ExMParser.ARRAY_PATH);
  assert(indexExpr.getChildCount() == 1);
  Type indexType=TypeChecker.findSingleExprType(context,indexExpr.child(0));
  if (!Types.isArrayKeyFuture(lval.var,indexType)) {
    throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + Types.arrayKeyType(lval.var) + ""String_Node_Str""+ ""String_Node_Str""+ indexType.typeName());
  }
  if (lval.indices.size() == 1) {
    Var lookedup=assignTo1DArray(context,origLval,lval,rValExpr,rValType,afterActions);
    return new LValue(lval.tree,lookedup,new ArrayList<SwiftAST>());
  }
 else {
    Var lvalArr=context.lookupVarUser(lval.varName);
    Type memberType=lval.getType(context,1);
    Var mVar;
    if (Types.isArray(memberType)) {
      Long literal=Literals.extractIntLit(context,indexExpr.child(0));
      if (literal != null) {
        long arrIx=literal;
        if (Types.isArray(lvalArr.type())) {
          mVar=varCreator.createTmpAlias(context,memberType);
          backend.arrayCreateNestedImm(mVar,lvalArr,Arg.createIntLit(arrIx));
        }
 else {
          assert(Types.isArrayRef(lvalArr.type()));
          mVar=varCreator.createTmp(context,new RefType(memberType));
          backend.arrayRefCreateNestedImm(mVar,origLval.var,lvalArr,Arg.createIntLit(arrIx));
        }
      }
 else {
        mVar=varCreator.createTmp(context,new RefType(memberType));
        Type keyType=Types.arrayKeyType(lvalArr);
        Var indexVar=exprWalker.eval(context,indexExpr.child(0),keyType,false,null);
        if (Types.isArray(lvalArr.type())) {
          backend.arrayCreateNestedFuture(mVar,lvalArr,indexVar);
        }
 else {
          assert(Types.isArrayRef(lvalArr.type()));
          backend.arrayRefCreateNestedFuture(mVar,origLval.var,lvalArr,indexVar);
        }
      }
    }
 else {
      mVar=varCreator.createTmp(context,new RefType(memberType));
    }
    return new LValue(lval.tree,mVar,lval.indices.subList(1,lval.indices.size()));
  }
}",0.9659691397167618
132911,"public static VariableDescriptor fromDeclareVariableRest(Context context,Type baseType,SwiftAST tree) throws UndefinedTypeException, InvalidSyntaxException {
  assert(tree.getType() == ExMParser.DECLARE_VARIABLE_REST);
  assert(tree.getChildCount() >= 1);
  SwiftAST nameTree=tree.child(0);
  assert(nameTree.getType() == ExMParser.ID);
  String varName=nameTree.getText();
  SwiftAST mappingExpr=null;
  Type varType=baseType;
  for (  SwiftAST subtree : tree.children(1)) {
    if (subtree.getType() == ExMParser.ARRAY) {
      Type keyType=getArrayKeyType(context,subtree);
      varType=new Types.ArrayType(keyType,varType);
    }
 else     if (subtree.getType() == ExMParser.MAPPING) {
      assert(mappingExpr == null);
      assert(subtree.getChildCount() == 1);
      mappingExpr=subtree.child(0);
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + LogHelper.tokName(subtree.getType()));
    }
  }
  return new VariableDescriptor(varType,varName,mappingExpr);
}","public static VariableDescriptor fromDeclareVariableRest(Context context,Type baseType,SwiftAST tree) throws UndefinedTypeException, InvalidSyntaxException {
  assert(tree.getType() == ExMParser.DECLARE_VARIABLE_REST);
  assert(tree.getChildCount() >= 1);
  SwiftAST nameTree=tree.child(0);
  assert(nameTree.getType() == ExMParser.ID);
  String varName=nameTree.getText();
  SwiftAST mappingExpr=null;
  Type varType=baseType;
  for (int i=tree.getChildCount() - 1; i >= 1; i--) {
    SwiftAST subtree=tree.child(i);
    if (subtree.getType() == ExMParser.ARRAY) {
      Type keyType=getArrayKeyType(context,subtree);
      varType=new Types.ArrayType(keyType,varType);
    }
 else     if (subtree.getType() == ExMParser.MAPPING) {
      assert(mappingExpr == null);
      assert(subtree.getChildCount() == 1);
      mappingExpr=subtree.child(0);
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + LogHelper.tokName(subtree.getType()));
    }
  }
  return new VariableDescriptor(varType,varName,mappingExpr);
}",0.9678048780487803
132912,"public static Instruction arrayCreateNestedImm(Var arrayResult,Var arrayVar,Arg arrIx){
  assert(arrIx.isImmediateInt());
  assert(arrayResult.storage() == VarStorage.ALIAS);
  return new TurbineOp(Opcode.ARRAY_CREATE_NESTED_IMM,Arg.createVar(arrayResult),Arg.createVar(arrayVar),arrIx,Arg.ZERO,Arg.ZERO);
}","public static Instruction arrayCreateNestedImm(Var arrayResult,Var arrayVar,Arg arrIx){
  assert(Types.isArrayKeyVal(arrayVar,arrIx));
  assert(arrayResult.storage() == VarStorage.ALIAS);
  return new TurbineOp(Opcode.ARRAY_CREATE_NESTED_IMM,Arg.createVar(arrayResult),Arg.createVar(arrayVar),arrIx,Arg.ZERO,Arg.ZERO);
}",0.9282296650717704
132913,"public static Instruction arrayCreateNestedComputed(Var arrayResult,Var array,Var ix){
  assert(Types.isArrayRef(arrayResult.type()));
  assert(arrayResult.storage() != VarStorage.ALIAS);
  assert(Types.isArray(array.type()));
  assert(Types.isInt(ix.type()));
  return new TurbineOp(Opcode.ARRAY_CREATE_NESTED_FUTURE,Arg.createVar(arrayResult),Arg.createVar(array),Arg.createVar(ix));
}","public static Instruction arrayCreateNestedComputed(Var arrayResult,Var array,Var ix){
  assert(Types.isArrayRef(arrayResult.type()));
  assert(arrayResult.storage() != VarStorage.ALIAS);
  assert(Types.isArray(array.type()));
  assert(Types.isArrayKeyFuture(array,ix));
  return new TurbineOp(Opcode.ARRAY_CREATE_NESTED_FUTURE,Arg.createVar(arrayResult),Arg.createVar(array),Arg.createVar(ix));
}",0.9591836734693876
132914,"/** 
 * Check that for loop is sensible semantically
 */
public void validateInit(Context context) throws UserException {
  for (  LoopVar lv : loopVars) {
    Var v=lv.var;
    Var outerV=context.lookupVarUser(v.name());
    if (lv.declaredOutsideLoop) {
      if (outerV == null) {
        throw UndefinedVarError.fromName(context,v.name(),""String_Node_Str"" + ""String_Node_Str"");
      }
    }
 else {
      context.checkNotDefined(v.name());
    }
    SwiftAST initExpr=initExprs.get(v.name());
    if (initExpr == null) {
      throw new UserException(context,""String_Node_Str"" + v.name() + ""String_Node_Str"");
    }
    Type initExprType=TypeChecker.findSingleExprType(context,initExpr);
    TypeChecker.checkAssignment(context,initExprType,v.type(),v.name());
  }
}","/** 
 * Check that for loop is sensible semantically
 */
public void validateInit(Context context) throws UserException {
  for (  LoopVar lv : loopVars) {
    Var v=lv.var;
    Var outerV=context.lookupVarUnsafe(v.name());
    if (lv.declaredOutsideLoop) {
      if (outerV == null) {
        throw UndefinedVarError.fromName(context,v.name(),""String_Node_Str"" + ""String_Node_Str"");
      }
    }
 else {
      context.checkNotDefined(v.name());
    }
    SwiftAST initExpr=initExprs.get(v.name());
    if (initExpr == null) {
      throw new UserException(context,""String_Node_Str"" + v.name() + ""String_Node_Str"");
    }
    Type initExprType=TypeChecker.findSingleExprType(context,initExpr);
    TypeChecker.checkAssignment(context,initExprType,v.type(),v.name());
  }
}",0.9948186528497408
132915,"/** 
 * Create dereferenced variable given a reference
 */
public static Var createDerefTmp(Var ref,VarStorage storage){
  assert(Types.isRef(ref.type()));
  Var res=new Var(ref.type().memberType(),DEREF_COMPILER_VAR_PREFIX + ref.name(),storage,DefType.LOCAL_COMPILER,null);
  assert(Types.isRefTo(ref.type(),res.type()));
  return res;
}","/** 
 * Create dereferenced variable given a reference
 */
public static Var createDerefTmp(Var ref,VarStorage storage){
  assert(Types.isRef(ref.type()));
  Var res=new Var(ref.type().memberType(),DEREF_COMPILER_VAR_PREFIX + ref.name(),storage,DefType.LOCAL_COMPILER,null);
  assert(Types.isAssignableRefTo(ref.type(),res.type()));
  return res;
}",0.9854227405247812
132916,"/** 
 * Dereference src into dst ie. dst = *src
 * @param dst
 * @param src
 * @throws UserException 
 * @throws UndefinedTypeException 
 */
public void dereference(Context context,Var dst,Var src) throws UndefinedTypeException, UserException {
  assert(Types.isRef(src.type()));
  assert(Types.isRefTo(src.type(),dst.type()));
  if (Types.isScalarFuture(dst.type())) {
    Type dstType=dst.type();
    if (dstType.equals(Types.F_INT)) {
      backend.dereferenceInt(dst,src);
    }
 else     if (dstType.equals(Types.F_STRING)) {
      backend.dereferenceString(dst,src);
    }
 else     if (dstType.equals(Types.F_FLOAT)) {
      backend.dereferenceFloat(dst,src);
    }
 else     if (dstType.equals(Types.F_BOOL)) {
      backend.dereferenceBool(dst,src);
    }
 else     if (dstType.equals(Types.F_FILE)) {
      backend.dereferenceFile(dst,src);
    }
 else     if (dstType.equals(Types.F_BLOB)) {
      backend.dereferenceBlob(dst,src);
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + src.type().toString());
    }
  }
 else   if (Types.isArray(dst.type())) {
    String wName=context.getFunctionContext().constructName(""String_Node_Str"");
    List<Var> waitVars=Arrays.asList(src);
    backend.startWaitStatement(wName,waitVars,WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL);
    Var derefed=varCreator.createTmpAlias(context,dst.type());
    backend.retrieveRef(derefed,src);
    copyArrayByValue(context,dst,derefed);
    backend.endWaitStatement();
  }
 else   if (Types.isStruct(dst.type())) {
    dereferenceStruct(context,dst,src);
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + src.type());
  }
}","/** 
 * Dereference src into dst ie. dst = *src
 * @param dst
 * @param src
 * @throws UserException 
 * @throws UndefinedTypeException 
 */
public void dereference(Context context,Var dst,Var src) throws UndefinedTypeException, UserException {
  assert(Types.isRef(src.type()));
  assert(Types.isAssignableRefTo(src.type(),dst.type()));
  if (Types.isScalarFuture(dst.type())) {
    Type dstType=dst.type();
    if (dstType.equals(Types.F_INT)) {
      backend.dereferenceInt(dst,src);
    }
 else     if (dstType.equals(Types.F_STRING)) {
      backend.dereferenceString(dst,src);
    }
 else     if (dstType.equals(Types.F_FLOAT)) {
      backend.dereferenceFloat(dst,src);
    }
 else     if (dstType.equals(Types.F_BOOL)) {
      backend.dereferenceBool(dst,src);
    }
 else     if (dstType.equals(Types.F_FILE)) {
      backend.dereferenceFile(dst,src);
    }
 else     if (dstType.equals(Types.F_BLOB)) {
      backend.dereferenceBlob(dst,src);
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + src.type().toString());
    }
  }
 else   if (Types.isArray(dst.type())) {
    String wName=context.getFunctionContext().constructName(""String_Node_Str"");
    List<Var> waitVars=Arrays.asList(src);
    backend.startWaitStatement(wName,waitVars,WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL);
    Var derefed=varCreator.createTmpAlias(context,dst.type());
    backend.retrieveRef(derefed,src);
    copyArrayByValue(context,dst,derefed);
    backend.endWaitStatement();
  }
 else   if (Types.isStruct(dst.type())) {
    dereferenceStruct(context,dst,src);
  }
 else {
    throw new STCRuntimeError(""String_Node_Str"" + src.type());
  }
}",0.9970023980815348
132917,"private void callFunction(Context context,String function,FunctionType concrete,List<Var> oList,List<Var> iList,TaskProps props) throws UndefinedTypeException, UserException {
  ArrayList<Var> realIList=new ArrayList<Var>(iList.size());
  ArrayList<Var> derefVars=new ArrayList<Var>();
  ArrayList<Var> waitVars=new ArrayList<Var>();
  Context waitContext=null;
  assert(concrete.getInputs().size() == iList.size());
  for (int i=0; i < iList.size(); i++) {
    Var input=iList.get(i);
    Type inputType=input.type();
    Type expType=concrete.getInputs().get(i);
    if (inputType.getImplType().assignableTo(expType.getImplType())) {
      realIList.add(input);
    }
 else     if (Types.isRefTo(inputType,expType)) {
      if (waitContext == null) {
        waitContext=new LocalContext(context);
      }
      Var derefed;
      derefed=waitContext.createAliasVariable(expType);
      waitVars.add(input);
      derefVars.add(derefed);
      realIList.add(derefed);
    }
 else     if (Types.isUpdateableEquiv(inputType,expType)) {
      realIList.add(snapshotUpdateable(context,input));
    }
 else {
      throw new STCRuntimeError(context.getFileLine() + ""String_Node_Str"" + ""String_Node_Str""+ inputType.toString()+ ""String_Node_Str""+ expType.toString());
    }
  }
  if (waitContext != null) {
    FunctionContext fc=context.getFunctionContext();
    TaskProps waitProps=props.filter(TaskPropKey.PRIORITY);
    backend.startWaitStatement(fc.constructName(""String_Node_Str"" + function),waitVars,WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL_CONTROL,waitProps);
    assert(waitVars.size() == derefVars.size());
    for (int i=0; i < waitVars.size(); i++) {
      Var derefVar=derefVars.get(i);
      varCreator.declare(derefVar);
      if (Types.isArrayRef(waitVars.get(i).type())) {
        backend.retrieveRef(derefVar,waitVars.get(i));
      }
 else {
        throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + derefVar);
      }
    }
  }
  backendFunctionCall(context,function,oList,realIList,props);
  if (waitContext != null) {
    backend.endWaitStatement();
  }
}","private void callFunction(Context context,String function,FunctionType concrete,List<Var> oList,List<Var> iList,TaskProps props) throws UndefinedTypeException, UserException {
  ArrayList<Var> realIList=new ArrayList<Var>(iList.size());
  ArrayList<Var> derefVars=new ArrayList<Var>();
  ArrayList<Var> waitVars=new ArrayList<Var>();
  Context waitContext=null;
  assert(concrete.getInputs().size() == iList.size());
  for (int i=0; i < iList.size(); i++) {
    Var input=iList.get(i);
    Type inputType=input.type();
    Type expType=concrete.getInputs().get(i);
    if (inputType.getImplType().assignableTo(expType.getImplType())) {
      realIList.add(input);
    }
 else     if (Types.isAssignableRefTo(inputType,expType)) {
      if (waitContext == null) {
        waitContext=new LocalContext(context);
      }
      Var derefed;
      derefed=waitContext.createAliasVariable(expType);
      waitVars.add(input);
      derefVars.add(derefed);
      realIList.add(derefed);
    }
 else     if (Types.isUpdateableEquiv(inputType,expType)) {
      realIList.add(snapshotUpdateable(context,input));
    }
 else {
      throw new STCRuntimeError(context.getFileLine() + ""String_Node_Str"" + ""String_Node_Str""+ inputType.toString()+ ""String_Node_Str""+ expType.toString());
    }
  }
  if (waitContext != null) {
    FunctionContext fc=context.getFunctionContext();
    TaskProps waitProps=props.filter(TaskPropKey.PRIORITY);
    backend.startWaitStatement(fc.constructName(""String_Node_Str"" + function),waitVars,WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL_CONTROL,waitProps);
    assert(waitVars.size() == derefVars.size());
    for (int i=0; i < waitVars.size(); i++) {
      Var derefVar=derefVars.get(i);
      varCreator.declare(derefVar);
      if (Types.isArrayRef(waitVars.get(i).type())) {
        backend.retrieveRef(derefVar,waitVars.get(i));
      }
 else {
        throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + derefVar);
      }
    }
  }
  backendFunctionCall(context,function,oList,realIList,props);
  if (waitContext != null) {
    backend.endWaitStatement();
  }
}",0.9976167778836988
132918,"/** 
 * Handle an expression which is an array access. Copies a member of an array, specified by index, into another variable. If the other variable is an alias variable, we can avoid the copy.
 * @param context
 * @param tree
 * @param oVar the variable to copy into
 * @throws UserException
 */
private void arrayLoad(Context context,SwiftAST tree,Var oVar,Map<String,String> renames) throws UserException {
  if (tree.getChildCount() != 2) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + tree.getChildCount());
  }
  SwiftAST arrayTree=tree.child(0);
  Type arrExprType=TypeChecker.findSingleExprType(context,arrayTree);
  Type arrType=null;
  for (  Type altType : UnionType.getAlternatives(arrExprType)) {
    assert(Types.isArray(altType) || Types.isArrayRef(altType));
    Type lookupRes=TypeChecker.dereferenceResultType(Types.arrayMemberType(altType));
    if (lookupRes.equals(oVar.type())) {
      arrType=altType;
      break;
    }
  }
  if (arrType == null) {
    throw new STCRuntimeError(""String_Node_Str"" + arrExprType + ""String_Node_Str""+ oVar);
  }
  Var arrayVar=eval(context,arrayTree,arrType,false,renames);
  Type memberType=Types.arrayMemberType(arrType);
  SwiftAST arrayIndexTree=tree.child(1);
  Type indexType=TypeChecker.findSingleExprType(context,arrayIndexTree);
  if (!indexType.assignableTo(Types.F_INT)) {
    throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + indexType.typeName());
  }
  Var lookupIntoVar;
  boolean doDereference;
  if (memberType.equals(oVar.type())) {
    lookupIntoVar=varCreator.createTmp(context,new RefType(memberType));
    doDereference=true;
  }
 else {
    assert(Types.isRefTo(oVar.type(),memberType));
    lookupIntoVar=oVar;
    doDereference=false;
  }
  Long arrayIndex=Literals.extractIntLit(context,arrayIndexTree);
  if (arrayIndex != null) {
    backend.arrayLookupRefImm(lookupIntoVar,arrayVar,Arg.createIntLit(arrayIndex),Types.isArrayRef(arrType));
  }
 else {
    Var indexVar=eval(context,arrayIndexTree,Types.F_INT,false,renames);
    backend.arrayLookupFuture(lookupIntoVar,arrayVar,indexVar,Types.isArrayRef(arrType));
  }
  if (doDereference) {
    dereference(context,oVar,lookupIntoVar);
  }
}","/** 
 * Handle an expression which is an array access. Copies a member of an array, specified by index, into another variable. If the other variable is an alias variable, we can avoid the copy.
 * @param context
 * @param tree
 * @param oVar the variable to copy into
 * @throws UserException
 */
private void arrayLoad(Context context,SwiftAST tree,Var oVar,Map<String,String> renames) throws UserException {
  if (tree.getChildCount() != 2) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + tree.getChildCount());
  }
  SwiftAST arrayTree=tree.child(0);
  Type arrExprType=TypeChecker.findSingleExprType(context,arrayTree);
  Type arrType=null;
  for (  Type altType : UnionType.getAlternatives(arrExprType)) {
    assert(Types.isArray(altType) || Types.isArrayRef(altType));
    Type lookupRes=TypeChecker.dereferenceResultType(Types.arrayMemberType(altType));
    if (lookupRes.equals(oVar.type())) {
      arrType=altType;
      break;
    }
  }
  if (arrType == null) {
    throw new STCRuntimeError(""String_Node_Str"" + arrExprType + ""String_Node_Str""+ oVar);
  }
  Var arrayVar=eval(context,arrayTree,arrType,false,renames);
  Type memberType=Types.arrayMemberType(arrType);
  SwiftAST arrayIndexTree=tree.child(1);
  Type indexType=TypeChecker.findSingleExprType(context,arrayIndexTree);
  if (!indexType.assignableTo(Types.F_INT)) {
    throw new TypeMismatchException(context,""String_Node_Str"" + ""String_Node_Str"" + indexType.typeName());
  }
  Var lookupIntoVar;
  boolean doDereference;
  if (memberType.equals(oVar.type())) {
    lookupIntoVar=varCreator.createTmp(context,new RefType(memberType));
    doDereference=true;
  }
 else {
    assert(Types.isAssignableRefTo(oVar.type(),memberType));
    lookupIntoVar=oVar;
    doDereference=false;
  }
  Long arrayIndex=Literals.extractIntLit(context,arrayIndexTree);
  if (arrayIndex != null) {
    backend.arrayLookupRefImm(lookupIntoVar,arrayVar,Arg.createIntLit(arrayIndex),Types.isArrayRef(arrType));
  }
 else {
    Var indexVar=eval(context,arrayIndexTree,Types.F_INT,false,renames);
    backend.arrayLookupFuture(lookupIntoVar,arrayVar,indexVar,Types.isArrayRef(arrType));
  }
  if (doDereference) {
    dereference(context,oVar,lookupIntoVar);
  }
}",0.9977628635346756
132919,"private Var derefOrCopyResult(Context context,Var lookupResult,Var outVar) throws UndefinedTypeException, UserException {
  try {
    if (outVar == null) {
      return lookupResult;
    }
 else     if (Types.isRefTo(lookupResult.type(),outVar.type())) {
      dereference(context,outVar,lookupResult);
      return outVar;
    }
 else {
      copyByValue(context,lookupResult,outVar,outVar.type());
      return outVar;
    }
  }
 catch (  RuntimeException e) {
    Logging.getSTCLogger().debug(""String_Node_Str"" + lookupResult + ""String_Node_Str""+ outVar);
    throw e;
  }
}","private Var derefOrCopyResult(Context context,Var lookupResult,Var outVar) throws UndefinedTypeException, UserException {
  try {
    if (outVar == null) {
      return lookupResult;
    }
 else     if (Types.isAssignableRefTo(lookupResult.type(),outVar.type())) {
      dereference(context,outVar,lookupResult);
      return outVar;
    }
 else {
      copyByValue(context,lookupResult,outVar,outVar.type());
      return outVar;
    }
  }
 catch (  RuntimeException e) {
    Logging.getSTCLogger().debug(""String_Node_Str"" + lookupResult + ""String_Node_Str""+ outVar);
    throw e;
  }
}",0.9914089347079038
132920,"/** 
 * Generate code for a call to a function, where the arguments might be expressions
 * @param context
 * @param tree
 * @param oList
 * @throws UserException
 * @throws UndefinedVariableException
 * @throws UndefinedFunctionException
 */
private void callFunctionExpression(Context context,SwiftAST tree,List<Var> oList,Map<String,String> renames) throws UserException {
}","/** 
 * Generate code for a call to a function, where the arguments might be expressions
 * @param context
 * @param tree
 * @param oList
 * @throws UserException
 * @throws UndefinedVariableException
 * @throws UndefinedFunctionException
 */
private void callFunctionExpression(Context context,SwiftAST tree,List<Var> oList,Map<String,String> renames) throws UserException {
  assert(tree.getType() == ExMParser.CALL_FUNCTION);
  FunctionCall f=FunctionCall.fromAST(context,tree,true);
  FunctionType concrete=TypeChecker.concretiseFunctionCall(context,f.function(),f.type(),f.args(),oList,false);
  try {
    if (Builtins.isAssertVariant(f.function()) && Settings.getBoolean(Settings.OPT_DISABLE_ASSERTS)) {
      return;
    }
  }
 catch (  InvalidOptionException e) {
    throw new STCRuntimeError(""String_Node_Str"" + e.toString());
  }
  ArrayList<Var> argVars=new ArrayList<Var>(f.args().size());
  for (int i=0; i < f.args().size(); i++) {
    SwiftAST argtree=f.args().get(i);
    Type expType=concrete.getInputs().get(i);
    Type exprType=TypeChecker.findSingleExprType(context,argtree);
    Type argType=TypeChecker.checkFunArg(context,f.function(),i,expType,exprType).val2;
    argVars.add(eval(context,argtree,argType,false,renames));
  }
  TaskProps propVals=new TaskProps();
  boolean openedWait=false;
  Context callContext=context;
  if (!f.annotations().isEmpty()) {
    List<Pair<TaskPropKey,Var>> propFutures=new ArrayList<Pair<TaskPropKey,Var>>();
    List<Var> waitVars=new ArrayList<Var>();
    for (    TaskPropKey ann : f.annotations().keySet()) {
      checkCallAnnotation(context,f,ann);
      SwiftAST expr=f.annotations().get(ann);
      Type exprType=TypeChecker.findSingleExprType(callContext,expr);
      Type concreteType=TaskProp.checkFrontendType(callContext,ann,exprType);
      Var future=eval(context,expr,concreteType,false,renames);
      waitVars.add(future);
      propFutures.add(Pair.create(ann,future));
    }
    backend.startWaitStatement(context.getFunctionContext().constructName(""String_Node_Str""),waitVars,WaitMode.WAIT_ONLY,false,false,TaskMode.LOCAL_CONTROL);
    openedWait=true;
    callContext=new LocalContext(context);
    for (    Pair<TaskPropKey,Var> x : propFutures) {
      Var value=varCreator.fetchValueOf(callContext,x.val2);
      propVals.put(x.val1,value.asArg());
    }
  }
  callFunction(context,f.function(),concrete,oList,argVars,propVals);
  if (openedWait) {
    backend.endWaitStatement();
  }
}",0.2647471910112359
132921,"/** 
 * Check if an expression type can be used for function argument
 * @param argType non-polymorphic function argument type
 * @param exprType type of argument expression
 * @return true if compatible
 */
public static boolean compatibleArgTypes(Type argType,Type exprType){
  if (exprType.assignableTo(argType)) {
    return true;
  }
 else   if (Types.isRefTo(exprType,argType)) {
    return true;
  }
 else {
    return false;
  }
}","/** 
 * Check if an expression type can be used for function argument
 * @param argType non-polymorphic function argument type
 * @param exprType type of argument expression
 * @return concretized argType if compatible, null if incompatible
 */
public static Type compatibleArgTypes(Type argType,Type exprType){
  if (exprType.assignableTo(argType)) {
    return exprType.concretize(argType);
  }
 else   if (Types.isAssignableRefTo(exprType,argType)) {
    return exprType.concretize(new RefType(argType));
  }
 else {
    return null;
  }
}",0.8367346938775511
132922,"/** 
 * @param formalArgT
 * @param argExprT
 * @return (selected type of argument, selected type of variable)
 */
public static Pair<Type,Type> whichAlternativeType(Type formalArgT,Type argExprT){
  for (  Type argExprAlt : UnionType.getAlternatives(argExprT)) {
    for (    Type formalArgAlt : UnionType.getAlternatives(formalArgT)) {
      if (compatibleArgTypes(formalArgAlt,argExprAlt)) {
        return Pair.create(formalArgAlt,argExprAlt);
      }
    }
  }
  return null;
}","/** 
 * @param formalArgT
 * @param argExprT
 * @return (selected type of argument, selected type of variable)
 */
public static Pair<Type,Type> whichAlternativeType(Type formalArgT,Type argExprT){
  for (  Type argExprAlt : UnionType.getAlternatives(argExprT)) {
    for (    Type formalArgAlt : UnionType.getAlternatives(formalArgT)) {
      Type concreteArgType=compatibleArgTypes(formalArgAlt,argExprAlt);
      if (concreteArgType != null) {
        return Pair.create(formalArgAlt,concreteArgType);
      }
    }
  }
  return null;
}",0.9167482859941234
132923,"public String toString(){
  StringBuilder sb=new StringBuilder();
  sb.append(""String_Node_Str"");
  int written=writeContents(sb,true);
  HierarchicalMap<K,V> ancestor=parent;
  while (ancestor != null) {
    ancestor.writeContents(sb,written == 0);
    ancestor=ancestor.parent;
  }
  sb.append(""String_Node_Str"");
  return sb.toString();
}","public String toString(){
  StringBuilder sb=new StringBuilder();
  sb.append(""String_Node_Str"");
  int written=writeContents(sb,true);
  HierarchicalMap<K,V> ancestor=parent;
  while (ancestor != null) {
    written+=ancestor.writeContents(sb,written == 0);
    ancestor=ancestor.parent;
  }
  sb.append(""String_Node_Str"");
  return sb.toString();
}",0.9869753979739508
132924,"/** 
 * @param logger
 * @param f
 * @param execCx
 * @param block
 * @param stmts
 * @param cv
 * @param replaceInputs
 * @param replaceAll
 * @return true if inlined. Returns immediately if inlining happens
 * @throws InvalidWriteException
 * @throws InvalidOptionException
 */
private boolean handleStatements(Logger logger,Program program,Function f,ExecContext execCx,Block block,ListIterator<Statement> stmts,ValueTracker cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll) throws InvalidWriteException, InvalidOptionException {
  while (stmts.hasNext()) {
    Statement stmt=stmts.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      handleInstruction(logger,f,execCx,block,stmts,stmt.instruction(),cv,replaceInputs,replaceAll);
    }
 else {
      assert(stmt.type() == StatementType.CONDITIONAL);
      UnifiedState condClosed=recurseOnContinuation(logger,program,f,execCx,stmt.conditional(),cv,replaceInputs,replaceAll);
      cv.addClosed(condClosed);
      cv.addComputedValues(condClosed.availableVals,Ternary.FALSE);
    }
  }
  return false;
}","/** 
 * @param logger
 * @param f
 * @param execCx
 * @param block
 * @param stmts
 * @param cv
 * @param replaceInputs
 * @param replaceAll
 * @throws InvalidWriteException
 * @throws InvalidOptionException
 */
private void handleStatements(Logger logger,Program program,Function f,ExecContext execCx,Block block,ListIterator<Statement> stmts,ValueTracker cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll) throws InvalidWriteException, InvalidOptionException {
  logger.trace(""String_Node_Str"");
  StringBuilder sb=new StringBuilder();
  for (  Statement stmt : block.getStatements()) {
    stmt.prettyPrint(sb,""String_Node_Str"");
  }
  logger.trace(sb);
  while (stmts.hasNext()) {
    Statement stmt=stmts.next();
    if (stmt.type() == StatementType.INSTRUCTION) {
      handleInstruction(logger,f,execCx,block,stmts,stmt.instruction(),cv,replaceInputs,replaceAll);
    }
 else {
      assert(stmt.type() == StatementType.CONDITIONAL);
      UnifiedState condClosed=recurseOnContinuation(logger,program,f,execCx,stmt.conditional(),cv,replaceInputs,replaceAll);
      cv.addClosed(condClosed);
      cv.addComputedValues(condClosed.availableVals,Ternary.FALSE);
    }
  }
  logger.trace(""String_Node_Str"");
  sb=new StringBuilder();
  for (  Statement stmt : block.getStatements()) {
    stmt.prettyPrint(sb,""String_Node_Str"");
  }
  logger.trace(sb);
}",0.8159484494563028
132925,"/** 
 * @param execCx
 * @param block
 * @param cv copy of cv from outer scope, or null if it should be initialized
 * @param replaceInputs : a set of variable replaces to do from this point in IC onwards
 * @return true if this should be called again
 * @throws InvalidOptionException
 * @throws InvalidWriteException
 */
private boolean forwardDataflow(Logger logger,Program program,Function f,ExecContext execCx,Block block,ValueTracker cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll) throws InvalidOptionException, InvalidWriteException {
  if (block.getType() == BlockType.MAIN_BLOCK) {
    for (    WaitVar wv : f.blockingInputs()) {
      cv.close(wv.var,false);
    }
    for (    Var v : f.getInputList()) {
      if (Types.isScalarUpdateable(v.type())) {
        cv.close(v,false);
      }
    }
  }
  for (  Var v : block.getVariables()) {
    if (v.isMapped() && Types.isFile(v.type())) {
      ResultVal filenameVal=ICInstructions.filenameCV(Arg.createVar(v.mapping()),v);
      cv.addComputedValue(filenameVal,Ternary.FALSE);
    }
    if (Types.isMappable(v.type()) && !v.isMapped() && v.storage() != VarStorage.ALIAS) {
      cv.setUnmapped(v);
    }
  }
  boolean inlined=handleStatements(logger,program,f,execCx,block,block.statementIterator(),cv,replaceInputs,replaceAll);
  if (inlined) {
    cleanupAfterInline(block,replaceInputs,replaceAll);
    return true;
  }
  block.renameCleanupActions(replaceInputs,RenameMode.VALUE);
  block.renameCleanupActions(replaceAll,RenameMode.REFERENCE);
  for (int i=0; i < block.getContinuations().size(); i++) {
    Continuation c=block.getContinuation(i);
    c.renameVars(replaceInputs,RenameMode.VALUE,false);
    c.renameVars(replaceAll,RenameMode.REFERENCE,false);
    Block toInline=c.tryInline(cv.getClosed(),cv.getRecursivelyClosed(),reorderingAllowed);
    if (toInline != null) {
      c.inlineInto(block,toInline);
      i--;
      inlined=true;
    }
  }
  if (inlined) {
    cleanupAfterInline(block,replaceInputs,replaceAll);
    return true;
  }
  for (  Continuation cont : block.getContinuations()) {
    recurseOnContinuation(logger,program,f,execCx,cont,cv,replaceInputs,replaceAll);
  }
  return false;
}","/** 
 * @param execCx
 * @param block
 * @param cv copy of cv from outer scope, or null if it should be initialized
 * @param replaceInputs : a set of variable replaces to do from this point in IC onwards
 * @return true if this should be called again
 * @throws InvalidOptionException
 * @throws InvalidWriteException
 */
private boolean forwardDataflow(Logger logger,Program program,Function f,ExecContext execCx,Block block,ValueTracker cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll) throws InvalidOptionException, InvalidWriteException {
  if (block.getType() == BlockType.MAIN_BLOCK) {
    for (    WaitVar wv : f.blockingInputs()) {
      cv.close(wv.var,false);
    }
    for (    Var v : f.getInputList()) {
      if (Types.isScalarUpdateable(v.type())) {
        cv.close(v,false);
      }
    }
  }
  for (  Var v : block.getVariables()) {
    if (v.isMapped() && Types.isFile(v.type())) {
      ResultVal filenameVal=ICInstructions.filenameCV(Arg.createVar(v.mapping()),v);
      cv.addComputedValue(filenameVal,Ternary.FALSE);
    }
    if (Types.isMappable(v.type()) && !v.isMapped() && v.storage() != VarStorage.ALIAS) {
      cv.setUnmapped(v);
    }
  }
  handleStatements(logger,program,f,execCx,block,block.statementIterator(),cv,replaceInputs,replaceAll);
  block.renameCleanupActions(replaceInputs,RenameMode.VALUE);
  block.renameCleanupActions(replaceAll,RenameMode.REFERENCE);
  boolean inlined=false;
  for (int i=0; i < block.getContinuations().size(); i++) {
    Continuation c=block.getContinuation(i);
    c.renameVars(replaceInputs,RenameMode.VALUE,false);
    c.renameVars(replaceAll,RenameMode.REFERENCE,false);
    Block toInline=c.tryInline(cv.getClosed(),cv.getRecursivelyClosed(),reorderingAllowed);
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + c.getType());
    }
    if (toInline != null) {
      prepareForInline(toInline,replaceInputs,replaceAll);
      c.inlineInto(block,toInline);
      i--;
      inlined=true;
    }
  }
  if (inlined) {
    return true;
  }
  for (  Continuation cont : block.getContinuations()) {
    recurseOnContinuation(logger,program,f,execCx,cont,cv,replaceInputs,replaceAll);
  }
  return false;
}",0.9225225225225224
132926,"private static void runPreprocessor(Logger logger,String input,String output,List<String> preprocArgs){
  List<String> cmd=new ArrayList<String>();
  if (useGCCProcessor()) {
    cmd.addAll(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",input,output));
  }
 else {
    cmd.addAll(Arrays.asList(""String_Node_Str"",input,output));
  }
  for (  String dir : Settings.getModulePath()) {
    cmd.add(""String_Node_Str"");
    cmd.add(dir);
  }
  for (  String macro : preprocArgs) {
    cmd.add(""String_Node_Str"");
    cmd.add(macro);
  }
  String cmdString=StringUtils.join(cmd,' ');
  try {
    logger.debug(""String_Node_Str"" + cmdString);
    Process cpp=Runtime.getRuntime().exec(cmd.toArray(new String[]{}));
    int cppExitCode=-1;
    boolean done=false;
    do {
      try {
        cppExitCode=cpp.waitFor();
        done=true;
      }
 catch (      InterruptedException ex) {
      }
    }
 while (!done);
    StringWriter sw=new StringWriter();
    IOUtils.copy(cpp.getErrorStream(),sw,""String_Node_Str"");
    String cppStderr=sw.toString();
    logger.debug(""String_Node_Str"" + cppExitCode);
    logger.debug(""String_Node_Str"" + cppStderr);
    if (cppExitCode != 0) {
      System.out.println(cppStderr);
      System.out.println(""String_Node_Str"" + cmdString + ""String_Node_Str""+ (""String_Node_Str"" + cppExitCode + ""String_Node_Str""));
      System.exit(1);
    }
  }
 catch (  IOException e) {
    System.out.println(""String_Node_Str"" + cmdString + ""String_Node_Str""+ e.getMessage());
    System.exit(1);
  }
}","private static void runPreprocessor(Logger logger,String input,String output,List<String> preprocArgs){
  List<String> cmd=new ArrayList<String>();
  if (useGCCProcessor()) {
    cmd.addAll(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",input,""String_Node_Str"",output));
  }
 else {
    cmd.addAll(Arrays.asList(""String_Node_Str"",input,output));
  }
  for (  String dir : Settings.getModulePath()) {
    cmd.add(""String_Node_Str"");
    cmd.add(dir);
  }
  for (  String macro : preprocArgs) {
    cmd.add(""String_Node_Str"");
    cmd.add(macro);
  }
  String cmdString=StringUtils.join(cmd,' ');
  try {
    logger.debug(""String_Node_Str"" + cmdString);
    Process cpp=Runtime.getRuntime().exec(cmd.toArray(new String[]{}));
    int cppExitCode=-1;
    boolean done=false;
    do {
      try {
        cppExitCode=cpp.waitFor();
        done=true;
      }
 catch (      InterruptedException ex) {
      }
    }
 while (!done);
    StringWriter sw=new StringWriter();
    IOUtils.copy(cpp.getErrorStream(),sw,""String_Node_Str"");
    String cppStderr=sw.toString();
    logger.debug(""String_Node_Str"" + cppExitCode);
    logger.debug(""String_Node_Str"" + cppStderr);
    if (cppExitCode != 0) {
      System.out.println(cppStderr);
      System.out.println(""String_Node_Str"" + cmdString + ""String_Node_Str""+ (""String_Node_Str"" + cppExitCode + ""String_Node_Str""));
      System.exit(1);
    }
  }
 catch (  IOException e) {
    System.out.println(""String_Node_Str"" + cmdString + ""String_Node_Str""+ e.getMessage());
    System.exit(1);
  }
}",0.9942492012779552
132927,"private static void compileTimeAssertCheck(BuiltinOpcode subop2,List<Arg> inputs2,Map<Var,Arg> knownConstants,String enclosingFnName){
  List<Arg> inputVals=new ArrayList<Arg>(inputs2.size());
  for (  Arg input : inputs2) {
    if (input.isConstant()) {
      inputVals.add(input);
    }
 else     if (knownConstants.containsKey(input.getVar())) {
      inputVals.add(input);
    }
 else {
      return;
    }
  }
  if (subop2 == BuiltinOpcode.ASSERT) {
    Arg cond=inputVals.get(0);
    assert(cond.isBoolVal());
    if (!cond.getBoolLit()) {
      compileTimeAssertWarn(enclosingFnName,""String_Node_Str"",inputs2.get(1),knownConstants);
    }
  }
 else {
    assert(subop2 == BuiltinOpcode.ASSERT_EQ);
    Arg a1=inputVals.get(0);
    Arg a2=inputVals.get(1);
    assert(a1.isConstant());
    assert(a2.isConstant());
    if (a1 != null && a2 != null) {
      if (!a1.equals(a2)) {
        String reason=a1.toString() + ""String_Node_Str"" + a2.toString();
        Arg msg=inputVals.get(2);
        compileTimeAssertWarn(enclosingFnName,reason,msg,knownConstants);
      }
    }
  }
}","private static void compileTimeAssertCheck(BuiltinOpcode subop2,List<Arg> inputs2,Map<Var,Arg> knownConstants,String enclosingFnName){
}",0.2227682227682227
132928,"/** 
 * Remove from candidates any variables that can't have the refcount decremented before this instruction executes
 * @param inst
 * @param type
 * @param candidates
 */
private void removeDecrCandidates(Instruction inst,RefCountType type,Set<Var> candidates){
  if (type == RefCountType.READERS) {
    for (    Arg in : inst.getInputs()) {
      if (in.isVar())       candidates.remove(in.getVar());
    }
    for (    Var read : inst.getReadOutputs()) {
      candidates.remove(read);
    }
  }
 else {
    assert(type == RefCountType.WRITERS);
    for (    Var modified : inst.getOutputs()) {
      candidates.remove(modified);
    }
  }
}","/** 
 * Remove from candidates any variables that can't have the refcount decremented before this instruction executes
 * @param inst
 * @param type
 * @param candidates
 */
private void removeDecrCandidates(Instruction inst,RefCountType type,Set<Var> candidates){
  if (type == RefCountType.READERS) {
    for (    Arg in : inst.getInputs()) {
      if (in.isVar())       candidates.remove(in.getVar());
    }
    for (    Var read : inst.getReadOutputs(functionMap)) {
      candidates.remove(read);
    }
  }
 else {
    assert(type == RefCountType.WRITERS);
    for (    Var modified : inst.getOutputs()) {
      candidates.remove(modified);
    }
  }
}",0.9915579432079816
132929,"/** 
 * @return list of outputs for which previous value is read
 */
public List<Var> getReadOutputs(){
switch (op) {
case ARRAY_CREATE_NESTED_IMM:
case ARRAY_CREATE_NESTED_FUTURE:
    return Arrays.asList(getOutput(1));
case ARRAYREF_CREATE_NESTED_IMM:
case ARRAYREF_CREATE_NESTED_FUTURE:
  return Arrays.asList(getOutput(2));
default :
return Var.NONE;
}
}","@Override public List<Var> getReadOutputs(Map<String,Function> fns){
switch (op) {
case CALL_BUILTIN:
{
      List<Var> res=new ArrayList<Var>();
      for (      Var o : outputs) {
        if (Types.hasReadableSideChannel(o.type())) {
          res.add(o);
        }
      }
      return res;
    }
case CALL_LOCAL:
case CALL_LOCAL_CONTROL:
case CALL_SYNC:
case CALL_CONTROL:
{
    List<Var> res=new ArrayList<Var>();
    Function f=fns == null ? null : fns.get(this.functionName);
    for (int i=0; i < outputs.size(); i++) {
      Var o=outputs.get(i);
      if (Types.hasReadableSideChannel(o.type()) && (f == null || !f.isOutputWriteOnly(i))) {
        res.add(o);
      }
    }
    return res;
  }
default :
throw new STCRuntimeError(""String_Node_Str"" + op);
}
}",0.1261101243339254
132930,"public boolean isOutputWriteOnly(int i){
  return oListWriteOnly.get(i);
}","public boolean isOutputWriteOnly(int i){
  return oListWriteOnly.contains(oList.get(i));
}",0.902439024390244
132931,"public void makeOutputWriteOnly(int i){
  assert(i >= 0 && i < oList.size());
  assert(!Types.isFile(oList.get(i).type()));
  oListWriteOnly.set(i,true);
}","public void makeOutputWriteOnly(int i){
  assert(i >= 0 && i < oList.size());
  assert(!Types.isFile(oList.get(i).type()));
  Var output=oList.get(i);
  if (!oListWriteOnly.contains(output)) {
    oListWriteOnly.add(output);
  }
}",0.7376623376623377
132932,"public Function(String name,List<Var> iList,List<WaitVar> blockingInputs,List<Var> oList,TaskMode mode,Block mainBlock){
  if (mainBlock.getType() != BlockType.MAIN_BLOCK) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"");
  }
  this.name=name;
  this.iList=new ArrayList<Var>(iList);
  this.oList=new ArrayList<Var>(oList);
  this.oListWriteOnly=new ArrayList<Boolean>(oList.size());
  for (int i=0; i < oList.size(); i++) {
    this.oListWriteOnly.add(false);
  }
  this.mode=mode;
  this.mainBlock=mainBlock;
  this.mainBlock.setParent(this);
  this.blockingInputs=new ArrayList<WaitVar>(blockingInputs);
}","public Function(String name,List<Var> iList,List<WaitVar> blockingInputs,List<Var> oList,TaskMode mode,Block mainBlock){
  if (mainBlock.getType() != BlockType.MAIN_BLOCK) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"");
  }
  this.name=name;
  this.iList=new ArrayList<Var>(iList);
  this.oList=new ArrayList<Var>(oList);
  this.oListWriteOnly=new ArrayList<Var>();
  this.mode=mode;
  this.mainBlock=mainBlock;
  this.mainBlock.setParent(this);
  this.blockingInputs=new ArrayList<WaitVar>(blockingInputs);
}",0.9045571797076526
132933,"/** 
 * list outputs augmented with info about whether they are write-only
 * @return
 */
public List<PassedVar> getPassedOutputList(){
  ArrayList<PassedVar> res=new ArrayList<PassedVar>();
  assert(oList.size() == oListWriteOnly.size());
  for (int i=0; i < oList.size(); i++) {
    res.add(new PassedVar(oList.get(i),oListWriteOnly.get(i)));
  }
  return res;
}","/** 
 * list outputs augmented with info about whether they are write-only
 * @return
 */
public List<PassedVar> getPassedOutputList(){
  ArrayList<PassedVar> res=new ArrayList<PassedVar>();
  for (int i=0; i < oList.size(); i++) {
    Var out=oList.get(i);
    res.add(new PassedVar(out,oListWriteOnly.contains(out)));
  }
  return res;
}",0.8193456614509246
132934,"private static Arg evalStringOp(BuiltinOpcode op,List<Arg> constInputs){
  if (op == BuiltinOpcode.STRCAT) {
    StringBuilder sb=new StringBuilder();
    for (    Arg oa : constInputs) {
      sb.append(oa.getStringLit());
    }
    return Arg.createStringLit(sb.toString());
  }
 else   if (constInputs.size() == 1) {
    String arg1=constInputs.get(0).getStringLit();
switch (op) {
case COPY_STRING:
      return Arg.createStringLit(arg1);
case STRTOINT:
    try {
      long val=Long.parseLong(arg1);
      return Arg.createIntLit(val);
    }
 catch (    NumberFormatException ex) {
    }
  break;
case STRTOFLOAT:
try {
  double val=Double.valueOf(arg1);
  return Arg.createFloatLit(val);
}
 catch (NumberFormatException ex) {
}
break;
}
}
 else if (constInputs.size() == 2) {
String arg1=constInputs.get(0).getStringLit();
String arg2=constInputs.get(1).getStringLit();
switch (op) {
case EQ_STRING:
return Arg.createBoolLit(arg1.equals(arg2));
case NEQ_STRING:
return Arg.createBoolLit(!arg1.equals(arg2));
default :
break;
}
}
return null;
}","private static Arg evalStringOp(BuiltinOpcode op,List<Arg> constInputs){
  if (op == BuiltinOpcode.STRCAT) {
    StringBuilder sb=new StringBuilder();
    for (    Arg oa : constInputs) {
      sb.append(oa.getStringLit());
    }
    return Arg.createStringLit(sb.toString());
  }
 else   if (constInputs.size() == 1) {
    String arg1=constInputs.get(0).getStringLit();
switch (op) {
case COPY_STRING:
      return Arg.createStringLit(arg1);
case STRTOINT:
    try {
      long val=Long.parseLong(arg1);
      return Arg.createIntLit(val);
    }
 catch (    NumberFormatException ex) {
    }
  break;
case STRTOFLOAT:
try {
  double val=Double.valueOf(arg1);
  return Arg.createFloatLit(val);
}
 catch (NumberFormatException ex) {
}
break;
default :
}
}
 else if (constInputs.size() == 2) {
String arg1=constInputs.get(0).getStringLit();
String arg2=constInputs.get(1).getStringLit();
switch (op) {
case EQ_STRING:
return Arg.createBoolLit(arg1.equals(arg2));
case NEQ_STRING:
return Arg.createBoolLit(!arg1.equals(arg2));
default :
break;
}
}
return null;
}",0.9952561669829222
132935,"/** 
 * Generate backend instruction for function call
 * @param context
 * @param function name of function
 * @param oList list of output variables
 * @param iList list of input variables (with correct types)
 * @param priorityVal optional priority value (can be null)
 * @throws UserException 
 */
private void backendFunctionCall(Context context,String function,List<Var> oList,ArrayList<Var> iList,TaskProps props) throws UserException {
  props.assertInternalTypesValid();
  FunctionType def=context.lookupFunction(function);
  if (def == null) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + function);
  }
  if (context.hasFunctionProp(function,FnProp.BUILTIN)) {
    if (Builtins.hasOpEquiv(function)) {
      assert(oList.size() <= 1);
      Var out=oList.size() == 0 ? null : oList.get(0);
      backend.asyncOp(Builtins.getOpEquiv(function),out,Arg.fromVarList(iList),props);
    }
 else {
      backend.builtinFunctionCall(function,iList,oList,props);
    }
  }
 else   if (context.hasFunctionProp(function,FnProp.COMPOSITE)) {
    System.err.println(function + ""String_Node_Str"");
    TaskMode mode;
    if (context.hasFunctionProp(function,FnProp.SYNC)) {
      mode=TaskMode.SYNC;
    }
 else {
      mode=TaskMode.CONTROL;
    }
    backend.functionCall(function,Var.asArgList(iList),oList,mode,props);
  }
 else {
    assert(context.hasFunctionProp(function,FnProp.WRAPPED_BUILTIN) || context.hasFunctionProp(function,FnProp.APP));
    List<Arg> realInputs=new ArrayList<Arg>();
    for (    Var in : iList) {
      realInputs.add(in.asArg());
    }
    if (context.hasFunctionProp(function,FnProp.PARALLEL)) {
      Arg par=props.get(TaskPropKey.PARALLELISM);
      if (par == null) {
        throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + function);
      }
      realInputs.add(par);
    }
    if (context.hasFunctionProp(function,FnProp.TARGETABLE)) {
      Arg target=props.getWithDefault(TaskPropKey.LOCATION);
      realInputs.add(target);
    }
    assert(context.hasFunctionProp(function,FnProp.SYNC));
    TaskMode mode=TaskMode.SYNC;
    backend.functionCall(function,realInputs,oList,mode,props);
  }
}","/** 
 * Generate backend instruction for function call
 * @param context
 * @param function name of function
 * @param oList list of output variables
 * @param iList list of input variables (with correct types)
 * @param priorityVal optional priority value (can be null)
 * @throws UserException 
 */
private void backendFunctionCall(Context context,String function,List<Var> oList,ArrayList<Var> iList,TaskProps props) throws UserException {
  props.assertInternalTypesValid();
  FunctionType def=context.lookupFunction(function);
  if (def == null) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + function);
  }
  if (context.hasFunctionProp(function,FnProp.BUILTIN)) {
    if (Builtins.hasOpEquiv(function)) {
      assert(oList.size() <= 1);
      Var out=oList.size() == 0 ? null : oList.get(0);
      backend.asyncOp(Builtins.getOpEquiv(function),out,Arg.fromVarList(iList),props);
    }
 else {
      backend.builtinFunctionCall(function,iList,oList,props);
    }
  }
 else   if (context.hasFunctionProp(function,FnProp.COMPOSITE)) {
    TaskMode mode;
    if (context.hasFunctionProp(function,FnProp.SYNC)) {
      mode=TaskMode.SYNC;
    }
 else {
      mode=TaskMode.CONTROL;
    }
    backend.functionCall(function,Var.asArgList(iList),oList,mode,props);
  }
 else {
    assert(context.hasFunctionProp(function,FnProp.WRAPPED_BUILTIN) || context.hasFunctionProp(function,FnProp.APP));
    List<Arg> realInputs=new ArrayList<Arg>();
    for (    Var in : iList) {
      realInputs.add(in.asArg());
    }
    if (context.hasFunctionProp(function,FnProp.PARALLEL)) {
      Arg par=props.get(TaskPropKey.PARALLELISM);
      if (par == null) {
        throw new UserException(context,""String_Node_Str"" + ""String_Node_Str"" + function);
      }
      realInputs.add(par);
    }
    if (context.hasFunctionProp(function,FnProp.TARGETABLE)) {
      Arg target=props.getWithDefault(TaskPropKey.LOCATION);
      realInputs.add(target);
    }
    assert(context.hasFunctionProp(function,FnProp.SYNC));
    TaskMode mode=TaskMode.SYNC;
    backend.functionCall(function,realInputs,oList,mode,props);
  }
}",0.9874651810584958
132936,"/** 
 * @return List of outputs that are piecewise assigned
 */
public List<Var> getPiecewiseAssignedOutputs(){
switch (op) {
case ARRAY_INSERT_FUTURE:
case ARRAY_INSERT_IMM:
case ARRAYREF_INSERT_FUTURE:
case ARRAYREF_INSERT_IMM:
    return getOutputs();
case ARRAY_CREATE_NESTED_FUTURE:
case ARRAY_CREATE_NESTED_IMM:
case ARRAYREF_CREATE_NESTED_FUTURE:
case ARRAYREF_CREATE_NESTED_IMM:
{
    List<Var> outputs=getOutputs();
    return outputs.subList(1,outputs.size());
  }
case STRUCT_INSERT:
case STRUCT_CLOSE:
return getOutputs();
default :
return Var.NONE;
}
}","/** 
 * @return List of outputs that are piecewise assigned
 */
public List<Var> getPiecewiseAssignedOutputs(){
switch (op) {
case ARRAY_INSERT_FUTURE:
case ARRAY_INSERT_IMM:
case ARRAYREF_INSERT_FUTURE:
case ARRAYREF_INSERT_IMM:
    return getOutputs();
case ARRAY_CREATE_NESTED_FUTURE:
case ARRAY_CREATE_NESTED_IMM:
case ARRAYREF_CREATE_NESTED_FUTURE:
case ARRAYREF_CREATE_NESTED_IMM:
{
    List<Var> outputs=getOutputs();
    return outputs.subList(1,outputs.size());
  }
case STRUCT_INSERT:
case STRUCT_CLOSE:
return getOutputs();
case GET_OUTPUT_FILENAME:
return Collections.singletonList(getOutput(1));
default :
return Var.NONE;
}
}",0.9385382059800664
132937,"public static void fixupFunction(Logger logger,Program prog,Function fn,Set<Var> referencedGlobals,boolean updateLists){
  HierarchicalSet<Var> fnargs=new HierarchicalSet<Var>();
  for (  Var v : fn.getInputList()) {
    fnargs.add(v);
  }
  for (  Var v : fn.getOutputList()) {
    fnargs.add(v);
  }
  for (  Entry<String,Arg> e : prog.getGlobalConsts().entrySet()) {
    Arg a=e.getValue();
    Var v=new Var(a.futureType(),e.getKey(),VarStorage.GLOBAL_CONST,DefType.GLOBAL_CONST,null);
    fnargs.add(v);
  }
  Pair<Set<Var>,Set<Var>> res=fixupBlockRec(logger,fn,fn.mainBlock(),ExecContext.CONTROL,fnargs,referencedGlobals,updateLists);
  Set<Var> read=res.val1;
  Set<Var> written=res.val2;
  if (updateLists) {
    for (int i=0; i < fn.getOutputList().size(); i++) {
      Var output=fn.getOutput(i);
      if (!read.contains(output)) {
        fn.makeOutputWriteOnly(i);
      }
    }
  }
  read.removeAll(fn.getInputList());
  read.removeAll(fn.getOutputList());
  if (read.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ read.toString());
  }
  written.removeAll(fn.getOutputList());
  for (  Var v : fn.getInputList()) {
    if (Types.isScalarUpdateable(v.type())) {
      written.remove(v);
    }
  }
  if (written.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ written.toString());
  }
}","public static void fixupFunction(Logger logger,Program prog,Function fn,Set<Var> referencedGlobals,boolean updateLists){
  HierarchicalSet<Var> fnargs=new HierarchicalSet<Var>();
  for (  Var v : fn.getInputList()) {
    fnargs.add(v);
  }
  for (  Var v : fn.getOutputList()) {
    fnargs.add(v);
  }
  for (  Entry<String,Arg> e : prog.getGlobalConsts().entrySet()) {
    Arg a=e.getValue();
    Var v=new Var(a.futureType(),e.getKey(),VarStorage.GLOBAL_CONST,DefType.GLOBAL_CONST,null);
    fnargs.add(v);
  }
  Pair<Set<Var>,Set<Var>> res=fixupBlockRec(logger,fn,fn.mainBlock(),ExecContext.CONTROL,fnargs,referencedGlobals,updateLists);
  Set<Var> read=res.val1;
  Set<Var> written=res.val2;
  if (updateLists) {
    for (int i=0; i < fn.getOutputList().size(); i++) {
      Var output=fn.getOutput(i);
      if (!read.contains(output) && !Types.hasReadableSideChannel(output.type())) {
        fn.makeOutputWriteOnly(i);
      }
    }
  }
  read.removeAll(fn.getInputList());
  read.removeAll(fn.getOutputList());
  if (read.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ read.toString());
  }
  written.removeAll(fn.getOutputList());
  for (  Var v : fn.getInputList()) {
    if (Types.isScalarUpdateable(v.type())) {
      written.remove(v);
    }
  }
  if (written.size() > 0) {
    throw new STCRuntimeError(""String_Node_Str"" + fn.getName() + ""String_Node_Str""+ written.toString());
  }
}",0.9831460674157304
132938,"public void makeOutputWriteOnly(int i){
  oListWriteOnly.set(i,true);
}","public void makeOutputWriteOnly(int i){
  assert(i >= 0 && i < oList.size());
  assert(!Types.isFile(oList.get(i).type()));
  oListWriteOnly.set(i,true);
}",0.6283185840707964
132939,"/** 
 * Called when we enter a construct that blocked on v
 * @param var
 */
public void close(Var var,boolean recursive){
  Stack<Var> work=new Stack<Var>();
  work.add(var);
  while (!work.empty()) {
    Var v=work.pop();
    closed.add(v);
    CopyOnWriteSmallSet<Var> deps=dependsOn.remove(v);
    if (deps != null) {
      work.addAll(deps);
    }
  }
  if (recursive) {
    recursivelyClosed.add(var);
  }
}","/** 
 * Called when we enter a construct that blocked on v
 * @param var
 */
public void close(Var var,boolean recursive){
}",0.4618249534450652
132940,"/** 
 * @param execCx 
 * @param block
 * @param cv copy of cv from outer scope, or null if it should be initialized
 * @param replaceInputs : a set of variable replaces to do from this point in IC onwards
 * @return true if this should be called again
 * @throws InvalidOptionException
 * @throws InvalidWriteException
 */
private boolean forwardDataflow(Logger logger,Program program,Function f,ExecContext execCx,Block block,State cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll) throws InvalidOptionException, InvalidWriteException {
  if (block.getType() == BlockType.MAIN_BLOCK) {
    for (    WaitVar wv : f.blockingInputs()) {
      cv.close(wv.var,false);
    }
    for (    Var v : f.getInputList()) {
      if (Types.isScalarUpdateable(v.type())) {
        cv.close(v,false);
      }
    }
  }
  for (  Var v : block.getVariables()) {
    if (v.isMapped() && Types.isFile(v.type())) {
      ResultVal filenameVal=ICInstructions.filenameCV(Arg.createVar(v.mapping()),v);
      cv.addComputedValue(filenameVal,false);
    }
    if (Types.isMappable(v.type()) && !v.isMapped() && v.storage() != VarStorage.ALIAS) {
      cv.setUnmapped(v);
    }
  }
  boolean inlined=handleStatements(logger,program,f,execCx,block,block.statementIterator(),cv,replaceInputs,replaceAll);
  if (inlined) {
    cleanupAfterInline(block,replaceInputs,replaceAll);
    return true;
  }
  block.renameCleanupActions(replaceInputs,RenameMode.VALUE);
  block.renameCleanupActions(replaceAll,RenameMode.REFERENCE);
  for (int i=0; i < block.getContinuations().size(); i++) {
    Continuation c=block.getContinuation(i);
    c.renameVars(replaceInputs,RenameMode.VALUE,false);
    c.renameVars(replaceAll,RenameMode.REFERENCE,false);
    Block toInline=c.tryInline(cv.getClosed(),cv.getRecursivelyClosed(),keepExplicitWaits);
    if (toInline != null) {
      c.inlineInto(block,toInline);
      i--;
      inlined=true;
    }
  }
  if (inlined) {
    cleanupAfterInline(block,replaceInputs,replaceAll);
    return true;
  }
  for (  Continuation cont : block.getContinuations()) {
    recurseOnContinuation(logger,program,f,execCx,cont,cv,replaceInputs,replaceAll);
  }
  return false;
}","/** 
 * @param execCx 
 * @param block
 * @param cv copy of cv from outer scope, or null if it should be initialized
 * @param replaceInputs : a set of variable replaces to do from this point in IC onwards
 * @return true if this should be called again
 * @throws InvalidOptionException
 * @throws InvalidWriteException
 */
private boolean forwardDataflow(Logger logger,Program program,Function f,ExecContext execCx,Block block,State cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll) throws InvalidOptionException, InvalidWriteException {
  if (block.getType() == BlockType.MAIN_BLOCK) {
    for (    WaitVar wv : f.blockingInputs()) {
      cv.close(wv.var,false);
    }
    for (    Var v : f.getInputList()) {
      if (Types.isScalarUpdateable(v.type())) {
        cv.close(v,false);
      }
    }
  }
  for (  Var v : block.getVariables()) {
    if (v.isMapped() && Types.isFile(v.type())) {
      ResultVal filenameVal=ICInstructions.filenameCV(Arg.createVar(v.mapping()),v);
      cv.addComputedValue(filenameVal,false);
    }
    if (Types.isMappable(v.type()) && !v.isMapped() && v.storage() != VarStorage.ALIAS) {
      cv.setUnmapped(v);
    }
  }
  boolean inlined=handleStatements(logger,program,f,execCx,block,block.statementIterator(),cv,replaceInputs,replaceAll);
  if (inlined) {
    cleanupAfterInline(block,replaceInputs,replaceAll);
    return true;
  }
  block.renameCleanupActions(replaceInputs,RenameMode.VALUE);
  block.renameCleanupActions(replaceAll,RenameMode.REFERENCE);
  for (int i=0; i < block.getContinuations().size(); i++) {
    Continuation c=block.getContinuation(i);
    c.renameVars(replaceInputs,RenameMode.VALUE,false);
    c.renameVars(replaceAll,RenameMode.REFERENCE,false);
    Block toInline=c.tryInline(cv.getClosed(),cv.getRecursivelyClosed(),reorderingAllowed);
    if (toInline != null) {
      c.inlineInto(block,toInline);
      i--;
      inlined=true;
    }
  }
  if (inlined) {
    cleanupAfterInline(block,replaceInputs,replaceAll);
    return true;
  }
  for (  Continuation cont : block.getContinuations()) {
    recurseOnContinuation(logger,program,f,execCx,cont,cv,replaceInputs,replaceAll);
  }
  return false;
}",0.9922727272727272
132941,"/** 
 * @param res
 * @param replace for debugging purposes, set to true if we intend to replace
 */
public void addComputedValue(ResultVal res,boolean replace){
  boolean outClosed=res.outClosed();
  if (isAvailable(res.value())) {
    if (!replace) {
      throw new STCRuntimeError(""String_Node_Str"" + getLocation(res.value()) + ""String_Node_Str""+ res);
    }
  }
 else   if (replace) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + res + ""String_Node_Str"");
  }
  Arg valLoc=res.location();
  availableVals.put(res.value(),valLoc);
  if (valLoc.isVar()) {
    varContents.put(valLoc.getVar(),res.value());
    if (outClosed) {
      close(valLoc.getVar(),false);
    }
  }
}","/** 
 * @param res
 * @param replace for debugging purposes, set to true if we intend to replace
 */
public void addComputedValue(ResultVal res,boolean replace){
  boolean outClosed=res.outClosed();
  if (isAvailable(res.value())) {
    if (!replace) {
      throw new STCRuntimeError(""String_Node_Str"" + getLocation(res.value()) + ""String_Node_Str""+ res);
    }
  }
 else   if (replace) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + res + ""String_Node_Str"");
  }
  Arg valLoc=res.location();
  availableVals.put(res.value(),valLoc);
  if (valLoc.isVar()) {
    varContents.put(valLoc.getVar(),res.value());
    if (outClosed) {
      if (logger.isTraceEnabled()) {
        logger.trace(""String_Node_Str"" + valLoc + ""String_Node_Str"");
      }
      close(valLoc.getVar(),false);
    }
  }
}",0.924292297564187
132942,"/** 
 * @param logger
 * @param program
 * @param fn
 * @param execCx
 * @param cont
 * @param cv
 * @param replaceInputs
 * @param replaceAll
 * @return any variables that are guaranteed to be closed in currentcontext after continuation is evaluated
 * @throws InvalidOptionException
 * @throws InvalidWriteException
 */
private UnifiedState recurseOnContinuation(Logger logger,Program program,Function fn,ExecContext execCx,Continuation cont,State cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll) throws InvalidOptionException, InvalidWriteException {
  State contCV=cv.makeChild(cont.inheritsParentVars());
  List<BlockingVar> contClosedVars=cont.blockingVars(true);
  if (contClosedVars != null) {
    for (    BlockingVar bv : contClosedVars) {
      contCV.close(bv.var,bv.recursive);
    }
  }
  boolean unifyBranches=cont.isExhaustiveSyncConditional();
  List<State> branchStates=unifyBranches ? new ArrayList<State>() : null;
  List<Block> contBlocks=cont.getBlocks();
  for (int i=0; i < contBlocks.size(); i++) {
    HierarchicalMap<Var,Arg> contReplaceInputs;
    HierarchicalMap<Var,Arg> contReplaceAll;
    if (cont.inheritsParentVars()) {
      contReplaceInputs=replaceInputs;
      contReplaceAll=replaceAll;
    }
 else {
      contReplaceInputs=replaceInputs.makeChildMap();
      contReplaceAll=replaceAll.makeChildMap();
      purgeUnpassableVars(contReplaceInputs);
      purgeUnpassableVars(contReplaceAll);
    }
    State blockCV;
    boolean again;
    int pass=1;
    do {
      logger.debug(""String_Node_Str"" + pass);
      blockCV=contCV.makeChild(true);
      again=forwardDataflow(logger,program,fn,cont.childContext(execCx),contBlocks.get(i),blockCV,contReplaceInputs.makeChildMap(),contReplaceAll.makeChildMap());
      pass++;
    }
 while (again);
    if (unifyBranches) {
      branchStates.add(blockCV);
    }
  }
  if (unifyBranches) {
    return UnifiedState.unify(cv,branchStates);
  }
 else {
    return UnifiedState.EMPTY;
  }
}","/** 
 * @param logger
 * @param program
 * @param fn
 * @param execCx
 * @param cont
 * @param cv
 * @param replaceInputs
 * @param replaceAll
 * @return any variables that are guaranteed to be closed in currentcontext after continuation is evaluated
 * @throws InvalidOptionException
 * @throws InvalidWriteException
 */
private UnifiedState recurseOnContinuation(Logger logger,Program program,Function fn,ExecContext execCx,Continuation cont,State cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll) throws InvalidOptionException, InvalidWriteException {
  logger.trace(""String_Node_Str"" + cont.getType());
  State contCV=cv.makeChild(cont.inheritsParentVars());
  List<BlockingVar> contClosedVars=cont.blockingVars(true);
  if (contClosedVars != null) {
    for (    BlockingVar bv : contClosedVars) {
      contCV.close(bv.var,bv.recursive);
    }
  }
  boolean unifyBranches=cont.isExhaustiveSyncConditional();
  List<State> branchStates=unifyBranches ? new ArrayList<State>() : null;
  List<Block> contBlocks=cont.getBlocks();
  for (int i=0; i < contBlocks.size(); i++) {
    HierarchicalMap<Var,Arg> contReplaceInputs;
    HierarchicalMap<Var,Arg> contReplaceAll;
    if (cont.inheritsParentVars()) {
      contReplaceInputs=replaceInputs;
      contReplaceAll=replaceAll;
    }
 else {
      contReplaceInputs=replaceInputs.makeChildMap();
      contReplaceAll=replaceAll.makeChildMap();
      purgeUnpassableVars(contReplaceInputs);
      purgeUnpassableVars(contReplaceAll);
    }
    State blockCV;
    boolean again;
    int pass=1;
    do {
      logger.debug(""String_Node_Str"" + pass);
      blockCV=contCV.makeChild(true);
      again=forwardDataflow(logger,program,fn,cont.childContext(execCx),contBlocks.get(i),blockCV,contReplaceInputs.makeChildMap(),contReplaceAll.makeChildMap());
      pass++;
    }
 while (again);
    if (unifyBranches) {
      branchStates.add(blockCV);
    }
  }
  if (unifyBranches) {
    return UnifiedState.unify(cv,branchStates);
  }
 else {
    return UnifiedState.EMPTY;
  }
}",0.987166831194472
132943,"public ForwardDataflow(boolean keepExplicitWaits){
  this.keepExplicitWaits=keepExplicitWaits;
}","public ForwardDataflow(boolean reorderingAllowed){
  this.reorderingAllowed=reorderingAllowed;
}",0.5416666666666666
132944,"private static void handleInstruction(Logger logger,Function f,ExecContext execCx,Block block,ListIterator<Statement> stmts,Instruction inst,State cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + replaceInputs);
    logger.trace(""String_Node_Str"" + replaceAll);
    logger.trace(""String_Node_Str"" + cv.availableVals);
    State ancestor=cv.parent;
    int up=1;
    while (ancestor != null) {
      logger.trace(""String_Node_Str"" + up + ""String_Node_Str""+ ancestor.availableVals);
      up++;
      ancestor=ancestor.parent;
    }
    logger.trace(""String_Node_Str"" + cv.closed);
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + inst);
  }
  inst.renameVars(replaceInputs,RenameMode.VALUE);
  inst.renameVars(replaceAll,RenameMode.REFERENCE);
  List<ResultVal> icvs=inst.getResults(cv);
  updateReplacements(logger,f,inst,cv,icvs,replaceInputs,replaceAll);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst);
  }
  if (switchToImmediate(logger,f,execCx,block,cv,inst,stmts)) {
    return;
  }
  List<Var> in=inst.getBlockingInputs();
  if (in != null) {
    for (    Var ov : inst.getOutputs()) {
      if (!Types.isScalarValue(ov.type())) {
        cv.setDependencies(ov,in);
      }
    }
  }
  for (  Var out : inst.getClosedOutputs()) {
    cv.close(out,false);
  }
}","private void handleInstruction(Logger logger,Function f,ExecContext execCx,Block block,ListIterator<Statement> stmts,Instruction inst,State cv,HierarchicalMap<Var,Arg> replaceInputs,HierarchicalMap<Var,Arg> replaceAll){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + replaceInputs);
    logger.trace(""String_Node_Str"" + replaceAll);
    logger.trace(""String_Node_Str"" + cv.availableVals);
    State ancestor=cv.parent;
    int up=1;
    while (ancestor != null) {
      logger.trace(""String_Node_Str"" + up + ""String_Node_Str""+ ancestor.availableVals);
      up++;
      ancestor=ancestor.parent;
    }
    logger.trace(""String_Node_Str"" + cv.closed);
    logger.trace(""String_Node_Str"");
    logger.trace(""String_Node_Str"" + inst);
  }
  inst.renameVars(replaceInputs,RenameMode.VALUE);
  inst.renameVars(replaceAll,RenameMode.REFERENCE);
  List<ResultVal> icvs=inst.getResults(cv);
  updateReplacements(logger,f,inst,cv,icvs,replaceInputs,replaceAll);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst);
  }
  if (switchToImmediate(logger,f,execCx,block,cv,inst,stmts)) {
    return;
  }
  if (!reorderingAllowed) {
    List<Var> in=inst.getBlockingInputs();
    if (in != null) {
      for (      Var ov : inst.getOutputs()) {
        if (!Types.isScalarValue(ov.type())) {
          cv.setDependencies(ov,in);
        }
      }
    }
  }
  for (  Var out : inst.getClosedOutputs()) {
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + out.name() + ""String_Node_Str"");
    }
    cv.close(out,false);
  }
}",0.93
132945,"public HoistTracking makeChild(Block childBlock,int maxHoist,int maxLoopHoist){
  return new HoistTracking(this,childBlock,maxHoist,maxLoopHoist,writeMap.makeChildMap(),piecewiseWriteMap.makeChildMap(),declareMap.makeChildMap(),initializedMap.makeChildMap());
}","public HoistTracking makeChild(Block childBlock,ExecContext newExecCx,int maxHoist,int maxLoopHoist){
  return new HoistTracking(this,childBlock,execCx,maxHoist,maxLoopHoist,writeMap.makeChildMap(),piecewiseWriteMap.makeChildMap(),declareMap.makeChildMap(),initializedMap.makeChildMap());
}",0.9473684210526316
132946,"@Override public void optimize(Logger logger,Program prog){
  for (  Function f : prog.getFunctions()) {
    HoistTracking global=new HoistTracking();
    for (    Var gv : prog.getGlobalVars()) {
      global.write(gv,false);
      global.declare(gv);
    }
    HoistTracking mainBlockState=global.makeChild(f.mainBlock(),0,0);
    for (    Var in : f.getInputList()) {
      mainBlockState.write(in,false);
      mainBlockState.declare(in);
    }
    for (    Var out : f.getOutputList()) {
      mainBlockState.declare(out);
    }
    hoistRec(logger,mainBlockState);
  }
}","@Override public void optimize(Logger logger,Program prog){
  for (  Function f : prog.getFunctions()) {
    HoistTracking global=new HoistTracking();
    for (    Var gv : prog.getGlobalVars()) {
      global.write(gv,false);
      global.declare(gv);
    }
    HoistTracking mainBlockState=global.makeChild(f.mainBlock(),ExecContext.CONTROL,0,0);
    for (    Var in : f.getInputList()) {
      mainBlockState.write(in,false);
      mainBlockState.declare(in);
    }
    for (    Var out : f.getOutputList()) {
      mainBlockState.declare(out);
    }
    hoistRec(logger,mainBlockState);
  }
}",0.9829351535836176
132947,"private HoistTracking(HoistTracking parent,Block block,int maxHoist,int maxLoopHoist,HierarchicalMap<Var,Block> writeMap,HierarchicalMap<Var,Block> piecewiseWriteMap,HierarchicalMap<Var,Block> declareMap,HierarchicalMap<Var,Block> initializedMap){
  super();
  this.parent=parent;
  this.block=block;
  this.maxHoist=maxHoist;
  this.maxLoopHoist=maxLoopHoist;
  this.writeMap=writeMap;
  this.piecewiseWriteMap=piecewiseWriteMap;
  this.declareMap=declareMap;
  this.initializedMap=initializedMap;
}","private HoistTracking(HoistTracking parent,Block block,ExecContext execCx,int maxHoist,int maxLoopHoist,HierarchicalMap<Var,Block> writeMap,HierarchicalMap<Var,Block> piecewiseWriteMap,HierarchicalMap<Var,Block> declareMap,HierarchicalMap<Var,Block> initializedMap){
  super();
  this.parent=parent;
  this.block=block;
  this.execCx=execCx;
  this.maxHoist=maxHoist;
  this.maxLoopHoist=maxLoopHoist;
  this.writeMap=writeMap;
  this.piecewiseWriteMap=piecewiseWriteMap;
  this.declareMap=declareMap;
  this.initializedMap=initializedMap;
}",0.9606147934678194
132948,"private RuleProps buildRuleProps(TaskProps props){
  Expression priority=TclUtil.argToExpr(props.get(TaskPropKey.PRIORITY),true);
  TclTarget target=TclTarget.fromArg(props.get(TaskPropKey.PRIORITY));
  Expression parallelism=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  RuleProps ruleProps=new RuleProps(target,parallelism,priority);
  return ruleProps;
}","private RuleProps buildRuleProps(TaskProps props){
  Expression priority=TclUtil.argToExpr(props.get(TaskPropKey.PRIORITY),true);
  TclTarget target=TclTarget.fromArg(props.get(TaskPropKey.TARGET));
  Expression parallelism=TclUtil.argToExpr(props.get(TaskPropKey.PARALLELISM),true);
  RuleProps ruleProps=new RuleProps(target,parallelism,priority);
  return ruleProps;
}",0.9811827956989247
132949,"@Override public Block branchPredict(Map<Var,Arg> knownConstants){
  if (switchVar.isVar() && knownConstants.containsKey(switchVar.getVar())) {
    this.switchVar=knownConstants.get(switchVar.getVar());
  }
  long val=switchVar.getIntLit();
  for (int i=0; i < caseLabels.size(); i++) {
    if (val == caseLabels.get(i)) {
      return caseBlocks.get(i);
    }
  }
  return defaultBlock;
}","@Override public Block branchPredict(Map<Var,Arg> knownConstants){
  if (switchVar.isVar()) {
    if (knownConstants.containsKey(switchVar.getVar())) {
      this.switchVar=knownConstants.get(switchVar.getVar());
    }
 else {
      return null;
    }
  }
  long val=switchVar.getIntLit();
  for (int i=0; i < caseLabels.size(); i++) {
    if (val == caseLabels.get(i)) {
      return caseBlocks.get(i);
    }
  }
  return defaultBlock;
}",0.9310761789600968
132950,"@Override public void appendTo(StringBuilder sb,ExprContext mode){
  assert(mode != ExprContext.VALUE_STRING);
  if (parenthesise)   sb.append(""String_Node_Str"");
  boolean first=true;
  for (  Expression e : exprs) {
    if (first) {
      first=false;
    }
 else {
      sb.append(""String_Node_Str"");
    }
    e.appendTo(sb,mode);
  }
  if (parenthesise)   sb.append(""String_Node_Str"");
}","@Override public void appendTo(StringBuilder sb,ExprContext mode){
  if (parenthesise)   sb.append(""String_Node_Str"");
  boolean first=true;
  for (  Expression e : exprs) {
    if (first) {
      first=false;
    }
 else {
      sb.append(""String_Node_Str"");
    }
    e.appendTo(sb,mode);
  }
  if (parenthesise)   sb.append(""String_Node_Str"");
}",0.727027027027027
132951,"/** 
 * Generate code for a rule
 * @param symbol
 * @param inputs
 * @param action the action, using a tcl list to ensure proper escaping
 * @param type
 * @return
 */
private static Sequence ruleHelper(String symbol,List<? extends Expression> inputs,Expression action,TaskMode type,Target target,Expression priority,ExecContext execCx){
  Sequence res=new Sequence();
  if (inputs.isEmpty()) {
    if (type != TaskMode.LOCAL && type != TaskMode.LOCAL_CONTROL) {
      res.add(spawnTask(action,type,target,priority,execCx));
      return res;
    }
  }
  if (priority != null)   res.add(setPriority(priority));
  Token ruleCmd=execCx == ExecContext.CONTROL ? RULE : SPAWN_RULE;
  res.add(new Command(ruleCmd,new Token(symbol),new TclList(inputs),tclRuleType(type),target.toTcl(),action));
  if (priority != null)   res.add(resetPriority());
  return res;
}","/** 
 * Generate code for a rule
 * @param symbol
 * @param inputs
 * @param action tokens making up the action
 * @param type
 * @return
 */
private static Sequence ruleHelper(String symbol,List<? extends Expression> inputs,List<Expression> action,TaskMode type,Target target,Expression priority,ExecContext execCx){
  if (inputs.isEmpty()) {
    if (type != TaskMode.LOCAL && type != TaskMode.LOCAL_CONTROL) {
      return spawnTask(action,type,target,priority,execCx);
    }
  }
  Sequence res=new Sequence();
  if (priority != null)   res.add(setPriority(priority));
  Token ruleCmd=execCx == ExecContext.CONTROL ? RULE : SPAWN_RULE;
  res.add(new Command(ruleCmd,new Token(symbol),new TclList(inputs),tclRuleType(type),target.toTcl(),TclUtil.tclStringAsList(action)));
  if (priority != null)   res.add(resetPriority());
  return res;
}",0.8798586572438163
132952,"/** 
 * @param symbol
 * @param blockOn
 * @param action
 * @param mode
 * @return
 */
public static Sequence rule(String symbol,List<? extends Expression> blockOn,Expression action,TaskMode mode,Target target,Expression priority,ExecContext execCx){
  return ruleHelper(symbol,blockOn,action,mode,target,priority,execCx);
}","/** 
 * @param symbol
 * @param blockOn
 * @param action
 * @param mode
 * @return
 */
public static Sequence rule(String symbol,List<? extends Expression> blockOn,List<Expression> action,TaskMode mode,Target target,Expression priority,ExecContext execCx){
  return ruleHelper(symbol,blockOn,action,mode,target,priority,execCx);
}",0.9908256880733946
132953,"public static Sequence loopRule(String symbol,List<Value> args,List<? extends Expression> blockOn,ExecContext execCx){
  assert(execCx == ExecContext.CONTROL);
  List<Expression> actionElems=new ArrayList<Expression>();
  actionElems.add(new Token(symbol));
  for (  Value arg : args) {
    actionElems.add(arg);
  }
  TclList action=new TclList(actionElems);
  return ruleHelper(symbol,blockOn,action,TaskMode.CONTROL,Target.rankAny(),null,execCx);
}","public static Sequence loopRule(String symbol,List<Value> args,List<? extends Expression> blockOn,ExecContext execCx){
  assert(execCx == ExecContext.CONTROL);
  List<Expression> action=new ArrayList<Expression>();
  action.add(new Token(symbol));
  for (  Value arg : args) {
    action.add(arg);
  }
  return ruleHelper(symbol,blockOn,action,TaskMode.CONTROL,Target.rankAny(),null,execCx);
}",0.4454976303317535
132954,"private static Command spawnTask(Expression action,TaskMode type,Target target,Expression priority,ExecContext execCx){
  Token ADLB_PUT=new Token(""String_Node_Str"");
  LiteralInt TURBINE_NULL_RULE=new LiteralInt(-1);
  if (priority == null) {
    priority=currentPriority();
  }
  assert(type == TaskMode.CONTROL || type == TaskMode.WORKER);
  return new Command(ADLB_PUT,target.toTcl(),adlbWorkType(type),new TclString(Arrays.asList(TURBINE_NULL_RULE,action)),priority);
}","private static Sequence spawnTask(List<Expression> action,TaskMode type,Target target,Expression priority,ExecContext execCx){
  Sequence res=new Sequence();
  Token ADLB_PUT=new Token(""String_Node_Str"");
  LiteralInt TURBINE_NULL_RULE=new LiteralInt(-1);
  if (priority == null) {
    priority=currentPriority();
  }
  Value priorityVar=new Value(TCLTMP_PRIO);
  res.add(new SetVariable(TCLTMP_PRIO,priority));
  List<Expression> taskTokens=new ArrayList<Expression>();
  if (type == TaskMode.WORKER) {
    taskTokens.add(TURBINE_NULL_RULE);
    taskTokens.addAll(action);
  }
 else {
    assert(type == TaskMode.CONTROL);
    taskTokens.add(new Token(""String_Node_Str""));
    taskTokens.add(new Token(""String_Node_Str""));
    taskTokens.add(priorityVar);
    taskTokens.addAll(action);
  }
  res.add(new Command(ADLB_PUT,target.toTcl(),adlbWorkType(type),TclUtil.tclStringAsList(taskTokens),priorityVar));
  return res;
}",0.4667143879742305
132955,"public static Sequence deepRule(String symbol,List<? extends Expression> inputs,int[] depths,boolean[] isFile,Expression action,TaskMode mode,Expression priority,ExecContext execCx){
  assert(inputs.size() == depths.length);
  assert(inputs.size() == isFile.length);
  List<Expression> depthExprs=new ArrayList<Expression>(depths.length);
  List<Expression> isFileExprs=new ArrayList<Expression>(isFile.length);
  for (  int depth : depths) {
    depthExprs.add(new LiteralInt(depth));
  }
  for (  boolean b : isFile) {
    isFileExprs.add(LiteralInt.boolValue(b));
  }
  Sequence res=new Sequence();
  if (priority != null)   res.add(setPriority(priority));
  res.add(new Command(DEEPRULE,new Token(symbol),new TclList(inputs),new TclList(depthExprs),new TclList(isFileExprs),tclRuleType(mode),action));
  if (priority != null)   res.add(resetPriority());
  return res;
}","public static Sequence deepRule(String symbol,List<? extends Expression> inputs,int[] depths,boolean[] isFile,List<Expression> action,TaskMode mode,Expression priority,ExecContext execCx){
  assert(inputs.size() == depths.length);
  assert(inputs.size() == isFile.length);
  List<Expression> depthExprs=new ArrayList<Expression>(depths.length);
  List<Expression> isFileExprs=new ArrayList<Expression>(isFile.length);
  for (  int depth : depths) {
    depthExprs.add(new LiteralInt(depth));
  }
  for (  boolean b : isFile) {
    isFileExprs.add(LiteralInt.boolValue(b));
  }
  Sequence res=new Sequence();
  if (priority != null)   res.add(setPriority(priority));
  res.add(new Command(DEEPRULE,new Token(symbol),new TclList(inputs),new TclList(depthExprs),new TclList(isFileExprs),tclRuleType(mode),TclUtil.tclStringAsList(action)));
  if (priority != null)   res.add(resetPriority());
  return res;
}",0.964546989307822
132956,"private Expression buildActionFromVars(String procName,List<Var> usedVariables){
  List<Expression> exprs=new ArrayList<Expression>();
  for (  Var v : usedVariables) {
    Type t=v.type();
    if (Types.isScalarFuture(t) || Types.isRef(t) || Types.isArray(t)|| Types.isStruct(t)|| Types.isScalarUpdateable(t)) {
      exprs.add(varToExpr(v));
    }
 else     if (Types.isScalarValue(t)) {
      PrimType pt=t.primType();
      if (pt == PrimType.INT || pt == PrimType.BOOL || pt == PrimType.FLOAT || pt == PrimType.STRING) {
        exprs.add(varToExpr(v));
      }
 else {
        throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + v);
      }
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + v);
    }
  }
  return buildAction(procName,exprs);
}","private List<Expression> buildActionFromVars(String procName,List<Var> usedVariables){
  List<Expression> exprs=new ArrayList<Expression>();
  for (  Var v : usedVariables) {
    Type t=v.type();
    if (Types.isScalarFuture(t) || Types.isRef(t) || Types.isArray(t)|| Types.isStruct(t)|| Types.isScalarUpdateable(t)) {
      exprs.add(varToExpr(v));
    }
 else     if (Types.isScalarValue(t)) {
      PrimType pt=t.primType();
      if (pt == PrimType.INT || pt == PrimType.BOOL || pt == PrimType.FLOAT || pt == PrimType.STRING) {
        exprs.add(varToExpr(v));
      }
 else {
        throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + v);
      }
    }
 else {
      throw new STCRuntimeError(""String_Node_Str"" + v);
    }
  }
  return buildAction(procName,exprs);
}",0.9961685823754788
132957,"/** 
 * Internal helper to implement constructs that need to wait for a number of variables, and then run some code
 * @param procName
 * @param waitVars
 * @param passIn
 * @param keepOpenVars
 * @param priority 
 * @param recursive
 */
private void startAsync(String procName,List<Var> waitVars,List<Var> passIn,Arg priority,boolean recursive,TaskMode mode){
  assert(priority == null || priority.isImmediateInt());
  mode.checkSpawn(execContextStack.peek());
  for (  Var v : passIn) {
    if (v.type().equals(Types.V_BLOB)) {
      throw new STCRuntimeError(""String_Node_Str"");
    }
  }
  List<String> args=new ArrayList<String>();
  args.add(Turbine.LOCAL_STACK_NAME);
  for (  Var v : passIn) {
    args.add(prefixVar(v.name()));
  }
  Sequence constructProc=new Sequence();
  String uniqueName=uniqueTCLFunctionName(procName);
  Proc proc=new Proc(uniqueName,usedTclFunctionNames,args,constructProc);
  tree.add(proc);
  boolean useDeepWait=false;
  List<Expression> waitFor=new ArrayList<Expression>();
  for (  Var w : waitVars) {
    if (recursive) {
      Type baseType=w.type();
      if (Types.isArray(w.type())) {
        baseType=new ArrayInfo(w.type()).baseType;
        useDeepWait=true;
      }
      if (Types.isScalarFuture(baseType)) {
      }
 else       if (Types.isRef(baseType)) {
      }
 else {
        throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + w.type().typeName());
      }
    }
    Expression waitExpr=getTurbineWaitId(w);
    waitFor.add(waitExpr);
  }
  Expression action=buildActionFromVars(uniqueName,passIn);
  Expression priorityExpr=TclUtil.argToExpr(priority,true);
  if (useDeepWait) {
    int depths[]=new int[waitVars.size()];
    boolean isFile[]=new boolean[waitVars.size()];
    for (int i=0; i < waitVars.size(); i++) {
      Type waitVarType=waitVars.get(i).type();
      Type baseType;
      if (Types.isArray(waitVarType)) {
        ArrayInfo ai=new ArrayInfo(waitVarType);
        depths[i]=ai.nesting;
        baseType=ai.baseType;
      }
 else {
        depths[i]=0;
        baseType=waitVarType;
      }
      isFile[i]=Types.isFile(baseType);
    }
    pointStack.peek().append(Turbine.deepRule(uniqueName,waitFor,depths,isFile,action,mode,priorityExpr,execContextStack.peek()));
  }
 else {
    pointStack.peek().append(Turbine.rule(uniqueName,waitFor,action,mode,Target.rankAny(),priorityExpr,execContextStack.peek()));
  }
  pointStack.push(constructProc);
  ExecContext newExecContext;
  if (mode == TaskMode.WORKER) {
    newExecContext=ExecContext.WORKER;
  }
 else   if (mode == TaskMode.CONTROL) {
    newExecContext=ExecContext.CONTROL;
  }
 else {
    newExecContext=execContextStack.peek();
  }
  execContextStack.push(newExecContext);
}","/** 
 * Internal helper to implement constructs that need to wait for a number of variables, and then run some code
 * @param procName
 * @param waitVars
 * @param passIn
 * @param keepOpenVars
 * @param priority 
 * @param recursive
 */
private void startAsync(String procName,List<Var> waitVars,List<Var> passIn,Arg priority,boolean recursive,TaskMode mode){
  assert(priority == null || priority.isImmediateInt());
  mode.checkSpawn(execContextStack.peek());
  for (  Var v : passIn) {
    if (v.type().equals(Types.V_BLOB)) {
      throw new STCRuntimeError(""String_Node_Str"");
    }
  }
  List<String> args=new ArrayList<String>();
  args.add(Turbine.LOCAL_STACK_NAME);
  for (  Var v : passIn) {
    args.add(prefixVar(v.name()));
  }
  Sequence constructProc=new Sequence();
  String uniqueName=uniqueTCLFunctionName(procName);
  Proc proc=new Proc(uniqueName,usedTclFunctionNames,args,constructProc);
  tree.add(proc);
  boolean useDeepWait=false;
  List<Expression> waitFor=new ArrayList<Expression>();
  for (  Var w : waitVars) {
    if (recursive) {
      Type baseType=w.type();
      if (Types.isArray(w.type())) {
        baseType=new ArrayInfo(w.type()).baseType;
        useDeepWait=true;
      }
      if (Types.isScalarFuture(baseType)) {
      }
 else       if (Types.isRef(baseType)) {
      }
 else {
        throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + w.type().typeName());
      }
    }
    Expression waitExpr=getTurbineWaitId(w);
    waitFor.add(waitExpr);
  }
  List<Expression> action=buildActionFromVars(uniqueName,passIn);
  Expression priorityExpr=TclUtil.argToExpr(priority,true);
  if (useDeepWait) {
    int depths[]=new int[waitVars.size()];
    boolean isFile[]=new boolean[waitVars.size()];
    for (int i=0; i < waitVars.size(); i++) {
      Type waitVarType=waitVars.get(i).type();
      Type baseType;
      if (Types.isArray(waitVarType)) {
        ArrayInfo ai=new ArrayInfo(waitVarType);
        depths[i]=ai.nesting;
        baseType=ai.baseType;
      }
 else {
        depths[i]=0;
        baseType=waitVarType;
      }
      isFile[i]=Types.isFile(baseType);
    }
    pointStack.peek().append(Turbine.deepRule(uniqueName,waitFor,depths,isFile,action,mode,priorityExpr,execContextStack.peek()));
  }
 else {
    pointStack.peek().append(Turbine.rule(uniqueName,waitFor,action,mode,Target.rankAny(),priorityExpr,execContextStack.peek()));
  }
  pointStack.push(constructProc);
  ExecContext newExecContext;
  if (mode == TaskMode.WORKER) {
    newExecContext=ExecContext.WORKER;
  }
 else   if (mode == TaskMode.CONTROL) {
    newExecContext=ExecContext.CONTROL;
  }
 else {
    newExecContext=execContextStack.peek();
  }
  execContextStack.push(newExecContext);
}",0.9989010989010988
132958,"/** 
 * After this function is called, in the TCL context at the top of the stack will be available the bottom, top (inclusive) and increment of the split in tcl values: TCLTMP_RANGE_LO TCLTMP_RANGE_HI and TCLTMP_RANGE_INC
 * @param loopName
 * @param splitDegree
 * @param leafDegree 
 * @param startE start of range (inclusive)
 * @param endE end of range (inclusive)
 * @param incrE
 * @param usedVariables
 * @param keepOpenVars
 */
private void startRangeSplit(String loopName,List<PassedVar> passedVars,List<RefCount> perIterIncrs,int splitDegree,int leafDegree,Expression startE,Expression endE,Expression incrE){
  List<String> commonFormalArgs=new ArrayList<String>();
  commonFormalArgs.add(Turbine.LOCAL_STACK_NAME);
  for (  PassedVar pv : passedVars) {
    commonFormalArgs.add(prefixVar(pv.var.name()));
  }
  commonFormalArgs.add(TCLTMP_RANGE_LO);
  commonFormalArgs.add(TCLTMP_RANGE_HI);
  commonFormalArgs.add(TCLTMP_RANGE_INC);
  List<String> outerFormalArgs=new ArrayList<String>(commonFormalArgs);
  Value loVal=new Value(TCLTMP_RANGE_LO);
  Value hiVal=new Value(TCLTMP_RANGE_HI);
  Value incVal=new Value(TCLTMP_RANGE_INC);
  List<Expression> commonArgs=new ArrayList<Expression>();
  commonArgs.add(new Value(Turbine.LOCAL_STACK_NAME));
  for (  PassedVar pv : passedVars) {
    commonArgs.add(varToExpr(pv.var));
  }
  List<Expression> outerCallArgs=new ArrayList<Expression>(commonArgs);
  outerCallArgs.add(startE);
  outerCallArgs.add(endE);
  outerCallArgs.add(incrE);
  List<Expression> innerCallArgs=new ArrayList<Expression>(commonArgs);
  innerCallArgs.add(loVal);
  innerCallArgs.add(hiVal);
  innerCallArgs.add(incVal);
  Sequence outer=new Sequence();
  String outerProcName=uniqueTCLFunctionName(loopName + ""String_Node_Str"");
  tree.add(new Proc(outerProcName,usedTclFunctionNames,outerFormalArgs,outer));
  Sequence inner=new Sequence();
  String innerProcName=uniqueTCLFunctionName(loopName + ""String_Node_Str"");
  tree.add(new Proc(innerProcName,usedTclFunctionNames,commonFormalArgs,inner));
  pointStack.peek().add(new Command(outerProcName,outerCallArgs));
  outer.add(new SetVariable(TCLTMP_ITERSLEFT,rangeItersLeft(new Value(TCLTMP_RANGE_LO),new Value(TCLTMP_RANGE_HI),new Value(TCLTMP_RANGE_INC))));
  Expression done=new TclExpr(new Value(TCLTMP_ITERSLEFT),TclExpr.LTE,LiteralInt.ZERO);
  Sequence thenDoneB=new Sequence();
  If finishedIf=new If(done,thenDoneB);
  thenDoneB.add(new Command(""String_Node_Str""));
  outer.add(finishedIf);
  Expression doneSplitting=new TclExpr(new Value(TCLTMP_ITERSLEFT),TclExpr.LTE,new LiteralInt(leafDegree));
  Sequence thenNoSplitB=new Sequence();
  Sequence elseSplitB=new Sequence();
  If splitIf=new If(doneSplitting,thenNoSplitB,elseSplitB);
  outer.add(splitIf);
  thenNoSplitB.add(new Command(innerProcName,innerCallArgs));
  Sequence splitBody=new Sequence();
  String splitStart=""String_Node_Str"";
  String skip=""String_Node_Str"";
  elseSplitB.add(new SetVariable(skip,TclExpr.mult(new Value(TCLTMP_RANGE_INC),TclExpr.max(new LiteralInt(leafDegree),TclExpr.group(TclExpr.paren(TclExpr.paren(new Value(TCLTMP_ITERSLEFT),TclExpr.MINUS,LiteralInt.ONE),TclExpr.DIV,new LiteralInt(splitDegree)),TclExpr.PLUS,LiteralInt.ONE)))));
  ForLoop splitLoop=new ForLoop(splitStart,loVal,hiVal,new Value(skip),splitBody);
  elseSplitB.add(splitLoop);
  ArrayList<Expression> outerRecCall=new ArrayList<Expression>();
  outerRecCall.add(new Token(outerProcName));
  outerRecCall.addAll(commonArgs);
  outerRecCall.add(new Value(splitStart));
  TclExpr splitEnd=new TclExpr(TclExpr.min(new Value(TCLTMP_RANGE_HI),TclExpr.group(new Value(splitStart),TclExpr.PLUS,new Value(skip),TclExpr.MINUS,LiteralInt.ONE)));
  outerRecCall.add(splitEnd);
  outerRecCall.add(incVal);
  splitBody.add(Turbine.rule(outerProcName,new ArrayList<Value>(0),new TclList(outerRecCall),TaskMode.CONTROL,Target.rankAny(),null,execContextStack.peek()));
  pointStack.push(inner);
}","/** 
 * After this function is called, in the TCL context at the top of the stack will be available the bottom, top (inclusive) and increment of the split in tcl values: TCLTMP_RANGE_LO TCLTMP_RANGE_HI and TCLTMP_RANGE_INC
 * @param loopName
 * @param splitDegree
 * @param leafDegree 
 * @param startE start of range (inclusive)
 * @param endE end of range (inclusive)
 * @param incrE
 * @param usedVariables
 * @param keepOpenVars
 */
private void startRangeSplit(String loopName,List<PassedVar> passedVars,List<RefCount> perIterIncrs,int splitDegree,int leafDegree,Expression startE,Expression endE,Expression incrE){
  List<String> commonFormalArgs=new ArrayList<String>();
  commonFormalArgs.add(Turbine.LOCAL_STACK_NAME);
  for (  PassedVar pv : passedVars) {
    commonFormalArgs.add(prefixVar(pv.var.name()));
  }
  commonFormalArgs.add(TCLTMP_RANGE_LO);
  commonFormalArgs.add(TCLTMP_RANGE_HI);
  commonFormalArgs.add(TCLTMP_RANGE_INC);
  List<String> outerFormalArgs=new ArrayList<String>(commonFormalArgs);
  Value loVal=new Value(TCLTMP_RANGE_LO);
  Value hiVal=new Value(TCLTMP_RANGE_HI);
  Value incVal=new Value(TCLTMP_RANGE_INC);
  List<Expression> commonArgs=new ArrayList<Expression>();
  commonArgs.add(new Value(Turbine.LOCAL_STACK_NAME));
  for (  PassedVar pv : passedVars) {
    commonArgs.add(varToExpr(pv.var));
  }
  List<Expression> outerCallArgs=new ArrayList<Expression>(commonArgs);
  outerCallArgs.add(startE);
  outerCallArgs.add(endE);
  outerCallArgs.add(incrE);
  List<Expression> innerCallArgs=new ArrayList<Expression>(commonArgs);
  innerCallArgs.add(loVal);
  innerCallArgs.add(hiVal);
  innerCallArgs.add(incVal);
  Sequence outer=new Sequence();
  String outerProcName=uniqueTCLFunctionName(loopName + ""String_Node_Str"");
  tree.add(new Proc(outerProcName,usedTclFunctionNames,outerFormalArgs,outer));
  Sequence inner=new Sequence();
  String innerProcName=uniqueTCLFunctionName(loopName + ""String_Node_Str"");
  tree.add(new Proc(innerProcName,usedTclFunctionNames,commonFormalArgs,inner));
  pointStack.peek().add(new Command(outerProcName,outerCallArgs));
  outer.add(new SetVariable(TCLTMP_ITERSLEFT,rangeItersLeft(new Value(TCLTMP_RANGE_LO),new Value(TCLTMP_RANGE_HI),new Value(TCLTMP_RANGE_INC))));
  Expression done=new TclExpr(new Value(TCLTMP_ITERSLEFT),TclExpr.LTE,LiteralInt.ZERO);
  Sequence thenDoneB=new Sequence();
  If finishedIf=new If(done,thenDoneB);
  thenDoneB.add(new Command(""String_Node_Str""));
  outer.add(finishedIf);
  Expression doneSplitting=new TclExpr(new Value(TCLTMP_ITERSLEFT),TclExpr.LTE,new LiteralInt(leafDegree));
  Sequence thenNoSplitB=new Sequence();
  Sequence elseSplitB=new Sequence();
  If splitIf=new If(doneSplitting,thenNoSplitB,elseSplitB);
  outer.add(splitIf);
  thenNoSplitB.add(new Command(innerProcName,innerCallArgs));
  Sequence splitBody=new Sequence();
  String splitStart=""String_Node_Str"";
  String skip=""String_Node_Str"";
  elseSplitB.add(new SetVariable(skip,TclExpr.mult(new Value(TCLTMP_RANGE_INC),TclExpr.max(new LiteralInt(leafDegree),TclExpr.group(TclExpr.paren(TclExpr.paren(new Value(TCLTMP_ITERSLEFT),TclExpr.MINUS,LiteralInt.ONE),TclExpr.DIV,new LiteralInt(splitDegree)),TclExpr.PLUS,LiteralInt.ONE)))));
  ForLoop splitLoop=new ForLoop(splitStart,loVal,hiVal,new Value(skip),splitBody);
  elseSplitB.add(splitLoop);
  ArrayList<Expression> outerRecCall=new ArrayList<Expression>();
  outerRecCall.add(new Token(outerProcName));
  outerRecCall.addAll(commonArgs);
  outerRecCall.add(new Value(splitStart));
  TclExpr splitEndExpr=new TclExpr(TclExpr.min(new Value(TCLTMP_RANGE_HI),TclExpr.group(new Value(splitStart),TclExpr.PLUS,new Value(skip),TclExpr.MINUS,LiteralInt.ONE)));
  splitBody.add(new SetVariable(TCLTMP_SPLITEND,splitEndExpr));
  outerRecCall.add(new Value(TCLTMP_SPLITEND));
  outerRecCall.add(incVal);
  splitBody.add(Turbine.rule(outerProcName,new ArrayList<Value>(0),outerRecCall,TaskMode.CONTROL,Target.rankAny(),null,execContextStack.peek()));
  pointStack.push(inner);
}",0.9855072463768116
132959,"private Expression buildAction(String procName,List<Expression> args){
  ArrayList<Expression> ruleTokens=new ArrayList<Expression>();
  ruleTokens.add(new Token(procName));
  ruleTokens.add(new Value(Turbine.LOCAL_STACK_NAME));
  ruleTokens.addAll(args);
  return TclUtil.tclStringAsList(ruleTokens);
}","private List<Expression> buildAction(String procName,List<Expression> args){
  ArrayList<Expression> ruleTokens=new ArrayList<Expression>();
  ruleTokens.add(new Token(procName));
  ruleTokens.add(new Value(Turbine.LOCAL_STACK_NAME));
  ruleTokens.addAll(args);
  return ruleTokens;
}",0.879045996592845
132960,"private Function switchToValuePassing(Logger logger,Function fn,Set<String> usedFunctionNames){
  if (fn.blockingInputs().isEmpty())   return null;
  List<Var> switchVars=new ArrayList<Var>();
  for (  WaitVar input : fn.blockingInputs()) {
    if (Types.isScalarFuture(input.var.type())) {
      Type valueT=Types.derefResultType(input.var.type());
      if (Semantics.canPassToChildTask(valueT)) {
        switchVars.add(input.var);
      }
    }
  }
  if (switchVars.isEmpty())   return null;
  List<Pair<Var,Var>> futValPairs=createValueVars(fn,switchVars);
  Map<Var,Var> switched=new HashMap<Var,Var>();
  for (  Pair<Var,Var> fv : futValPairs) {
    switched.put(fv.val1,fv.val2);
    assert(fv.val2 != null);
  }
  List<Var> newIList=buildNewInputList(fn,switched);
  String newName=selectUniqueName(fn.getName(),usedFunctionNames);
  Block callNewFunction=callNewFunctionCode(fn,newName,switchVars);
  Block newBlock=fn.swapBlock(callNewFunction);
  for (  Pair<Var,Var> fv : futValPairs) {
    Var tmpfuture=new Var(fv.val1.type(),fv.val1.name(),VarStorage.STACK,DefType.LOCAL_USER);
    newBlock.renameVars(Collections.singletonMap(fv.val1,tmpfuture.asArg()),RenameMode.REPLACE_VAR,true);
    newBlock.addVariable(tmpfuture);
    Instruction store=ICInstructions.futureSet(fv.val1,fv.val2.asArg());
    newBlock.addInstructionFront(store);
  }
  List<WaitVar> newBlocking=new ArrayList<WaitVar>();
  for (  WaitVar wv : fn.blockingInputs()) {
    if (!switchVars.contains(wv.var)) {
      newBlocking.add(wv);
    }
  }
  return new Function(newName,newIList,newBlocking,fn.getOutputList(),fn.mode(),newBlock);
}","private Function switchToValuePassing(Logger logger,Function fn,Set<String> usedFunctionNames){
  if (fn.blockingInputs().isEmpty())   return null;
  List<Var> switchVars=new ArrayList<Var>();
  for (  WaitVar input : fn.blockingInputs()) {
    if (Types.isScalarFuture(input.var.type())) {
      Type valueT=Types.derefResultType(input.var.type());
      if (Semantics.canPassToChildTask(valueT)) {
        switchVars.add(input.var);
      }
    }
  }
  if (switchVars.isEmpty())   return null;
  List<Pair<Var,Var>> futValPairs=createValueVars(fn,switchVars);
  Map<Var,Var> switched=new HashMap<Var,Var>();
  for (  Pair<Var,Var> fv : futValPairs) {
    switched.put(fv.val1,fv.val2);
    assert(fv.val2 != null);
  }
  List<Var> newIList=buildNewInputList(fn,switched);
  String newName=selectUniqueName(fn.getName(),usedFunctionNames);
  Block callNewFunction=callNewFunctionCode(fn,newName,switchVars);
  Block newBlock=fn.swapBlock(callNewFunction);
  for (  Pair<Var,Var> fv : futValPairs) {
    Var tmpfuture=new Var(fv.val1.type(),fv.val1.name(),VarStorage.STACK,DefType.LOCAL_USER);
    newBlock.renameVars(Collections.singletonMap(fv.val1,tmpfuture.asArg()),RenameMode.REPLACE_VAR,true);
    newBlock.addVariable(tmpfuture);
    Instruction store=ICInstructions.futureSet(tmpfuture,fv.val2.asArg());
    newBlock.addInstructionFront(store);
  }
  List<WaitVar> newBlocking=new ArrayList<WaitVar>();
  for (  WaitVar wv : fn.blockingInputs()) {
    if (!switchVars.contains(wv.var)) {
      newBlocking.add(wv);
    }
  }
  return new Function(newName,newIList,newBlocking,fn.getOutputList(),fn.mode(),newBlock);
}",0.9950738916256158
132961,"private boolean rearrangeWaitsRec(Logger logger,Function fn,Block block,ExecContext currContext){
  List<WaitStatement> toInline=new ArrayList<WaitStatement>();
  boolean changed=false;
  for (  Continuation c : block.getContinuations()) {
    ExecContext newContext=c.childContext(currContext);
    for (    Block childB : c.getBlocks()) {
      if (rearrangeWaitsRec(logger,fn,childB,newContext)) {
        changed=true;
      }
    }
    if (c.getType() == ContinuationType.WAIT_STATEMENT) {
      WaitStatement wait=(WaitStatement)c;
      if (tryReduce(logger,fn,currContext,newContext,wait)) {
        toInline.add(wait);
      }
 else       if (squashWaits(logger,fn,wait,newContext)) {
        changed=true;
      }
    }
  }
  for (  WaitStatement w : toInline) {
    w.inlineInto(block);
    changed=true;
  }
  return changed;
}","private boolean rearrangeWaitsRec(Logger logger,Function fn,Block block,ExecContext currContext){
  List<WaitStatement> toInline=new ArrayList<WaitStatement>();
  boolean changed=false;
  for (  Continuation c : block.getContinuations()) {
    ExecContext newContext=c.childContext(currContext);
    for (    Block childB : c.getBlocks()) {
      if (rearrangeWaits(logger,fn,childB,newContext)) {
        changed=true;
      }
    }
    if (c.getType() == ContinuationType.WAIT_STATEMENT) {
      WaitStatement wait=(WaitStatement)c;
      if (tryReduce(logger,fn,currContext,newContext,wait)) {
        toInline.add(wait);
      }
 else       if (squashWaits(logger,fn,wait,newContext)) {
        changed=true;
      }
    }
  }
  for (  WaitStatement w : toInline) {
    w.inlineInto(block);
    changed=true;
  }
  return changed;
}",0.9982089552238806
132962,"private static boolean mergeWaits(Logger logger,Function fn,Block block){
  boolean changed=false;
  boolean fin;
  do {
    fin=true;
    MultiMap<Var,WaitStatement> waitMap=buildWaitMap(block);
    Var winner=mostSharedVar(waitMap);
    if (winner != null) {
      fin=false;
      changed=true;
      List<WaitStatement> waits=waitMap.get(winner);
      assert(waits != null && waits.size() >= 2);
      logger.trace(""String_Node_Str"" + waits.size() + ""String_Node_Str"");
      boolean explicit=false;
      boolean allRecursive=true;
      Set<Var> intersection=null;
      for (      WaitStatement wait : waits) {
        Set<Var> waitVars=new HashSet<Var>();
        for (        WaitVar wv : wait.getWaitVars()) {
          waitVars.add(wv.var);
        }
        if (intersection == null) {
          intersection=waitVars;
        }
 else {
          intersection.retainAll(waitVars);
        }
        explicit=explicit || wait.getMode() != WaitMode.WAIT_ONLY;
        allRecursive=allRecursive && wait.isRecursive();
      }
      assert(intersection != null && !intersection.isEmpty());
      WaitStatement newWait=new WaitStatement(fn.getName() + ""String_Node_Str"",WaitVar.makeList(intersection,false),PassedVar.NONE,Var.NONE,null,WaitMode.WAIT_ONLY,allRecursive,TaskMode.LOCAL);
      for (      WaitStatement wait : waits) {
        if (allRecursive) {
          wait.tryInline(Collections.<Var>emptySet(),intersection,false);
        }
 else {
          wait.tryInline(intersection,Collections.<Var>emptySet(),false);
        }
        if (wait.getWaitVars().isEmpty() && wait.getMode() != WaitMode.TASK_DISPATCH) {
          newWait.getBlock().insertInline(wait.getBlock());
        }
 else {
          wait.setParent(newWait.getBlock());
          newWait.getBlock().addContinuation(wait);
        }
      }
      block.addContinuation(newWait);
      block.removeContinuations(waits);
    }
    changed=changed || !fin;
  }
 while (!fin);
  return changed;
}","private boolean mergeWaits(Logger logger,Function fn,Block block){
  boolean changed=false;
  boolean fin;
  do {
    fin=true;
    MultiMap<Var,WaitStatement> waitMap=buildWaitMap(block);
    Var winner=mostSharedVar(waitMap);
    if (winner != null) {
      fin=false;
      changed=true;
      List<WaitStatement> waits=waitMap.get(winner);
      assert(waits != null && waits.size() >= 2);
      logger.trace(""String_Node_Str"" + waits.size() + ""String_Node_Str"");
      boolean explicit=false;
      boolean allRecursive=true;
      Set<Var> explicitVars=new HashSet<Var>();
      Set<Var> notExplicitVars=new HashSet<Var>();
      Set<Var> intersection=null;
      for (      WaitStatement wait : waits) {
        Set<Var> waitVars=new HashSet<Var>();
        for (        WaitVar wv : wait.getWaitVars()) {
          waitVars.add(wv.var);
          if (wv.explicit) {
            explicitVars.add(wv.var);
          }
 else {
            notExplicitVars.add(wv.var);
          }
        }
        if (intersection == null) {
          intersection=waitVars;
        }
 else {
          intersection.retainAll(waitVars);
        }
        explicit=explicit || wait.getMode() != WaitMode.WAIT_ONLY;
        allRecursive=allRecursive && wait.isRecursive();
      }
      assert(intersection != null && !intersection.isEmpty());
      List<WaitVar> mergedWaitVars=new ArrayList<WaitVar>();
      for (      Var v : intersection) {
        boolean mergedExplicit=explicitVars.contains(v) && !notExplicitVars.contains(v);
        mergedWaitVars.add(new WaitVar(v,mergedExplicit));
      }
      WaitStatement newWait=new WaitStatement(fn.getName() + ""String_Node_Str"",mergedWaitVars,PassedVar.NONE,Var.NONE,null,WaitMode.WAIT_ONLY,allRecursive,TaskMode.LOCAL);
      for (      WaitStatement wait : waits) {
        wait.removeWaitVars(mergedWaitVars,allRecursive,retainExplicit);
        if (wait.getWaitVars().isEmpty() && wait.getMode() != WaitMode.TASK_DISPATCH) {
          newWait.getBlock().insertInline(wait.getBlock());
        }
 else {
          wait.setParent(newWait.getBlock());
          newWait.getBlock().addContinuation(wait);
        }
      }
      block.addContinuation(newWait);
      block.removeContinuations(waits);
    }
    changed=changed || !fin;
  }
 while (!fin);
  return changed;
}",0.7774411559077138
132963,"/** 
 * @param newCV
 * @param replace for debugging purposes, set to true if we intend to replace
 */
public void addComputedValue(ComputedValue newCV,boolean replace){
  boolean outClosed=newCV.isOutClosed();
  if (isAvailable(newCV)) {
    if (!replace) {
      throw new STCRuntimeError(""String_Node_Str"" + getLocation(newCV) + ""String_Node_Str""+ newCV);
    }
  }
 else   if (replace) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + newCV + ""String_Node_Str"");
  }
  Arg valLoc=newCV.getValLocation();
  Opcode op=newCV.getOp();
  availableVals.put(newCV,valLoc);
  if (valLoc.isVar()) {
    varContents.put(valLoc.getVar(),newCV);
  }
  if (valLoc.isVar() && outClosed) {
    close(valLoc.getVar(),false);
  }
  if (op == Opcode.LOAD_BOOL || op == Opcode.LOAD_FLOAT || op == Opcode.LOAD_INT || op == Opcode.LOAD_STRING || op == Opcode.LOAD_VOID || op == Opcode.LOAD_FILE) {
    close(newCV.getInput(0).getVar(),true);
  }
}","/** 
 * @param newCV
 * @param replace for debugging purposes, set to true if we intend to replace
 */
public void addComputedValue(ComputedValue newCV,boolean replace){
  boolean outClosed=newCV.isOutClosed();
  if (isAvailable(newCV)) {
    if (!replace) {
      throw new STCRuntimeError(""String_Node_Str"" + getLocation(newCV) + ""String_Node_Str""+ newCV);
    }
  }
 else   if (replace) {
    throw new STCRuntimeError(""String_Node_Str"" + ""String_Node_Str"" + newCV + ""String_Node_Str"");
  }
  Arg valLoc=newCV.getValLocation();
  Opcode op=newCV.getOp();
  availableVals.put(newCV,valLoc);
  if (valLoc.isVar()) {
    varContents.put(valLoc.getVar(),newCV);
    if (outClosed) {
      close(valLoc.getVar(),false);
    }
  }
  if (op == Opcode.LOAD_BOOL || op == Opcode.LOAD_FLOAT || op == Opcode.LOAD_INT || op == Opcode.LOAD_STRING || op == Opcode.LOAD_VOID || op == Opcode.LOAD_FILE) {
    close(newCV.getInput(0).getVar(),true);
  }
}",0.9852164730728616
132964,"/** 
 * Do a kind of dataflow analysis where we try to identify which futures are closed at different points in the program. This allows us to switch to lower-overhead versions of many operations.
 * @param logger
 * @param program
 * @throws InvalidOptionException
 * @throws InvalidWriteException
 */
public void optimize(Logger logger,Program program) throws InvalidOptionException, InvalidWriteException {
  for (  Function f : program.getFunctions()) {
    boolean changes;
    int pass=1;
    do {
      logger.trace(""String_Node_Str"" + f.getName() + ""String_Node_Str""+ pass);
      changes=forwardDataflow(logger,program,f,ExecContext.CONTROL,f.mainBlock(),null,null,null);
      liftWait(logger,program,f);
      pass++;
    }
 while (changes);
  }
}","/** 
 * Do a kind of dataflow analysis where we try to identify which futures are closed at different points in the program. This allows us to switch to lower-overhead versions of many operations.
 * @param logger
 * @param program
 * @throws InvalidOptionException
 * @throws InvalidWriteException
 */
public void optimize(Logger logger,Program program) throws InvalidOptionException, InvalidWriteException {
}",0.7031650983746792
132965,"@Override public Block tryInline(Set<Var> closedVars,Set<Var> recClosedVars,boolean keepExplicitDependencies){
  boolean varsLeft=false;
  ListIterator<WaitVar> it=waitVars.listIterator();
  while (it.hasNext()) {
    WaitVar wv=it.next();
    if (keepExplicitDependencies && wv.explicit) {
      varsLeft=true;
    }
 else     if ((closedVars.contains(wv) && !recursionRequired(wv.var)) || recClosedVars.contains(wv)) {
      it.remove();
    }
 else {
      varsLeft=true;
    }
  }
  if (varsLeft || mode == WaitMode.TASK_DISPATCH) {
    return null;
  }
 else {
    return block;
  }
}","@Override public Block tryInline(Set<Var> closedVars,Set<Var> recClosedVars,boolean keepExplicitDependencies){
  boolean varsLeft=false;
  ListIterator<WaitVar> it=waitVars.listIterator();
  while (it.hasNext()) {
    WaitVar wv=it.next();
    if (keepExplicitDependencies && wv.explicit) {
      varsLeft=true;
    }
 else     if ((closedVars.contains(wv.var) && !recursionRequired(wv.var)) || recClosedVars.contains(wv.var)) {
      it.remove();
    }
 else {
      varsLeft=true;
    }
  }
  if (varsLeft || mode == WaitMode.TASK_DISPATCH) {
    return null;
  }
 else {
    return block;
  }
}",0.9932546374367622
132966,"@Override public boolean equals(Object other){
  if (!(other instanceof WaitVar))   return false;
  WaitVar owv=(WaitVar)other;
  return owv.explicit == explicit && owv.var.equals(this.var);
}","@Override public boolean equals(Object other){
  if (!(other instanceof WaitVar))   throw new STCRuntimeError(""String_Node_Str"" + other + ""String_Node_Str""+ other.getClass());
  WaitVar owv=(WaitVar)other;
  return owv.explicit == explicit && owv.var.equals(this.var);
}",0.7792207792207793
132967,"/** 
 * If we have something of the below form, can just block  on a as far of function call protocol (..) f (a, b, c) { wait (a) { } }
 * @param logger
 * @param program
 * @param f
 */
private static void liftWait(Logger logger,Program program,Function f){
  if (!f.isAsync()) {
    return;
  }
  Block main=f.mainBlock();
  List<WaitVar> blockingVariables=findBlockingVariables(main);
  if (blockingVariables != null) {
    List<Var> locals=f.getInputList();
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + f.getName() + ""String_Node_Str""+ blockingVariables);
    }
    for (    WaitVar wv : blockingVariables) {
      boolean isConst=program.lookupGlobalConst(wv.var.name()) != null;
      if (!isConst && locals.contains(wv)) {
        f.addBlockingInput(wv);
      }
    }
  }
}","/** 
 * If we have something of the below form, can just block  on a as far of function call protocol (..) f (a, b, c) { wait (a) { } }
 * @param logger
 * @param program
 * @param f
 */
private static void liftWait(Logger logger,Program program,Function f){
  if (!f.isAsync()) {
    return;
  }
  Block main=f.mainBlock();
  List<WaitVar> blockingVariables=findBlockingVariables(main);
  if (blockingVariables != null) {
    List<Var> locals=f.getInputList();
    if (logger.isTraceEnabled()) {
      logger.trace(""String_Node_Str"" + f.getName() + ""String_Node_Str""+ blockingVariables);
    }
    for (    WaitVar wv : blockingVariables) {
      boolean isConst=program.lookupGlobalConst(wv.var.name()) != null;
      if (!isConst && locals.contains(wv.var)) {
        f.addBlockingInput(wv);
      }
    }
  }
}",0.9975369458128078
132968,"public boolean varNameUsed(String name){
  Deque<Block> blocks=new ArrayDeque<Block>();
  blocks.add(this.mainBlock);
  while (!blocks.isEmpty()) {
    Block curr=blocks.pop();
    for (    Var v : curr.variables) {
      if (v.name().equals(name))       return true;
    }
    for (    Continuation c : curr.getContinuations()) {
      for (      Var cv : c.constructDefinedVars())       if (cv.name().equals(name))       return true;
      for (      Block inner : c.getBlocks()) {
        blocks.push(inner);
      }
    }
  }
  return false;
}","public boolean varNameUsed(String name){
  if (Var.findByName(iList,name) != null || Var.findByName(oList,name) != null) {
    return true;
  }
  Deque<Block> blocks=new ArrayDeque<Block>();
  blocks.add(this.mainBlock);
  while (!blocks.isEmpty()) {
    Block curr=blocks.pop();
    if (Var.findByName(curr.variables,name) != null) {
      return true;
    }
    for (    Continuation c : curr.getContinuations()) {
      if (Var.findByName(c.constructDefinedVars(),name) != null) {
        return true;
      }
      for (      Block inner : c.getBlocks()) {
        blocks.push(inner);
      }
    }
  }
  return false;
}",0.5038428693424424
132969,"private void startForeachSplit(String procName,Var arrayVar,String contentsVar,int splitDegree,int leafDegree,boolean haveKeys,List<PassedVar> usedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  pointStack.peek().add(Turbine.containerSize(TCLTMP_CONTAINER_SIZE,varToExpr(arrayVar)));
  Value containerSize=new Value(TCLTMP_CONTAINER_SIZE);
  Expression lastIndex=Square.arithExpr(containerSize,new Token(""String_Node_Str""),new LiteralInt(1));
  handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  ArrayList<PassedVar> splitUsedVars=new ArrayList<PassedVar>(usedVars);
  splitUsedVars.add(new PassedVar(arrayVar,false));
  startRangeSplit(procName,splitUsedVars,perIterIncrs,splitDegree,leafDegree,LiteralInt.ZERO,lastIndex,LiteralInt.ONE);
  pointStack.peek().add(new SetVariable(TCLTMP_SPLITLEN,Square.arithExpr(new Token(String.format(""String_Node_Str"",TCLTMP_RANGE_HI,TCLTMP_RANGE_LO)))));
  pointStack.peek().add(Turbine.containerContents(contentsVar,varToExpr(arrayVar),haveKeys,new Value(TCLTMP_SPLITLEN),TCLTMP_RANGE_LO_V));
}","private void startForeachSplit(String procName,Var arrayVar,String contentsVar,int splitDegree,int leafDegree,boolean haveKeys,List<PassedVar> usedVars,List<RefCount> perIterIncrs,MultiMap<Var,RefCount> constIncrs){
  pointStack.peek().add(Turbine.containerSize(TCLTMP_CONTAINER_SIZE,varToExpr(arrayVar)));
  Value containerSize=new Value(TCLTMP_CONTAINER_SIZE);
  Expression lastIndex=Square.arithExpr(containerSize,new Token(""String_Node_Str""),new LiteralInt(1));
  handleForeachContainerRefcounts(perIterIncrs,constIncrs,containerSize);
  ArrayList<PassedVar> splitUsedVars=new ArrayList<PassedVar>(usedVars);
  if (!PassedVar.contains(splitUsedVars,arrayVar)) {
    splitUsedVars.add(new PassedVar(arrayVar,false));
  }
  startRangeSplit(procName,splitUsedVars,perIterIncrs,splitDegree,leafDegree,LiteralInt.ZERO,lastIndex,LiteralInt.ONE);
  pointStack.peek().add(new SetVariable(TCLTMP_SPLITLEN,Square.arithExpr(new Token(String.format(""String_Node_Str"",TCLTMP_RANGE_HI,TCLTMP_RANGE_LO)))));
  pointStack.peek().add(Turbine.containerContents(contentsVar,varToExpr(arrayVar),haveKeys,new Value(TCLTMP_SPLITLEN),TCLTMP_RANGE_LO_V));
}",0.9733634311512416
132970,"private void decrementReaders(List<Var> vars,Expression incr){
  incrementReaders(vars,incr);
}","private void decrementReaders(List<Var> vars,Expression decr){
  pointStack.peek().append(buildIncReaders(vars,decr,true));
}",0.8
132971,"public static SetVariable integerDecrGet(String target,Value src,Expression decr){
  return new SetVariable(target,new Square(RETRIEVE_INTEGER,src,decr));
}","public static SetVariable integerDecrGet(String target,Value src,Expression decr){
  return new SetVariable(target,new Square(RETRIEVE_INTEGER,src,CACHED,decr));
}",0.9780564263322884
132972,"public static SetVariable floatDecrGet(String target,Value src,Expression decr){
  return new SetVariable(target,new Square(RETRIEVE_FLOAT,src,decr));
}","public static SetVariable floatDecrGet(String target,Value src,Expression decr){
  return new SetVariable(target,new Square(RETRIEVE_FLOAT,src,CACHED,decr));
}",0.977491961414791
132973,"public static SetVariable stringDecrGet(String target,Value src,Expression decr){
  return new SetVariable(target,new Square(RETRIEVE_STRING,src,decr));
}","public static SetVariable stringDecrGet(String target,Value src,Expression decr){
  return new SetVariable(target,new Square(RETRIEVE_STRING,src,CACHED,decr));
}",0.9777777777777776
132974,"/** 
 * @param logger
 * @param fn
 * @param block
 * @param readIncrements pre-existing decrements
 * @param writeIncrements -existing decrements
 * @param parentAssignedAliasVars assign alias vars from parent blocks that we can immediately manipulate refcount of
 */
private void fixBlockRefCounting(Logger logger,Function fn,Block block,Counters<Var> readIncrements,Counters<Var> writeIncrements,Set<Var> parentAssignedAliasVars){
  boolean cancelEnabled=cancelEnabled();
  ListIterator<Instruction> instIt=block.instructionIterator();
  while (instIt.hasNext()) {
    Instruction inst=instIt.next();
    updateInstructionRefCount(inst,readIncrements,writeIncrements);
    if (!cancelEnabled) {
      dumpIncrements(instIt,readIncrements,writeIncrements);
    }
  }
  for (  Continuation cont : block.getContinuations()) {
    addIncrementsForCont(cont,readIncrements,writeIncrements);
    if (!cancelEnabled) {
      dumpIncrements(instIt,readIncrements,writeIncrements);
    }
  }
  updateDecrementCounts(block,fn,readIncrements,writeIncrements);
  dumpDecrements(block,readIncrements,writeIncrements);
  if (cancelEnabled()) {
    updateBlockRefcounting(logger,fn,block,readIncrements,writeIncrements,parentAssignedAliasVars);
  }
}","/** 
 * @param logger
 * @param fn
 * @param block
 * @param readIncrements pre-existing decrements
 * @param writeIncrements -existing decrements
 * @param parentAssignedAliasVars assign alias vars from parent blocks that we can immediately manipulate refcount of
 */
private void fixBlockRefCounting(Logger logger,Function fn,Block block,Counters<Var> readIncrements,Counters<Var> writeIncrements,Set<Var> parentAssignedAliasVars){
  boolean cancelEnabled=cancelEnabled();
  ListIterator<Instruction> instIt=block.instructionIterator();
  while (instIt.hasNext()) {
    Instruction inst=instIt.next();
    updateInstructionRefCount(inst,readIncrements,writeIncrements);
    if (!cancelEnabled) {
      dumpIncrements(instIt,readIncrements,writeIncrements);
    }
  }
  for (  Continuation cont : block.getContinuations()) {
    addIncrementsForCont(cont,readIncrements,writeIncrements);
    if (!cancelEnabled) {
      dumpIncrements(instIt,readIncrements,writeIncrements);
    }
  }
  updateDecrementCounts(block,fn,readIncrements,writeIncrements);
  if (!cancelEnabled()) {
    dumpDecrements(block,readIncrements,writeIncrements);
  }
  if (cancelEnabled()) {
    updateBlockRefcounting(logger,fn,block,readIncrements,writeIncrements,parentAssignedAliasVars);
  }
}",0.9872408293460924
132975,"private void dumpDecrements(Block block,Counters<Var> readIncrements,Counters<Var> writeIncrements){
  for (  Entry<Var,Long> e : readIncrements.entries()) {
    Var var=e.getKey();
    Arg amount=Arg.createIntLit(e.getValue() * -1);
    block.addCleanup(var,TurbineOp.decrRef(var,amount));
  }
  for (  Entry<Var,Long> e : writeIncrements.entries()) {
    Var var=e.getKey();
    Arg amount=Arg.createIntLit(e.getValue() * -1);
    block.addCleanup(var,TurbineOp.decrWriters(var,amount));
  }
}","private void dumpDecrements(Block block,Counters<Var> readIncrements,Counters<Var> writeIncrements){
  for (  Entry<Var,Long> e : readIncrements.entries()) {
    Var var=e.getKey();
    Arg amount=Arg.createIntLit(e.getValue() * -1);
    block.addCleanup(var,TurbineOp.decrRef(var,amount));
  }
  for (  Entry<Var,Long> e : writeIncrements.entries()) {
    Var var=e.getKey();
    Arg amount=Arg.createIntLit(e.getValue() * -1);
    block.addCleanup(var,TurbineOp.decrWriters(var,amount));
  }
  readIncrements.resetAll();
  writeIncrements.resetAll();
}",0.9437559580552908
132976,"/** 
 * Update variable passing for nested continuation
 * @param logger
 * @param function
 * @param outerCx exec context outside of continuation
 * @param continuation
 * @param visible
 * @param referencedGlobals
 * @param outerBlockVars
 * @param neededVars
 * @param updateLists 
 */
private static void fixupContinuationRec(Logger logger,Function function,ExecContext outerCx,Continuation continuation,HierarchicalSet<Var> visible,Set<Var> referencedGlobals,Set<Var> outerBlockVars,Set<Var> read,Set<Var> written,boolean updateLists){
  List<Var> constructVars=continuation.constructDefinedVars();
  ExecContext innerCx=continuation.childContext(outerCx);
  for (  Block innerBlock : continuation.getBlocks()) {
    HierarchicalSet<Var> childVisible=visible.makeChild();
    for (    Var v : constructVars) {
      childVisible.add(v);
    }
    Pair<Set<Var>,Set<Var>> inner=fixupBlockRec(logger,function,innerBlock,innerCx,childVisible,referencedGlobals,updateLists);
    Set<Var> innerRead=inner.val1;
    Set<Var> innerWritten=inner.val2;
    if (!constructVars.isEmpty()) {
      innerRead.removeAll(constructVars);
      innerWritten.removeAll(constructVars);
    }
    if (continuation.inheritsParentVars()) {
      innerRead.removeAll(outerBlockVars);
      read.addAll(innerRead);
      innerWritten.removeAll(outerBlockVars);
      written.addAll(innerWritten);
    }
 else     if (updateLists) {
      rebuildContinuationPassedVars(function,continuation,visible,outerBlockVars,read,innerRead,written,innerWritten);
      rebuildContinuationKeepOpenVars(function,continuation,visible,outerBlockVars,written,innerWritten);
    }
  }
}","/** 
 * Update variable passing for nested continuation
 * @param logger
 * @param function
 * @param outerCx exec context outside of continuation
 * @param continuation
 * @param visible
 * @param referencedGlobals
 * @param outerBlockVars
 * @param neededVars
 * @param updateLists 
 */
private static void fixupContinuationRec(Logger logger,Function function,ExecContext outerCx,Continuation continuation,HierarchicalSet<Var> visible,Set<Var> referencedGlobals,Set<Var> outerBlockVars,Set<Var> read,Set<Var> written,boolean updateLists){
  List<Var> constructVars=continuation.constructDefinedVars(ContVarDefType.NEW_DEF);
  ExecContext innerCx=continuation.childContext(outerCx);
  for (  Block innerBlock : continuation.getBlocks()) {
    HierarchicalSet<Var> childVisible=visible.makeChild();
    for (    Var v : constructVars) {
      childVisible.add(v);
    }
    Pair<Set<Var>,Set<Var>> inner=fixupBlockRec(logger,function,innerBlock,innerCx,childVisible,referencedGlobals,updateLists);
    Set<Var> innerRead=inner.val1;
    Set<Var> innerWritten=inner.val2;
    if (!constructVars.isEmpty()) {
      innerRead.removeAll(constructVars);
      innerWritten.removeAll(constructVars);
    }
    if (continuation.inheritsParentVars()) {
      innerRead.removeAll(outerBlockVars);
      read.addAll(innerRead);
      innerWritten.removeAll(outerBlockVars);
      written.addAll(innerWritten);
    }
 else     if (updateLists) {
      rebuildContinuationPassedVars(function,continuation,visible,outerBlockVars,read,innerRead,written,innerWritten);
      rebuildContinuationKeepOpenVars(function,continuation,visible,outerBlockVars,written,innerWritten);
    }
  }
}",0.9933734939759036
132977,"private void checkUniqueVarNames(Logger logger,Program program,Function fn,Block block,Map<String,Var> declared){
  for (  Var v : block.getVariables()) {
    checkVarUnique(logger,fn,declared,v);
    if (v.isMapped()) {
      assert(declared.containsKey(v.mapping().name()));
    }
  }
  checkVarReferences(logger,block,declared);
  if (checkCleanups)   checkCleanups(fn,block);
  for (  Continuation c : block.getContinuations()) {
    for (    Var v : c.constructDefinedVars(true)) {
      checkVarUnique(logger,fn,declared,v);
    }
    for (    Block inner : c.getBlocks()) {
      checkUniqueVarNames(logger,program,fn,inner,declared);
    }
  }
}","private void checkUniqueVarNames(Logger logger,Program program,Function fn,Block block,Map<String,Var> declared){
  for (  Var v : block.getVariables()) {
    checkVarUnique(logger,fn,declared,v);
    if (v.isMapped()) {
      assert(declared.containsKey(v.mapping().name()));
    }
  }
  checkVarReferences(logger,block,declared);
  if (checkCleanups)   checkCleanups(fn,block);
  for (  Continuation c : block.getContinuations()) {
    for (    Var v : c.constructDefinedVars(ContVarDefType.NEW_DEF)) {
      checkVarUnique(logger,fn,declared,v);
    }
    for (    Block inner : c.getBlocks()) {
      checkUniqueVarNames(logger,program,fn,inner,declared);
    }
  }
}",0.9803625377643505
132978,"@Override public List<Var> constructDefinedVars(boolean includeRedefs){
  if (loopCounterVar != null) {
    return Arrays.asList(loopVar,loopCounterVar);
  }
 else {
    return Arrays.asList(loopVar);
  }
}","@Override public List<Var> constructDefinedVars(ContVarDefType type){
  if (type.includesNewDefs()) {
    if (loopCounterVar != null) {
      return Arrays.asList(loopVar,loopCounterVar);
    }
 else {
      return Arrays.asList(loopVar);
    }
  }
 else {
    return Collections.emptyList();
  }
}",0.6984126984126984
132979,"@Override public void removeRedef(Var oldV,Var newV){
  for (int i=0; i < loopVars.size(); i++) {
    Var loopVar=loopVars.get(i);
    if (loopVar.equals(oldV)) {
      assert(!this.definedHere.get(i));
      this.loopVars.set(i,newV);
      this.definedHere.set(i,true);
    }
  }
}","@Override public void removeRedef(Var oldV,Var newV){
}",0.3254437869822485
132980,"@Override public List<Var> constructDefinedVars(boolean includeRedefs){
  ArrayList<Var> defVars=new ArrayList<Var>();
  for (int i=0; i < this.loopVars.size(); i++) {
    if (includeRedefs || this.definedHere.get(i)) {
      defVars.add(this.loopVars.get(i));
    }
  }
  return defVars;
}","@Override public List<Var> constructDefinedVars(ContVarDefType type){
  ArrayList<Var> defVars=new ArrayList<Var>();
  for (int i=0; i < this.loopVars.size(); i++) {
    Boolean defHere=this.definedHere.get(i);
    Var loopVar=this.loopVars.get(i);
    if (type.includesNewDefs() && defHere) {
      defVars.add(loopVar);
    }
 else     if (type.includesRedefs() && !defHere) {
      defVars.add(loopVar);
    }
  }
  return defVars;
}",0.6115702479338843
132981,"/** 
 * Try to hoist this instruction
 * @param logger
 * @param inst
 * @param state
 * @return true if hoisted
 */
private boolean tryHoist(Logger logger,Instruction inst,HoistTracking state){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ state.maxHoist+ ""String_Node_Str""+ state.maxLoopHoist);
  }
  if (inst.hasSideEffects()) {
    logger.trace(""String_Node_Str"");
    return false;
  }
  int maxHoist=state.maxHoist;
  for (  Arg in : inst.getInputs()) {
    if (maxHoist <= 0)     return false;
    if (in.isVar()) {
      Var inVar=in.getVar();
      maxHoist=Math.min(maxHoist,maxInputHoist(logger,state,inVar));
    }
  }
  for (  Var readOutput : inst.getReadOutputs()) {
    maxHoist=Math.min(maxHoist,maxInputHoist(logger,state,readOutput));
  }
  for (  Var out : inst.getOutputs()) {
    if (trackDeclares(out)) {
      int declareDepth=state.declareMap.getDepth(out);
      if (logger.isTraceEnabled())       logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ declareDepth);
      assert(declareDepth >= 0);
      if (declareDepth > state.maxLoopHoist && !inst.isIdempotent()) {
        maxHoist=Math.min(maxHoist,state.maxLoopHoist);
        if (logger.isTraceEnabled())         logger.trace(""String_Node_Str"" + state.maxLoopHoist);
      }
    }
  }
  for (  Var out : inst.getOutputs()) {
    if (out.storage() == VarStorage.ALIAS) {
      if (!inst.getInitializedAliases().contains(out)) {
        int initDepth=state.initializedMap.getDepth(out);
        if (logger.isTraceEnabled())         logger.trace(""String_Node_Str"" + initDepth + ""String_Node_Str""+ ""String_Node_Str""+ out);
        maxHoist=Math.min(maxHoist,initDepth);
        return false;
      }
    }
  }
  if (logger.isTraceEnabled())   logger.trace(""String_Node_Str"" + maxHoist);
  if (maxHoist > 0) {
    doHoist(logger,inst,maxHoist,state);
    return true;
  }
 else {
    return false;
  }
}","/** 
 * Try to hoist this instruction
 * @param logger
 * @param inst
 * @param state
 * @return true if hoisted
 */
private boolean tryHoist(Logger logger,Instruction inst,HoistTracking state){
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + inst + ""String_Node_Str""+ state.maxHoist+ ""String_Node_Str""+ state.maxLoopHoist);
  }
  if (inst.hasSideEffects()) {
    logger.trace(""String_Node_Str"");
    return false;
  }
  int maxHoist=state.maxHoist;
  for (  Arg in : inst.getInputs()) {
    if (maxHoist <= 0)     return false;
    if (in.isVar()) {
      Var inVar=in.getVar();
      maxHoist=Math.min(maxHoist,maxInputHoist(logger,state,inVar));
    }
  }
  for (  Var readOutput : inst.getReadOutputs()) {
    maxHoist=Math.min(maxHoist,maxInputHoist(logger,state,readOutput));
  }
  for (  Var out : inst.getOutputs()) {
    if (trackDeclares(out)) {
      int declareDepth=state.declareMap.getDepth(out);
      if (logger.isTraceEnabled())       logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ declareDepth);
      assert(declareDepth >= 0);
      if (declareDepth > state.maxLoopHoist && !inst.isIdempotent()) {
        maxHoist=Math.min(maxHoist,state.maxLoopHoist);
        if (logger.isTraceEnabled())         logger.trace(""String_Node_Str"" + state.maxLoopHoist);
      }
    }
  }
  for (  Var out : inst.getPiecewiseAssignedOutputs()) {
    maxHoist=Math.min(maxHoist,state.maxLoopHoist);
    if (logger.isTraceEnabled())     logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ ""String_Node_Str""+ state.maxLoopHoist);
  }
  for (  Var out : inst.getOutputs()) {
    if (out.storage() == VarStorage.ALIAS) {
      if (!inst.getInitializedAliases().contains(out)) {
        int initDepth=state.initializedMap.getDepth(out);
        if (logger.isTraceEnabled())         logger.trace(""String_Node_Str"" + initDepth + ""String_Node_Str""+ ""String_Node_Str""+ out);
        maxHoist=Math.min(maxHoist,initDepth);
        return false;
      }
    }
  }
  if (logger.isTraceEnabled())   logger.trace(""String_Node_Str"" + maxHoist);
  if (maxHoist > 0) {
    doHoist(logger,inst,maxHoist,state);
    return true;
  }
 else {
    return false;
  }
}",0.9398350315380885
132982,"private void updateInstructionRefCount(Instruction inst,Counters<Var> readIncrements,Counters<Var> writeIncrements){
  Pair<List<Var>,List<Var>> refIncrs=inst.getIncrVars(functionMap);
  List<Var> readIncrVars=refIncrs.val1;
  List<Var> writeIncrVars=refIncrs.val2;
  for (  Var v : readIncrVars) {
    if (RefCounting.hasReadRefCount(v)) {
      readIncrements.increment(v);
    }
  }
  for (  Var v : writeIncrVars) {
    if (RefCounting.hasWriteRefCount(v)) {
      writeIncrements.increment(v);
    }
  }
  if (inst.op == Opcode.LOOP_BREAK) {
    LoopBreak loopBreak=(LoopBreak)inst;
    for (    Var ko : loopBreak.getKeepOpenVars()) {
      assert(RefCounting.hasWriteRefCount(ko));
      writeIncrements.decrement(ko);
    }
    for (    PassedVar pass : loopBreak.getLoopUsedVars()) {
      if (!pass.writeOnly && RefCounting.hasReadRefCount(pass.var)) {
        readIncrements.decrement(pass.var);
      }
    }
  }
}","private void updateInstructionRefCount(Instruction inst,Counters<Var> readIncrements,Counters<Var> writeIncrements){
  Pair<List<Var>,List<Var>> refIncrs=inst.getIncrVars(functionMap);
  List<Var> readIncrVars=refIncrs.val1;
  List<Var> writeIncrVars=refIncrs.val2;
  for (  Var v : readIncrVars) {
    if (RefCounting.hasReadRefCount(v)) {
      readIncrements.increment(v);
    }
  }
  for (  Var v : writeIncrVars) {
    if (RefCounting.hasWriteRefCount(v)) {
      writeIncrements.increment(v);
    }
  }
  if (inst.op == Opcode.COPY_REF) {
    Var newAlias=inst.getOutput(0);
    if (RefCounting.hasReadRefCount(newAlias)) {
      readIncrements.decrement(newAlias);
    }
    if (RefCounting.hasWriteRefCount(newAlias)) {
      writeIncrements.decrement(newAlias);
    }
  }
  if (inst.op == Opcode.LOOP_BREAK) {
    LoopBreak loopBreak=(LoopBreak)inst;
    for (    Var ko : loopBreak.getKeepOpenVars()) {
      assert(RefCounting.hasWriteRefCount(ko));
      writeIncrements.decrement(ko);
    }
    for (    PassedVar pass : loopBreak.getLoopUsedVars()) {
      if (!pass.writeOnly && RefCounting.hasReadRefCount(pass.var)) {
        readIncrements.decrement(pass.var);
      }
    }
  }
}",0.871939736346516
132983,"public Pair<Var,Var> getComponentAlias(){
switch (op) {
case ARRAY_CREATE_NESTED_IMM:
case ARRAY_CREATE_NESTED_FUTURE:
    return Pair.create(getOutput(0),getOutput(1));
case ARRAYREF_CREATE_NESTED_IMM:
case ARRAYREF_CREATE_NESTED_FUTURE:
  return Pair.create(getOutput(0),getOutput(2));
case LOAD_REF:
return Pair.create(getOutput(0),getInput(0).getVar());
case ADDRESS_OF:
return Pair.create(getOutput(0),getInput(0).getVar());
case STRUCT_LOOKUP:
case STRUCTREF_LOOKUP:
return Pair.create(getOutput(0),getInput(0).getVar());
default :
return null;
}
}","public Pair<Var,Var> getComponentAlias(){
switch (op) {
case ARRAY_CREATE_NESTED_IMM:
case ARRAY_CREATE_NESTED_FUTURE:
    return Pair.create(getOutput(0),getOutput(1));
case ARRAYREF_CREATE_NESTED_IMM:
case ARRAYREF_CREATE_NESTED_FUTURE:
  return Pair.create(getOutput(0),getOutput(2));
case LOAD_REF:
return Pair.create(getOutput(0),getInput(0).getVar());
case COPY_REF:
return Pair.create(getOutput(0),getInput(0).getVar());
case ADDRESS_OF:
return Pair.create(getOutput(0),getInput(0).getVar());
case STRUCT_LOOKUP:
case STRUCTREF_LOOKUP:
return Pair.create(getOutput(0),getInput(0).getVar());
default :
return null;
}
}",0.9405772495755518
132984,"private void checkUniqueVarNames(Logger logger,Program program,Function fn,Block block,Map<String,Var> declared){
  for (  Var v : block.getVariables()) {
    checkVarUnique(logger,fn,declared,v);
    if (v.isMapped()) {
      assert(declared.containsKey(v.mapping().name()));
    }
  }
  checkVarReferences(logger,block,declared);
  if (checkCleanups)   checkCleanups(fn,block);
  for (  Continuation c : block.getContinuations()) {
    for (    Var v : c.constructDefinedVars()) {
      checkVarUnique(logger,fn,declared,v);
    }
    for (    Block inner : c.getBlocks()) {
      checkUniqueVarNames(logger,program,fn,inner,declared);
    }
  }
}","private void checkUniqueVarNames(Logger logger,Program program,Function fn,Block block,Map<String,Var> declared){
  for (  Var v : block.getVariables()) {
    checkVarUnique(logger,fn,declared,v);
    if (v.isMapped()) {
      assert(declared.containsKey(v.mapping().name()));
    }
  }
  checkVarReferences(logger,block,declared);
  if (checkCleanups)   checkCleanups(fn,block);
  for (  Continuation c : block.getContinuations()) {
    for (    Var v : c.constructDefinedVars(true)) {
      checkVarUnique(logger,fn,declared,v);
    }
    for (    Block inner : c.getBlocks()) {
      checkUniqueVarNames(logger,program,fn,inner,declared);
    }
  }
}",0.9969278033794164
132985,"@Override public List<Var> constructDefinedVars(){
  if (loopCounterVar != null) {
    return Arrays.asList(loopVar,loopCounterVar);
  }
 else {
    return Arrays.asList(loopVar);
  }
}","@Override public List<Var> constructDefinedVars(boolean includeRedefs){
  if (loopCounterVar != null) {
    return Arrays.asList(loopVar,loopCounterVar);
  }
 else {
    return Arrays.asList(loopVar);
  }
}",0.9462915601023018
132986,"@Override public List<Var> constructDefinedVars(){
  return Var.NONE;
}","@Override public List<Var> constructDefinedVars(boolean includeRedefs){
  ArrayList<Var> defVars=new ArrayList<Var>();
  for (int i=0; i < this.loopVars.size(); i++) {
    if (includeRedefs || this.definedHere.get(i)) {
      defVars.add(this.loopVars.get(i));
    }
  }
  return defVars;
}",0.299168975069252
132987,"/** 
 * Find the set of variables required to be closed (recursively or not) to make progress in block.   If function inlining is enabled, exclude explicit waits since blocking inputs to a function are treated as data_only upon inlining and this can cause problems if they were previously explicit waits.
 * @param block
 * @return
 */
private static Set<Var> findBlockingVariables(Block block){
  HashSet<Var> blockingVariables=null;
  for (  Instruction inst : block.getInstructions()) {
    if (!ProgressOpcodes.isNonProgressOpcode(inst.op)) {
      return null;
    }
  }
  for (  Continuation c : block.getContinuations()) {
    List<BlockingVar> waitOnVars=c.blockingVars();
    List<Var> waitOn;
    if (waitOnVars == null) {
      waitOn=Var.NONE;
    }
 else {
      waitOn=new ArrayList<Var>(waitOnVars.size());
      for (      BlockingVar bv : waitOnVars) {
        waitOn.add(bv.var);
      }
    }
    assert(waitOn != null);
    if (blockingVariables == null) {
      blockingVariables=new HashSet<Var>(waitOn);
    }
 else {
      blockingVariables.retainAll(waitOn);
    }
    try {
      if (Settings.getBoolean(Settings.OPT_FUNCTION_INLINE)) {
        if (c.getType() == ContinuationType.WAIT_STATEMENT && ((WaitStatement)c).getMode() == WaitMode.EXPLICIT) {
          blockingVariables.removeAll(waitOn);
        }
      }
    }
 catch (    InvalidOptionException e) {
      throw new STCRuntimeError(e.getMessage());
    }
  }
  return blockingVariables;
}","/** 
 * Find the set of variables required to be closed (recursively or not) to make progress in block.   If function inlining is enabled, exclude explicit waits since blocking inputs to a function are treated as data_only upon inlining and this can cause problems if they were previously explicit waits.
 * @param block
 * @return
 */
private static Set<Var> findBlockingVariables(Block block){
  HashSet<Var> blockingVariables=null;
  for (  Instruction inst : block.getInstructions()) {
    if (!ProgressOpcodes.isNonProgressOpcode(inst.op)) {
      return null;
    }
  }
  for (  Continuation c : block.getContinuations()) {
    List<BlockingVar> waitOnVars=c.blockingVars(false);
    List<Var> waitOn;
    if (waitOnVars == null) {
      waitOn=Var.NONE;
    }
 else {
      waitOn=new ArrayList<Var>(waitOnVars.size());
      for (      BlockingVar bv : waitOnVars) {
        waitOn.add(bv.var);
      }
    }
    assert(waitOn != null);
    if (blockingVariables == null) {
      blockingVariables=new HashSet<Var>(waitOn);
    }
 else {
      blockingVariables.retainAll(waitOn);
    }
    try {
      if (Settings.getBoolean(Settings.OPT_FUNCTION_INLINE)) {
        if (c.getType() == ContinuationType.WAIT_STATEMENT && ((WaitStatement)c).getMode() == WaitMode.EXPLICIT) {
          blockingVariables.removeAll(waitOn);
        }
      }
    }
 catch (    InvalidOptionException e) {
      throw new STCRuntimeError(e.getMessage());
    }
  }
  return blockingVariables;
}",0.9983102399459276
132988,"private void addIncrementsForCont(Continuation cont,Counters<Var> readIncrements,Counters<Var> writeIncrements){
  if (cont.isAsync() && isSingleSpawnCont(cont)) {
    long incr=1;
    for (    Var keepOpen : cont.getKeepOpenVars()) {
      if (RefCounting.hasWriteRefCount(keepOpen)) {
        writeIncrements.add(keepOpen,incr);
      }
    }
    Set<Var> readIncrTmp=new HashSet<Var>();
    for (    PassedVar passedIn : cont.getPassedVars()) {
      if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
        readIncrTmp.add(passedIn.var);
      }
    }
    for (    BlockingVar blockingVar : cont.blockingVars()) {
      if (RefCounting.hasReadRefCount(blockingVar.var)) {
        readIncrTmp.add(blockingVar.var);
      }
    }
    for (    Var v : readIncrTmp) {
      readIncrements.add(v,incr);
    }
  }
}","private void addIncrementsForCont(Continuation cont,Counters<Var> readIncrements,Counters<Var> writeIncrements){
  if (cont.isAsync() && isSingleSpawnCont(cont)) {
    long incr=1;
    for (    Var keepOpen : cont.getKeepOpenVars()) {
      if (RefCounting.hasWriteRefCount(keepOpen)) {
        writeIncrements.add(keepOpen,incr);
      }
    }
    Set<Var> readIncrTmp=new HashSet<Var>();
    for (    PassedVar passedIn : cont.getPassedVars()) {
      if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
        readIncrTmp.add(passedIn.var);
      }
    }
    for (    BlockingVar blockingVar : cont.blockingVars(false)) {
      if (RefCounting.hasReadRefCount(blockingVar.var)) {
        readIncrTmp.add(blockingVar.var);
      }
    }
    for (    Var v : readIncrTmp) {
      readIncrements.add(v,incr);
    }
  }
}",0.9970184853905784
132989,"/** 
 * Add decrements that have to happen for continuation inside block
 * @param cont
 * @param readIncrements
 * @param writeIncrements
 */
private void addDecrementsForCont(Continuation cont,Counters<Var> readIncrements,Counters<Var> writeIncrements){
  if ((isSingleSpawnCont(cont) || isAsyncForeachLoop(cont)) && cont.getType() != ContinuationType.LOOP) {
    long amount=1;
    for (    Var keepOpen : cont.getKeepOpenVars()) {
      if (RefCounting.hasWriteRefCount(keepOpen)) {
        writeIncrements.decrement(keepOpen,amount);
      }
    }
    Set<Var> readIncrTmp=new HashSet<Var>();
    for (    PassedVar passedIn : cont.getPassedVars()) {
      if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
        readIncrTmp.add(passedIn.var);
      }
    }
    for (    BlockingVar blockingVar : cont.blockingVars()) {
      if (RefCounting.hasReadRefCount(blockingVar.var)) {
        readIncrTmp.add(blockingVar.var);
      }
    }
    for (    Var v : readIncrTmp) {
      readIncrements.decrement(v,amount);
    }
  }
}","/** 
 * Add decrements that have to happen for continuation inside block
 * @param cont
 * @param readIncrements
 * @param writeIncrements
 */
private void addDecrementsForCont(Continuation cont,Counters<Var> readIncrements,Counters<Var> writeIncrements){
  if ((isSingleSpawnCont(cont) || isAsyncForeachLoop(cont)) && cont.getType() != ContinuationType.LOOP) {
    long amount=1;
    for (    Var keepOpen : cont.getKeepOpenVars()) {
      if (RefCounting.hasWriteRefCount(keepOpen)) {
        writeIncrements.decrement(keepOpen,amount);
      }
    }
    Set<Var> readIncrTmp=new HashSet<Var>();
    for (    PassedVar passedIn : cont.getPassedVars()) {
      if (!passedIn.writeOnly && RefCounting.hasReadRefCount(passedIn.var)) {
        readIncrTmp.add(passedIn.var);
      }
    }
    for (    BlockingVar blockingVar : cont.blockingVars(false)) {
      if (RefCounting.hasReadRefCount(blockingVar.var)) {
        readIncrTmp.add(blockingVar.var);
      }
    }
    for (    Var v : readIncrTmp) {
      readIncrements.decrement(v,amount);
    }
  }
}",0.997629208155524
132990,"private static void findBlockingContinuations(Block block,MultiMap<Var,InstOrCont> waitMap){
  for (  Continuation c : block.getContinuations()) {
    List<BlockingVar> blockingVars=c.blockingVars();
    if (blockingVars != null) {
      for (      BlockingVar v : blockingVars) {
        waitMap.put(v.var,new InstOrCont(c));
      }
    }
  }
}","private static void findBlockingContinuations(Block block,MultiMap<Var,InstOrCont> waitMap){
  for (  Continuation c : block.getContinuations()) {
    List<BlockingVar> blockingVars=c.blockingVars(false);
    if (blockingVars != null) {
      for (      BlockingVar v : blockingVars) {
        waitMap.put(v.var,new InstOrCont(c));
      }
    }
  }
}",0.9928263988522238
132991,"@Override public List<BlockingVar> blockingVars(){
  return Collections.emptyList();
}","@Override public List<BlockingVar> blockingVars(boolean includeConstructDefined){
  return Collections.emptyList();
}",0.8472906403940886
132992,"/** 
 * Try to piggyback constant incrs/decrs from outside continuation. Reset counters in increments for any that are changed
 * @param increments
 * @param type
 * @param if true, try to piggyback decrements, if false, increments
 */
public void tryPiggyBack(Counters<Var> increments,RefCountType type,boolean decrement){
  for (  RefCount startIncr : startIncrements) {
    if (startIncr.type == type) {
      long incr=increments.getCount(startIncr.var);
      if ((decrement && incr < 0) || (!decrement && incr > 0)) {
        addConstantStartIncrement(startIncr.var,type,Arg.createIntLit(incr));
        increments.add(startIncr.var,-1 * incr);
      }
    }
  }
}","/** 
 * Try to piggyback constant incrs/decrs from outside continuation. Reset counters in increments for any that are changed
 * @param increments
 * @param type
 * @param decrement if true, try to piggyback decrements, if false, increments
 */
public void tryPiggyBack(Counters<Var> increments,RefCountType type,boolean decrement){
  for (  RefCount startIncr : startIncrements) {
    if (startIncr.type == type) {
      long incr=increments.getCount(startIncr.var);
      if ((decrement && incr < 0) || (!decrement && incr > 0)) {
        addConstantStartIncrement(startIncr.var,type,Arg.createIntLit(incr));
        increments.add(startIncr.var,-1 * incr);
      }
    }
  }
}",0.9925925925925926
132993,"@Override public List<BlockingVar> blockingVars(){
  ArrayList<BlockingVar> res=new ArrayList<BlockingVar>(waitVars.size());
  for (  Var wv : waitVars) {
    res.add(new BlockingVar(wv,this.recursive));
  }
  return res;
}","@Override public List<BlockingVar> blockingVars(boolean includeConstructDefined){
  ArrayList<BlockingVar> res=new ArrayList<BlockingVar>(waitVars.size());
  for (  Var wv : waitVars) {
    res.add(new BlockingVar(wv,this.recursive));
  }
  return res;
}",0.9350104821802936
132994,"/** 
 * Rename variables in block (and nested blocks) according to map. If the map doesn't have an entry, we don't rename anything
 * @param renames OldName -> NewName
 * @param inputsOnly  if true, only change references which are readingthe var.  if false, completely remove the old variable and replace  with new
 * @param recursive if it should be done on child blocks
 */
public void renameVars(Map<Var,Arg> renames,RenameMode mode,boolean recursive){
  if (renames.isEmpty())   return;
  renameInDefs(renames,mode);
  renameInCode(renames,mode,recursive);
}","/** 
 * Rename variables in block (and nested blocks) according to map. If the map doesn't have an entry, we don't rename anything
 * @param renames OldName -> NewName
 * @param mode
 * @param recursive if it should be done on child blocks
 */
public void renameVars(Map<Var,Arg> renames,RenameMode mode,boolean recursive){
  if (renames.isEmpty())   return;
  renameInDefs(renames,mode);
  renameInCode(renames,mode,recursive);
}",0.8580060422960725
132995,"/** 
 * @param logger
 * @param f
 * @return true if changes made
 */
private static boolean eliminateIter(Logger logger,Function f){
  HashSet<Var> removeCandidates=new HashSet<Var>();
  HashSet<Var> needed=new HashSet<Var>();
  MultiMap<Var,Var> dependencyGraph=new MultiMap<Var,Var>();
  walkFunction(logger,f,removeCandidates,needed,dependencyGraph);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + f.getName() + ""String_Node_Str""+ ""String_Node_Str""+ removeCandidates+ ""String_Node_Str""+ ""String_Node_Str""+ needed+ ""String_Node_Str""+ ""String_Node_Str""+ printDepGraph(dependencyGraph));
  }
  ArrayDeque<Var> workStack=new ArrayDeque<Var>();
  workStack.addAll(needed);
  while (!workStack.isEmpty()) {
    Var neededVar=workStack.pop();
    List<Var> deps=dependencyGraph.remove(neededVar);
    if (deps != null) {
      needed.addAll(deps);
      workStack.addAll(deps);
    }
  }
  removeCandidates.removeAll(needed);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + removeCandidates);
  }
  if (removeCandidates.isEmpty()) {
    return false;
  }
 else {
    f.getMainblock().removeVars(removeCandidates);
    return true;
  }
}","/** 
 * @param logger
 * @param f
 * @return true if changes made
 */
private static boolean eliminateIter(Logger logger,Function f){
  HashSet<Var> removeCandidates=new HashSet<Var>();
  HashSet<Var> needed=new HashSet<Var>();
  MultiMap<Var,Var> dependencyGraph=new MultiMap<Var,Var>();
  walkFunction(logger,f,removeCandidates,needed,dependencyGraph);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + f.getName() + ""String_Node_Str""+ ""String_Node_Str""+ removeCandidates+ ""String_Node_Str""+ ""String_Node_Str""+ needed+ ""String_Node_Str""+ ""String_Node_Str""+ printDepGraph(dependencyGraph));
  }
  ArrayDeque<Var> workStack=new ArrayDeque<Var>();
  workStack.addAll(needed);
  while (!workStack.isEmpty()) {
    Var neededVar=workStack.pop();
    List<Var> deps=dependencyGraph.remove(neededVar);
    if (deps != null) {
      needed.addAll(deps);
      workStack.addAll(deps);
    }
  }
  removeCandidates.removeAll(needed);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + removeCandidates);
  }
  if (removeCandidates.isEmpty()) {
    return false;
  }
 else {
    f.getMainblock().removeVars(removeCandidates);
    return true;
  }
}",0.9915110356536504
132996,"/** 
 * @param logger
 * @param f
 * @return true if changes made
 */
private static boolean eliminateIter(Logger logger,Function f){
  HashSet<Var> removeCandidates=new HashSet<Var>();
  HashSet<Var> needed=new HashSet<Var>();
  MultiMap<Var,Var> dependencyGraph=new MultiMap<Var,Var>();
  Map<Var,Var> componentOf=new HashMap<Var,Var>();
  walkFunction(logger,f,removeCandidates,needed,dependencyGraph,componentOf);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + f.getName() + ""String_Node_Str""+ ""String_Node_Str""+ removeCandidates+ ""String_Node_Str""+ ""String_Node_Str""+ needed+ ""String_Node_Str""+ ""String_Node_Str""+ printDepGraph(dependencyGraph,4)+ ""String_Node_Str""+ ICUtil.prettyPrintMap(componentOf,4));
  }
  ArrayDeque<Var> workStack=new ArrayDeque<Var>();
  workStack.addAll(needed);
  while (!workStack.isEmpty()) {
    Var neededVar=workStack.pop();
    List<Var> deps=dependencyGraph.remove(neededVar);
    if (deps != null) {
      needed.addAll(deps);
      workStack.addAll(deps);
    }
  }
  removeCandidates.removeAll(needed);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + removeCandidates);
  }
  if (removeCandidates.isEmpty()) {
    return false;
  }
 else {
    f.getMainblock().removeVars(removeCandidates);
    return true;
  }
}","/** 
 * @param logger
 * @param f
 * @return true if changes made
 */
private static boolean eliminateIter(Logger logger,Function f){
  HashSet<Var> removeCandidates=new HashSet<Var>();
  HashSet<Var> needed=new HashSet<Var>();
  List<Var> modified=new ArrayList<Var>();
  MultiMap<Var,Var> dependencyGraph=new MultiMap<Var,Var>();
  Map<Var,Var> componentOf=new HashMap<Var,Var>();
  walkFunction(logger,f,removeCandidates,needed,dependencyGraph,modified,componentOf);
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + f.getName() + ""String_Node_Str""+ ""String_Node_Str""+ removeCandidates+ ""String_Node_Str""+ ""String_Node_Str""+ needed+ ""String_Node_Str""+ ""String_Node_Str""+ printDepGraph(dependencyGraph,4)+ ""String_Node_Str""+ ICUtil.prettyPrintMap(componentOf,4));
  }
  for (  Var written : modified) {
    Var whole=componentOf.get(written);
    while (whole != null) {
      if (logger.isTraceEnabled())       logger.trace(""String_Node_Str"" + whole + ""String_Node_Str""+ written);
      dependencyGraph.put(whole,written);
      whole=componentOf.get(whole);
    }
  }
  ArrayDeque<Var> workStack=new ArrayDeque<Var>();
  workStack.addAll(needed);
  while (!workStack.isEmpty()) {
    Var neededVar=workStack.pop();
    List<Var> deps=dependencyGraph.remove(neededVar);
    if (deps != null) {
      needed.addAll(deps);
      workStack.addAll(deps);
    }
  }
  removeCandidates.removeAll(needed);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + removeCandidates);
  }
  if (removeCandidates.isEmpty()) {
    return false;
  }
 else {
    f.getMainblock().removeVars(removeCandidates);
    return true;
  }
}",0.8801624915368991
132997,"private static void walkInstructions(Logger logger,Block block,HashSet<Var> needed,MultiMap<Var,Var> dependencyGraph,Map<Var,Var> componentOf){
}","private static void walkInstructions(Logger logger,Block block,HashSet<Var> needed,MultiMap<Var,Var> dependencyGraph,List<Var> modified,Map<Var,Var> componentOf){
}",0.9385113268608414
132998,"private static void addOutputDep(Logger logger,Instruction inst,MultiMap<Var,Var> dependencyGraph,Map<Var,Var> componentOf,Var out,Var in){
  if (logger.isTraceEnabled())   logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ in+ ""String_Node_Str""+ inst);
  dependencyGraph.put(out,in);
  Var whole=componentOf.get(out);
  while (whole != null) {
    if (logger.isTraceEnabled())     logger.trace(""String_Node_Str"" + whole + ""String_Node_Str""+ out+ ""String_Node_Str""+ inst);
    dependencyGraph.put(whole,out);
    whole=componentOf.get(whole);
  }
}","private static void addOutputDep(Logger logger,Instruction inst,MultiMap<Var,Var> dependencyGraph,Map<Var,Var> componentOf,Var out,Var in){
  if (logger.isTraceEnabled())   logger.trace(""String_Node_Str"" + out + ""String_Node_Str""+ in+ ""String_Node_Str""+ inst);
  dependencyGraph.put(out,in);
}",0.6910377358490566
132999,"/** 
 * Collect information for dead code elimination
 * @param logger
 * @param f
 * @param removeCandidates list of vars declared in function thatcould be removed
 * @param needed
 * @param dependencyGraph
 * @param componentOf
 */
private static void walkFunction(Logger logger,Function f,HashSet<Var> removeCandidates,HashSet<Var> needed,MultiMap<Var,Var> dependencyGraph,Map<Var,Var> componentOf){
  ArrayDeque<Block> workStack=new ArrayDeque<Block>();
  workStack.push(f.getMainblock());
  needed.addAll(f.getOutputList());
  while (!workStack.isEmpty()) {
    Block block=workStack.pop();
    walkBlockVars(block,removeCandidates,dependencyGraph);
    walkInstructions(logger,block,needed,dependencyGraph,componentOf);
    ListIterator<Continuation> it=block.continuationIterator();
    while (it.hasNext()) {
      Continuation c=it.next();
      if (c.isNoop()) {
        it.remove();
      }
 else {
        needed.addAll(c.requiredVars(true));
        for (        Block inner : c.getBlocks()) {
          workStack.push(inner);
        }
      }
    }
  }
}","/** 
 * Collect information for dead code elimination
 * @param logger
 * @param f
 * @param removeCandidates list of vars declared in function thatcould be removed
 * @param needed
 * @param dependencyGraph
 * @param modified 
 * @param componentOf
 */
private static void walkFunction(Logger logger,Function f,HashSet<Var> removeCandidates,HashSet<Var> needed,MultiMap<Var,Var> dependencyGraph,List<Var> modified,Map<Var,Var> componentOf){
  ArrayDeque<Block> workStack=new ArrayDeque<Block>();
  workStack.push(f.getMainblock());
  needed.addAll(f.getOutputList());
  while (!workStack.isEmpty()) {
    Block block=workStack.pop();
    walkBlockVars(block,removeCandidates,dependencyGraph);
    walkInstructions(logger,block,needed,dependencyGraph,modified,componentOf);
    ListIterator<Continuation> it=block.continuationIterator();
    while (it.hasNext()) {
      Continuation c=it.next();
      if (c.isNoop()) {
        it.remove();
      }
 else {
        needed.addAll(c.requiredVars(true));
        for (        Block inner : c.getBlocks()) {
          workStack.push(inner);
        }
      }
    }
  }
}",0.9752973467520586
133000,"public ForwardDataflow(boolean eliminateExplicitWaits){
  this.eliminateExplicitWaits=eliminateExplicitWaits;
}","public ForwardDataflow(boolean keepExplicitWaits){
  this.keepExplicitWaits=keepExplicitWaits;
}",0.8695652173913043
