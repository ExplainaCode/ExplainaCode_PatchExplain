{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanations have been generated, evaluated, refined, and saved to output_data/bug_fix_explain_data0_sample_5_output_gpt.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained sentence-transformers model for semantic similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can use other models from Hugging Face based on your need\n",
    "\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "source_name = \"data0\"\n",
    "sample_size = \"5\"\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv(f'source_data/{source_name}_sample_{sample_size}.csv')  # Replace with the actual path to your dataframe\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = OPEN_AI_API_KEY  # Replace with your actual OpenAI API key\n",
    "\n",
    "def get_batch_explanations(batch_prompts, model_name):\n",
    "    \"\"\"\n",
    "    Function to get explanations for a batch of prompts using the OpenAI Chat API.\n",
    "    Allows specifying the model name for cost-effective usage.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for prompt in batch_prompts:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides clear, brief explanations of why the fixed code is correct.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            n=1,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        explanations = response.choices[0].message['content'].strip()\n",
    "        responses.append(explanations)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def generate_reference_explanation(row):\n",
    "    \"\"\"\n",
    "    Generates a reference explanation for the fixed code using a more cost-effective AI model (e.g., GPT-3.5).\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Explain in detailed, clear language why the fixed code is correct and how it resolves the issue present in the buggy code. Provide a reference explanation that can be used to compare with other explanations. Be specific and concise.\n",
    "\n",
    "    Buggy code:\n",
    "    {row['buggy_code']}\n",
    "\n",
    "    Fixed code:\n",
    "    {row['fixed_code']}\n",
    "\n",
    "    Why the fixed code is correct:\"\"\"\n",
    "    \n",
    "    # Use GPT-3.5 for cost-effective reference explanation generation\n",
    "    return get_batch_explanations([prompt], model_name=\"gpt-3.5-turbo\")[0]\n",
    "\n",
    "def evaluate_explanation_nlp(generated_explanation, reference_explanation, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Function to evaluate the quality of the explanation using semantic similarity.\n",
    "    Returns the similarity score and whether the explanation is satisfactory.\n",
    "    \"\"\"\n",
    "    # Encode both explanations to get embeddings\n",
    "    generated_embedding = model.encode(generated_explanation, convert_to_tensor=True)\n",
    "    reference_embedding = model.encode(reference_explanation, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity between the embeddings\n",
    "    similarity_score = util.pytorch_cos_sim(generated_embedding, reference_embedding).item()\n",
    "\n",
    "    # If the similarity score is above the threshold, consider it satisfactory\n",
    "    return similarity_score, similarity_score >= threshold\n",
    "\n",
    "def analyze_explanation_issue(generated_explanation, reference_explanation):\n",
    "    \"\"\"\n",
    "    Analyzes why an explanation might be considered \"Needs Refinement.\"\n",
    "    Returns an issue type that can be used to refine the prompt.\n",
    "    \"\"\"\n",
    "    # If the explanation is very short or lacks detail\n",
    "    if len(generated_explanation.split()) < 10:\n",
    "        return \"too_short\"\n",
    "    \n",
    "    # If the explanation doesn't mention key concepts from the reference\n",
    "    if any(keyword not in generated_explanation for keyword in reference_explanation.split()[:5]):\n",
    "        return \"missing_key_details\"\n",
    "    \n",
    "    # Check if the explanation is too vague\n",
    "    vague_terms = [\"good\", \"correct\", \"proper\", \"appropriate\"]\n",
    "    if any(term in generated_explanation for term in vague_terms):\n",
    "        return \"too_vague\"\n",
    "    \n",
    "    # Default case: issue is unclear\n",
    "    return \"general_issue\"\n",
    "\n",
    "def refine_prompt(prompt, issue_type):\n",
    "    \"\"\"\n",
    "    Function to refine the prompt based on the detected issue in the generated explanation.\n",
    "    \"\"\"\n",
    "    if issue_type == \"too_short\":\n",
    "        refined_prompt = prompt + \"\\nThe previous explanation was too short. Provide a more detailed explanation that covers the key points.\"\n",
    "    \n",
    "    elif issue_type == \"missing_key_details\":\n",
    "        refined_prompt = prompt + \"\\nThe previous explanation missed key details. Focus specifically on how the fixed code resolves the problem in the buggy code.\"\n",
    "    \n",
    "    elif issue_type == \"too_vague\":\n",
    "        refined_prompt = prompt + \"\\nThe previous explanation was too vague. Provide a more specific explanation, avoiding generic terms like 'good' or 'correct'.\"\n",
    "\n",
    "    else:\n",
    "        # General refinement\n",
    "        refined_prompt = prompt + \"\\nThe previous explanation was not clear enough. Please rephrase the explanation to be more informative and detailed.\"\n",
    "    \n",
    "    return refined_prompt\n",
    "\n",
    "def refine_until_satisfactory(df):\n",
    "    \"\"\"\n",
    "    Function to refine the prompts and explanations for rows marked as 'Needs Refinement'.\n",
    "    The function continues until all rows are marked as 'Satisfactory'.\n",
    "    \"\"\"\n",
    "    cycle = 1\n",
    "    while \"Needs Refinement\" in df['evaluation_result'].values:\n",
    "        print(f\"Cycle {cycle}: Refining prompts for 'Needs Refinement' rows.\")\n",
    "        needs_refinement_rows = df[df['evaluation_result'] == \"Needs Refinement\"]\n",
    "        \n",
    "        for index, row in needs_refinement_rows.iterrows():\n",
    "            # Analyze the issue with the generated explanation\n",
    "            issue_type = analyze_explanation_issue(row['generated_explanation'], row['reference_explanation'])\n",
    "            \n",
    "            # Refine the prompt based on the issue\n",
    "            refined_prompt = refine_prompt(row['prompt'], issue_type)\n",
    "            \n",
    "            # Generate a new explanation using the more accurate model (e.g., gpt-4o-mini)\n",
    "            refined_explanation = get_batch_explanations([refined_prompt], model_name=\"gpt-4-turbo\")[0]\n",
    "            \n",
    "            # Evaluate the refined explanation\n",
    "            reference_explanation = row['reference_explanation']\n",
    "            similarity_score, is_satisfactory = evaluate_explanation_nlp(refined_explanation, reference_explanation)\n",
    "            \n",
    "            # Update DataFrame with the new explanation and results\n",
    "            df.at[index, 'generated_explanation'] = refined_explanation\n",
    "            df.at[index, 'similarity_score'] = similarity_score\n",
    "            df.at[index, 'evaluation_result'] = \"Satisfactory\" if is_satisfactory else \"Needs Refinement\"\n",
    "            \n",
    "            # If it's satisfactory, record the cycle number\n",
    "            if is_satisfactory:\n",
    "                df.at[index, 'completed_cycle'] = cycle\n",
    "        \n",
    "        cycle += 1\n",
    "\n",
    "# Create prompts for each row and generate reference explanations using GPT-3.5\n",
    "df['prompt'] = df.apply(\n",
    "    lambda row: f\"\"\"Explain in simple, clear language why the fixed code is correct and how it resolves the issue present in the buggy code. \n",
    "    Keep the explanation short, around 20 words. don't include things as 'The fixed code correctly uses' like unwanted words, only include useful words, simply why the fixed_code is correct compared to buggy_code.\n",
    "\n",
    "    Buggy code:\n",
    "    {row['buggy_code']}\n",
    "\n",
    "    Fixed code:\n",
    "    {row['fixed_code']}\n",
    "\n",
    "    Why the fixed code is correct:\"\"\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Generate AI-based reference explanations for each buggy/fixed code pair using GPT-3.5\n",
    "df['reference_explanation'] = df.apply(generate_reference_explanation, axis=1)\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "df['generated_explanation'] = \"\"\n",
    "df['similarity_score'] = 0.0\n",
    "df['evaluation_result'] = \"\"\n",
    "df['completed_cycle'] = 0  # To track the cycle in which it became satisfactory\n",
    "\n",
    "# First batch of explanations using gpt-4o-mini for higher accuracy\n",
    "df['generated_explanation'] = get_batch_explanations(df['prompt'].tolist(), model_name=\"gpt-4-turbo\")\n",
    "\n",
    "# First evaluation\n",
    "for index, row in df.iterrows():\n",
    "    reference_explanation = row['reference_explanation']\n",
    "    generated_explanation = row['generated_explanation']\n",
    "    similarity_score, is_satisfactory = evaluate_explanation_nlp(generated_explanation, reference_explanation)\n",
    "    \n",
    "    df.at[index, 'similarity_score'] = similarity_score\n",
    "    df.at[index, 'evaluation_result'] = \"Satisfactory\" if is_satisfactory else \"Needs Refinement\"\n",
    "    \n",
    "    # Mark completed cycle for satisfactory rows (first cycle is 1)\n",
    "    if is_satisfactory:\n",
    "        df.at[index, 'completed_cycle'] = 1\n",
    "\n",
    "# Refine until all rows are satisfactory\n",
    "refine_until_satisfactory(df)\n",
    "\n",
    "# Drop the 'prompt' column as it's no longer needed\n",
    "df.drop(columns=['prompt'], inplace=True)\n",
    "\n",
    "# Save the dataframe with the new columns\n",
    "output_file = f'output_data/bug_fix_explain_{source_name}_sample_{sample_size}_output_gpt.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Explanations have been generated, evaluated, refined, and saved to {output_file} successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
