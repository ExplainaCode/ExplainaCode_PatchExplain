{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load environment variables\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "source_name = \"data0\"\n",
    "sample_size = \"5\"\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv(f'../Data/1000_record_chunks/1000_chunk_1.csv')  # Replace with the actual path to your dataframe\n",
    "df = df.sample(100)\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = OPEN_AI_API_KEY\n",
    "\n",
    "\n",
    "\n",
    "def get_gpt_explanation(buggy_code, fixed_code):\n",
    "    \"\"\"\n",
    "    Function to get a brief explanation using GPT-3.5-Turbo of why the fixed code is correct compared to the buggy code.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a senior software engineer explaining to a junior developer. \n",
    "                First, identify the main issue in the buggy code, then explain how the fixed code resolves it, focusing on clarity and brevity. \n",
    "                Keep the explanation concise, no more than 20 words.\n",
    "\n",
    "    Buggy code:\n",
    "    {buggy_code}\n",
    "\n",
    "    Fixed code:\n",
    "    {fixed_code}\n",
    "\n",
    "    Why the fixed code is correct:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides clear, brief explanations of why the fixed code is correct.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=50,  # Limit the output to around 50 tokens\n",
    "        temperature=0.5\n",
    "    )\n",
    "    explanation = response['choices'][0]['message']['content'].strip()\n",
    "    return explanation\n",
    "\n",
    "# Apply both functions to each row in the DataFrame and store the results in new columns\n",
    "df['gpt_explanation'] = df.apply(lambda row: get_gpt_explanation(row['buggy_code'], row['fixed_code']), axis=1)\n",
    "df.to_csv(f'../Data/evaluations/100_record_eval_only_gpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 5 out of 100 records.\n",
      "Completed 10 out of 100 records.\n",
      "Completed 15 out of 100 records.\n",
      "Completed 20 out of 100 records.\n",
      "Completed 25 out of 100 records.\n",
      "Completed 30 out of 100 records.\n",
      "Completed 35 out of 100 records.\n",
      "Completed 40 out of 100 records.\n",
      "Completed 45 out of 100 records.\n",
      "Completed 50 out of 100 records.\n",
      "Completed 55 out of 100 records.\n",
      "Completed 60 out of 100 records.\n",
      "Completed 65 out of 100 records.\n",
      "Completed 70 out of 100 records.\n",
      "Completed 75 out of 100 records.\n",
      "Completed 80 out of 100 records.\n",
      "Completed 85 out of 100 records.\n",
      "Completed 90 out of 100 records.\n",
      "Completed 95 out of 100 records.\n",
      "Completed 100 out of 100 records.\n",
      "Batch processing complete. Explanations and similarity scores have been generated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Configure the Gemini API key\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Define the model configuration for Gemini\n",
    "generation_config = {\n",
    "  \"temperature\": 0.5,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 50,  # Adjust as needed to fit your requirements\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "# Create the Gemini model\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",  # Replace with the correct model name as needed\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "df = pd.read_csv(f'../Data/evaluations/100_record_eval_only_gpt.csv')  # Replace with the actual path to your dataframe\n",
    "df = df [['record_number', 'buggy_code', 'fixed_code', 'gpt_explanation']]\n",
    "\n",
    "\n",
    "\n",
    "def get_gemini_explanation(buggy_code, fixed_code):\n",
    "    \"\"\"\n",
    "    Function to get a brief explanation using Gemini API of why the fixed code is correct compared to the buggy code.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a senior software engineer explaining to a junior developer. \n",
    "                First, identify the main issue in the buggy code, then explain how the fixed code resolves it, focusing on clarity and brevity. \n",
    "                Keep the explanation concise, no more than 20 words, only need to have one sentence in the explanation.\n",
    "\n",
    "    Buggy code:\n",
    "    {buggy_code}\n",
    "\n",
    "    Fixed code:\n",
    "    {fixed_code}\n",
    "\n",
    "    Why the fixed code is correct:\"\"\"\n",
    "\n",
    "    # Start a chat session\n",
    "    chat_session = model.start_chat(history=[])\n",
    "    \n",
    "    # Send the message (prompt) to the chat session\n",
    "    response = chat_session.send_message(prompt)\n",
    "\n",
    "    # Extract the text from the response\n",
    "    explanation = response.text.strip()\n",
    "    return explanation\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two pieces of text using SentenceTransformer.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model from SentenceTransformers\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Encode the two texts\n",
    "    embeddings = model.encode([text1, text2])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "\n",
    "# df['gemini_explanation'] = df.apply(lambda row: get_gemini_explanation(row['buggy_code'], row['fixed_code']), axis=1)\n",
    "\n",
    "# # Calculate similarity score between 'gpt_explanation' and 'gemini_explanation' for each row\n",
    "# df['similarity_score'] = df.apply(lambda row: calculate_similarity(row['gpt_explanation'], row['gemini_explanation']), axis=1)\n",
    "\n",
    "# Process the DataFrame in batches of 5 rows at a time\n",
    "# Process the DataFrame in batches of 5 rows at a time\n",
    "# List to collect processed batches with new columns\n",
    "processed_batches = []\n",
    "\n",
    "# Process the DataFrame in batches of 5 rows at a time\n",
    "batch_size = 5\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch = df.iloc[i:i + batch_size].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Generate explanations for each record in the batch\n",
    "    batch['gemini_explanation'] = batch.apply(lambda row: get_gemini_explanation(row['buggy_code'], row['fixed_code']), axis=1)\n",
    "\n",
    "    # Calculate similarity score between 'gpt_explanation' and 'gemini_explanation' for each row\n",
    "    batch['similarity_score'] = batch.apply(lambda row: calculate_similarity(row['gpt_explanation'], row['gemini_explanation']), axis=1)\n",
    "\n",
    "    # Append the processed batch to the list\n",
    "    processed_batches.append(batch)\n",
    "\n",
    "    # Print progress\n",
    "    completed_records = min(i + batch_size, len(df))\n",
    "    print(f\"Completed {completed_records} out of {len(df)} records.\")\n",
    "\n",
    "    # Pause for 1 seconds to avoid hitting API limits\n",
    "    time.sleep(5)\n",
    "\n",
    "# Concatenate all processed batches\n",
    "result_df = pd.concat(processed_batches, ignore_index=True)\n",
    "\n",
    "# Save the DataFrame with explanations and similarity scores to a new CSV file\n",
    "result_df.to_csv(f'../Data/evaluations/100_record_eval_gpt_gemini.csv', index=False)\n",
    "\n",
    "print(\"Batch processing complete. Explanations and similarity scores have been generated and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f'../Data/evaluations/100_record_eval_gpt_gemini.csv') \n",
    "\n",
    "df['result_status'] = df['similarity_score'].apply(lambda x: 'Similarity Passed' if x > 0.6 else '')\n",
    "\n",
    "df.to_csv(f'../Data/evaluations/100_record_eval_gpt_gemini.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
