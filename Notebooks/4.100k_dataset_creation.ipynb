{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1000_chunk_1.csv...\n",
      "Explanations saved to ../Data/new_100k_dataset/gpt/1000_chunk_1_only_gpt.csv\n",
      "Processing 1000_chunk_2.csv...\n",
      "Explanations saved to ../Data/new_100k_dataset/gpt/1000_chunk_2_only_gpt.csv\n",
      "Processing 1000_chunk_3.csv...\n",
      "Explanations saved to ../Data/new_100k_dataset/gpt/1000_chunk_3_only_gpt.csv\n",
      "Processing 1000_chunk_4.csv...\n",
      "Explanations saved to ../Data/new_100k_dataset/gpt/1000_chunk_4_only_gpt.csv\n",
      "Processing 1000_chunk_5.csv...\n",
      "Explanations saved to ../Data/new_100k_dataset/gpt/1000_chunk_5_only_gpt.csv\n",
      "Processing 1000_chunk_6.csv...\n",
      "Attempt 1 failed: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Attempt 2 failed: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002477F7B8D40>: Failed to resolve 'api.openai.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Explanations saved to ../Data/new_100k_dataset/gpt/1000_chunk_6_only_gpt.csv\n",
      "Processing 1000_chunk_7.csv...\n",
      "Explanations saved to ../Data/new_100k_dataset/gpt/1000_chunk_7_only_gpt.csv\n",
      "Processing 1000_chunk_8.csv...\n",
      "Explanations saved to ../Data/new_100k_dataset/gpt/1000_chunk_8_only_gpt.csv\n",
      "Processing 1000_chunk_9.csv...\n",
      "Attempt 1 failed: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Attempt 1 failed: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai.api_key = OPEN_AI_API_KEY\n",
    "\n",
    "# Directories\n",
    "input_dir = '../Data/new_1000_record_chunks/'  # Input folder with chunk files\n",
    "output_dir = '../Data/new_100k_dataset/gpt/'  # Output folder to save explanations\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "# sample_size = 1  # Sample size for each chunk\n",
    "\n",
    "def retry_request(func, max_retries=5, delay=2):\n",
    "    \"\"\"\n",
    "    Retry mechanism for API requests with exponential backoff.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                raise Exception(\"All retries failed\") from e\n",
    "\n",
    "def get_gpt_explanation(buggy_code, fixed_code):\n",
    "    \"\"\"\n",
    "    Function to get a concise explanation using OpenAI's GPT model with retry logic.\n",
    "    \"\"\"\n",
    "    def make_request():\n",
    "        prompt = f\"\"\"You are a senior software engineer explaining a bug fix to a junior developer. \n",
    "        Your task is to provide a concise explanation of the changes made to fix the bug in the provided code snippets. Follow these guidelines:\n",
    "\n",
    "        ### Guidelines:\n",
    "        1. **Bug Identification**: Describe the error in the original code, including its type (e.g., logic error, runtime error) and its impact.\n",
    "        2. **Problem Analysis**: Explain why the bug is problematic and under what conditions it causes issues.\n",
    "        3. **Fix Explanation**: Describe the changes in the fixed code and how they address the issue.\n",
    "        4. **Justification**: Justify why the fix is necessary and how it resolves the problem.\n",
    "        5. **Improvement Highlight**: Summarize how the fix improves code reliability, functionality, or performance.\n",
    "\n",
    "        ### Constraints:\n",
    "        - The explanation must be **concise**, using no more than **100 words**.\n",
    "        - The explanation should consist of **exactly three sentences**:\n",
    "        1. Why the buggy code is incorrect.\n",
    "        2. What changes were made in the fixed code and why they are correct.\n",
    "        3. How the fixed code improves upon the buggy code.\n",
    "        - Do not include meta-descriptions such as \"This is a three-sentence explanation\" or mention word counts in the response.\n",
    "        - Ensure your explanation aligns with the code context and focuses solely on technical details relevant to the fix.\n",
    "\n",
    "        ### Example Explanations:\n",
    "\n",
    "        #### Example 1:\n",
    "        **Buggy Code:**\n",
    "        @Override protected void afterTests(){{\n",
    "            try {{\n",
    "                context.shutdown();\n",
    "            }}\n",
    "            catch (Exception e) {{\n",
    "                throw new RuntimeException(\"String_Node_Str\", e);\n",
    "            }}\n",
    "            super.afterTests();\n",
    "        }}\n",
    "\n",
    "        **Fixed Code:**\n",
    "        @Override protected void afterTests(){{\n",
    "            try {{\n",
    "                context.shutdown();\n",
    "            }}\n",
    "            catch (Exception e) {{\n",
    "                throw new RuntimeException(\"String_Node_Str\", e);\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        **Explanation:**\n",
    "        The bug in the original code is the unconditional call to `super.afterTests()`, which executes even if `context.shutdown()` fails, risking inconsistent state. \n",
    "        The fixed code removes this call, ensuring `super.afterTests()` is not invoked when an exception occurs, preventing potential errors. \n",
    "        This fix ensures predictable cleanup behavior, improving code reliability.\n",
    "\n",
    "        #### Example 2:\n",
    "        **Buggy Code:**\n",
    "        private void updateTreeView(Tree tree){{\n",
    "            Iterator it = tree.getDepthFirstIterator(false);\n",
    "            while (it.hasNext()) {{\n",
    "                ((Tree<JsonTreeNode>) it.next()).setExpanded(true);\n",
    "            }}\n",
    "            editorTreeView.setModel(tree.copy());\n",
    "        }}\n",
    "\n",
    "        **Fixed Code:**\n",
    "        private void updateTreeView(JsonTree tree){{\n",
    "            JsonTree fixedTree = JsonTreeConverter.serialize(JsonTreeConverter.deserialize(tree));\n",
    "            Iterator it = fixedTree.getDepthFirstIterator(false);\n",
    "            while (it.hasNext()) {{\n",
    "                ((JsonTree) it.next()).setExpanded(true);\n",
    "            }}\n",
    "            editorTreeView.setModel(fixedTree.copy());\n",
    "        }}\n",
    "\n",
    "        **Explanation:**\n",
    "        The original code has a bug where it improperly casts a generic Tree to `Tree<JsonTreeNode>`, which can cause runtime errors if the types don’t match. \n",
    "        The fix uses a `JsonTree` with serialization and deserialization to ensure the tree structure is correct and safe to work with. \n",
    "        This makes the code more reliable and prevents runtime type errors.\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Your Task:\n",
    "\n",
    "        **Buggy Code:**\n",
    "        {buggy_code}\n",
    "\n",
    "        **Fixed Code:**\n",
    "        {fixed_code}\n",
    "\n",
    "        **Why the fixed code is correct:**\"\"\"\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains bug fixes clearly.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    return retry_request(make_request)\n",
    "\n",
    "def natural_sort_key(file_name):\n",
    "    \"\"\"\n",
    "    Extract numeric parts of file names for natural sorting.\n",
    "    Example: '1000_chunk_10.csv' -> [10]\n",
    "    \"\"\"\n",
    "    return [int(text) if text.isdigit() else text for text in re.split(r'(\\d+)', file_name)]\n",
    "\n",
    "# Process all chunk files\n",
    "# files = [file for file in os.listdir(input_dir) if file.endswith('.csv')]\n",
    "# files.sort(key=natural_sort_key)  # Sort files in natural order\n",
    "\n",
    "files = [f\"1000_chunk_{i}.csv\" for i in range(1, 100)]\n",
    "# files.sort(key=natural_sort_key)  # Sort files in natural order\n",
    "\n",
    "for file in files:\n",
    "    input_path = os.path.join(input_dir, file)\n",
    "    output_file_name = file.replace(\".csv\", \"_only_gpt.csv\")\n",
    "    output_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    print(f\"Processing {file}...\")\n",
    "\n",
    "    # Load the data\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # df = df.sample(min(sample_size, len(df)))  # Take a sample of the data\n",
    "\n",
    "    # Apply explanation generation\n",
    "    df['gpt_explanation'] = df.apply(lambda row: get_gpt_explanation(row['buggy_code'], row['fixed_code']), axis=1)\n",
    "\n",
    "    # Save the output\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Explanations saved to {output_path}\")\n",
    "\n",
    "print(\"Processing complete for all chunk files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing conversation with the static prompt...\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 170\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 170\u001b[0m     \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 142\u001b[0m, in \u001b[0;36mprocess_files\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_files\u001b[39m():\n\u001b[0;32m    140\u001b[0m     files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1000_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m100\u001b[39m)]\n\u001b[1;32m--> 142\u001b[0m     \u001b[43minitialize_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Send the static prompt once\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m    145\u001b[0m         input_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, file)\n",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m, in \u001b[0;36minitialize_conversation\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_conversation\u001b[39m():\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing conversation with the static prompt...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-5-haiku-latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add max_tokens (required)\u001b[39;49;00m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTATIC_PROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversation initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\suwan\\Downloads\\Github\\FYP\\ExplainaCode_PatchExplain\\new_venv\\Lib\\site-packages\\anthropic\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\suwan\\Downloads\\Github\\FYP\\ExplainaCode_PatchExplain\\new_venv\\Lib\\site-packages\\anthropic\\resources\\messages.py:888\u001b[0m, in \u001b[0;36mMessages.create\u001b[1;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[0;32m    882\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    884\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    885\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    886\u001b[0m     )\n\u001b[1;32m--> 888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\suwan\\Downloads\\Github\\FYP\\ExplainaCode_PatchExplain\\new_venv\\Lib\\site-packages\\anthropic\\_base_client.py:1279\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1267\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1275\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1276\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1277\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1278\u001b[0m     )\n\u001b[1;32m-> 1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\suwan\\Downloads\\Github\\FYP\\ExplainaCode_PatchExplain\\new_venv\\Lib\\site-packages\\anthropic\\_base_client.py:956\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    954\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\suwan\\Downloads\\Github\\FYP\\ExplainaCode_PatchExplain\\new_venv\\Lib\\site-packages\\anthropic\\_base_client.py:1060\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1057\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1059\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1063\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1064\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1068\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1069\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "import time\n",
    "\n",
    "# Load API key from environment variables\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "client = anthropic.Client(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "# Directories\n",
    "input_dir = '../Data/new_1000_record_chunks/'  # Input folder\n",
    "output_dir = '../Data/new_100k_dataset/anthropic/'  # Output folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Static prompt (sent once per session)\n",
    "STATIC_PROMPT = \"\"\"You are a senior software engineer explaining a bug fix to a junior developer. \n",
    "        Your task is to provide a concise explanation of the changes made to fix the bug in the provided code snippets. Follow these guidelines:\n",
    "\n",
    "        ### Guidelines:\n",
    "        1. **Bug Identification**: Describe the error in the original code, including its type (e.g., logic error, runtime error) and its impact.\n",
    "        2. **Problem Analysis**: Explain why the bug is problematic and under what conditions it causes issues.\n",
    "        3. **Fix Explanation**: Describe the changes in the fixed code and how they address the issue.\n",
    "        4. **Justification**: Justify why the fix is necessary and how it resolves the problem.\n",
    "        5. **Improvement Highlight**: Summarize how the fix improves code reliability, functionality, or performance.\n",
    "\n",
    "        ### Constraints:\n",
    "        - The explanation must be **concise**, using no more than **100 words**.\n",
    "        - The explanation should consist of **exactly three sentences**:\n",
    "        1. Why the buggy code is incorrect.\n",
    "        2. What changes were made in the fixed code and why they are correct.\n",
    "        3. How the fixed code improves upon the buggy code.\n",
    "        - Do not include meta-descriptions such as \"This is a three-sentence explanation\" or mention word counts in the response.\n",
    "        - Ensure your explanation aligns with the code context and focuses solely on technical details relevant to the fix.\n",
    "\n",
    "        ### Example Explanations:\n",
    "\n",
    "        #### Example 1:\n",
    "        **Buggy Code:**\n",
    "        @Override protected void afterTests(){{\n",
    "            try {{\n",
    "                context.shutdown();\n",
    "            }}\n",
    "            catch (Exception e) {{\n",
    "                throw new RuntimeException(\"String_Node_Str\", e);\n",
    "            }}\n",
    "            super.afterTests();\n",
    "        }}\n",
    "\n",
    "        **Fixed Code:**\n",
    "        @Override protected void afterTests(){{\n",
    "            try {{\n",
    "                context.shutdown();\n",
    "            }}\n",
    "            catch (Exception e) {{\n",
    "                throw new RuntimeException(\"String_Node_Str\", e);\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        **Explanation:**\n",
    "        The bug in the original code is the unconditional call to `super.afterTests()`, which executes even if `context.shutdown()` fails, risking inconsistent state. \n",
    "        The fixed code removes this call, ensuring `super.afterTests()` is not invoked when an exception occurs, preventing potential errors. \n",
    "        This fix ensures predictable cleanup behavior, improving code reliability.\n",
    "\n",
    "        #### Example 2:\n",
    "        **Buggy Code:**\n",
    "        private void updateTreeView(Tree tree){{\n",
    "            Iterator it = tree.getDepthFirstIterator(false);\n",
    "            while (it.hasNext()) {{\n",
    "                ((Tree<JsonTreeNode>) it.next()).setExpanded(true);\n",
    "            }}\n",
    "            editorTreeView.setModel(tree.copy());\n",
    "        }}\n",
    "\n",
    "        **Fixed Code:**\n",
    "        private void updateTreeView(JsonTree tree){{\n",
    "            JsonTree fixedTree = JsonTreeConverter.serialize(JsonTreeConverter.deserialize(tree));\n",
    "            Iterator it = fixedTree.getDepthFirstIterator(false);\n",
    "            while (it.hasNext()) {{\n",
    "                ((JsonTree) it.next()).setExpanded(true);\n",
    "            }}\n",
    "            editorTreeView.setModel(fixedTree.copy());\n",
    "        }}\n",
    "\n",
    "        **Explanation:**\n",
    "        The original code has a bug where it improperly casts a generic Tree to `Tree<JsonTreeNode>`, which can cause runtime errors if the types don’t match. \n",
    "        The fix uses a `JsonTree` with serialization and deserialization to ensure the tree structure is correct and safe to work with. \n",
    "        This makes the code more reliable and prevents runtime type errors.\"\"\"\n",
    "\n",
    "# Initialize conversation with the static prompt\n",
    "def initialize_conversation():\n",
    "    print(\"Initializing conversation with the static prompt...\")\n",
    "    client.messages.create(\n",
    "        model=\"claude-3-5-haiku-latest\",\n",
    "        max_tokens=200,  # Add max_tokens (required)\n",
    "        messages=[{\"role\": \"system\", \"content\": STATIC_PROMPT}]\n",
    "    )\n",
    "    print(\"Conversation initialized.\")\n",
    "\n",
    "\n",
    "# Generate explanation for a specific buggy and fixed code pair\n",
    "def get_explanation_dynamic(buggy_code, fixed_code):\n",
    "    dynamic_prompt = f\"\"\"\n",
    "### Your Task:\n",
    "\n",
    "**Buggy Code:**\n",
    "{buggy_code}\n",
    "\n",
    "**Fixed Code:**\n",
    "{fixed_code}\n",
    "\n",
    "**Why the fixed code is correct:**\"\"\"\n",
    "    \n",
    "    # Make the API call with only the dynamic content\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-haiku-latest\",\n",
    "        max_tokens=200,\n",
    "        temperature=0.5,\n",
    "        messages=[{\"role\": \"user\", \"content\": dynamic_prompt}]\n",
    "    )\n",
    "    return response.content[0].text.strip()\n",
    "\n",
    "# Retry mechanism for robustness\n",
    "def retry_request(func, max_retries=5, delay=2):\n",
    "    \"\"\"\n",
    "    Retry mechanism for API requests with exponential backoff.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                raise Exception(\"All retries failed\") from e\n",
    "\n",
    "# Process all evaluation files\n",
    "def process_files():\n",
    "    files = [f\"1000_chunk_{i}.csv\" for i in range(25, 100)]\n",
    "    \n",
    "    initialize_conversation()  # Send the static prompt once\n",
    "    \n",
    "    for file in files:\n",
    "        input_path = os.path.join(input_dir, file)\n",
    "        output_file_name = file.replace(\".csv\", \"_only_anthropic.csv\")\n",
    "        output_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "        print(f\"Processing {file}...\")\n",
    "        \n",
    "        # Load the input data\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "        # Generate explanations for each row\n",
    "        df['anthropic_explanation'] = df.apply(\n",
    "            lambda row: retry_request(\n",
    "                lambda: get_explanation_dynamic(row['buggy_code'], row['fixed_code'])\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Save the output\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Explanations saved to {output_path}\")\n",
    "\n",
    "    print(\"Processing complete for all evaluation files!\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    process_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1000_chunk_1_only_gpt_4-0_mini_with_gpt_and_anthropic.csv...\n",
      "Reading 1000_chunk_2_only_gpt_4-0_mini_with_gpt_and_anthropic.csv...\n",
      "Reading 1000_chunk_3_only_gpt_4-0_mini_with_gpt_and_anthropic.csv...\n",
      "Reading 1000_chunk_4_only_gpt_4-0_mini_with_gpt_and_anthropic.csv...\n",
      "Reading 1000_chunk_5_only_gpt_4-0_mini_with_gpt_and_anthropic.csv...\n",
      "Combined CSV saved to ../Data/100k_dataset/combined_gpt_anthropic_explanations_without_similarity.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Directories\n",
    "input_dir = '../Data/100k_dataset/anthropic/'  # Folder with individual CSV files\n",
    "output_file = '../Data/100k_dataset/combined_gpt_anthropic_explanations_without_similarity.csv'  # Final combined output\n",
    "\n",
    "\n",
    "def natural_sort_key(file_name):\n",
    "    \"\"\"\n",
    "    Extract numeric parts of file names for natural sorting.\n",
    "    Example: '1000_chunk_10_with_gpt_and_anthropic.csv' -> [10]\n",
    "    \"\"\"\n",
    "    return [int(text) if text.isdigit() else text for text in re.split(r'(\\d+)', file_name)]\n",
    "\n",
    "# Find and sort all CSV files\n",
    "files = [file for file in os.listdir(input_dir) if file.endswith('_anthropic.csv')]\n",
    "files.sort(key=natural_sort_key)  # Sort files in natural order\n",
    "\n",
    "# Combine all files into one DataFrame\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    input_path = os.path.join(input_dir, file)\n",
    "    print(f\"Reading {file}...\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a single CSV\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"Combined CSV saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suwan\\Downloads\\Github\\FYP\\ExplainaCode_PatchExplain\\new_venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# sample_size = \"100\"\n",
    "\n",
    "df = pd.read_csv(f'../Data/100k_dataset/combined_gpt_anthropic_explanations_without_similarity.csv')  # Replace with the actual path to your dataframe\n",
    "# Function to filter sentences from the text\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two pieces of text using SentenceTransformer.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model from SentenceTransformers\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Encode the two texts\n",
    "    embeddings = model.encode([text1, text2])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "# Apply the function to the 'anthropic_explanation' column\n",
    "df['explanation_similarity_score'] = df.apply(lambda row: calculate_similarity(row['gpt_explanation'], row['anthropic_explanation']), axis=1)\n",
    "\n",
    "df.to_csv(f'../Data/100k_dataset/combined_gpt_anthropic_explanations_with_similarity.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
